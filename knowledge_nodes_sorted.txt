第1章：引言
    1.1：本书面向的读者
        - 摘要：本书对各类读者都有一定的用处，但主要是为两类受众而写的。其中，；一类受众是学习机器学习的大学生（本科或研究生），包括那些已经开；始职业生涯的深度学习和人工智能研究者。另一类受众是没有机器学习
          關鍵詞：一类受众是学习机器学习的大学生, 本书对各类读者都有一定的用处, 其中, 另一类受众是没有机器学习, 但主要是为两类受众而写的
        - 摘要：为了更好地服务各类读者，我们将本书组织为3个部分。第1部分介绍基
          關鍵詞：我们将本书组织为, 个部分, 为了更好地服务各类读者, 部分介绍基
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；本的数学工具和机器学习的概念。第2部分介绍最成熟的深度学习算；法，这些技术基本上已经得到解决。第3部分讨论某些具有展望性的想
          關鍵詞：这些技术基本上已经得到解决, 部分介绍最成熟的深度学习算, 本的数学工具和机器学习的概念, 部分讨论某些具有展望性的想
        - 摘要：读者可以随意跳过不感兴趣或与自己背景不相关的部分。熟悉线性代；数、概率和基本机器学习概念的读者可以跳过第1部分。若读者只是想；实现一个能工作的系统，则不需要阅读超出第2部分的内容。为了帮助
          關鍵詞：为了帮助, 部分, 若读者只是想, 部分的内容, 实现一个能工作的系统
        - 摘要：图1.6　本书的高层组织结构的流程图。从一章到另一章的箭头表示前一章是理解后一章的必备；内容
          關鍵詞：内容, 本书的高层组织结构的流程图, 从一章到另一章的箭头表示前一章是理解后一章的必备
        - 摘要：我们假设所有读者都具备计算机科学背景。也假设读者熟悉编程，并且；对计算的性能问题、复杂性理论、入门级微积分和一些图论术语有基本；的了解。
          關鍵詞：也假设读者熟悉编程, 我们假设所有读者都具备计算机科学背景, 的了解, 入门级微积分和一些图论术语有基本, 复杂性理论
        - 摘要：《深度学习》英文版配套网站是www.deeplearningbook.org。网站上提供
          關鍵詞：英文版配套网站是, 深度学习, 网站上提供
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；了各种补充材料，包括练习、讲义幻灯片、错误更正以及其他应该对读；者和讲师有用的资源。
          關鍵詞：者和讲师有用的资源, 了各种补充材料, 讲义幻灯片, 包括练习, 错误更正以及其他应该对读
        - 摘要：《深度学习》中文版的读者，可访问人民邮电出版社异步社区网站；www.epubit.com.cn，获取更多图书信息。
          關鍵詞：可访问人民邮电出版社异步社区网站, 中文版的读者, 深度学习, 获取更多图书信息
    1.2：深度学习的历史趋势
        - 摘要：1.2.1　神经网络的众多名称和命运变迁
          關鍵詞：神经网络的众多名称和命运变迁
        - 摘要：1.2.2　与日俱增的数据量
          關鍵詞：与日俱增的数据量
        - 摘要：1.2.3　与日俱增的模型规模
          關鍵詞：与日俱增的模型规模
        - 摘要：1.2.4　与日俱增的精度、复杂度和对现实世界的冲击
          關鍵詞：与日俱增的精度, 复杂度和对现实世界的冲击
        - 摘要：第1部分　应用数学与机器学习基础
          關鍵詞：应用数学与机器学习基础, 部分
        - 摘要：通过历史背景了解深度学习是最简单的方式。这里我们仅指出深度学习；的几个关键趋势，而不是提供其详细的历史：
          關鍵詞：的几个关键趋势, 这里我们仅指出深度学习, 而不是提供其详细的历史, 通过历史背景了解深度学习是最简单的方式
        - 摘要：深度学习有着悠久而丰富的历史，但随着许多不同哲学观点的渐渐；消逝，与之对应的名称也渐渐尘封。；随着可用的训练数据量不断增加，深度学习变得更加有用。
          關鍵詞：但随着许多不同哲学观点的渐渐, 消逝, 深度学习有着悠久而丰富的历史, 与之对应的名称也渐渐尘封, 随着可用的训练数据量不断增加
        - 摘要：1.2.1　神经网络的众多名称和命运变迁
          關鍵詞：神经网络的众多名称和命运变迁
        - 摘要：我们期待这本书的许多读者都听说过深度学习这一激动人心的新技术，；并对一本书提及一个新兴领域的“历史”而感到惊讶。事实上，深度学习；的历史可以追溯到20世纪40年代。深度学习看似是一个全新的领域，只
          關鍵詞：历史, 深度学习, 事实上, 世纪, 我们期待这本书的许多读者都听说过深度学习这一激动人心的新技术
        - 摘要：全面地讲述深度学习的历史超出了本书的范围。然而，一些基本的背景；对理解深度学习是有用的。一般认为，迄今为止深度学习已经经历了3；次发展浪潮：20世纪40年代到60年代，深度学习的雏形出现在控制论
          關鍵詞：深度学习的雏形出现在控制论, 一些基本的背景, 对理解深度学习是有用的, 世纪, 迄今为止深度学习已经经历了
        - 摘要：图1.7　根据Google图书中短语“控制论”“联结主义”或“神经网络”频率衡量的人工神经网络研究；的历史浪潮（图中展示了3次浪潮的前两次，第3次最近才出现）。第1次浪潮开始于20世纪40年；代到20世纪60年代的控制论，随着生物学习理论的发展（McCulloch and Pitts，1943；Hebb，
          關鍵詞：图书中短语, 图中展示了, 联结主义, 控制论, 根据
        - 摘要：我们今天知道的一些最早的学习算法，旨在模拟生物学习的计算模型，；即大脑怎样学习或为什么能学习的模型。其结果是深度学习以人工神经；网络 （artificial neural network，ANN）之名而淡去。彼时，深度学习模
          關鍵詞：网络, 之名而淡去, 其结果是深度学习以人工神经, 即大脑怎样学习或为什么能学习的模型, 彼时
        - 摘要：现代术语“深度学习”超越了目前机器学习模型的神经科学观点。它诉诸；于学习多层次组合这一更普遍的原理，这一原理也可以应用于那些并非；受神经科学启发的机器学习框架。
          關鍵詞：现代术语, 超越了目前机器学习模型的神经科学观点, 深度学习, 这一原理也可以应用于那些并非, 受神经科学启发的机器学习框架
        - 摘要：现代深度学习最早的前身是从神经科学的角度出发的简单线性模型。这；些模型设计为使用一组n个输入x  1  ，···，x  n  ，并将它们与一个输出y相
          關鍵詞：并将它们与一个输出, 些模型设计为使用一组, 现代深度学习最早的前身是从神经科学的角度出发的简单线性模型, 个输入
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；关联。这些模型希望学习一组权重w  1  ，···，w  n  ，并计算它们的输出；f(x,w)=x 1 w 1 +···+x n w n 。如图1.7所示，第一次神经网络研究浪潮称为
          關鍵詞：关联, 所示, 第一次神经网络研究浪潮称为, 如图, 并计算它们的输出
        - 摘要：McCulloch-Pitts神经元（McCulloch  and  Pitts，1943）是脑功能的早期模；型。该线性模型通过检验函数f(x,w)的正负来识别两种不同类别的输；入。显然，模型的权重需要正确设置后才能使模型的输出对应于期望的
          關鍵詞：显然, 是脑功能的早期模, 的正负来识别两种不同类别的输, 神经元, 该线性模型通过检验函数
        - 摘要：这些简单的学习算法大大影响了机器学习的现代景象。用于调节；ADALINE权重的训练算法是被称为随机梯度下降  （stochastic  gradient；descent）的一种特例。稍加改进后的随机梯度下降算法仍然是当今深度
          關鍵詞：稍加改进后的随机梯度下降算法仍然是当今深度, 用于调节, 的一种特例, 这些简单的学习算法大大影响了机器学习的现代景象, 权重的训练算法是被称为随机梯度下降
        - 摘要：基于感知机和ADALINE中使用的函数f(x,w)的模型称为线性模型；（linear  model）。尽管在许多情况下，这些模型以不同于原始模型的方；式进行训练，但仍是目前最广泛使用的机器学习模型。
          關鍵詞：基于感知机和, 的模型称为线性模型, 中使用的函数, 但仍是目前最广泛使用的机器学习模型, 尽管在许多情况下
        - 摘要：线性模型有很多局限性。最著名的是，它们无法学习异或（XOR）函；数，即f([0,1],w)=1和f([1,0],w)=1，但f([1,1],w)=0和f([0,0],w)=0。观察到；线性模型这个缺陷的批评者对受生物学启发的学习普遍地产生了抵触
          關鍵詞：线性模型这个缺陷的批评者对受生物学启发的学习普遍地产生了抵触, 最著名的是, 它们无法学习异或, 线性模型有很多局限性, 观察到
        - 摘要：现在，神经科学被视为深度学习研究的一个重要灵感来源，但它已不再；是该领域的主要指导。
          關鍵詞：是该领域的主要指导, 现在, 神经科学被视为深度学习研究的一个重要灵感来源, 但它已不再
        - 摘要：如今神经科学在深度学习研究中的作用被削弱，主要原因是我们根本没；有足够的关于大脑的信息来作为指导去使用它。要获得对被大脑实际使；用算法的深刻理解，我们需要有能力同时监测（至少是）数千相连神经
          關鍵詞：主要原因是我们根本没, 如今神经科学在深度学习研究中的作用被削弱, 我们需要有能力同时监测, 用算法的深刻理解, 至少是
        - 摘要：神经科学已经给了我们依靠单一深度学习算法解决许多不同任务的理；由。神经学家们发现，如果将雪貂的大脑重新连接，使视觉信号传送到；听觉区域，它们可以学会用大脑的听觉处理区域去“看”（Von  Melchner
          關鍵詞：神经科学已经给了我们依靠单一深度学习算法解决许多不同任务的理, 如果将雪貂的大脑重新连接, 它们可以学会用大脑的听觉处理区域去, 使视觉信号传送到, 听觉区域
        - 摘要：linear
          關鍵詞：
        - 摘要：我们能够从神经科学得到一些粗略的指南。仅通过计算单元之间的相互；作用而变得智能的基本思想是受大脑启发的。新认知机（Fukushima，；1980）受哺乳动物视觉系统的结构启发，引入了一个处理图片的强大模
          關鍵詞：作用而变得智能的基本思想是受大脑启发的, 仅通过计算单元之间的相互, 引入了一个处理图片的强大模, 新认知机, 受哺乳动物视觉系统的结构启发
        - 摘要：媒体报道经常强调深度学习与大脑的相似性。的确，深度学习研究者比；其他机器学习领域（如核方法或贝叶斯统计）的研究者更可能地引用大；脑作为影响，但是大家不应该认为深度学习在尝试模拟大脑。现代深度
          關鍵詞：深度学习研究者比, 的研究者更可能地引用大, 其他机器学习领域, 脑作为影响, 媒体报道经常强调深度学习与大脑的相似性
        - 摘要：值得注意的是，了解大脑是如何在算法层面上工作的尝试确实存在且发；展良好。这项尝试主要被称为“计算神经科学”，并且是独立于深度学习
          關鍵詞：并且是独立于深度学习, 计算神经科学, 这项尝试主要被称为, 值得注意的是, 展良好
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；的领域。研究人员在两个领域之间来回研究是很常见的。深度学习领域；主要关注如何构建计算机系统，从而成功解决需要智能才能解决的任
          關鍵詞：主要关注如何构建计算机系统, 的领域, 深度学习领域, 从而成功解决需要智能才能解决的任, 研究人员在两个领域之间来回研究是很常见的
        - 摘要：20世纪80年代，神经网络研究的第二次浪潮在很大程度上是伴随一个被；称为联结主义  （connectionism）或并行分布处理  （parallel  distributed；processing）潮流而出现的（Rumelhart et  al.  ，1986d；McClelland  et  al.
          關鍵詞：称为联结主义, 神经网络研究的第二次浪潮在很大程度上是伴随一个被, 世纪, 年代, 或并行分布处理
        - 摘要：联结主义的中心思想是，当网络将大量简单的计算单元连接在一起时可；以实现智能行为。这种见解同样适用于生物神经系统中的神经元，因为；它和计算模型中隐藏单元起着类似的作用。
          關鍵詞：因为, 它和计算模型中隐藏单元起着类似的作用, 联结主义的中心思想是, 当网络将大量简单的计算单元连接在一起时可, 以实现智能行为
        - 摘要：在20世纪80年代的联结主义期间形成的几个关键概念在今天的深度学习；中仍然是非常重要的。
          關鍵詞：中仍然是非常重要的, 年代的联结主义期间形成的几个关键概念在今天的深度学习, 世纪
        - 摘要：其中一个概念是分布式表示  （distributed  representation）（Hinton  et  al.；，1986）。其思想是：系统的每一个输入都应该由多个特征表示，并且；每一个特征都应该参与到多个可能输入的表示。例如，假设我们有一个
          關鍵詞：每一个特征都应该参与到多个可能输入的表示, 假设我们有一个, 系统的每一个输入都应该由多个特征表示, 例如, 其中一个概念是分布式表示
        - 摘要：联结主义潮流的另一个重要成就是反向传播在训练具有内部表示的深度；神经网络中的成功使用以及反向传播算法的普及（Rumelhart  et  al.  ，；1986c；LeCun，1987）。这个算法虽然曾黯然失色且不再流行，但截至
          關鍵詞：但截至, 联结主义潮流的另一个重要成就是反向传播在训练具有内部表示的深度, 神经网络中的成功使用以及反向传播算法的普及, 这个算法虽然曾黯然失色且不再流行
        - 摘要：20世纪90年代，研究人员在使用神经网络进行序列建模的方面取得了重；要进展。Hochreiter（1991b）和Bengio  et  al.  （1994b）指出了对长序列；进行建模的一些根本性数学难题，这将在第10.7节中描述。Hochreiter和
          關鍵詞：世纪, 进行建模的一些根本性数学难题, 研究人员在使用神经网络进行序列建模的方面取得了重, 这将在第, 年代
        - 摘要：神经网络研究的第二次浪潮一直持续到20世纪90年代中期。基于神经网；络和其他AI技术的创业公司开始寻求投资，其做法野心勃勃但不切实；际。当AI研究不能实现这些不合理的期望时，投资者感到失望。同时，
          關鍵詞：技术的创业公司开始寻求投资, 络和其他, 研究不能实现这些不合理的期望时, 同时, 世纪
        - 摘要：在此期间，神经网络继续在某些任务上获得令人印象深刻的表现；（LeCun  et  al.  ，1998c；Bengio  et  al.  ，2001a）。加拿大高级研究所；（CIFAR）通过其神经计算和自适应感知（NCAP）研究计划帮助维持
          關鍵詞：神经网络继续在某些任务上获得令人印象深刻的表现, 通过其神经计算和自适应感知, 加拿大高级研究所, 在此期间, 研究计划帮助维持
        - 摘要：在那个时候，人们普遍认为深度网络是难以训练的。现在我们知道，20；世纪80年代就存在的算法能工作得非常好，但是直到2006年前后都没有；体现出来。这可能仅仅由于其计算代价太高，而以当时可用的硬件难以
          關鍵詞：这可能仅仅由于其计算代价太高, 世纪, 而以当时可用的硬件难以, 但是直到, 年前后都没有
        - 摘要：神经网络研究的第三次浪潮始于2006年的突破。Geoffrey  Hinton表明名；为“深度信念网络”的神经网络可以使用一种称为“贪婪逐层预训练”的策；略来有效地训练（Hinton  et  al.  ，2006a），我们将在第15.1节中更详细
          關鍵詞：年的突破, 表明名, 的神经网络可以使用一种称为, 我们将在第, 贪婪逐层预训练
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；地描述。其他CIFAR附属研究小组很快表明，同样的策略可以被用来训；练许多其他类型的深度网络（Bengio  and  LeCun，2007a；Ranzato  et  al.
          關鍵詞：附属研究小组很快表明, 练许多其他类型的深度网络, 同样的策略可以被用来训, 地描述, 其他
        - 摘要：1.2.2　与日俱增的数据量
          關鍵詞：与日俱增的数据量
        - 摘要：人们可能想问，既然人工神经网络的第一个实验在20世纪50年代就完成；了，但为什么深度学习直到最近才被认为是关键技术？自20世纪90年代；以来，深度学习就已经成功用于商业应用，但通常被视为一种只有专家
          關鍵詞：但为什么深度学习直到最近才被认为是关键技术, 年代就完成, 世纪, 以来, 但通常被视为一种只有专家
        - 摘要：的未标注样本。
          關鍵詞：的未标注样本
        - 摘要：图1.8　与日俱增的数据量。20世纪初，统计学家使用数百或数千的手动制作的度量来研究数据；集（Garson，1900；Gosset，1908；Anderson，1935；Fisher，1936）。20世纪50年代到80年；代，受生物启发的机器学习开拓者通常使用小的合成数据集，如低分辨率的字母位图，设计为
          關鍵詞：设计为, 世纪, 统计学家使用数百或数千的手动制作的度量来研究数据, 如低分辨率的字母位图, 与日俱增的数据量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图1.9　MNIST数据集的输入样例。“NIST”代表国家标准和技术研究所（National Institute of；Standards and Technology），是最初收集这些数据的机构。“M”代表“修改的（Modified）”，为
          關鍵詞：代表国家标准和技术研究所, 代表, 数据集的输入样例, 修改的, 是最初收集这些数据的机构
        - 摘要：1.2.3　与日俱增的模型规模
          關鍵詞：与日俱增的模型规模
        - 摘要：20世纪80年代，神经网络只能取得相对较小的成功，而现在神经网络非；常成功的另一个重要原因是我们现在拥有的计算资源可以运行更大的模；型。联结主义的主要见解之一是，当动物的许多神经元一起工作时会变
          關鍵詞：常成功的另一个重要原因是我们现在拥有的计算资源可以运行更大的模, 世纪, 而现在神经网络非, 联结主义的主要见解之一是, 当动物的许多神经元一起工作时会变
        - 摘要：生物神经元不是特别稠密地连接在一起。如图1.10所示，几十年来，我；们的机器学习模型中每个神经元的连接数量已经与哺乳动物的大脑在同
          關鍵詞：所示, 们的机器学习模型中每个神经元的连接数量已经与哺乳动物的大脑在同, 生物神经元不是特别稠密地连接在一起, 如图, 几十年来
        - 摘要：一数量级上。
          關鍵詞：一数量级上
        - 摘要：图1.10　与日俱增的每个神经元的连接数。最初，人工神经网络中神经元之间的连接数受限于；硬件能力。而现在，神经元之间的连接数大多是出于设计考虑。一些人工神经网络中每个神经；元的连接数与猫一样多，并且对于其他神经网络来说，每个神经元的连接数与较小哺乳动物
          關鍵詞：与日俱增的每个神经元的连接数, 而现在, 人工神经网络中神经元之间的连接数受限于, 并且对于其他神经网络来说, 硬件能力
        - 摘要：1．自适应线性单元（Widrow and Hoff，1960）；2．神经认知机（Fukushima，1980）；3．；GPU-加速卷积网络（Chellapilla et al. ，2006）；4．深度玻尔兹曼机（Salakhutdinov and；Hinton，2009a）；5．无监督卷积网络（Jarrett et al. ，2009b）；6．GPU-加速多层感知机
          關鍵詞：无监督卷积网络, 深度玻尔兹曼机, 自适应线性单元, 神经认知机, 加速卷积网络
        - 摘要：如图1.11所示，就神经元的总数目而言，直到最近神经网络都是惊人的；小。自从隐藏单元引入以来，人工神经网络的规模大约每2.4年扩大一；倍。这种增长是由更大内存、更快的计算机和更大的可用数据集驱动
          關鍵詞：这种增长是由更大内存, 更快的计算机和更大的可用数据集驱动, 所示, 就神经元的总数目而言, 直到最近神经网络都是惊人的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图1.11　与日俱增的神经网络规模。自从引入隐藏单元，人工神经网络的规模大约每2.4年翻一；倍。生物神经网络规模来自Wikipedia（2015）
          關鍵詞：生物神经网络规模来自, 年翻一, 自从引入隐藏单元, 与日俱增的神经网络规模, 人工神经网络的规模大约每
        - 摘要：1．感知机（Rosenblatt，1958，1962）；2.自适应线性单元（Widrow and Hoff，1960）；3.神经；认知机（Fukushima，1980）；4.早期后向传播网络（Rumelhart et al. ，1986b）；5.用于语音识；别的循环神经网络（Robinson and Fallside，1991）；6.用于语音识别的多层感知机（Bengio et al.
          關鍵詞：认知机, 感知机, 自适应线性单元, 用于语音识, 神经
        - 摘要：现在看来，神经元数量比一个水蛭还少的神经网络不能解决复杂的人工；智能问题，这是不足为奇的。即使现在的网络，从计算系统角度来看它；可能相当大，但实际上它比相对原始的脊椎动物（如青蛙）的神经系统
          關鍵詞：可能相当大, 但实际上它比相对原始的脊椎动物, 神经元数量比一个水蛭还少的神经网络不能解决复杂的人工, 从计算系统角度来看它, 的神经系统
        - 摘要：由于更快的CPU、通用GPU的出现（在第12.1.2节中讨论）、更快的网；络连接和更好的分布式计算的软件基础设施，模型规模随着时间的推移；不断增加是深度学习历史中最重要的趋势之一。人们普遍预计这种趋势
          關鍵詞：模型规模随着时间的推移, 不断增加是深度学习历史中最重要的趋势之一, 由于更快的, 的出现, 在第
        - 摘要：1.2.4　与日俱增的精度、复杂度和对现实世界的冲击
          關鍵詞：与日俱增的精度, 复杂度和对现实世界的冲击
        - 摘要：20世纪80年代以来，深度学习提供精确识别和预测的能力一直在提高。；而且，深度学习持续成功地应用于越来越广泛的实际问题中。
          關鍵詞：世纪, 而且, 年代以来, 深度学习提供精确识别和预测的能力一直在提高, 深度学习持续成功地应用于越来越广泛的实际问题中
        - 摘要：最早的深度模型被用来识别裁剪紧凑且非常小的图像中的单个对象；（Rumelhart  et  al.  ，1986d）。此后，神经网络可以处理的图像尺寸逐；渐增加。现代对象识别网络能处理丰富的高分辨率照片，并且不需要在
          關鍵詞：最早的深度模型被用来识别裁剪紧凑且非常小的图像中的单个对象, 渐增加, 并且不需要在, 现代对象识别网络能处理丰富的高分辨率照片, 此后
        - 摘要：图1.12　日益降低的错误率。由于深度网络达到了在ImageNet大规模视觉识别挑战中竞争所必；需的规模，它们每年都能赢得胜利，并且产生越来越低的错误率。数据来源于Russakovsky et al.；（2014b）和He et al. （2015）
          關鍵詞：并且产生越来越低的错误率, 它们每年都能赢得胜利, 日益降低的错误率, 大规模视觉识别挑战中竞争所必, 由于深度网络达到了在
        - 摘要：深度学习也对语音识别产生了巨大影响。语音识别在20世纪90年代得到；提高后，直到约2000年都停滞不前。深度学习的引入（Dahl  et  al.  ，；2010；Deng et al. ，2010b；Seide et al. ，2011；Hinton et al. ，2012a）
          關鍵詞：语音识别在, 深度学习的引入, 年代得到, 直到约, 世纪
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；第12.3节更详细地探讨这个历史。
          關鍵詞：节更详细地探讨这个历史
        - 摘要：深度网络在行人检测和图像分割中也取得了引人注目的成功（Sermanet；et al. ，2013；Fara-bet et al. ，2013；Couprie et al. ，2013），并且在交；通标志分类上取得了超越人类的表现（Ciresan et al. ，2012）。
          關鍵詞：通标志分类上取得了超越人类的表现, 深度网络在行人检测和图像分割中也取得了引人注目的成功, 并且在交
        - 摘要：在深度网络的规模和精度有所提高的同时，它们可以解决的任务也日益；复杂。Goodfellow et  al.  （2014d）表明，神经网络可以学习输出描述图；像的整个字符序列，而不是仅仅识别单个对象。此前，人们普遍认为，
          關鍵詞：神经网络可以学习输出描述图, 它们可以解决的任务也日益, 人们普遍认为, 像的整个字符序列, 而不是仅仅识别单个对象
        - 摘要：这种复杂性日益增加的趋势已将其推向逻辑结论，即神经图灵机；（Graves  et  al.  ，2014）的引入，它能学习读取存储单元和向存储单元；写入任意内容。这样的神经网络可以从期望行为的样本中学习简单的程
          關鍵詞：这种复杂性日益增加的趋势已将其推向逻辑结论, 它能学习读取存储单元和向存储单元, 的引入, 即神经图灵机, 这样的神经网络可以从期望行为的样本中学习简单的程
        - 摘要：深度学习的另一个最大的成就是其在强化学习；learning）领域的扩展。在强化学习中，一个自主的智能体必须在没有；人类操作者指导的情况下，通过试错来学习执行任务。DeepMind表
          關鍵詞：深度学习的另一个最大的成就是其在强化学习, 一个自主的智能体必须在没有, 人类操作者指导的情况下, 通过试错来学习执行任务, 领域的扩展
        - 摘要：（reinforcement
          關鍵詞：
        - 摘要：许多深度学习应用都是高利润的。现在深度学习被许多顶级的技术公司；使用，包括Google、Microsoft、Facebook、IBM、Baidu、Apple、；Adobe、Netflix、NVIDIA和NEC等。
          關鍵詞：使用, 许多深度学习应用都是高利润的, 现在深度学习被许多顶级的技术公司, 包括
        - 摘要：深度学习的进步也严重依赖于软件基础架构的进展。软件库如；et  al.  ，2012a）、；Theano（Bergstra
          關鍵詞：深度学习的进步也严重依赖于软件基础架构的进展, 软件库如
        - 摘要：et  al.  ，2010a；Bastien
          關鍵詞：
        - 摘要：MXNet（Chen et al. ，2015）和Tensor-Flow（Abadi et al. ，2015）都能；支持重要的研究项目或商业产品。
          關鍵詞：支持重要的研究项目或商业产品, 都能
        - 摘要：深度学习也为其他科学做出了贡献。用于对象识别的现代卷积网络为神；经科学家们提供了可以研究的视觉处理模型（DiCarlo，2013）。深度；学习也为处理海量数据以及在科学领域做出有效的预测提供了非常有用
          關鍵詞：深度学习也为其他科学做出了贡献, 学习也为处理海量数据以及在科学领域做出有效的预测提供了非常有用, 经科学家们提供了可以研究的视觉处理模型, 用于对象识别的现代卷积网络为神, 深度
        - 摘要：总之，深度学习是机器学习的一种方法。在过去几十年的发展中，它大；量借鉴了我们关于人脑、统计学和应用数学的知识。近年来，得益于更；强大的计算机、更大的数据集和能够训练更深网络的技术，深度学习的
          關鍵詞：它大, 深度学习是机器学习的一种方法, 得益于更, 在过去几十年的发展中, 近年来
        - 摘要：第1部分　应用数学与机器学习基础
          關鍵詞：应用数学与机器学习基础, 部分
        - 摘要：本书这一部分将介绍理解深度学习所需的基本数学概念。我们从应用数；学的一般概念开始，这能使我们定义拥有许多变量的函数，找到这些函；数的最高点和最低点，并量化信念度。
          關鍵詞：这能使我们定义拥有许多变量的函数, 并量化信念度, 学的一般概念开始, 本书这一部分将介绍理解深度学习所需的基本数学概念, 数的最高点和最低点
        - 摘要：接着，我们描述机器学习的基本目标，并描述如何实现这些目标。我们；需要指定代表某些信念的模型、设计衡量这些信念与现实对应程度的代；价函数以及使用训练算法最小化这个代价函数。
          關鍵詞：我们描述机器学习的基本目标, 接着, 并描述如何实现这些目标, 我们, 需要指定代表某些信念的模型
        - 摘要：这个基本框架是广泛多样的机器学习算法的基础，其中也包括非深度的；机器学习方法。在本书的后续章节，我们将在这个框架下开发深度学习；算法。
          關鍵詞：算法, 其中也包括非深度的, 这个基本框架是广泛多样的机器学习算法的基础, 我们将在这个框架下开发深度学习, 机器学习方法
        - 摘要：1.2.1　神经网络的众多名称和命运变迁
          關鍵詞：神经网络的众多名称和命运变迁
        - 摘要：1.2.2　与日俱增的数据量
          關鍵詞：与日俱增的数据量
        - 摘要：1.2.3　与日俱增的模型规模
          關鍵詞：与日俱增的模型规模
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；1.2.4　与日俱增的精度、复杂度和对现实世；界的冲击
          關鍵詞：与日俱增的精度, 复杂度和对现实世, 界的冲击
        - 摘要：第1部分　应用数学与机器学习基础
          關鍵詞：应用数学与机器学习基础, 部分
    20.15：结论
        - 摘要：远在古希腊时期，发明家就梦想着创造能自主思考的机器。神话人物皮；格马利翁（Pygmalion）、代达罗斯（Daedalus）和赫淮斯托斯；（Hephaestus）可以被看作传说中的发明家，而加拉蒂亚（Galatea）、
          關鍵詞：远在古希腊时期, 和赫淮斯托斯, 可以被看作传说中的发明家, 代达罗斯, 格马利翁
        - 摘要：当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能；（尽管这距造出第一台计算机还有一百多年）（Lovelace，1842）。如；今，人工智能  （artificial  intelligence，AI）已经成为一个具有众多实际
          關鍵詞：就已经在思考计算机能否变得智能, 尽管这距造出第一台计算机还有一百多年, 当人类第一次构思可编程计算机时, 人工智能, 已经成为一个具有众多实际
        - 摘要：在人工智能的早期，那些对人类智力来说非常困难、但对计算机来说相；对简单的问题得到迅速解决，比如，那些可以通过一系列形式化的数学；规则来描述的问题。人工智能的真正挑战在于解决那些对人来说很容易
          關鍵詞：在人工智能的早期, 那些对人类智力来说非常困难, 但对计算机来说相, 比如, 人工智能的真正挑战在于解决那些对人来说很容易
        - 摘要：针对这些比较直观的问题，本书讨论一种解决方案。该方案可以让计算；机从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则；通过与某些相对简单的概念之间的关系来定义。让计算机从经验获取知
          關鍵詞：本书讨论一种解决方案, 该方案可以让计算, 机从经验中学习, 让计算机从经验获取知, 并根据层次化的概念体系来理解世界
        - 摘要：AI许多早期的成功发生在相对朴素且形式化的环境中，而且不要求计算；机具备很多关于世界的知识。例如，IBM的深蓝（Deep Blue）国际象棋；系统在1997年击败了世界冠军Garry  Kasparov（Hsu，2002）。显然国际
          關鍵詞：而且不要求计算, 国际象棋, 系统在, 机具备很多关于世界的知识, 例如
        - 摘要：具有讽刺意义的是，抽象和形式化的任务对人类而言是最困难的脑力任；务之一，但对计算机而言却属于最容易的。计算机早就能够打败人类最；好的国际象棋选手，但直到最近计算机才在识别对象或语音任务中达到
          關鍵詞：但直到最近计算机才在识别对象或语音任务中达到, 但对计算机而言却属于最容易的, 务之一, 抽象和形式化的任务对人类而言是最困难的脑力任, 计算机早就能够打败人类最
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；算机需要获取同样的知识才能表现出智能。人工智能的一个关键挑战就；是如何将这些非形式化的知识传达给计算机。
          關鍵詞：人工智能的一个关键挑战就, 算机需要获取同样的知识才能表现出智能, 是如何将这些非形式化的知识传达给计算机
        - 摘要：一些人工智能项目力求将关于世界的知识用形式化的语言进行硬编码；（hard-code）。计算机可以使用逻辑推理规则来自动地理解这些形式化；（knowledge
          關鍵詞：一些人工智能项目力求将关于世界的知识用形式化的语言进行硬编码, 计算机可以使用逻辑推理规则来自动地理解这些形式化
        - 摘要：依靠硬编码的知识体系面临的困难表明，AI系统需要具备自己获取知识；的能力，即从原始数据中提取模式的能力。这种能力称为机器学习（；machine  learning）。引入机器学习使计算机能够解决涉及现实世界知识
          關鍵詞：引入机器学习使计算机能够解决涉及现实世界知识, 依靠硬编码的知识体系面临的困难表明, 这种能力称为机器学习, 系统需要具备自己获取知识, 的能力
        - 摘要：这些简单的机器学习算法的性能在很大程度上依赖于给定数据的表示；（representation）。例如，当逻辑回归用于判断产妇是否适合剖腹产；时，AI系统不会直接检查患者。相反，医生需要告诉系统几条相关的信
          關鍵詞：系统不会直接检查患者, 例如, 这些简单的机器学习算法的性能在很大程度上依赖于给定数据的表示, 当逻辑回归用于判断产妇是否适合剖腹产, 医生需要告诉系统几条相关的信
        - 摘要：在整个计算机科学乃至日常生活中，对表示的依赖都是一个普遍现象。；在计算机科学中，如果数据集合被精巧地结构化并被智能地索引，那么；诸如搜索之类的操作的处理速度就可以成指数级地加快。人们可以很容
          關鍵詞：诸如搜索之类的操作的处理速度就可以成指数级地加快, 对表示的依赖都是一个普遍现象, 人们可以很容, 那么, 在整个计算机科学乃至日常生活中
        - 摘要：易地在阿拉伯数字的表示下进行算术运算，但在罗马数字的表示下，运；算会比较耗时。因此，毫不奇怪，表示的选择会对机器学习算法的性能；产生巨大的影响。图1.1展示了一个简单的可视化例子。
          關鍵詞：表示的选择会对机器学习算法的性能, 但在罗马数字的表示下, 展示了一个简单的可视化例子, 因此, 产生巨大的影响
        - 摘要：图1.1　不同表示的例子：假设我们想在散点图中画一条线来分隔两类数据。在左图中，我们使；用笛卡儿坐标表示数据，这个任务是不可能的。在右图中，我们用极坐标表示数据，可以用垂；直线简单地解决这个任务（与David Warde-Farley合作绘制此图）
          關鍵詞：用笛卡儿坐标表示数据, 在左图中, 这个任务是不可能的, 假设我们想在散点图中画一条线来分隔两类数据, 可以用垂
        - 摘要：许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征；集，然后将这些特征提供给简单的机器学习算法。例如，对于通过声音；鉴别说话者的任务来说，一个有用的特征是对其声道大小的估计。这个
          關鍵詞：许多人工智能任务都可以通过以下方式解决, 鉴别说话者的任务来说, 一个有用的特征是对其声道大小的估计, 先提取一个合适的特征, 例如
        - 摘要：然而，对于许多任务来说，我们很难知道应该提取哪些特征。例如，假；设我们想编写一个程序来检测照片中的车。我们知道，汽车有轮子，所；以我们可能会想用车轮的存在与否作为特征。遗憾的是，我们难以准确
          關鍵詞：我们很难知道应该提取哪些特征, 以我们可能会想用车轮的存在与否作为特征, 然而, 对于许多任务来说, 例如
        - 摘要：解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把；表示映射到输出。这种方法我们称之为表示学习；（representation
          關鍵詞：解决这个问题的途径之一是使用机器学习来发掘表示本身, 而不仅仅把, 这种方法我们称之为表示学习, 表示映射到输出
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；务则需要几小时到几个月。手动为一个复杂的任务设计特征需要耗费大；量的人工、时间和精力，甚至需要花费整个社群研究人员几十年的时
          關鍵詞：量的人工, 甚至需要花费整个社群研究人员几十年的时, 务则需要几小时到几个月, 手动为一个复杂的任务设计特征需要耗费大, 时间和精力
        - 摘要：表示学习算法的典型例子是自编码器  （autoencoder）。自编码器由一；个编码器 （encoder）函数和一个解码器  （decoder）函数组合而成。编；码器函数将输入数据转换为一种不同的表示，而解码器函数则将这个新
          關鍵詞：而解码器函数则将这个新, 表示学习算法的典型例子是自编码器, 函数组合而成, 码器函数将输入数据转换为一种不同的表示, 函数和一个解码器
        - 摘要：当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能；解释观察数据的变差因素  （factors  of  variation）。在此背景下，“因；素”这个词仅指代影响的不同来源；因素通常不是乘性组合。这些因素
          關鍵詞：这个词仅指代影响的不同来源, 解释观察数据的变差因素, 在此背景下, 我们的目标通常是分离出能, 当设计特征或设计用于学习特征的算法时
        - 摘要：在许多现实的人工智能应用中，困难主要源于多个变差因素同时影响着；我们能够观察到的每一个数据。比如，在一张包含红色汽车的图片中，；其单个像素在夜间可能会非常接近黑色。汽车轮廓的形状取决于视角。
          關鍵詞：在一张包含红色汽车的图片中, 汽车轮廓的形状取决于视角, 困难主要源于多个变差因素同时影响着, 其单个像素在夜间可能会非常接近黑色, 比如
        - 摘要：显然，从原始数据中提取如此高层次、抽象的特征是非常困难的。许多；诸如说话口音这样的变差因素，只能通过对数据进行复杂的、接近人类；水平的理解来辨识。这几乎与获得原问题的表示一样困难，因此，乍一
          關鍵詞：显然, 抽象的特征是非常困难的, 只能通过对数据进行复杂的, 许多, 水平的理解来辨识
        - 摘要：深度学习  （deep  learning）通过其他较简单的表示来表达复杂表示，解；决了表示学习中的核心问题。
          關鍵詞：通过其他较简单的表示来表达复杂表示, 深度学习, 决了表示学习中的核心问题
        - 摘要：深度学习让计算机通过较简单的概念构建复杂的概念。图1.2展示了深
          關鍵詞：深度学习让计算机通过较简单的概念构建复杂的概念, 展示了深
        - 摘要：度学习系统如何通过组合较简单的概念（例如角和轮廓，它们反过来由；边线定义）来表示图像中人的概念。深度学习模型的典型例子是前馈深；度网络或或多层感知机 （multilayer perceptron，MLP）。多层感知机仅
          關鍵詞：多层感知机仅, 来表示图像中人的概念, 深度学习模型的典型例子是前馈深, 例如角和轮廓, 它们反过来由
        - 摘要：图1.2　深度学习模型的示意图。计算机难以理解原始感观输入数据的含义，如表示为像素值集；合的图像。将一组像素映射到对象标识的函数非常复杂。如果直接处理，学习或评估此映射似；乎是不可能的。深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个由模型的不同
          關鍵詞：如表示为像素值集, 如果直接处理, 学习或评估此映射似, 深度学习将所需的复杂映射分解为一系列嵌套的简单映射, 合的图像
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；学习数据的正确表示的想法是解释深度学习的一个视角。另一个视角是；深度促使计算机学习一个多步骤的计算机程序。每一层表示都可以被认
          關鍵詞：另一个视角是, 深度促使计算机学习一个多步骤的计算机程序, 学习数据的正确表示的想法是解释深度学习的一个视角, 每一层表示都可以被认
        - 摘要：目前主要有两种度量模型深度的方式。一种方式是基于评估架构所需执；行的顺序指令的数目。假设我们将模型表示为给定输入后，计算对应输；出的流程图，则可以将这张流程图中的最长路径视为模型的深度。正如
          關鍵詞：一种方式是基于评估架构所需执, 出的流程图, 则可以将这张流程图中的最长路径视为模型的深度, 行的顺序指令的数目, 目前主要有两种度量模型深度的方式
        - 摘要：图1.3　将输入映射到输出的计算图表的示意图，其中每个节点执行一个操作。深度是从输入到；输出的最长路径的长度，但这取决于可能的计算步骤的定义。这些图中所示的计算是逻辑回归；模型的输出，σ(w T x)，其中σ是logistic sigmoid函数。如果使用加法、乘法和logistic sigmoid作
          關鍵詞：如果使用加法, 深度是从输入到, 输出的最长路径的长度, 乘法和, 将输入映射到输出的计算图表的示意图
        - 摘要：另一种是在深度概率模型中使用的方法，它不是将计算图的深度视为模；型深度，而是将描述概念彼此如何关联的图的深度视为模型深度。在这；种情况下，计算每个概念表示的计算流程图的深度可能比概念本身的图
          關鍵詞：而是将描述概念彼此如何关联的图的深度视为模型深度, 计算每个概念表示的计算流程图的深度可能比概念本身的图, 在这, 型深度, 另一种是在深度概率模型中使用的方法
        - 摘要：更深。这是因为系统对较简单概念的理解在给出更复杂概念的信息后可；以进一步精细化。例如，一个AI系统观察其中一只眼睛在阴影中的脸部；图像时，它最初可能只看到一只眼睛。但当检测到脸部的存在后，系统
          關鍵詞：以进一步精细化, 但当检测到脸部的存在后, 这是因为系统对较简单概念的理解在给出更复杂概念的信息后可, 例如, 系统观察其中一只眼睛在阴影中的脸部
        - 摘要：由于并不总是清楚计算图的深度和概率模型图的深度哪一个是最有意义；的，并且由于不同的人选择不同的最小元素集来构建相应的图，所以就；像计算机程序的长度不存在单一的正确值一样，架构的深度也不存在单
          關鍵詞：并且由于不同的人选择不同的最小元素集来构建相应的图, 像计算机程序的长度不存在单一的正确值一样, 所以就, 架构的深度也不存在单, 由于并不总是清楚计算图的深度和概率模型图的深度哪一个是最有意义
        - 摘要：总之，这本书的主题——深度学习是通向人工智能的途径之一。具体来；说，它是机器学习的一种，一种能够使计算机系统从经验和数据中得到；提高的技术。我们坚信机器学习可以构建出在复杂实际环境下运行的AI
          關鍵詞：具体来, 我们坚信机器学习可以构建出在复杂实际环境下运行的, 提高的技术, 一种能够使计算机系统从经验和数据中得到, 这本书的主题
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图1.4　维恩图展示了深度学习既是一种表示学习，也是一种机器学习，可以用于许多（但不是；全部）AI方法。维恩图的每个部分包括一个AI技术的实例
          關鍵詞：全部, 也是一种机器学习, 维恩图展示了深度学习既是一种表示学习, 但不是, 技术的实例
        - 摘要：图1.5　流程图展示了AI系统的不同部分如何在不同的AI学科中彼此相关。阴影框表示能从数据；中学习的组件
          關鍵詞：学科中彼此相关, 流程图展示了, 中学习的组件, 系统的不同部分如何在不同的, 阴影框表示能从数据
第2章：线性代数
    1.2：深度学习的历史趋势
        - 摘要：线性代数作为数学的一个分支，广泛应用于科学和工程中。然而，因为；线性代数主要是面向连续数学，而非离散数学，所以很多计算机科学家
          關鍵詞：因为, 而非离散数学, 然而, 线性代数作为数学的一个分支, 广泛应用于科学和工程中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；很少接触它。掌握好线性代数对于理解和从事机器学习算法相关工作是；很有必要的，尤其对于深度学习算法而言。因此，在开始介绍深度学习
          關鍵詞：掌握好线性代数对于理解和从事机器学习算法相关工作是, 因此, 尤其对于深度学习算法而言, 在开始介绍深度学习, 很有必要的
        - 摘要：如果你已经很熟悉线性代数，那么可以轻松地跳过本章。如果你已经了；解这些概念，但是需要一份索引表来回顾一些重要公式，那么我们推；荐The Matrix Cookbook （Petersen and Pedersen，2006）。如果你没有接
          關鍵詞：那么可以轻松地跳过本章, 解这些概念, 但是需要一份索引表来回顾一些重要公式, 如果你已经了, 如果你没有接
    2.1：标量、向量、矩阵和张量
        - 摘要：学习线性代数，会涉及以下几个数学概念：
          關鍵詞：学习线性代数, 会涉及以下几个数学概念
        - 摘要：标量  （scalar）：一个标量就是一个单独的数，它不同于线性代数；中研究的其他大部分对象（通常是多个数的数组）。我们用斜体表；示标量。标量通常被赋予小写的变量名称。在介绍标量时，我们会
          關鍵詞：标量, 我们用斜体表, 在介绍标量时, 通常是多个数的数组, 示标量
        - 摘要：，并且该向量有n个元素，那么该向量属于实数集   的n次笛；。当需要明确表示向量中的元素
          關鍵詞：并且该向量有, 那么该向量属于实数集, 次笛, 个元素, 当需要明确表示向量中的元素
        - 摘要：卡儿乘积构成的集合，记为；时，我们会将元素排列成一个方括号包围的纵列：
          關鍵詞：记为, 我们会将元素排列成一个方括号包围的纵列, 卡儿乘积构成的集合
        - 摘要：我们可以把向量看作空间中的点，每个元素是不同坐标轴上的坐；标。
          關鍵詞：我们可以把向量看作空间中的点, 每个元素是不同坐标轴上的坐
        - 摘要：有时我们需要索引向量中的一些元素。在这种情况下，我们定义一；个包含这些元素索引的集合，然后将该集合写在脚标处。比如，指；定x 1 、x 3 和x 6 ，我们定义集合S={1，3，6}，然后写作x  S  。我们
          關鍵詞：然后将该集合写在脚标处, 有时我们需要索引向量中的一些元素, 我们定义集合, 在这种情况下, 我们
        - 摘要：有时我们需要矩阵值表达式的索引，而不是单个元素。在这种情况；下，我们在表达式后面接下标，但不必将矩阵的变量名称小写化。
          關鍵詞：在这种情况, 我们在表达式后面接下标, 而不是单个元素, 但不必将矩阵的变量名称小写化, 有时我们需要矩阵值表达式的索引
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；比如，f( A ) i,j 表示函数f作用在 A 上输出的矩阵的第i行第j列元素。；张量  （tensor）：在某些情况下，我们会讨论坐标超过两维的数
          關鍵詞：我们会讨论坐标超过两维的数, 列元素, 作用在, 张量, 上输出的矩阵的第
        - 摘要：转置 （transpose）是矩阵的重要操作之一。矩阵的转置是以对角线为轴；的镜像，这条从左上角到右下角的对角线被称为主对角线；（main
          關鍵詞：矩阵的转置是以对角线为轴, 这条从左上角到右下角的对角线被称为主对角线, 的镜像, 转置, 是矩阵的重要操作之一
        - 摘要：向量可以看作只有一列的矩阵。对应地，向量的转置可以看作只有一行；的矩阵。有时，我们通过将向量元素作为行矩阵写在文本行中，然后使；用转置操作将其变为标准的列向量，来定义一个向量，比如
          關鍵詞：对应地, 我们通过将向量元素作为行矩阵写在文本行中, 向量可以看作只有一列的矩阵, 然后使, 有时
        - 摘要：。
          關鍵詞：
        - 摘要：图2.1　矩阵的转置可以看作以主对角线为轴的一个镜像
          關鍵詞：矩阵的转置可以看作以主对角线为轴的一个镜像
        - 摘要：标量可以看作只有一个元素的矩阵。因此，标量的转置等于它本身，
          關鍵詞：标量的转置等于它本身, 因此, 标量可以看作只有一个元素的矩阵
        - 摘要：。
          關鍵詞：
        - 摘要：只要矩阵的形状一样，我们可以把两个矩阵相加。两个矩阵相加是指对；应位置的元素相加，比如 C = A ﹢ B ，其中C i,j =A i,j ﹢B i,j 。
          關鍵詞：我们可以把两个矩阵相加, 两个矩阵相加是指对, 应位置的元素相加, 其中, 比如
        - 摘要：标量和矩阵相乘，或是和矩阵相加时，我们只需将其与矩阵的每个元素；相乘或相加，比如 D =a · B ﹢c ，其中 D i,j =a · B i,j ﹢c 。
          關鍵詞：我们只需将其与矩阵的每个元素, 其中, 比如, 标量和矩阵相乘, 或是和矩阵相加时
        - 摘要：在深度学习中，我们也使用一些不那么常规的符号。我们允许矩阵和向；量相加，产生另一个矩阵：  C = A ﹢ b ，其中 C  i,j  = A  i,j  ﹢ b  j  。换言；之，向量 b 和矩阵 A 的每一行相加。这个简写方法使我们无须在加法操
          關鍵詞：我们允许矩阵和向, 的每一行相加, 其中, 我们也使用一些不那么常规的符号, 量相加
    2.2：矩阵和向量相乘
        - 摘要：矩阵乘法是矩阵运算中最重要的操作之一。两个矩阵  A  和  B  的矩阵乘；积  （matrix  product）是第三个矩阵  C  。为了使乘法可被定义，矩阵  A；的列数必须和矩阵  B  的行数相等。如果矩阵  A  的形状是m×n，矩阵  B
          關鍵詞：两个矩阵, 的矩阵乘, 的形状是, 矩阵, 的行数相等
        - 摘要：具体地，该乘法操作定义为
          關鍵詞：该乘法操作定义为, 具体地
        - 摘要：需要注意的是，两个矩阵的标准乘积不是指两个矩阵中对应元素的乘；积。不过，那样的矩阵操作确实是存在的，称为元素对应乘积；（element-wise product）或者Hadamard乘积 （Hadamard product），记
          關鍵詞：需要注意的是, 两个矩阵的标准乘积不是指两个矩阵中对应元素的乘, 不过, 称为元素对应乘积, 或者
        - 摘要：。
          關鍵詞：
        - 摘要：两个相同维数的向量  x  和  y  的点积  （dot  product）可看作矩阵乘积；。我们可以把矩阵乘积 C = AB 中计算C  i,j  的步骤看作 A 的第i行
          關鍵詞：的第, 的点积, 两个相同维数的向量, 的步骤看作, 我们可以把矩阵乘积
        - 摘要：和 B 的第j列之间的点积。
          關鍵詞：的第, 列之间的点积
        - 摘要：矩阵乘积运算有许多有用的性质，从而使矩阵的数学分析更加方便。比；如，矩阵乘积服从分配律：
          關鍵詞：矩阵乘积运算有许多有用的性质, 从而使矩阵的数学分析更加方便, 矩阵乘积服从分配律
        - 摘要：矩阵乘积也服从结合律：
          關鍵詞：矩阵乘积也服从结合律
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；不同于标量乘积，矩阵乘积并不满足交换律（  AB=BA  的情况并非总是；满足）。然而，两个向量的点积 满足交换律：
          關鍵詞：的情况并非总是, 满足交换律, 满足, 两个向量的点积, 然而
        - 摘要：矩阵乘积的转置有着简单的形式：
          關鍵詞：矩阵乘积的转置有着简单的形式
        - 摘要：利用两个向量点积的结果是标量、标量转置是自身的事实，我们可以证；明式（2.8）：
          關鍵詞：利用两个向量点积的结果是标量, 我们可以证, 标量转置是自身的事实, 明式
        - 摘要：由于本书的重点不是线性代数，我们并不想展示矩阵乘积的所有重要性；质，但读者应该知道矩阵乘积还有很多有用的性质。
          關鍵詞：我们并不想展示矩阵乘积的所有重要性, 由于本书的重点不是线性代数, 但读者应该知道矩阵乘积还有很多有用的性质
        - 摘要：现在我们已经知道了足够多的线性代数符号，可以表达下列线性方程；组：
          關鍵詞：现在我们已经知道了足够多的线性代数符号, 可以表达下列线性方程
        - 摘要：是一个已知矩阵，
          關鍵詞：是一个已知矩阵
        - 摘要：是一个已知向；其中；量，
          關鍵詞：是一个已知向, 其中
        - 摘要：或者，更明确地，写作
          關鍵詞：写作, 更明确地, 或者
        - 摘要：矩阵向量乘积符号为这种形式的方程提供了更紧凑的表示。
          關鍵詞：矩阵向量乘积符号为这种形式的方程提供了更紧凑的表示
    2.3：单位矩阵和逆矩阵
        - 摘要：线性代数提供了称为矩阵逆  （matrix  inversion）的强大工具。对于大多；数矩阵 A ，我们都能通过矩阵逆解析地求解式（2.11）。
          關鍵詞：数矩阵, 我们都能通过矩阵逆解析地求解式, 线性代数提供了称为矩阵逆, 对于大多, 的强大工具
        - 摘要：为了描述矩阵逆，我们首先需要定义单位矩阵  （identity  matrix）的概；念。任意向量和单位矩阵相乘，都不会改变。我们将保持n维向量不变；的单位矩阵记作 I n 。形式上，
          關鍵詞：我们首先需要定义单位矩阵, 我们将保持, 为了描述矩阵逆, 维向量不变, 任意向量和单位矩阵相乘
        - 摘要：，
          關鍵詞：
        - 摘要：单位矩阵的结构很简单：所有沿主对角线的元素都是1，而其他位置的；所有元素都是0，如图2.2所示。
          關鍵詞：而其他位置的, 所示, 所有元素都是, 单位矩阵的结构很简单, 如图
        - 摘要：图2.2　单位矩阵的一个样例：这是 I 3
          關鍵詞：这是, 单位矩阵的一个样例
        - 摘要：矩阵 A 的矩阵逆 记作 A −1 ，其定义的矩阵满足如下条件：
          關鍵詞：记作, 矩阵, 的矩阵逆, 其定义的矩阵满足如下条件
        - 摘要：现在我们可以通过以下步骤求解式（2.11）：
          關鍵詞：现在我们可以通过以下步骤求解式
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；当然，这取决于我们能否找到一个逆矩阵 A −1 。在接下来的章节中，我；们会讨论逆矩阵 A −1 存在的条件。
          關鍵詞：存在的条件, 们会讨论逆矩阵, 在接下来的章节中, 这取决于我们能否找到一个逆矩阵, 当然
        - 摘要：当逆矩阵 A −1 存在时，有几种不同的算法都能找到它的闭解形式。理论；上，相同的逆矩阵可用于多次求解不同向量b的方程。然而，逆矩阵  A；−1  主要是作为理论工具使用的，并不会在大多数软件应用程序中实际使
          關鍵詞：相同的逆矩阵可用于多次求解不同向量, 存在时, 逆矩阵, 然而, 理论
    2.4：线性相关和生成子空间
        - 摘要：如果逆矩阵 A −1 存在，那么式（2.11）肯定对于每一个向量 b 恰好存在；一个解。但是，对于方程组而言，对于向量 b 的某些值，有可能不存在；解，或者存在无限多个解。存在多于一个解但是少于无限多个解的情况
          關鍵詞：如果逆矩阵, 那么式, 有可能不存在, 对于向量, 但是
        - 摘要：（其中α取任意实数）也是该方程组的解。
          關鍵詞：取任意实数, 其中, 也是该方程组的解
        - 摘要：为了分析方程有多少个解，我们可以将；（origin）（元素都是零的向量）出发的不同方向，确定有多少种方法；可以到达向量 b 。在这个观点下，向量 x 中的每个元素表示我们应该沿
          關鍵詞：为了分析方程有多少个解, 我们可以将, 出发的不同方向, 确定有多少种方法, 向量
        - 摘要：的列向量看作从原点
          關鍵詞：的列向量看作从原点
        - 摘要：A
          關鍵詞：
        - 摘要：一般而言，这种操作称为线性组合  （linear  combination）。形式上，一；组向量的线性组合，是指每个向量乘以对应标量系数之后的和，即
          關鍵詞：是指每个向量乘以对应标量系数之后的和, 组向量的线性组合, 形式上, 一般而言, 这种操作称为线性组合
        - 摘要：一组向量的生成子空间 （span）是原始向量线性组合后所能抵达的点的；集合。
          關鍵詞：集合, 是原始向量线性组合后所能抵达的点的, 一组向量的生成子空间
        - 摘要：确定 Ax=b 是否有解，相当于确定向量 b 是否在 A 列向量的生成子空间；中。这个特殊的生成子空间被称为 A 的列空间 （column space）或者 A；的值域 （range）。
          關鍵詞：是否在, 确定, 列向量的生成子空间, 是否有解, 这个特殊的生成子空间被称为
        - 摘要：都存在解，我们要求  A；为了使方程  Ax=b  对于任意向量；中的某个点不在  A  的列空间
          關鍵詞：为了使方程, 都存在解, 我们要求, 对于任意向量, 的列空间
        - 摘要：的要求，意味着 A 至少有m列，即
          關鍵詞：至少有, 的要求, 意味着
        - 摘要：不等式；仅是方程对每一点都有解的必要条件。这不是一个；中的矩
          關鍵詞：仅是方程对每一点都有解的必要条件, 不等式, 这不是一个, 中的矩
        - 摘要：正式地说，这种冗余称为线性相关  （linear  dependence）。如果一组向；量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量；称为线性无关  （linearly  independent）。如果某个向量是一组向量中某
          關鍵詞：这种冗余称为线性相关, 称为线性无关, 如果某个向量是一组向量中某, 如果一组向, 那么这组向量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；要想使矩阵可逆，我们还需要保证式（2.11）对于每一个  b  值至多有一；个解。为此，我们需要确保该矩阵至多有m个列向量。否则，该方程会
          關鍵詞：个列向量, 值至多有一, 对于每一个, 否则, 我们还需要保证式
        - 摘要：综上所述，这意味着该矩阵必须是一个方阵  （square），即m=n，并且；所有列向量都是线性无关的。一个列向量线性相关的方阵被称为奇异的；（singular）。
          關鍵詞：综上所述, 一个列向量线性相关的方阵被称为奇异的, 并且, 这意味着该矩阵必须是一个方阵, 所有列向量都是线性无关的
        - 摘要：如果矩阵  A  不是一个方阵或者是一个奇异的方阵，该方程仍然可能有；解。但是我们不能使用矩阵逆去求解。
          關鍵詞：不是一个方阵或者是一个奇异的方阵, 该方程仍然可能有, 如果矩阵, 但是我们不能使用矩阵逆去求解
        - 摘要：目前为止，我们已经讨论了逆矩阵左乘。我们也可以定义逆矩阵右乘：
          關鍵詞：我们已经讨论了逆矩阵左乘, 目前为止, 我们也可以定义逆矩阵右乘
        - 摘要：对于方阵而言，它的左逆和右逆是相等的。
          關鍵詞：它的左逆和右逆是相等的, 对于方阵而言
    2.5：范数
        - 摘要：有时我们需要衡量一个向量的大小。在机器学习中，我们经常使用称为；范数 （norm）的函数来衡量向量大小。形式上，L p 范数定义如下
          關鍵詞：我们经常使用称为, 在机器学习中, 的函数来衡量向量大小, 有时我们需要衡量一个向量的大小, 形式上
        - 摘要：其中
          關鍵詞：其中
        - 摘要：。
          關鍵詞：
        - 摘要：范数（包括L  p  范数）是将向量映射到非负值的函数。直观上来说，向；量 x 的范数衡量从原点到点 x 的距离。更严格地说，范数是满足下列性；质的任意函数：
          關鍵詞：是将向量映射到非负值的函数, 的范数衡量从原点到点, 的距离, 更严格地说, 范数是满足下列性
        - 摘要：；
          關鍵詞：
        - 摘要：（三角不等式  （triangle
          關鍵詞：三角不等式
        - 摘要：inequality））；
          關鍵詞：
        - 摘要：。
          關鍵詞：
        - 摘要：当p=2时，L  2  范数称为欧几里得范数  （Euclidean  norm）。它表示从原；点出发到向量  x  确定的点的欧几里得距离。L  2  范数在机器学习中出现；得十分频繁，经常简化表示为  ，略去了下标2。平方L 2 范数也经常
          關鍵詞：范数也经常, 范数在机器学习中出现, 它表示从原, 范数称为欧几里得范数, 略去了下标
        - 摘要：计算。
          關鍵詞：计算
        - 摘要：平方L 2 范数在数学和计算上都比L 2 范数本身更方便。例如，平方L 2 范；数对  x  中每个元素的导数只取决于对应的元素，而L  2  范数对每个元素；的导数和整个向量相关。但是在很多情况下，平方L  2  范数也可能不受
          關鍵詞：的导数和整个向量相关, 例如, 中每个元素的导数只取决于对应的元素, 范数对每个元素, 但是在很多情况下
        - 摘要：当机器学习问题中零和非零元素之间的差异非常重要时，通常会使用L 1；范数。每当 x 中某个元素从0增加  ，对应的L 1 范数也会增加  。
          關鍵詞：对应的, 中某个元素从, 当机器学习问题中零和非零元素之间的差异非常重要时, 通常会使用, 范数也会增加
        - 摘要：有时候我们会统计向量中非零元素的个数来衡量向量的大小。有些作者；将这种函数称为“L  0  范数”，但是这个术语在数学意义上是不对的。向；量的非零元素的数目不是范数，因为对向量缩放α倍不会改变该向量非
          關鍵詞：有些作者, 有时候我们会统计向量中非零元素的个数来衡量向量的大小, 但是这个术语在数学意义上是不对的, 量的非零元素的数目不是范数, 范数
        - 摘要：1  范数经常作为表示非零元素数目的替代函
          關鍵詞：范数经常作为表示非零元素数目的替代函
        - 摘要：另外一个经常在机器学习中出现的范数是L  ∞  范数，也被称为最大范数；（max norm）。这个范数表示向量中具有最大幅值的元素的绝对值：
          關鍵詞：这个范数表示向量中具有最大幅值的元素的绝对值, 另外一个经常在机器学习中出现的范数是, 也被称为最大范数, 范数
        - 摘要：有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法；是使用Frobenius范数 （Frobenius norm），即
          關鍵詞：有时候我们可能也希望衡量矩阵的大小, 最常见的做法, 是使用, 在深度学习中, 范数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；其类似于向量的L 2 范数。
          關鍵詞：其类似于向量的, 范数
        - 摘要：两个向量的点积 可以用范数来表示，具体如下
          關鍵詞：两个向量的点积, 可以用范数来表示, 具体如下
        - 摘要：其中θ表示 x 和 y 之间的夹角。
          關鍵詞：表示, 之间的夹角, 其中
    2.6：特殊类型的矩阵和向量
        - 摘要：有些特殊类型的矩阵和向量是特别有用的。
          關鍵詞：有些特殊类型的矩阵和向量是特别有用的
        - 摘要：D
          關鍵詞：
        - 摘要：对角矩阵  （diagonal  matrix）只在主对角线上含有非零元素，其他位置；是对角矩阵，当且仅当对于所有的；都是零。形式上，矩阵
          關鍵詞：是对角矩阵, 都是零, 当且仅当对于所有的, 矩阵, 只在主对角线上含有非零元素
        - 摘要：并非所有的对角矩阵都是方阵。长方形的矩阵也有可能是对角矩阵。非；方阵的对角矩阵没有逆矩阵，但我们仍然可以高效地计算它们的乘法。；对于一个长方形对角矩阵  D  而言，乘法  Dx  会涉及  x  中每个元素的缩
          關鍵詞：对于一个长方形对角矩阵, 乘法, 但我们仍然可以高效地计算它们的乘法, 并非所有的对角矩阵都是方阵, 而言
        - 摘要：对称 （symmetric）矩阵是转置和自己相等的矩阵，即
          關鍵詞：对称, 矩阵是转置和自己相等的矩阵
        - 摘要：当某些不依赖参数顺序的双参数函数生成元素时，对称矩阵经常会出
          關鍵詞：当某些不依赖参数顺序的双参数函数生成元素时, 对称矩阵经常会出
        - 摘要：现。例如，如果 A 是一个距离度量矩阵， A  i,j  表示点i到点j的距离，那；么 A i,j = A j,i ，因为距离函数是对称的。
          關鍵詞：如果, 是一个距离度量矩阵, 例如, 的距离, 到点
        - 摘要：单位向量 （unit vector）是具有单位范数 （unit norm）的向量，即
          關鍵詞：的向量, 是具有单位范数, 单位向量
        - 摘要：，那么向量 x 和向量 y 互相正交 （orthogonal）。如果
          關鍵詞：如果, 互相正交, 和向量, 那么向量
        - 摘要：如果；两个向量都有非零范数，那么这两个向量之间的夹角是90◦。在；中，至多有n个范数非零向量互相正交。如果这些向量不但互相正交，
          關鍵詞：至多有, 如果, 那么这两个向量之间的夹角是, 个范数非零向量互相正交, 如果这些向量不但互相正交
        - 摘要：正交矩阵  （orthogonal  matrix）指行向量和列向量是分别标准正交的方；阵，即
          關鍵詞：指行向量和列向量是分别标准正交的方, 正交矩阵
        - 摘要：这意味着
          關鍵詞：这意味着
        - 摘要：正交矩阵受到关注是因为求逆计算代价小。我们需要注意正交矩阵的定；义。违反直觉的是，正交矩阵的行向量不仅是正交的，还是标准正交；的。对于行向量或列向量互相正交但不是标准正交的矩阵，没有对应的
          關鍵詞：正交矩阵受到关注是因为求逆计算代价小, 我们需要注意正交矩阵的定, 违反直觉的是, 还是标准正交, 没有对应的
    2.7：特征分解
        - 摘要：许多数学对象可以通过将它们分解成多个组成部分或者找到它们的一些；属性来更好地理解。这些属性是通用的，而不是由我们选择表示它们的；方式所产生的。
          關鍵詞：这些属性是通用的, 属性来更好地理解, 许多数学对象可以通过将它们分解成多个组成部分或者找到它们的一些, 方式所产生的, 而不是由我们选择表示它们的
        - 摘要：例如，整数可以分解为质因数。我们可以用十进制或二进制等不同方式；表示整数12，但是12=2×3×3永远是对的。从这个表示中我们可以获得一；些有用的信息，比如12不能被5整除，或者12的倍数可以被3整除。
          關鍵詞：整数可以分解为质因数, 但是, 例如, 从这个表示中我们可以获得一, 我们可以用十进制或二进制等不同方式
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；正如我们可以通过分解质因数来发现整数的一些内在性质，我们也可以；通过分解矩阵来发现矩阵表示成数组元素时不明显的函数性质。
          關鍵詞：我们也可以, 通过分解矩阵来发现矩阵表示成数组元素时不明显的函数性质, 正如我们可以通过分解质因数来发现整数的一些内在性质
        - 摘要：特征分解  （eigendecomposition）是使用最广的矩阵分解之一，即我们；将矩阵分解成一组特征向量和特征值。
          關鍵詞：即我们, 是使用最广的矩阵分解之一, 将矩阵分解成一组特征向量和特征值, 特征分解
        - 摘要：方阵  A  的特征向量  （eigenvector）是指与  A  相乘后相当于对该向量进；行缩放的非零向量ν：
          關鍵詞：相乘后相当于对该向量进, 方阵, 的特征向量, 是指与, 行缩放的非零向量
        - 摘要：其中标量λ称为这个特征向量对应的特征值 （eigenvalue）。（类似地，；，；我们也可以定义左特征向量  （left  eigenvector）
          關鍵詞：称为这个特征向量对应的特征值, 其中标量, 我们也可以定义左特征向量, 类似地
        - 摘要：如果
          關鍵詞：如果
        - 摘要：ν
          關鍵詞：
        - 摘要：是
          關鍵詞：
        - 摘要：的特征向量，那么任何缩放后的向量；A；也是 A 的特征向量。此外， sν 和 ν 有相同的
          關鍵詞：此外, 也是, 那么任何缩放后的向量, 的特征向量, 有相同的
        - 摘要：特征值。基于这个原因，通常我们只考虑单位特征向量。
          關鍵詞：通常我们只考虑单位特征向量, 特征值, 基于这个原因
        - 摘要：假设矩阵  A  有n个线性无关的特征向量；征值；是一个特征向量：
          關鍵詞：个线性无关的特征向量, 假设矩阵, 是一个特征向量, 征值
        - 摘要：，对应着特；。我们将特征向量连接成一个矩阵，使得每一列；。类似地，我们也可以将特
          關鍵詞：使得每一列, 我们将特征向量连接成一个矩阵, 对应着特, 我们也可以将特, 类似地
        - 摘要：我们已经看到了构建具有特定特征值和特征向量的矩阵，能够使我们在；目标方向上延伸空间。然而，我们也常常希望将矩阵分解；（decompose）成特征值和特征向量。这样可以帮助我们分析矩阵的特
          關鍵詞：成特征值和特征向量, 目标方向上延伸空间, 能够使我们在, 然而, 我们也常常希望将矩阵分解
        - 摘要：不是每一个矩阵都可以分解成特征值和特征向量。在某些情况下，特征；分解存在，但是会涉及复数而非实数。幸运的是，在本书中，我们通常；只需要分解一类有简单分解的矩阵。具体来讲，每个实对称矩阵都可以
          關鍵詞：但是会涉及复数而非实数, 具体来讲, 幸运的是, 在本书中, 我们通常
        - 摘要：其中 Q 是 A 的特征向量组成的正交矩阵， Λ 是对角矩阵。特征值Λ  i，i；对应的特征向量是矩阵 Q 的第i列，记作 Q ：，i 。因为 Q 是正交矩阵，；我们可以将 A 看作沿方向 ν (i) 延展λ i 倍的空间，如图2.3所示。
          關鍵詞：因为, 的第, 特征值, 是对角矩阵, 我们可以将
        - 摘要：图2.3　特征向量和特征值的作用效果。特征向量和特征值的作用效果的一个实例。在这里，矩；阵 A 有两个标准正交的特征向量，对应特征值为λ 1 的 ν (1) 以及对应特征值为λ 2 的 ν (2) 。；的集合，构成一个单位圆。（右）我们画出了所有
          關鍵詞：在这里, 对应特征值为, 特征向量和特征值的作用效果的一个实例, 特征向量和特征值的作用效果, 以及对应特征值为
        - 摘要：虽然任意一个实对称矩阵  A  都有特征分解，但是特征分解可能并不唯；一。如果两个或多个特征向量拥有相同的特征值，那么在由这些特征向；量产生的生成子空间中，任意一组正交向量都是该特征值对应的特征向
          關鍵詞：那么在由这些特征向, 如果两个或多个特征向量拥有相同的特征值, 量产生的生成子空间中, 但是特征分解可能并不唯, 任意一组正交向量都是该特征值对应的特征向
        - 摘要：矩阵的特征分解给了我们很多关于矩阵的有用信息。矩阵是奇异的，当；且仅当含有零特征值。实对称矩阵的特征分解也可以用于优化二次方程；。当 x 等于  A 的某个特征向
          關鍵詞：矩阵的特征分解给了我们很多关于矩阵的有用信息, 矩阵是奇异的, 实对称矩阵的特征分解也可以用于优化二次方程, 且仅当含有零特征值, 等于
        - 摘要：，其中限制
          關鍵詞：其中限制
        - 摘要：所有特征值都是正数的矩阵称为正定  （positive  definite）；所有特征值
          關鍵詞：所有特征值都是正数的矩阵称为正定, 所有特征值
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；都是非负数的矩阵称为半正定  （positive  semidefinite）。同样地，所有；特征值都是负数的矩阵称为负定 （negative definite）；所有特征值都是
          關鍵詞：所有特征值都是, 同样地, 所有, 都是非负数的矩阵称为半正定, 特征值都是负数的矩阵称为负定
        - 摘要：。
          關鍵詞：
    2.8：奇异值分解
        - 摘要：在第2.7节，我们探讨了如何将矩阵分解成特征向量和特征值。还有另；一种分解矩阵的方法，称为奇异值分解 （singular value decomposition，；vector）和奇异值
          關鍵詞：一种分解矩阵的方法, 称为奇异值分解, 和奇异值, 还有另, 在第
        - 摘要：回想一下，我们使用特征分解去分析矩阵 A 时，得到特征向量构成的矩；阵 V 和特征值构成的向量 λ ，我们可以重新将 A 写作
          關鍵詞：和特征值构成的向量, 写作, 我们可以重新将, 得到特征向量构成的矩, 回想一下
        - 摘要：奇异值分解是类似的，只不过这回我们将矩阵  A  分解成三个矩阵的乘；积：
          關鍵詞：只不过这回我们将矩阵, 奇异值分解是类似的, 分解成三个矩阵的乘
        - 摘要：假设 A 是一个m×n的矩阵，那么 U  是一个m×m的矩阵，  D  是一个m×n；的矩阵， V 是一个n×n矩阵。
          關鍵詞：的矩阵, 矩阵, 那么, 是一个, 假设
        - 摘要：这些矩阵中的每一个经定义后都拥有特殊的结构。矩阵  U  和  V  都定义；为正交矩阵，而矩阵  D  定义为对角矩阵。注意，矩阵  D  不一定是方；阵。
          關鍵詞：定义为对角矩阵, 注意, 矩阵, 不一定是方, 为正交矩阵
        - 摘要：对角矩阵 D  对角线上的元素称为矩阵  A  的奇异值  （singular  value）。；矩阵 U 的列向量称为左奇异向量 （left  singular  vector），矩阵  V  的列；向量称右奇异向量 （right singular vector）。
          關鍵詞：矩阵, 的列, 的奇异值, 对角线上的元素称为矩阵, 向量称右奇异向量
        - 摘要：事实上，我们可以用与  A  相关的特征分解去解释  A  的奇异值分解。  A；的左奇异向量 （left singular vector）是；的特征向量。 A 的右奇
          關鍵詞：我们可以用与, 事实上, 的奇异值分解, 相关的特征分解去解释, 的左奇异向量
        - 摘要：特征值的平方根，同时也是
          關鍵詞：特征值的平方根, 同时也是
        - 摘要：SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。我们将在下；一节中探讨。
          關鍵詞：最有用的一个性质可能是拓展矩阵求逆到非方矩阵上, 一节中探讨, 我们将在下
    2.9：Moore-Penrose伪逆
        - 摘要：对于非方矩阵而言，其逆矩阵没有定义。假设在下面的问题中，我们希；望通过矩阵 A 的左逆 B 来求解线性方程：
          關鍵詞：对于非方矩阵而言, 其逆矩阵没有定义, 来求解线性方程, 望通过矩阵, 的左逆
        - 摘要：等式两边左乘左逆B后，我们得到
          關鍵詞：等式两边左乘左逆, 我们得到
        - 摘要：取决于问题的形式，我们可能无法设计一个唯一的映射将  A  映射到  B；。
          關鍵詞：我们可能无法设计一个唯一的映射将, 映射到, 取决于问题的形式
        - 摘要：如果矩阵  A  的行数大于列数，那么上述方程可能没有解。如果矩阵  A；的行数小于列数，那么上述矩阵可能有多个解。
          關鍵詞：那么上述矩阵可能有多个解, 如果矩阵, 的行数小于列数, 那么上述方程可能没有解, 的行数大于列数
        - 摘要：Moore-Penrose伪逆 （Moore-Penrose pseudoinverse）使我们在这类问题；上取得了一定的进展。矩阵 A 的伪逆定义为
          關鍵詞：上取得了一定的进展, 伪逆, 矩阵, 使我们在这类问题, 的伪逆定义为
        - 摘要：计算伪逆的实际算法没有基于这个定义，而是使用下面的公式
          關鍵詞：计算伪逆的实际算法没有基于这个定义, 而是使用下面的公式
        - 摘要：其中，矩阵  U、D  和  V  是矩阵  A  奇异值分解后得到的矩阵。对角矩阵；D 的伪逆 D + 是其非零元素取倒数之后再转置得到的。
          關鍵詞：矩阵, 其中, 奇异值分解后得到的矩阵, 是其非零元素取倒数之后再转置得到的, 是矩阵
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中；是方程所有可行解中欧几里得范数
          關鍵詞：当矩阵, 的列数多于行数时, 是方程所有可行解中欧几里得范数, 使用伪逆求解线性方程是众多可能解法中
        - 摘要：最小的一个。
          關鍵詞：最小的一个
        - 摘要：当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得；到的 x 使得 Ax 和 y 的欧几里得距离
          關鍵詞：可能没有解, 的欧几里得距离, 到的, 在这种情况下, 当矩阵
        - 摘要：最小。
          關鍵詞：最小
    2.10：迹运算
        - 摘要：迹运算返回的是矩阵对角元素的和：
          關鍵詞：迹运算返回的是矩阵对角元素的和
        - 摘要：迹运算因为很多原因而有用。若不使用求和符号，有些矩阵运算很难描；述，而通过矩阵乘法和迹运算符号可以清楚地表示。例如，迹运算提供；了另一种描述矩阵Frobenius范数的方式：
          關鍵詞：若不使用求和符号, 迹运算提供, 有些矩阵运算很难描, 了另一种描述矩阵, 例如
        - 摘要：用迹运算表示表达式，我们可以使用很多有用的等式巧妙地处理表达；式。例如，迹运算在转置运算下是不变的：
          關鍵詞：迹运算在转置运算下是不变的, 用迹运算表示表达式, 例如, 我们可以使用很多有用的等式巧妙地处理表达
        - 摘要：多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面；之后相乘的迹是相同的。当然，我们需要考虑挪动之后矩阵乘积依然定；义良好：
          關鍵詞：之后相乘的迹是相同的, 义良好, 我们需要考虑挪动之后矩阵乘积依然定, 当然, 多个矩阵相乘得到的方阵的迹
        - 摘要：或者更一般地，
          關鍵詞：或者更一般地
        - 摘要：即使循环置换后矩阵乘积得到的矩阵形状变了，迹运算的结果依然不
          關鍵詞：迹运算的结果依然不, 即使循环置换后矩阵乘积得到的矩阵形状变了
        - 摘要：变。例如，假设矩阵；们可以得到
          關鍵詞：们可以得到, 假设矩阵, 例如
        - 摘要：，矩阵
          關鍵詞：矩阵
        - 摘要：，我
          關鍵詞：
        - 摘要：尽管
          關鍵詞：尽管
        - 摘要：。
          關鍵詞：
        - 摘要：另一个有用的事实是标量在迹运算后仍然是它自己：a=Tr(a)。
          關鍵詞：另一个有用的事实是标量在迹运算后仍然是它自己
    2.11：行列式
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；2.12　实例：主成分分析
          關鍵詞：主成分分析, 实例
        - 摘要：行列式，记作det(  A  )，是一个将方阵  A  映射到实数的函数。行列式等；于矩阵特征值的乘积。行列式的绝对值可以用来衡量矩阵参与矩阵乘法；后空间扩大或者缩小了多少。如果行列式是0，那么空间至少沿着某一
          關鍵詞：行列式, 于矩阵特征值的乘积, 记作, 后空间扩大或者缩小了多少, 映射到实数的函数
    2.12：实例：主成分分析
        - 摘要：主成分分析 （principal components analysis，PC A ）是一个简单的机器；学习算法，可以通过基础的线性代数知识推导。
          關鍵詞：主成分分析, 可以通过基础的线性代数知识推导, 学习算法, 是一个简单的机器
        - 摘要：空间中有m个点
          關鍵詞：个点, 空间中有
        - 摘要：，我们希望对这些点；假设在；进行有损压缩。有损压缩表示我们使用更少的内存，但损失一些精度去
          關鍵詞：假设在, 但损失一些精度去, 我们希望对这些点, 进行有损压缩, 有损压缩表示我们使用更少的内存
        - 摘要：，会有；编码这些点的一种方式是用低维表示。对于每个点；。如果l比n小，那么我们便使用了更
          關鍵詞：如果, 对于每个点, 会有, 那么我们便使用了更, 编码这些点的一种方式是用低维表示
        - 摘要：PCA由我们选择的解码函数而定。具体来讲，为了简化解码器，我们使；是；用矩阵乘法将编码映射回
          關鍵詞：为了简化解码器, 由我们选择的解码函数而定, 具体来讲, 用矩阵乘法将编码映射回, 我们使
        - 摘要：，即g (c )=Dc ，其中
          關鍵詞：其中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；到目前为止，所描述的问题可能会有多个解。因为如果我们按比例地缩；小所有点对应的编码向量c i ，那么只需按比例放大 D :,i ，即可保持结果
          關鍵詞：小所有点对应的编码向量, 即可保持结果, 那么只需按比例放大, 因为如果我们按比例地缩, 所描述的问题可能会有多个解
        - 摘要：计算这个解码器的最优编码可能是一个困难的问题。为了使编码问题简；单一些，PCA限制  D  的列向量彼此正交（注意，除非l=n，否则严格意；义上 D 不是一个正交矩阵）。
          關鍵詞：除非, 义上, 不是一个正交矩阵, 的列向量彼此正交, 注意
        - 摘要：为了将这个基本想法变为我们能够实现的算法，首先我们需要明确如何；根据每一个输入 x 得到一个最优编码c ∗ 。一种方法是最小化原始输入向；量  x  和重构向量g(c  ∗  )之间的距离。我们使用范数来衡量它们之间的距
          關鍵詞：得到一个最优编码, 和重构向量, 我们使用范数来衡量它们之间的距, 一种方法是最小化原始输入向, 为了将这个基本想法变为我们能够实现的算法
        - 摘要：我们可以用平方L 2 范数替代L 2 范数，因为两者在相同的值c上取得最小；值。这是因为L  2  范数是非负的，并且平方运算在非负值上是单调递增；的。
          關鍵詞：这是因为, 我们可以用平方, 上取得最小, 范数是非负的, 并且平方运算在非负值上是单调递增
        - 摘要：该最小化函数可以简化成
          關鍵詞：该最小化函数可以简化成
        - 摘要：（式（2.30）中L 2 范数的定义）
          關鍵詞：范数的定义
        - 摘要：（分配律）
          關鍵詞：分配律
        - 摘要：（因为标量
          關鍵詞：因为标量
        - 摘要：的转置等于自己）。
          關鍵詞：的转置等于自己
        - 摘要：因为第一项
          關鍵詞：因为第一项
        - 摘要：不依赖于c，所以我们可以忽略它，得到如下的优化
          關鍵詞：不依赖于, 所以我们可以忽略它, 得到如下的优化
        - 摘要：目标：
          關鍵詞：目标
        - 摘要：更进一步，代入g(c)的定义：
          關鍵詞：的定义, 更进一步, 代入
        - 摘要：（矩阵 D 的正交性和单位范数约束）
          關鍵詞：矩阵, 的正交性和单位范数约束
        - 摘要：我们可以通过向量微积分来求解这个最优化问题（如果你不清楚怎么；做，请参考第4.3节）。
          關鍵詞：请参考第, 我们可以通过向量微积分来求解这个最优化问题, 如果你不清楚怎么
        - 摘要：这使得算法很高效：最优编码  x  只需要一个矩阵-向量乘法操作。为了；编码向量，我们使用编码函数
          關鍵詞：最优编码, 只需要一个矩阵, 为了, 编码向量, 向量乘法操作
        - 摘要：进一步使用矩阵乘法，我们也可以定义PCA重构操作：
          關鍵詞：重构操作, 进一步使用矩阵乘法, 我们也可以定义
        - 摘要：接下来，我们需要挑选编码矩阵  D  。要做到这一点，先来回顾最小化；输入和重构之间L  2  距离的这个想法。因为用相同的矩阵  D  对所有点进；行解码，我们不能再孤立地看待每个点。反之，我们必须最小化所有维
          關鍵詞：接下来, 先来回顾最小化, 因为用相同的矩阵, 反之, 对所有点进
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；为了推导用于寻求  D  ∗  的算法，我们首先考虑l=1的情况。在这种情况；下， D 是一个单一向量 d 。将式（2.67）代入式（2.68），简化 D 为 d
          關鍵詞：在这种情况, 简化, 的情况, 的算法, 为了推导用于寻求
        - 摘要：上述公式是直接代入得到的，但不是表述上最美观的方式。在上述公式；中，我们将标量；放在向量  d  的右边。将该标量放在左边的写
          關鍵詞：我们将标量, 将该标量放在左边的写, 上述公式是直接代入得到的, 但不是表述上最美观的方式, 在上述公式
        - 摘要：或者，考虑到标量的转置和自身相等，我们也可以写作
          關鍵詞：我们也可以写作, 或者, 考虑到标量的转置和自身相等
        - 摘要：读者应该对这些重排写法慢慢熟悉起来。
          關鍵詞：读者应该对这些重排写法慢慢熟悉起来
        - 摘要：此时，使用单一矩阵来重述问题，比将问题写成求和形式更有帮助。这；有助于我们使用更紧凑的符号。将表示各点的向量堆叠成一个矩阵，记；为
          關鍵詞：将表示各点的向量堆叠成一个矩阵, 有助于我们使用更紧凑的符号, 比将问题写成求和形式更有帮助, 此时, 使用单一矩阵来重述问题
        - 摘要：。原问题可以重新表述为
          關鍵詞：原问题可以重新表述为
        - 摘要：，其中
          關鍵詞：其中
        - 摘要：暂时不考虑约束，我们可以将Frobenius范数简化成下面的形式：
          關鍵詞：范数简化成下面的形式, 暂时不考虑约束, 我们可以将
        - 摘要：（式（2.49））
          關鍵詞：
        - 摘要：（因为与d无关的项不影响arg min）
          關鍵詞：因为与, 无关的项不影响
        - 摘要：（因为循环改变迹运算中相乘矩阵的顺序不影响结果，如式（2.52）所；示）
          關鍵詞：因为循环改变迹运算中相乘矩阵的顺序不影响结果, 如式
        - 摘要：（再次使用上述性质）。
          關鍵詞：再次使用上述性质
        - 摘要：此时，我们再来考虑约束条件
          關鍵詞：我们再来考虑约束条件, 此时
        - 摘要：（因为约束条件）
          關鍵詞：因为约束条件
        - 摘要：这个优化问题可以通过特征分解来求解。具体来讲，最优的
          關鍵詞：具体来讲, 这个优化问题可以通过特征分解来求解, 最优的
        - 摘要：d  是
          關鍵詞：
        - 摘要：最大特征值对应的特征向量。
          關鍵詞：最大特征值对应的特征向量
        - 摘要：以上推导特定于l=1的情况，仅得到了第一个主成分。更一般地，当我；们希望得到主成分的基时，矩阵  D  由前l个最大的特征值对应的特征向；量组成。这个结论可以通过归纳法证明，我们建议将此证明作为练习。
          關鍵詞：当我, 仅得到了第一个主成分, 以上推导特定于, 们希望得到主成分的基时, 个最大的特征值对应的特征向
        - 摘要：线性代数是理解深度学习所必须掌握的基础数学学科之一。另一门在机
          關鍵詞：线性代数是理解深度学习所必须掌握的基础数学学科之一, 另一门在机
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；器学习中无处不在的重要数学学科是概率论，我们将在下一章探讨。
          關鍵詞：器学习中无处不在的重要数学学科是概率论, 我们将在下一章探讨
第3章：概率与信息论
    2.12：实例：主成分分析
        - 摘要：本章讨论概率论和信息论。
          關鍵詞：本章讨论概率论和信息论
        - 摘要：概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定；性的方法，也提供了用于导出新的不确定性声明 （statement）的公理。；在人工智能领域，概率论主要有两种用途：首先，概率法则告诉我们AI
          關鍵詞：在人工智能领域, 也提供了用于导出新的不确定性声明, 的公理, 概率论主要有两种用途, 性的方法
        - 摘要：概率论是众多科学学科和工程学科的基本工具。之所以讲述这章的内；容，是为了确保那些背景偏软件工程而较少接触概率论的读者也可以理；解本书的内容。
          關鍵詞：概率论是众多科学学科和工程学科的基本工具, 是为了确保那些背景偏软件工程而较少接触概率论的读者也可以理, 之所以讲述这章的内, 解本书的内容
        - 摘要：概率论使我们能够提出不确定的声明以及在不确定性存在的情况下进行；推理，而信息论使我们能够量化概率分布中的不确定性总量。
          關鍵詞：推理, 而信息论使我们能够量化概率分布中的不确定性总量, 概率论使我们能够提出不确定的声明以及在不确定性存在的情况下进行
        - 摘要：如果你已经对概率论和信息论很熟悉了，那么除了第3.14节，本章其余；内容你都可以跳过。而在第3.14节中，我们会介绍用来描述机器学习中；结构化概率模型的图。即使你对这些主题没有任何的先验知识，本章对
          關鍵詞：我们会介绍用来描述机器学习中, 内容你都可以跳过, 本章其余, 而在第, 节中
    3.1：为什么要使用概率
        - 摘要：计算机科学的许多分支处理的实体大部分都是完全确定且必然的。程序；员通常可以安全地假定CPU将完美地执行每条机器指令。虽然硬件错误；确实会发生，但它们非常罕见，以至于大部分软件应用在设计时并不需
          關鍵詞：虽然硬件错误, 员通常可以安全地假定, 计算机科学的许多分支处理的实体大部分都是完全确定且必然的, 但它们非常罕见, 程序
        - 摘要：这是因为机器学习通常必须处理不确定量，有时也可能需要处理随机
          關鍵詞：有时也可能需要处理随机, 这是因为机器学习通常必须处理不确定量
        - 摘要：（非确定性的）量。不确定性和随机性可能来自多个方面。至少从20世；纪80年代开始，研究人员就对使用概率论来量化不确定性提出了令人信；服的论据。这里给出的许多论据都是根据Pearl（1988）的工作总结或启
          關鍵詞：的工作总结或启, 这里给出的许多论据都是根据, 非确定性的, 年代开始, 不确定性和随机性可能来自多个方面
        - 摘要：几乎所有活动都需要一些在不确定性存在的情况下进行推理的能力。事；实上，除了那些被定义为真的数学声明，我们很难认定某个命题是千真；万确的或者确保某件事一定会发生。
          關鍵詞：几乎所有活动都需要一些在不确定性存在的情况下进行推理的能力, 除了那些被定义为真的数学声明, 实上, 万确的或者确保某件事一定会发生, 我们很难认定某个命题是千真
        - 摘要：不确定性有3种可能的来源：
          關鍵詞：不确定性有, 种可能的来源
        - 摘要：（1）被建模系统内在的随机性。例如，大多数量子力学的解释，都将；亚原子粒子的动力学描述为概率的。我们还可以创建一些假设具有随机；动态的理论情境，例如一个假想的纸牌游戏，在这个游戏中，我们假设
          關鍵詞：大多数量子力学的解释, 动态的理论情境, 亚原子粒子的动力学描述为概率的, 例如一个假想的纸牌游戏, 例如
        - 摘要：（2）不完全观测。即使是确定的系统，当我们不能观测到所有驱动系；统行为的变量时，该系统也会呈现随机性。例如，在Monty；中，一个游戏节目的参与者被要求在3个门之间选择，并且会赢得放置
          關鍵詞：统行为的变量时, 该系统也会呈现随机性, 即使是确定的系统, 不完全观测, 当我们不能观测到所有驱动系
        - 摘要：Hall问题
          關鍵詞：问题
        - 摘要：（3）不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，；舍弃的信息会导致模型的预测出现不确定性。例如，假设我们制作了一；个机器人，它可以准确地观察周围每一个对象的位置。在对这些对象将
          關鍵詞：它可以准确地观察周围每一个对象的位置, 舍弃的信息会导致模型的预测出现不确定性, 例如, 个机器人, 当我们使用一些必须舍弃某些观测信息的模型时
        - 摘要：在很多情况下，使用一些简单而不确定的规则要比复杂而确定的规则更；为实用，即使真正的规则是确定的并且我们建模的系统可以足够精确地；容纳复杂的规则。例如，“多数鸟儿都会飞”这个简单的规则描述起来很
          關鍵詞：这个简单的规则描述起来很, 容纳复杂的规则, 为实用, 例如, 即使真正的规则是确定的并且我们建模的系统可以足够精确地
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；即使经过这么多的努力，这个规则还是很脆弱而且容易失效。
          關鍵詞：即使经过这么多的努力, 这个规则还是很脆弱而且容易失效
        - 摘要：尽管我们的确需要一种用以对不确定性进行表示和推理的方法，但是概；率论并不能明显地提供我们在人工智能领域需要的所有工具。概率论最；初的发展是为了分析事件发生的频率。我们可以很容易地看出概率论，
          關鍵詞：我们可以很容易地看出概率论, 但是概, 概率论最, 率论并不能明显地提供我们在人工智能领域需要的所有工具, 初的发展是为了分析事件发生的频率
        - 摘要：关于不确定性的常识推理，如果我们已经列出了若干条期望它具有的性；质，那么满足这些性质的唯一一种方法就是将贝叶斯概率和频率派概率；视为等同的。例如，如果我们要在扑克牌游戏中根据玩家手上的牌计算
          關鍵詞：关于不确定性的常识推理, 如果我们已经列出了若干条期望它具有的性, 那么满足这些性质的唯一一种方法就是将贝叶斯概率和频率派概率, 例如, 视为等同的
        - 摘要：概率可以被看作用于处理不确定性的逻辑扩展。逻辑提供了一套形式化；的规则，可以在给定某些命题是真或假的假设下，判断另外一些命题是；真的还是假的。概率论提供了一套形式化的规则，可以在给定一些命题
          關鍵詞：概率可以被看作用于处理不确定性的逻辑扩展, 的规则, 判断另外一些命题是, 可以在给定一些命题, 概率论提供了一套形式化的规则
    3.2：随机变量
        - 摘要：随机变量  （random  variable）是可以随机地取不同值的变量。我们通常；用无格式字体（plain  typeface）中的小写字母来表示随机变量本身，而；用手写体中的小写字母来表示随机变量能够取到的值。例如，x  1  和x  2
          關鍵詞：中的小写字母来表示随机变量本身, 是可以随机地取不同值的变量, 用无格式字体, 例如, 我们通常
        - 摘要：都是随机变量x可能的取值。对于向量值变量，我们会将随机变量写成x；，它的一个可能取值为 x  。就其本身而言，一个随机变量只是对可能的；状态的描述；它必须伴随着一个概率分布来指定每个状态的可能性。
          關鍵詞：就其本身而言, 我们会将随机变量写成, 都是随机变量, 对于向量值变量, 一个随机变量只是对可能的
        - 摘要：随机变量可以是离散的或者连续的。离散随机变量拥有有限或者可数无；限多的状态。注意：这些状态不一定非要是整数，它们也可能只是一些；被命名的状态而没有数值。连续随机变量伴随着实数值。
          關鍵詞：这些状态不一定非要是整数, 随机变量可以是离散的或者连续的, 注意, 被命名的状态而没有数值, 它们也可能只是一些
    3.3：概率分布
        - 摘要：3.3.1　离散型变量和概率质量函数
          關鍵詞：离散型变量和概率质量函数
        - 摘要：3.3.2　连续型变量和概率密度函数
          關鍵詞：连续型变量和概率密度函数
        - 摘要：概率分布  （probability  distribution）用来描述随机变量或一簇随机变量；在每一个可能取到的状态的可能性大小。我们描述概率分布的方式取决；于随机变量是离散的还是连续的。
          關鍵詞：于随机变量是离散的还是连续的, 在每一个可能取到的状态的可能性大小, 用来描述随机变量或一簇随机变量, 概率分布, 我们描述概率分布的方式取决
        - 摘要：3.3.1　离散型变量和概率质量函数
          關鍵詞：离散型变量和概率质量函数
        - 摘要：离散型变量的概率分布可以用概率质量函数；mass；function，PMF）
          關鍵詞：离散型变量的概率分布可以用概率质量函数
        - 摘要：（probability
          關鍵詞：
        - 摘要：概率质量函数将随机变量能够取得的每个状态映射到随机变量取得该状；态的概率。x=x的概率用P(x)来表示，概率为1表示x=x是确定的，概率；为0表示x=x是不可能发生的。有时为了使得PMF的使用不相互混淆，我
          關鍵詞：表示, 的概率用, 有时为了使得, 是不可能发生的, 态的概率
        - 摘要：概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布；被称为联合概率分布  （joint  probability  distribution）。P(x=x,y=y)表示；x=x和y=y同时发生的概率。我们也可以简写为P(x,y)。
          關鍵詞：表示, 这种多个变量的概率分布, 我们也可以简写为, 被称为联合概率分布, 概率质量函数可以同时作用于多个随机变量
        - 摘要：如果一个函数P是随机变量x的PMF，必须满足下面这几个条件：
          關鍵詞：必须满足下面这几个条件, 如果一个函数, 是随机变量
        - 摘要：P的定义域必须是x所有可能状态的集合。
          關鍵詞：的定义域必须是, 所有可能状态的集合
        - 摘要：。不可能发生的事件概率为0，并且
          關鍵詞：并且, 不可能发生的事件概率为
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；不存在比这概率更低的状态。类似地，能够确保一定发生的事件概；率为1，而且不存在比这概率更高的状态。
          關鍵詞：率为, 不存在比这概率更低的状态, 能够确保一定发生的事件概, 而且不存在比这概率更高的状态, 类似地
        - 摘要：。我们把这条性质称之为归一化的
          關鍵詞：我们把这条性质称之为归一化的
        - 摘要：（normalized）。如果没有这条性质，当我们计算很多事件其中之；一发生的概率时，可能会得到大于1的概率。
          關鍵詞：如果没有这条性质, 一发生的概率时, 当我们计算很多事件其中之, 可能会得到大于, 的概率
        - 摘要：例如，考虑一个离散型随机变量x有k个不同的状态。我们可以假设x是；均匀分布  （uniform  distribution）的（也就是将它的每个状态视为等可；能的），通过将它的PMF设为
          關鍵詞：也就是将它的每个状态视为等可, 均匀分布, 个不同的状态, 考虑一个离散型随机变量, 能的
        - 摘要：对于所有的i都成立。我们可以看出这满足上述成为概率质量函数的条
          關鍵詞：对于所有的, 都成立, 我们可以看出这满足上述成为概率质量函数的条
        - 摘要：件。因为k是一个正整数，所以  是正的。我们也可以看出
          關鍵詞：因为, 是一个正整数, 是正的, 所以, 我们也可以看出
        - 摘要：因此分布也满足归一化条件。
          關鍵詞：因此分布也满足归一化条件
        - 摘要：3.3.2　连续型变量和概率密度函数
          關鍵詞：连续型变量和概率密度函数
        - 摘要：当研究的对象是连续型随机变量时，我们用概率密度函数  （probability；density  function，P D  F）而不是概率质量函数来描述它的概率分布。如；果一个函数p是概率密度函数，必须满足下面这几个条件：
          關鍵詞：当研究的对象是连续型随机变量时, 必须满足下面这几个条件, 我们用概率密度函数, 而不是概率质量函数来描述它的概率分布, 是概率密度函数
        - 摘要：p的定义域必须是x所有可能状态的集合。
          關鍵詞：的定义域必须是, 所有可能状态的集合
        - 摘要：。注意，我们并不要求
          關鍵詞：注意, 我们并不要求
        - 摘要：。
          關鍵詞：
        - 摘要：。
          關鍵詞：
        - 摘要：概率密度函数P(x)并没有直接对特定的状态给出概率，相对的，它给出；了落在面积为δx的无限小的区域内的概率为P(x)δx。
          關鍵詞：了落在面积为, 它给出, 的无限小的区域内的概率为, 并没有直接对特定的状态给出概率, 相对的
        - 摘要：我们可以对概率密度函数求积分来获得点集的真实概率质量。特别是，
          關鍵詞：我们可以对概率密度函数求积分来获得点集的真实概率质量, 特别是
        - 摘要：x落在集合   中的概率可以通过P(x)对这个集合求积分来得到。在单变；量的例子中，x落在区间［a，b］的概率是
          關鍵詞：落在区间, 落在集合, 中的概率可以通过, 的概率是, 量的例子中
        - 摘要：。
          關鍵詞：
        - 摘要：为了给出一个连续型随机变量的PDF的例子，我们可以考虑实数区间上；的均匀分布。我们可以使用函数u(x;a,b)，其中a和b是区间的端点且满足；b＞a。符号“；”表示“以什么为参数”；我们把x作为函数的自变量，a和b
          關鍵詞：表示, 我们可以考虑实数区间上, 符号, 其中, 是区间的端点且满足
        - 摘要：。在［a，b］内，有
          關鍵詞：
        - 摘要：。可以看出，任何一点都非负。另
          關鍵詞：可以看出, 任何一点都非负
        - 摘要：外，它的积分为1。我们通常用x∼U(a,b)表示x在［a，b］上是均匀分布；的。
          關鍵詞：表示, 它的积分为, 我们通常用, 上是均匀分布
        - 摘要：3.3.1　离散型变量和概率；质量函数
          關鍵詞：离散型变量和概率, 质量函数
        - 摘要：3.3.2　连续型变量和概率；密度函数
          關鍵詞：连续型变量和概率, 密度函数
    3.4：边缘概率
        - 摘要：有时，我们知道了一组变量的联合概率分布，但想要了解其中一个子集；的概率分布。这种定义在子集上的概率分布被称为边缘概率分布；（marginal probability distribution）。
          關鍵詞：的概率分布, 有时, 但想要了解其中一个子集, 这种定义在子集上的概率分布被称为边缘概率分布, 我们知道了一组变量的联合概率分布
        - 摘要：例如，假设有离散型随机变量x和y，并且我们知道P(x,y)。可以依据下；面的求和法则 （sum rule）来计算P(x)：
          關鍵詞：可以依据下, 并且我们知道, 来计算, 例如, 假设有离散型随机变量
        - 摘要：“边缘概率”的名称来源于手算边缘概率的计算过程。当P(x,y)的每个值；被写在由每行表示不同的x值、每列表示不同的y值形成的网格中时，对；网格中的每行求和是很自然的事情，然后将求和的结果P(x)写在每行右
          關鍵詞：的每个值, 值形成的网格中时, 然后将求和的结果, 边缘概率, 每列表示不同的
        - 摘要：对于连续型变量，我们需要用积分替代求和：
          關鍵詞：对于连续型变量, 我们需要用积分替代求和
    3.5：条件概率
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在很多情况下，我们感兴趣的是某个事件在给定其他事件发生时出现的；概率。这种概率叫作条件概率。我们将给定x=x，y=y发生的条件概率记
          關鍵詞：发生的条件概率记, 这种概率叫作条件概率, 概率, 在很多情况下, 我们感兴趣的是某个事件在给定其他事件发生时出现的
        - 摘要：条件概率只在P(x=x)＞0时有定义。我们不能计算给定在永远不会发生；的事件上的条件概率。
          關鍵詞：的事件上的条件概率, 时有定义, 我们不能计算给定在永远不会发生, 条件概率只在
        - 摘要：这里需要注意的是，不要把条件概率和计算当采用某个动作后会发生什；么相混淆。假定某个人说德语，那么他是德国人的条件概率是非常高；的，但是如果随机选择的一个人会说德语，他的国籍不会因此而改变。
          關鍵詞：但是如果随机选择的一个人会说德语, 他的国籍不会因此而改变, 那么他是德国人的条件概率是非常高, 这里需要注意的是, 假定某个人说德语
    3.6：条件概率的链式法则
        - 摘要：任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件；概率相乘的形式：
          關鍵詞：概率相乘的形式, 任何多维随机变量的联合概率分布, 都可以分解成只有一个变量的条件
        - 摘要：这个规则被称为概率的链式法则  （chain  rule）或者乘法法则  （product；rule）。它可以直接从式（3.5）条件概率的定义中得到。例如，使用两；次定义可以得到
          關鍵詞：使用两, 它可以直接从式, 例如, 这个规则被称为概率的链式法则, 或者乘法法则
    3.7：独立性和条件独立性
        - 摘要：两个随机变量x和y，如果它们的概率分布可以表示成两个因子的乘积形
          關鍵詞：两个随机变量, 如果它们的概率分布可以表示成两个因子的乘积形
        - 摘要：式，并且一个因子只包含x，另一个因子只包含y，我们就称这两个随机；变量是相互独立的 （independent）：
          關鍵詞：我们就称这两个随机, 变量是相互独立的, 并且一个因子只包含, 另一个因子只包含
        - 摘要：如果关于x和y的条件概率分布对于z的每一个值都可以写成乘积的形；式，那么这两个随机变量x和y在给定随机变量z时是条件独立的；（conditionally independent）：
          關鍵詞：的条件概率分布对于, 时是条件独立的, 那么这两个随机变量, 的每一个值都可以写成乘积的形, 如果关于
        - 摘要：我们可以采用一种简化形式来表示独立性和条件独立性：x⊥y表示x和y；相互独立，x⊥y｜z表示x和y在给定z时条件独立。
          關鍵詞：表示, 时条件独立, 我们可以采用一种简化形式来表示独立性和条件独立性, 在给定, 相互独立
    3.8：期望、方差和协方差
        - 摘要：函数f(  x  )关于某分布P(x)的期望  （expectation）或者期望值  （expected；value）是指，当x由P产生，f作用于x时，f( x )的平均值。对于离散型随；机变量，这可以通过求和得到
          關鍵詞：的平均值, 对于离散型随, 函数, 是指, 产生
        - 摘要：对于连续型随机变量，可以通过求积分得到
          關鍵詞：对于连续型随机变量, 可以通过求积分得到
        - 摘要：当概率分布在上下文中指明时，我们可以只写出期望作用的随机变量的；名称来进行简化，例如；。如果期望作用的随机变量也很明
          關鍵詞：当概率分布在上下文中指明时, 如果期望作用的随机变量也很明, 例如, 名称来进行简化, 我们可以只写出期望作用的随机变量的
        - 摘要：。默认地，我们假设
          關鍵詞：我们假设, 默认地
        - 摘要：期望是线性的，例如，
          關鍵詞：例如, 期望是线性的
        - 摘要：其中α和β不依赖于x。
          關鍵詞：不依赖于, 其中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；方差 （variance）衡量的是当我们对x依据它的概率分布进行采样时，随；机变量x的函数值会呈现多大的差异：
          關鍵詞：衡量的是当我们对, 依据它的概率分布进行采样时, 方差, 机变量, 的函数值会呈现多大的差异
        - 摘要：当方差很小时，f( x  )的值形成的簇比较接近它们的期望值。方差的平方；根被称为标准差 （standard deviation）。
          關鍵詞：当方差很小时, 方差的平方, 根被称为标准差, 的值形成的簇比较接近它们的期望值
        - 摘要：协方差  （covariance）在某种意义上给出了两个变量线性相关性的强度；以及这些变量的尺度：
          關鍵詞：以及这些变量的尺度, 协方差, 在某种意义上给出了两个变量线性相关性的强度
        - 摘要：协方差的绝对值如果很大，则意味着变量值变化很大，并且它们同时距；离各自的均值很远。如果协方差是正的，那么两个变量都倾向于同时取；得相对较大的值。如果协方差是负的，那么其中一个变量倾向于取得相
          關鍵詞：并且它们同时距, 则意味着变量值变化很大, 协方差的绝对值如果很大, 那么两个变量都倾向于同时取, 得相对较大的值
        - 摘要：协方差和相关性是有联系的，但实际上是不同的概念。它们是有联系；的：如果两个变量相互独立，那么它们的协方差为零；如果两个变量的；协方差不为零，那么它们一定是相关的。然而，独立性又是和协方差完
          關鍵詞：它们是有联系, 如果两个变量相互独立, 如果两个变量的, 协方差不为零, 那么它们的协方差为零
        - 摘要：机变量s进行采样。s以   的概率值为1，否则为−1。我们可以通过令
          關鍵詞：进行采样, 否则为, 我们可以通过令, 机变量, 的概率值为
        - 摘要：y=sx来生成一个随机变量y。显然，x和y不是相互独立的，因为x完全决；定了y的尺度。然而，Cov(x,y)=0。
          關鍵詞：显然, 因为, 的尺度, 不是相互独立的, 定了
        - 摘要：随机向量；矩阵，并且满足
          關鍵詞：并且满足, 矩阵, 随机向量
        - 摘要：的协方差矩阵 （covariance matrix）是一个n×n的
          關鍵詞：是一个, 的协方差矩阵
        - 摘要：协方差矩阵的对角元是方差：
          關鍵詞：协方差矩阵的对角元是方差
    3.9：常用概率分布
        - 摘要：3.9.1　Bernoulli分布
          關鍵詞：分布
        - 摘要：3.9.2　Multinoulli分布
          關鍵詞：分布
        - 摘要：3.9.3　高斯分布
          關鍵詞：高斯分布
        - 摘要：3.9.4　指数分布和Laplace分布
          關鍵詞：分布, 指数分布和
        - 摘要：3.9.5　Dirac分布和经验分布
          關鍵詞：分布和经验分布
        - 摘要：3.9.6　分布的混合
          關鍵詞：分布的混合
        - 摘要：许多简单的概率分布在机器学习的众多领域中都是有用的。
          關鍵詞：许多简单的概率分布在机器学习的众多领域中都是有用的
        - 摘要：3.9.1　Bernoulli分布
          關鍵詞：分布
        - 摘要：Bernoulli分布 （Bernoulli  distribution）是单个二值随机变量的分布。它；由单个参数φ∈［0，1］控制，φ给出了随机变量等于1的概率。它具有；如下的一些性质。
          關鍵詞：它具有, 如下的一些性质, 控制, 是单个二值随机变量的分布, 分布
        - 摘要：3.9.2　Multinoulli分布
          關鍵詞：分布
        - 摘要：Multinoulli分布  （multinoulli  distribution）或者范畴分布  （categorical；distribution）是指在具有k个不同状态的单个离散型随机变量上的分布，；其中k是一个有限值。 (2) Multinoulli分布由向量p∈［0，1］ k−1 参数化，
          關鍵詞：参数化, 或者范畴分布, 是一个有限值, 其中, 个不同状态的单个离散型随机变量上的分布
        - 摘要：给出。注意我们必须限制
          關鍵詞：注意我们必须限制, 给出
        - 摘要：Bernoulli分布和Multinoulli分布足够用来描述在它们领域内的任意分；布。它们能够描述这些分布，不是因为它们特别强大，而是因为它们的；领域很简单。它们可以对那些能够将所有的状态进行枚举的离散型随机
          關鍵詞：分布足够用来描述在它们领域内的任意分, 不是因为它们特别强大, 领域很简单, 而是因为它们的, 它们可以对那些能够将所有的状态进行枚举的离散型随机
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；变量进行建模。当处理的是连续型随机变量时，会有不可数无限多的状；态，所以任何通过少量参数描述的概率分布都必须在分布上加以严格的
          關鍵詞：变量进行建模, 会有不可数无限多的状, 所以任何通过少量参数描述的概率分布都必须在分布上加以严格的, 当处理的是连续型随机变量时
        - 摘要：3.9.3　高斯分布
          關鍵詞：高斯分布
        - 摘要：实数上最常用的分布就是正态分布 （normal distribution），也称为高斯；分布 （Gaussian dis-tribution）：
          關鍵詞：也称为高斯, 分布, 实数上最常用的分布就是正态分布
        - 摘要：图3.1画出了正态分布的概率密度函数。
          關鍵詞：画出了正态分布的概率密度函数
        - 摘要：图3.1　正态分布。正态分布；由µ给出，峰的宽度受σ控制。在这个示例中，我们展示的是 标准正态分布 （standard normal；distribution），其中µ=0，σ=1
          關鍵詞：正态分布, 其中, 控制, 我们展示的是, 峰的宽度受
        - 摘要：呈现经典的“钟形曲线”的形状，其中中心峰的x坐标
          關鍵詞：的形状, 坐标, 呈现经典的, 其中中心峰的, 钟形曲线
        - 摘要：正态分布由两个参数控制，；心峰值的坐标，这也是分布的均值：；示，方差用σ 2 表示。
          關鍵詞：表示, 正态分布由两个参数控制, 方差用, 这也是分布的均值, 心峰值的坐标
        - 摘要：。参数µ给出了中；。分布的标准差用σ表
          關鍵詞：分布的标准差用, 参数, 给出了中
        - 摘要：当我们要对概率密度函数求值时，需要对σ平方并且取倒数。当我们需；要经常对不同参数下的概率密度函数求值时，一种更高效的参数化分布；的方式是使用参数β∈(0,∞)来控制分布的精度  （precision）（或方差的
          關鍵詞：当我们需, 来控制分布的精度, 的方式是使用参数, 一种更高效的参数化分布, 当我们要对概率密度函数求值时
        - 摘要：采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于；某个实数上分布的先验知识而不知道该选择怎样的形式时，正态分布是；默认的比较好的选择，其中有两个原因。
          關鍵詞：正态分布是, 当我们由于缺乏关于, 采用正态分布在很多应用中都是一个明智的选择, 其中有两个原因, 默认的比较好的选择
        - 摘要：第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。中；心极限定理  （central  limit  theorem）说明很多独立随机变量的和近似服；从正态分布。这意味着在实际中，很多复杂系统都可以被成功地建模成
          關鍵詞：这意味着在实际中, 说明很多独立随机变量的和近似服, 第一, 心极限定理, 很多复杂系统都可以被成功地建模成
        - 摘要：第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具；有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验；知识量最少的分布。充分利用和证明这个想法需要更多的数学工具，我
          關鍵詞：充分利用和证明这个想法需要更多的数学工具, 我们可以认为正态分布是对模型加入的先验, 在具有相同方差的所有可能的概率分布中, 有最大的不确定性, 因此
        - 摘要：空间，这种情况下被称为多维正态分布；正态分布可以推广到；（multivariate normal dis-tribution）。它的参数是一个正定对称矩阵Σ：
          關鍵詞：正态分布可以推广到, 它的参数是一个正定对称矩阵, 空间, 这种情况下被称为多维正态分布
        - 摘要：参数µ仍然表示分布的均值，只不过现在是向量值。参数Σ给出了分布的；协方差矩阵。和单变量的情况类似，当我们希望对很多不同参数下的概；率密度函数多次求值时，协方差矩阵并不是一个很高效的参数化分布的
          關鍵詞：只不过现在是向量值, 参数, 和单变量的情况类似, 给出了分布的, 仍然表示分布的均值
        - 摘要：我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是各向同；性 （isotropic）高斯分布，它的协方差矩阵是一个标量乘以单位阵。
          關鍵詞：它的协方差矩阵是一个标量乘以单位阵, 我们常常把协方差矩阵固定成一个对角阵, 高斯分布, 一个更简单的版本是各向同
        - 摘要：3.9.4　指数分布和Laplace分布
          關鍵詞：分布, 指数分布和
        - 摘要：在深度学习中，我们经常会需要一个在x=0点处取得边界点（sharp
          關鍵詞：我们经常会需要一个在, 在深度学习中, 点处取得边界点
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；point）的分布。为了实现这一目的，我们可以使用指数分布；（exponential distribution）：
          關鍵詞：的分布, 为了实现这一目的, 我们可以使用指数分布
        - 摘要：指数分布用指示函数（indicator  function）；概率为零。
          關鍵詞：概率为零, 指数分布用指示函数
        - 摘要：来使得当x取负值时的
          關鍵詞：取负值时的, 来使得当
        - 摘要：一个联系紧密的概率分布是Laplace分布  （Laplace  distribution），它允；许我们在任意一点µ处设置概率质量的峰值：
          關鍵詞：处设置概率质量的峰值, 许我们在任意一点, 一个联系紧密的概率分布是, 它允, 分布
        - 摘要：3.9.5　Dirac分布和经验分布
          關鍵詞：分布和经验分布
        - 摘要：在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这；可以通过Dirac  delta函数 （Dirac  delta  function）δ(x)定义概率密度函数；来实现：
          關鍵詞：定义概率密度函数, 函数, 我们希望概率分布中的所有质量都集中在一个点上, 来实现, 可以通过
        - 摘要：Dirac  delta函数被定义成在除了0以外的所有点的值都为0，但是积分为；1。Dirac  delta函数不像普通函数一样对x的每一个值都有一个实数值的；输出，它是一种不同类型的数学对象，被称为广义函数  （generalized
          關鍵詞：以外的所有点的值都为, 被称为广义函数, 函数不像普通函数一样对, 但是积分为, 的每一个值都有一个实数值的
        - 摘要：通过把P(x)定义成δ函数左移−µ个单位，我们得到了一个在x=µ处具有无；限窄也无限高的峰值的概率质量。
          關鍵詞：我们得到了一个在, 定义成, 处具有无, 通过把, 函数左移
        - 摘要：Dirac分布经常作为经验分布 （empirical distribution）的一个组成部分出；现：
          關鍵詞：分布经常作为经验分布, 的一个组成部分出
        - 摘要：经验分布将概率密度   赋给m个点
          關鍵詞：经验分布将概率密度, 个点, 赋给
        - 摘要：中的每一个，这
          關鍵詞：中的每一个
        - 摘要：些点是给定的数据集或者采样的集合。只有在定义连续型随机变量的经；验分布时，Dirac  delta函数才是必要的。对于离散型随机变量，情况更；加简单：经验分布可以被定义成一个Multinoulli分布，对于每一个可能
          關鍵詞：对于离散型随机变量, 加简单, 只有在定义连续型随机变量的经, 经验分布可以被定义成一个, 函数才是必要的
        - 摘要：当我们在训练集上训练模型时，可以认为从这个训练集上得到的经验分；布指明了采样来源的分布。关于经验分布另外一种重要的观点是，它是；训练数据的似然最大的那个概率密度函数（见第5.5节）。
          關鍵詞：可以认为从这个训练集上得到的经验分, 关于经验分布另外一种重要的观点是, 当我们在训练集上训练模型时, 训练数据的似然最大的那个概率密度函数, 见第
        - 摘要：3.9.6　分布的混合
          關鍵詞：分布的混合
        - 摘要：通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种；通用的组合方法是构造混合分布  （mixture  distribution）。混合分布由；一些组件（component）分布构成。每次实验，样本是由哪个组件分布
          關鍵詞：一些组件, 分布构成, 通用的组合方法是构造混合分布, 混合分布由, 样本是由哪个组件分布
        - 摘要：这里P(c)是对各组件的一个Multinoulli分布。
          關鍵詞：这里, 分布, 是对各组件的一个
        - 摘要：我们已经看过一个混合分布的例子了：实值变量的经验分布对于每一个；训练实例来说，就是以Dirac分布为组件的混合分布。
          關鍵詞：训练实例来说, 就是以, 实值变量的经验分布对于每一个, 我们已经看过一个混合分布的例子了, 分布为组件的混合分布
        - 摘要：混合模型是组合简单概率分布来生成更丰富的分布的一种简单策略。在；第16章中，我们更加详细地探讨从简单概率分布构建复杂模型的技术。
          關鍵詞：章中, 我们更加详细地探讨从简单概率分布构建复杂模型的技术, 混合模型是组合简单概率分布来生成更丰富的分布的一种简单策略
        - 摘要：混合模型使我们能够一瞥以后会用到的一个非常重要的概念——潜变量；（latent  variable）。潜变量是我们不能直接观测到的随机变量。混合模；型的组件标识变量c就是其中一个例子。潜变量在联合分布中可能和x有
          關鍵詞：潜变量, 混合模型使我们能够一瞥以后会用到的一个非常重要的概念, 型的组件标识变量, 就是其中一个例子, 潜变量在联合分布中可能和
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；一个非常强大且常见的混合模型是高斯混合模型  （Gaussian  Mixture；Model），它的组件p(x｜c=i)是高斯分布。每个组件都有各自的参数，
          關鍵詞：是高斯分布, 一个非常强大且常见的混合模型是高斯混合模型, 每个组件都有各自的参数, 它的组件
        - 摘要：除了均值和协方差以外，高斯混合模型的参数指明了给每个组件i的先；验概率  （prior  probability）α  i  =P(c=i)。“先验”一词表明了在观测到x之；前传递给模型关于c的信念。作为对比，P(c｜x)是后验概率  （posterior
          關鍵詞：一词表明了在观测到, 前传递给模型关于, 是后验概率, 验概率, 的信念
        - 摘要：图3.2展示了某个高斯混合模型生成的样本。
          關鍵詞：展示了某个高斯混合模型生成的样本
        - 摘要：图3.2　来自高斯混合模型的样本。在这个示例中，有3个组件。从左到右，第1个组件具有各向
          關鍵詞：来自高斯混合模型的样本, 个组件, 个组件具有各向, 在这个示例中, 从左到右
        - 摘要：同性的协方差矩阵，这意味着它在每个方向上具有相同的方差。第2个组件具有对角的协方差矩；阵，这意味着它可以沿着每个轴的对齐方向单独控制方差。该示例中，沿着x 2 轴的方差要比沿；着x 1 轴的方差大。第3个组件具有满秩的协方差矩阵，使它能够沿着任意基的方向单独地控制
          關鍵詞：同性的协方差矩阵, 个组件具有对角的协方差矩, 轴的方差要比沿, 使它能够沿着任意基的方向单独地控制, 沿着
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；3.10　常用函数的有用性质
          關鍵詞：常用函数的有用性质
        - 摘要：某些函数在处理概率分布时经常会出现，尤其是深度学习的模型中用到；的概率分布。
          關鍵詞：的概率分布, 某些函数在处理概率分布时经常会出现, 尤其是深度学习的模型中用到
        - 摘要：其中一个函数是logistic sigmoid函数：
          關鍵詞：函数, 其中一个函数是
        - 摘要：logistic  sigmoid函数通常用来产生Bernoulli分布中的参数φ，因为它的范；围是（0,1），处在φ的有效取值范围内。图3.3给出了sigmoid函数的图；示。sigmoid函数在变量取绝对值非常大的正值或负值时会出现饱和
          關鍵詞：分布中的参数, 函数通常用来产生, 函数的图, 因为它的范, 围是
        - 摘要：图3.3　logistic sigmoid函数
          關鍵詞：函数
        - 摘要：另外一个经常遇到的函数是softplus函数 （softplus function）（Dugas et；al. ，2001）：
          關鍵詞：函数, 另外一个经常遇到的函数是
        - 摘要：softplus函数可以用来产生正态分布的β和σ参数，因为它的范围是；（0,∞）。当处理包含sigmoid函数的表达式时，它也经常出现。softplus；函数名来源于它是另外一个函数的平滑（或“软化”）形式，这个函数是
          關鍵詞：形式, 参数, 当处理包含, 因为它的范围是, 函数可以用来产生正态分布的
        - 摘要：图3.4给出了softplus函数的图示。
          關鍵詞：函数的图示, 给出了
        - 摘要：下面一些性质非常有用，你可能要记下来。
          關鍵詞：你可能要记下来, 下面一些性质非常有用
        - 摘要：图3.4　softplus函数
          關鍵詞：函数
        - 摘要：函数σ −1 (x)在统计学中被称为分对数 （logit），但这个函数在机器学习；中很少用到。
          關鍵詞：函数, 中很少用到, 但这个函数在机器学习, 在统计学中被称为分对数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；+
          關鍵詞：
        - 摘要：式（3.41）为函数名“softplus”提供了其他的正当理由。softplus函数被设；计成正部函数 （positive part function）的平滑版本，这个正部函数是指；x
          關鍵詞：的平滑版本, 函数被设, 计成正部函数, 提供了其他的正当理由, 这个正部函数是指
        - 摘要：3.9.1　Bernoulli分布
          關鍵詞：分布
        - 摘要：3.9.2　Multinoulli分布
          關鍵詞：分布
        - 摘要：3.9.3　高斯分布
          關鍵詞：高斯分布
        - 摘要：3.9.4　指数分布和Laplace；分布
          關鍵詞：分布, 指数分布和
        - 摘要：3.9.5　Dirac分布和经验；分布
          關鍵詞：分布和经验, 分布
        - 摘要：3.9.6　分布的混合
          關鍵詞：分布的混合
    3.11：贝叶斯规则
        - 摘要：我们经常会需要在已知P(y｜x)时计算P(x｜y)。幸运的是，如果还知道；P(x)，我们可以用贝叶斯规则 （Bayes' rule）来实现这一目的：
          關鍵詞：我们经常会需要在已知, 我们可以用贝叶斯规则, 幸运的是, 来实现这一目的, 时计算
        - 摘要：注意到P(y)出现在上面的公式中，它通常使用
          關鍵詞：注意到, 它通常使用, 出现在上面的公式中
        - 摘要：来计算，所以我们并不需要事先知道
          關鍵詞：来计算, 所以我们并不需要事先知道
        - 摘要：P(y)的信息。
          關鍵詞：的信息
        - 摘要：贝叶斯规则可以从条件概率的定义直接推导得出，但我们最好记住这个；公式的名字，因为很多文献通过名字来引用这个公式。这个公式是以牧；师Thomas  Bayes的名字来命名的，他是第一个发现这个公式特例的人。
          關鍵詞：贝叶斯规则可以从条件概率的定义直接推导得出, 这个公式是以牧, 但我们最好记住这个, 的名字来命名的, 因为很多文献通过名字来引用这个公式
    3.12：连续型变量的技术细节
        - 摘要：连续型随机变量和概率密度函数的深入理解需要用到数学分支测度论；（measure；畴，但我们可以简要介绍一些测度论用来解决的问题。
          關鍵詞：但我们可以简要介绍一些测度论用来解决的问题, 连续型随机变量和概率密度函数的深入理解需要用到数学分支测度论
        - 摘要：theory）的相关内容来扩展概率论。测度论超出了本书的范
          關鍵詞：测度论超出了本书的范, 的相关内容来扩展概率论
        - 摘要：在第3.3.2节中，我们已经看到连续型向量值随机变量x  落在某个集合；中的概率是通过p( x )对集合  积分得到的。对于集合  的一些选择可；使得
          關鍵詞：的一些选择可, 对集合, 积分得到的, 对于集合, 节中
        - 摘要：并且
          關鍵詞：并且
        - 摘要：和
          關鍵詞：
        - 摘要：的。这些集合通常是大量使用了实数的无限精度来构造的，例如通过构；造分形形状（fractal-shaped）的集合或者是通过有理数相关集合的变换；定义的集合。 (3) 测度论的一个重要贡献就是提供了一些集合的特征，使
          關鍵詞：的集合或者是通过有理数相关集合的变换, 测度论的一个重要贡献就是提供了一些集合的特征, 定义的集合, 造分形形状, 这些集合通常是大量使用了实数的无限精度来构造的
        - 摘要：对于我们的目的，测度论更多的是用来描述那些适用于；上的大多；数点，却不适用于一些边界情况的定理。测度论提供了一种严格的方式
          關鍵詞：却不适用于一些边界情况的定理, 对于我们的目的, 数点, 测度论提供了一种严格的方式, 测度论更多的是用来描述那些适用于
        - 摘要：另外一个有用的测度论中的术语是“几乎处处  （almost  everywhere）”。；某个性质如果是几乎处处都成立的，那么它在整个空间中除了一个测度；为零的集合以外都是成立的。因为这些例外只在空间中占有极其微小的
          關鍵詞：那么它在整个空间中除了一个测度, 几乎处处, 另外一个有用的测度论中的术语是, 为零的集合以外都是成立的, 某个性质如果是几乎处处都成立的
        - 摘要：连续型随机变量的另一技术细节涉及处理那种相互之间有确定性函数关；系的连续型变量。假设有两个随机变量x 和y 满足 y =g( x )，其中g是可；逆的、连续可微的函数。可能有人会想
          關鍵詞：是可, 系的连续型变量, 满足, 连续型随机变量的另一技术细节涉及处理那种相互之间有确定性函数关, 假设有两个随机变量
        - 摘要：举一个简单的例子，假设有两个标量值随机变量x和y，并且满足
          關鍵詞：并且满足, 假设有两个标量值随机变量, 举一个简单的例子
        - 摘要：以及x∼U(0,1)。如果我们使用p  y  (y)=p  x  (2y)，那么p  y  除
          關鍵詞：那么, 如果我们使用, 以及
        - 摘要：了区间
          關鍵詞：了区间
        - 摘要：以外都为0，并且在这个区间上的值为1。这意味着
          關鍵詞：并且在这个区间上的值为, 这意味着, 以外都为
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；而这违背了概率密度的定义（积分为1）。这个常见错误之所以错，是；因为它没有考虑到引入函数g后造成的空间变形。回忆一下，  x  落在无
          關鍵詞：回忆一下, 而这违背了概率密度的定义, 这个常见错误之所以错, 因为它没有考虑到引入函数, 积分为
        - 摘要：为了看出如何改正这个问题，我们回到标量值的情况。我们需要保持下；面这个性质：
          關鍵詞：面这个性质, 我们回到标量值的情况, 为了看出如何改正这个问题, 我们需要保持下
        - 摘要：求解上式，我们得到
          關鍵詞：我们得到, 求解上式
        - 摘要：或者等价地，
          關鍵詞：或者等价地
        - 摘要：在高维空间中，微分运算扩展为Jacobian矩阵  （Jacobian  matrix）的行
          關鍵詞：矩阵, 在高维空间中, 的行, 微分运算扩展为
        - 摘要：列式——矩阵的每个元素为
          關鍵詞：列式, 矩阵的每个元素为
        - 摘要：。因此，对于实值向量  x
          關鍵詞：因此, 对于实值向量
        - 摘要：和 y ，
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；3.13　信息论
          關鍵詞：信息论
    3.13：信息论
        - 摘要：信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多；少进行量化。它最初被发明是用来研究在一个含有噪声的信道上用离散；的字母表来发送消息，例如通过无线电传输来通信。在这种情况下，信
          關鍵詞：少进行量化, 主要研究的是对一个信号包含信息的多, 例如通过无线电传输来通信, 在这种情况下, 信息论是应用数学的一个分支
        - 摘要：的解释不再适用。信息论是电子工程和计算机科学中许多领域的基础。；在本书中，我们主要使用信息论的一些关键思想来描述概率分布或者量；化概率分布之间的相似性。有关信息论的更多细节，参见Cover
          關鍵詞：参见, 的解释不再适用, 信息论是电子工程和计算机科学中许多领域的基础, 我们主要使用信息论的一些关键思想来描述概率分布或者量, 在本书中
        - 摘要：信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可；能的事件发生，能提供更多的信息。消息说：“今天早上太阳升起”，信；息量是如此之少，以至于没有必要发送；但一条消息说：“今天早上有
          關鍵詞：息量是如此之少, 能的事件发生, 今天早上太阳升起, 能提供更多的信息, 要比一个非常可
        - 摘要：我们想要通过这种基本想法来量化信息。特别是：
          關鍵詞：我们想要通过这种基本想法来量化信息, 特别是
        - 摘要：非常可能发生的事件信息量要比较少，并且极端情况下，确保能够；发生的事件应该没有信息量。；较不可能发生的事件具有更高的信息量。
          關鍵詞：并且极端情况下, 确保能够, 非常可能发生的事件信息量要比较少, 较不可能发生的事件具有更高的信息量, 发生的事件应该没有信息量
        - 摘要：为了满足上述3个性质，我们定义一个事件x=x  的自信息  （self-；information）为
          關鍵詞：为了满足上述, 我们定义一个事件, 个性质, 的自信息
        - 摘要：在本书中，我们总是用log来表示自然对数，其底数为e。因此我们定义
          關鍵詞：其底数为, 在本书中, 我们总是用, 来表示自然对数, 因此我们定义
        - 摘要：的I(x)单位是奈特  （nats）。一奈特是以   的概率观测到一个事件时获
          關鍵詞：一奈特是以, 单位是奈特, 的概率观测到一个事件时获
        - 摘要：得的信息量。其他的材料中使用底数为2的对数，单位是比特  （bit）或；者香农  （shannons）；通过比特度量的信息只是通过奈特度量信息的常；数倍。
          關鍵詞：得的信息量, 的对数, 数倍, 通过比特度量的信息只是通过奈特度量信息的常, 其他的材料中使用底数为
        - 摘要：当x是连续的，我们使用类似的关于信息的定义，但有些来源于离散形；式的性质就丢失了。例如，一个具有单位密度的事件信息量仍然为0，；但是不能保证它一定发生。
          關鍵詞：一个具有单位密度的事件信息量仍然为, 我们使用类似的关于信息的定义, 但有些来源于离散形, 是连续的, 例如
        - 摘要：自信息只处理单个的输出。我们可以用香农熵 （Shannon entropy）来对；整个概率分布中的不确定性总量进行量化：
          關鍵詞：我们可以用香农熵, 整个概率分布中的不确定性总量进行量化, 来对, 自信息只处理单个的输出
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；也记作H(P)。换言之，一个分布的香农熵是指遵循这个分布的事件所产；生的期望信息总量。它给出了对依据概率分布P生成的符号进行编码所
          關鍵詞：生成的符号进行编码所, 换言之, 一个分布的香农熵是指遵循这个分布的事件所产, 也记作, 生的期望信息总量
        - 摘要：图3.5　二值随机变量的香农熵。该图说明了更接近确定性的分布是如何具有较低的香农熵，而；更接近均匀分布的分布是如何具有较高的香农熵。水平轴是p，表示二值随机变量等于1的概；率。熵由(p−1)log(1−p)−p log p给出。当p接近0时，分布几乎是确定的，因为随机变量几乎总是
          關鍵詞：熵由, 水平轴是, 因为随机变量几乎总是, 表示二值随机变量等于, 分布几乎是确定的
        - 摘要：如果对于同一个随机变量x有两个单独的概率分布P(x)和Q(x)，可以使用；KL散度  （Kullback-Leibler（KL）divergence）来衡量这两个分布的差；异：
          關鍵詞：如果对于同一个随机变量, 可以使用, 散度, 来衡量这两个分布的差, 有两个单独的概率分布
        - 摘要：在离散型变量的情况下，KL散度衡量的是，当我们使用一种被设计成；能够使得概率分布Q产生的消息的长度最小的编码，发送包含由概率分；布P产生的符号的消息时，所需要的额外信息量（如果我们使用底数为2
          關鍵詞：能够使得概率分布, 当我们使用一种被设计成, 所需要的额外信息量, 发送包含由概率分, 在离散型变量的情况下
        - 摘要：。为了说明每种选择的效果，我们
          關鍵詞：为了说明每种选择的效果, 我们
        - 摘要：或最小化
          關鍵詞：或最小化
        - 摘要：图3.6　KL散度是不对称的。假设我们有一个分布P(x)，并且希望用另一个分布q(x)来近似它。；我们可以选择最小化；令p是两个高斯分布的混合，令q为单个高斯分布。选择使用KL散度的哪个方向是取决于问题
          關鍵詞：为单个高斯分布, 来近似它, 散度的哪个方向是取决于问题, 散度是不对称的, 是两个高斯分布的混合
        - 摘要：KL散度有很多有用的性质，最重要的是，它是非负的。KL散度为0，当；且仅当P和Q在离散型变量的情况下是相同的分布，或者在连续型变量；的情况下是“几乎处处”相同的。因为KL散度是非负的并且衡量的是两
          關鍵詞：因为, 散度有很多有用的性质, 散度是非负的并且衡量的是两, 散度为, 几乎处处
        - 摘要：。这种非对称性意味着选择；影响很大。更多细节可以看图3.6。
          關鍵詞：影响很大, 这种非对称性意味着选择, 更多细节可以看图
        - 摘要：还是
          關鍵詞：还是
        - 摘要：一个和KL散度密切联系的量是交叉熵
          關鍵詞：一个和, 散度密切联系的量是交叉熵
        - 摘要：（cross-entropy），即；，它和KL散度很像，但是缺
          關鍵詞：散度很像, 但是缺, 它和
        - 摘要：少左边一项：
          關鍵詞：少左边一项
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；针对Q最小化交叉熵等价于最小化KL散度，因为Q并不参与被省略的那；一项。
          關鍵詞：因为, 针对, 散度, 一项, 最小化交叉熵等价于最小化
        - 摘要：当我们计算这些量时，经常会遇到0  log  0这个表达式。按照惯例，在信；息论中，我们将这个表达式处理为lim x→0 x log x=0。
          關鍵詞：息论中, 当我们计算这些量时, 这个表达式, 在信, 我们将这个表达式处理为
    3.14：结构化概率模型
        - 摘要：机器学习的算法经常会涉及在非常多的随机变量上的概率分布。通常，；这些概率分布涉及的直接相互作用都是介于非常少的变量之间的。使用；单个函数来描述整个联合概率分布是非常低效的（无论是计算上还是统
          關鍵詞：通常, 机器学习的算法经常会涉及在非常多的随机变量上的概率分布, 无论是计算上还是统, 这些概率分布涉及的直接相互作用都是介于非常少的变量之间的, 单个函数来描述整个联合概率分布是非常低效的
        - 摘要：我们可以把概率分布分解成许多因子的乘积形式，而不是使用单一的函；数来表示概率分布。例如，假设我们有3个随机变量a、b和c，并且a影；响b的取值，b影响c的取值，但是a和c在给定b时是条件独立的。我们可
          關鍵詞：时是条件独立的, 的取值, 而不是使用单一的函, 影响, 但是
        - 摘要：这种分解可以极大地减少用来描述一个分布的参数数量。每个因子使用；的参数数目是其变量数目的指数倍。这意味着，如果我们能够找到一种；使每个因子分布具有更少变量的分解方法，就能极大地降低表示联合分
          關鍵詞：如果我们能够找到一种, 每个因子使用, 的参数数目是其变量数目的指数倍, 这种分解可以极大地减少用来描述一个分布的参数数量, 使每个因子分布具有更少变量的分解方法
        - 摘要：可以用图来描述这种分解。这里我们使用的是图论中的“图”的概念：由；一些可以通过边互相连接的顶点的集合构成。当用图来表示这种概率分；布的分解时，我们把它称为结构化概率模型  （structured  probabilistic
          關鍵詞：布的分解时, 可以用图来描述这种分解, 当用图来表示这种概率分, 我们把它称为结构化概率模型, 这里我们使用的是图论中的
        - 摘要：有两种主要的结构化概率模型：有向的和无向的。两种图模型都使用图；，其中图的每个节点对应着一个随机变量，连接两个随机变量的边意
          關鍵詞：两种图模型都使用图, 连接两个随机变量的边意, 有两种主要的结构化概率模型, 有向的和无向的, 其中图的每个节点对应着一个随机变量
        - 摘要：味着概率分布可以表示成这两个随机变量之间的直接作用。
          關鍵詞：味着概率分布可以表示成这两个随机变量之间的直接作用
        - 摘要：有向  （directed）模型使用带有有向边的图，它们用条件概率分布来表
          關鍵詞：有向, 模型使用带有有向边的图, 它们用条件概率分布来表
        - 摘要：示分解，就像上面的例子。特别地，有向模型对于分布中的每一个随机；变量x i 都包含着一个影响因子，这个组成x i 条件概率的影响因子被称为；x i 的父节点，记为
          關鍵詞：有向模型对于分布中的每一个随机, 记为, 这个组成, 特别地, 就像上面的例子
        - 摘要：。
          關鍵詞：
        - 摘要：图3.7给出了一个有向图的例子以及它表示的概率分布的分解。
          關鍵詞：给出了一个有向图的例子以及它表示的概率分布的分解
        - 摘要：图3.7　关于随机变量a、b、c、d和e的有向图模型。这幅图对应的概率分布可以分解为
          關鍵詞：的有向图模型, 关于随机变量, 这幅图对应的概率分布可以分解为
        - 摘要：该图模型使我们能够快速看出此分布的一些性质。例如，a和c直接相互；影响，但a和e只有通过c间接相互影响
          關鍵詞：直接相互, 影响, 例如, 该图模型使我们能够快速看出此分布的一些性质, 间接相互影响
        - 摘要：无向  （undirected）模型使用带有无向边的图，它们将分解表示成一组；函数：不像有向模型那样，这些函数通常不是任何类型的概率分布。；中任何满足两两之间有边连接的顶点的集合被称为团。无向模型中的每
          關鍵詞：无向模型中的每, 模型使用带有无向边的图, 函数, 这些函数通常不是任何类型的概率分布, 中任何满足两两之间有边连接的顶点的集合被称为团
        - 摘要：都伴随着一个因子
          關鍵詞：都伴随着一个因子
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；中那样要求因子的和或者积分为1。
          關鍵詞：中那样要求因子的和或者积分为
        - 摘要：随机变量的联合概率与所有这些因子的乘积成比例  （proportional）；——这意味着因子的值越大，则可能性越大。当然，不能保证这种乘积；的求和为1。所以我们需要除以一个归一化常数Z来得到归一化的概率分
          關鍵詞：所以我们需要除以一个归一化常数, 的求和为, 不能保证这种乘积, 随机变量的联合概率与所有这些因子的乘积成比例, 则可能性越大
        - 摘要：图3.8给出了一个无向图的例子以及它表示的概率分布的分解。
          關鍵詞：给出了一个无向图的例子以及它表示的概率分布的分解
        - 摘要：图3.8　关于随机变量a、b、c、d和e的无向图模型。这幅图对应的概率分布可以分解为
          關鍵詞：关于随机变量, 这幅图对应的概率分布可以分解为, 的无向图模型
        - 摘要：该图模型使我们能够快速看出此分布的一些性质。例如，a和c直接相互；影响，但a和e只有通过c间接相互影响
          關鍵詞：直接相互, 影响, 例如, 该图模型使我们能够快速看出此分布的一些性质, 间接相互影响
        - 摘要：请记住，这些图模型表示的分解仅仅是描述概率分布的一种语言。它们
          關鍵詞：这些图模型表示的分解仅仅是描述概率分布的一种语言, 请记住, 它们
        - 摘要：不是互相排斥的概率分布族。有向或者无向不是概率分布的特性；它是；概率分布的一种特殊描述  （description）所具有的特性，而任何概率分；布都可以用这两种方式进行描述。
          關鍵詞：布都可以用这两种方式进行描述, 概率分布的一种特殊描述, 有向或者无向不是概率分布的特性, 所具有的特性, 而任何概率分
        - 摘要：在本书第1部分和第2部分中，我们仅仅将结构化概率模型视作一门语；言，来描述不同的机器学习算法选择表示的直接的概率关系。在讨论研；究课题之前，读者不需要更深入地理解结构化概率模型。在第3部分的
          關鍵詞：在讨论研, 部分和第, 究课题之前, 在本书第, 我们仅仅将结构化概率模型视作一门语
        - 摘要：本章复习了概率论中与深度学习最为相关的一些基本概念。我们还剩下；一些基本的数学工具需要讨论：数值计算。
          關鍵詞：一些基本的数学工具需要讨论, 我们还剩下, 本章复习了概率论中与深度学习最为相关的一些基本概念, 数值计算
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)  译者注：国内有些教材也将PMF翻译成概率分布律。
          關鍵詞：翻译成概率分布律, 译者注, 国内有些教材也将
        - 摘要：“multinoulli”这个术语是最近被Gustavo
          關鍵詞：这个术语是最近被
        - 摘要：(2)；Lacerdo发明、被Murphy（2012）推广的。；Multinoulli分布是 多项式分布 （multinomial distribution）的一个特例。多项式分布是{0，···，n}
          關鍵詞：推广的, 的一个特例, 分布是, 多项式分布, 发明
        - 摘要：(3)；Banach-Tarski定理给出了这类集合的一个有趣的例子。译者注：我们这里把“the  set  of；rational  numbers”翻译成“有理数相关集合”，理解为“一些有理数组成的集合”，如果直接用后面
          關鍵詞：翻译成, 定理给出了这类集合的一个有趣的例子, 有理数相关集合, 译者注, 理解为
第4章：数值计算
    3.14：结构化概率模型
        - 摘要：机器学习算法通常需要大量的数值计算。这通常是指通过迭代过程更新；解的估计值来解决数学问题的算法，而不是通过解析过程推导出公式来；提供正确解的方法。常见的操作包括优化（找到最小化或最大化函数值
          關鍵詞：找到最小化或最大化函数值, 机器学习算法通常需要大量的数值计算, 提供正确解的方法, 解的估计值来解决数学问题的算法, 常见的操作包括优化
    4.1：上溢和下溢
        - 摘要：连续数学在数字计算机上的根本困难是，我们需要通过有限数量的位模
          關鍵詞：我们需要通过有限数量的位模, 连续数学在数字计算机上的根本困难是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；式来表示无限多的实数。这意味着我们在计算机中表示实数时，几乎总；会引入一些近似误差。在许多情况下，这仅仅是舍入误差。舍入误差会
          關鍵詞：会引入一些近似误差, 在许多情况下, 式来表示无限多的实数, 几乎总, 这仅仅是舍入误差
        - 摘要：一种极具毁灭性的舍入误差是下溢  （underflow）。当接近零的数被四；舍五入为零时发生下溢。许多函数在其参数为零而不是一个很小的正数；时才会表现出质的不同。例如，我们通常要避免被零除（一些软件环境
          關鍵詞：我们通常要避免被零除, 例如, 许多函数在其参数为零而不是一个很小的正数, 当接近零的数被四, 一些软件环境
        - 摘要：另一个极具破坏力的数值错误形式是上溢  （overflow）。当大量级的数；被近似为∞或−∞时发生上溢。进一步的运算通常会导致这些无限值变为；非数字。
          關鍵詞：进一步的运算通常会导致这些无限值变为, 当大量级的数, 非数字, 时发生上溢, 另一个极具破坏力的数值错误形式是上溢
        - 摘要：必须对上溢和下溢进行数值稳定的一个例子是softmax函数  （softmax；function）。softmax函数经常用于预测与Multinoulli分布相关联的概率，；定义为
          關鍵詞：分布相关联的概率, 必须对上溢和下溢进行数值稳定的一个例子是, 定义为, 函数, 函数经常用于预测与
        - 摘要：考虑一下当所有x  i  都等于某个常数c时会发生什么。从理论分析上说，
          關鍵詞：从理论分析上说, 都等于某个常数, 时会发生什么, 考虑一下当所有
        - 摘要：我们可以发现所有的输出都应该为   。从数值计算上说，当c量级很大
          關鍵詞：量级很大, 从数值计算上说, 我们可以发现所有的输出都应该为
        - 摘要：时，这可能不会发生。如果c是很小的负数，exp(c)就会下溢。这意味着；softmax函数的分母会变成0，所以最后的结果是未定义的。当c是非常大；的正数时，exp(c)的上溢再次导致整个表达式未定义。这两个困难能通
          關鍵詞：如果, 这两个困难能通, 函数的分母会变成, 就会下溢, 的上溢再次导致整个表达式未定义
        - 摘要：i  导致exp的最大参数为0，这排除了上溢的可能性。同样
          關鍵詞：这排除了上溢的可能性, 同样, 的最大参数为, 导致
        - 摘要：i  x
          關鍵詞：
        - 摘要：还有一个小问题。分子中的下溢仍可以导致整体表达式被计算为零。这；意味着，如果我们在计算log  softmax(  x  )时，先计算softmax再把结果传；给log函数，会错误地得到−∞。相反，我们必须实现一个单独的函数，
          關鍵詞：会错误地得到, 如果我们在计算, 函数, 还有一个小问题, 再把结果传
        - 摘要：在大多数情况下，我们没有明确地对本书描述的各种算法所涉及的数值；考虑进行详细说明。在实现深度学习算法时，底层库的开发者应该牢记；数值问题。本书的大多数读者可以简单地依赖保证数值稳定的底层库。
          關鍵詞：在大多数情况下, 在实现深度学习算法时, 底层库的开发者应该牢记, 我们没有明确地对本书描述的各种算法所涉及的数值, 本书的大多数读者可以简单地依赖保证数值稳定的底层库
    4.2：病态条件
        - 摘要：条件数指的是函数相对于输入的微小变化而变化的快慢程度。输入被轻；微扰动而迅速改变的函数对于科学计算来说可能是有问题的，因为输入；中的舍入误差可能导致输出的巨大变化。
          關鍵詞：因为输入, 中的舍入误差可能导致输出的巨大变化, 条件数指的是函数相对于输入的微小变化而变化的快慢程度, 输入被轻, 微扰动而迅速改变的函数对于科学计算来说可能是有问题的
        - 摘要：考虑函数；其条件数为
          關鍵詞：考虑函数, 其条件数为
        - 摘要：。当
          關鍵詞：
        - 摘要：具有特征值分解时，
          關鍵詞：具有特征值分解时
        - 摘要：这是最大和最小特征值的模之比 (1) 。当该数很大时，矩阵求逆对输入的；误差特别敏感。
          關鍵詞：这是最大和最小特征值的模之比, 误差特别敏感, 当该数很大时, 矩阵求逆对输入的
        - 摘要：这种敏感性是矩阵本身的固有特性，而不是矩阵求逆期间舍入误差的结；果。即使我们乘以完全正确的矩阵逆，病态条件的矩阵也会放大预先存；在的误差。在实践中，该错误将与求逆过程本身的数值误差进一步复
          關鍵詞：病态条件的矩阵也会放大预先存, 即使我们乘以完全正确的矩阵逆, 而不是矩阵求逆期间舍入误差的结, 该错误将与求逆过程本身的数值误差进一步复, 在的误差
    4.3：基于梯度的优化方法
        - 摘要：4.3.1　梯度之上：Jacobian和Hessian矩阵
          關鍵詞：矩阵, 梯度之上
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；大多数深度学习算法都涉及某种形式的优化。优化指的是改变 x 以最小；化或最大化某个函数f( x )的任务。我们通常以最小化f( x )指代大多数最
          關鍵詞：的任务, 以最小, 化或最大化某个函数, 优化指的是改变, 大多数深度学习算法都涉及某种形式的优化
        - 摘要：我们把要最小化或最大化的函数称为目标函数  （objective  function）或；准则  （criterion）。当我们对其进行最小化时，也把它称为代价函数；（cost
          關鍵詞：当我们对其进行最小化时, 准则, 我们把要最小化或最大化的函数称为目标函数, 也把它称为代价函数
        - 摘要：function）、损失函数  （loss
          關鍵詞：损失函数
        - 摘要：我们通常使用一个上标∗表示最小化或最大化函数的 x 值，如记 x ∗ =arg；min f( x )。
          關鍵詞：我们通常使用一个上标, 如记, 表示最小化或最大化函数的
        - 摘要：我们假设读者已经熟悉微积分，这里简要回顾微积分概念如何与优化联；系。
          關鍵詞：我们假设读者已经熟悉微积分, 这里简要回顾微积分概念如何与优化联
        - 摘要：假设有一个函数y=f(
          關鍵詞：假设有一个函数
        - 摘要：x
          關鍵詞：
        - 摘要：)，其中x和y是实数。这个函数的导数
          關鍵詞：这个函数的导数, 其中, 是实数
        - 摘要：（derivative）记为f ′( x )或  。导数f ′( x )代表f( x )在点x处的斜率。
          關鍵詞：记为, 导数, 在点, 代表, 处的斜率
        - 摘要：换句话说，它表明如何缩放输入的小变化才能在输出获得相应的变化：；。
          關鍵詞：换句话说, 它表明如何缩放输入的小变化才能在输出获得相应的变化
        - 摘要：因此导数对于最小化一个函数很有用，因为它告诉我们如何更改x来略；微地改善y。例如，我们知道对于足够小的   来说，f(x−sign（f  ′(  x  )))；是比f( x )小的。因此我们可以将x往导数的反方向移动一小步来减小f( x
          關鍵詞：微地改善, 是比, 来略, 小的, 例如
        - 摘要：图4.1　梯度下降。梯度下降算法如何使用函数导数的示意图，即沿着函数的下坡方向（导数反；方向）直到最小
          關鍵詞：直到最小, 即沿着函数的下坡方向, 梯度下降算法如何使用函数导数的示意图, 方向, 梯度下降
        - 摘要：当f ′( x )=0时，导数无法提供往哪个方向移动的信息。f ′( x )=0的点称为；临界点  （critical  point）或驻点  （stationary  point）。一个局部极小点；（local minimum）意味着这个点的f( x )小于所有邻近点，因此不可能通
          關鍵詞：的点称为, 一个局部极小点, 意味着这个点的, 因此不可能通, 或驻点
        - 摘要：图4.2　临界点的类型。一维情况下，3种临界点的示例。临界点是斜率为零的点。这样的点可；以是： 局部极小点 （local minimum），其值低于相邻点； 局部极大点 （local maximum），其；值高于相邻点；鞍点，同时存在更高和更低的相邻点
          關鍵詞：临界点的类型, 一维情况下, 种临界点的示例, 其值低于相邻点, 鞍点
        - 摘要：使f(
          關鍵詞：
        - 摘要：x
          關鍵詞：
        - 摘要：)取得绝对的最小值（相对所有其他值）的点是全局最小点
          關鍵詞：相对所有其他值, 取得绝对的最小值, 的点是全局最小点
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；（global  minimum）。函数可能只有一个全局最小点或存在多个全局最；小点，还可能存在不是全局最优的全局极小点。在深度学习的背景下，
          關鍵詞：函数可能只有一个全局最小点或存在多个全局最, 还可能存在不是全局最优的全局极小点, 小点, 在深度学习的背景下
        - 摘要：图4.3　近似最小化。当存在多个全局极小点或平坦区域时，优化算法可能无法找到全局最小；点。在深度学习的背景下，即使找到的解不是真正最小的，但只要它们对应于代价函数显著低；的值，我们通常就能接受这样的解
          關鍵詞：我们通常就能接受这样的解, 近似最小化, 的值, 优化算法可能无法找到全局最小, 即使找到的解不是真正最小的
        - 摘要：我们经常最小化具有多维输入的函数：；小化”的概念有意义，输出必须是一维的（标量）。
          關鍵詞：小化, 标量, 的概念有意义, 输出必须是一维的, 我们经常最小化具有多维输入的函数
        - 摘要：。为了使“最
          關鍵詞：为了使
        - 摘要：针对具有多维输入的函数，我们需要用到偏导数 （partial derivative）的
          關鍵詞：针对具有多维输入的函数, 我们需要用到偏导数
        - 摘要：概念。偏导数
          關鍵詞：偏导数, 概念
        - 摘要：衡量点 x 处只有x i 增加时f( x )如何变化。
          關鍵詞：处只有, 如何变化, 衡量点, 增加时
        - 摘要：梯度  （gradient）是相对一个向量求导的导数：f的导数是包含所有偏导；数的向量，记为；。梯度的第i个元素是f关于x  i  的偏导数。在
          關鍵詞：梯度, 的导数是包含所有偏导, 记为, 是相对一个向量求导的导数, 数的向量
        - 摘要：在 u （单位向量）方向的方向导数 （directional derivative）是函数f在 u；关于α的导数；方向的斜率。换句话说，方向导数是函数
          關鍵詞：方向导数是函数, 单位向量, 是函数, 方向的方向导数, 方向的斜率
        - 摘要：为了最小化f，我们希望找到使f下降得最快的方向。计算方向导数：
          關鍵詞：计算方向导数, 下降得最快的方向, 我们希望找到使, 为了最小化
        - 摘要：。
          關鍵詞：
        - 摘要：其中θ是  u  与梯度的夹角。将
          關鍵詞：与梯度的夹角, 其中
        - 摘要：代入，并忽略与  u  无关的
          關鍵詞：并忽略与, 无关的, 代入
        - 摘要：项，就能简化得到
          關鍵詞：就能简化得到
        - 摘要：这在  u  与梯度方向相反时取得最
          關鍵詞：这在, 与梯度方向相反时取得最
        - 摘要：小。换句话说，梯度向量指向上坡，负梯度向量指向下坡。我们在负梯；度方向上移动可以减小f。这被称为最速下降法  （method  of  steepest；descent）或梯度下降 （gradient descent）。
          關鍵詞：这被称为最速下降法, 我们在负梯, 梯度向量指向上坡, 度方向上移动可以减小, 或梯度下降
        - 摘要：最速下降建议新的点为
          關鍵詞：最速下降建议新的点为
        - 摘要：其中  为学习率  （learning  rate），是一个确定步长大小的正标量。我；们可以通过几种不同的方式选择   。普遍的方式是选择一个小常数。；有时我们通过计算，选择使方向导数消失的步长。还有一种方法是根据
          關鍵詞：们可以通过几种不同的方式选择, 是一个确定步长大小的正标量, 普遍的方式是选择一个小常数, 其中, 还有一种方法是根据
        - 摘要：最速下降在梯度的每一个元素为零时收敛（或在实践中，很接近零；时）。在某些情况下，我们也许能够避免运行该迭代算法，并通过解方；程
          關鍵詞：最速下降在梯度的每一个元素为零时收敛, 或在实践中, 我们也许能够避免运行该迭代算法, 很接近零, 在某些情况下
        - 摘要：直接跳到临界点。
          關鍵詞：直接跳到临界点
        - 摘要：虽然梯度下降被限制在连续空间中的优化问题，但不断向更好的情况移；动一小步（即近似最佳的小移动）的一般概念可以推广到离散空间。递；增带有离散参数的目标函数称为爬山 （hill  climbing）算法（Russel  and
          關鍵詞：的一般概念可以推广到离散空间, 算法, 即近似最佳的小移动, 增带有离散参数的目标函数称为爬山, 但不断向更好的情况移
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；4.3.1　梯度之上：Jacobian和Hessian矩阵
          關鍵詞：矩阵, 梯度之上
        - 摘要：有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有；这样的偏导数的矩阵被称为Jacobian 矩阵。具体来说，如果我们有一个；定义为
          關鍵詞：定义为, 这样的偏导数的矩阵被称为, 矩阵, 包含所有, 具体来说
        - 摘要：。
          關鍵詞：
        - 摘要：有时，我们也对导数的导数感兴趣，即二阶导数；derivative）。例如，有一个函数
          關鍵詞：有一个函数, 即二阶导数, 有时, 例如, 我们也对导数的导数感兴趣
        - 摘要：（second；，f的一阶导数（关
          關鍵詞：的一阶导数
        - 摘要：于x  j  ）关于x  i  的导数记为
          關鍵詞：的导数记为, 关于
        - 摘要：。在一维情况下，我们可以将
          關鍵詞：在一维情况下, 我们可以将
        - 摘要：。二阶导数告诉我们，一阶导数将如何随
          關鍵詞：二阶导数告诉我们, 一阶导数将如何随
        - 摘要：着输入的变化而改变。它表示只基于梯度信息的梯度下降步骤是否会产；生如我们预期的那样大的改善，因此它是重要的。我们可以认为，二阶；导数是对曲率的衡量。假设我们有一个二次函数（虽然很多实践中的函
          關鍵詞：虽然很多实践中的函, 因此它是重要的, 它表示只基于梯度信息的梯度下降步骤是否会产, 二阶, 假设我们有一个二次函数
        - 摘要：图4.4　二阶导数确定函数的曲率。这里我们展示具有各种曲率的二次函数。虚线表示我们仅根；据梯度信息进行梯度下降后预期的代价函数值。对于负曲率，代价函数实际上比梯度预测下降；得更快。没有曲率时，梯度正确预测下降值。对于正曲率，代价函数比预期下降得更慢，并且
          關鍵詞：代价函数实际上比梯度预测下降, 对于负曲率, 据梯度信息进行梯度下降后预期的代价函数值, 没有曲率时, 虚线表示我们仅根
        - 摘要：当我们的函数具有多维输入时，二阶导数也有很多。我们可以将这些导；数合并成一个矩阵，称为Hessian  矩阵。Hessian矩阵；定义
          關鍵詞：称为, 定义, 矩阵, 当我们的函数具有多维输入时, 二阶导数也有很多
        - 摘要：Hessian等价于梯度的Jacobian矩阵。
          關鍵詞：等价于梯度的, 矩阵
        - 摘要：微分算子在任何二阶偏导连续的点处可交换，也就是它们的顺序可以互；换：
          關鍵詞：也就是它们的顺序可以互, 微分算子在任何二阶偏导连续的点处可交换
        - 摘要：这意味着H i,j =H j,i ，因此Hessian矩阵在这些点上是对称的。在深度学习；背景下，我们遇到的大多数函数的Hessian几乎处处都是对称的。因为；Hessian矩阵是实对称的，我们可以将其分解成一组实特征值和一组特征
          關鍵詞：几乎处处都是对称的, 因为, 因此, 矩阵是实对称的, 矩阵在这些点上是对称的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；于其他的方向 d  ，方向二阶导数是所有特征值的加权平均，权重在0和1；之间，且与 d 夹角越小的特征向量的权重越大。最大特征值确定最大二
          關鍵詞：方向二阶导数是所有特征值的加权平均, 之间, 于其他的方向, 最大特征值确定最大二, 夹角越小的特征向量的权重越大
        - 摘要：我们可以通过（方向）二阶导数预期一个梯度下降步骤能表现得多好。；我们在当前点 x (0) 处做函数f ( x )的近似二阶泰勒级数：
          關鍵詞：处做函数, 二阶导数预期一个梯度下降步骤能表现得多好, 我们可以通过, 方向, 我们在当前点
        - 摘要：其中 g 是梯度， H 是 x (0) 点的Hessian。如果我们使用学习率  ，那么；新的点 x 将会是
          關鍵詞：是梯度, 其中, 将会是, 那么, 如果我们使用学习率
        - 摘要：。代入上述的近似，可得
          關鍵詞：代入上述的近似, 可得
        - 摘要：其中有3项：函数的原始值、函数斜率导致的预期改善和函数曲率导致；的校正。当最后一项太大时，梯度下降实际上是可能向上移动的。当
          關鍵詞：函数斜率导致的预期改善和函数曲率导致, 其中有, 函数的原始值, 的校正, 当最后一项太大时
        - 摘要：为零或负时，近似的泰勒级数表明增加   将永远使f下降。在；实践中，泰勒级数不会在   大的时候也保持准确，因此在这种情况下；为正时，通过计算可得，
          關鍵詞：为正时, 大的时候也保持准确, 因此在这种情况下, 通过计算可得, 下降
        - 摘要：最坏的情况下， g 与  H  最大特征值λ  max  对应的特征向量对齐，则最优
          關鍵詞：最坏的情况下, 对应的特征向量对齐, 最大特征值, 则最优
        - 摘要：步长是
          關鍵詞：步长是
        - 摘要：。当我们要最小化的函数能用二次函数很好地近似的情
          關鍵詞：当我们要最小化的函数能用二次函数很好地近似的情
        - 摘要：况下，Hessian的特征值决定了学习率的量级。
          關鍵詞：况下, 的特征值决定了学习率的量级
        - 摘要：二阶导数还可以用于确定一个临界点是否是局部极大点、全局极小点或；鞍点。回想一下，在临界点处f ′( x )=0。而f ″( x )>0意味着f ′( x )会随着；我们移向右边而增加，移向左边而减小，也就是f ′( x -ε)<0和f ′( x +ε)>0
          關鍵詞：也就是, 全局极小点或, 会随着, 二阶导数还可以用于确定一个临界点是否是局部极大点, 鞍点
        - 摘要：论，当f ′( x )=0且f ″( x )>0时， x 是一个全局极小点。同理，当f ′( x )=0；且f  ″(  x  )<0时，  x  是一个局部极大点。这就是所谓的二阶导数测试；（second  derivative  test）。不幸的是，当f  ″(  x  )=0时，测试是不确定
          關鍵詞：同理, 测试是不确定, 这就是所谓的二阶导数测试, 是一个全局极小点, 不幸的是
        - 摘要：在多维情况下，我们需要检测函数的所有二阶导数。利用Hessian的特征；值分解，我们可以将二阶导数测试扩展到多维情况。在临界点处
          關鍵詞：我们可以将二阶导数测试扩展到多维情况, 的特征, 在多维情况下, 我们需要检测函数的所有二阶导数, 值分解
        - 摘要：，我们通过检测Hessian的特征值来判断该临界点是；一个局部极大点、全局极小点还是鞍点。当Hessian是正定的（所有特征；值都是正的），则该临界点是全局极小点。因为方向二阶导数在任意方
          關鍵詞：则该临界点是全局极小点, 因为方向二阶导数在任意方, 的特征值来判断该临界点是, 是正定的, 全局极小点还是鞍点
        - 摘要：图4.5　既有正曲率又有负曲率的鞍点。示例中的函数是；上弯曲。x 1 轴是Hessian的一个特征向量，并且具有正特征值。函数沿x 2 轴向下弯曲。该方向；对应于Hessian负特征值的特征向量。名称“鞍点”源自该处函数的鞍状形状。这是具有鞍点函数
          關鍵詞：并且具有正特征值, 这是具有鞍点函数, 鞍点, 函数沿, 既有正曲率又有负曲率的鞍点
        - 摘要：。函数沿x 1 轴向
          關鍵詞：轴向, 函数沿
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；个横截面内是全局极小点
          關鍵詞：个横截面内是全局极小点
        - 摘要：多维情况下，单个点处每个方向上的二阶导数是不同的。Hessian的条件；数衡量这些二阶导数的变化范围。当Hessian的条件数很差时，梯度下降；法也会表现得很差。这是因为一个方向上的导数增加得很快，而在另一
          關鍵詞：的条件数很差时, 的条件, 单个点处每个方向上的二阶导数是不同的, 数衡量这些二阶导数的变化范围, 多维情况下
        - 摘要：我们可以使用Hessian矩阵的信息来指导搜索，以解决这个问题。其中最；简单的方法是牛顿法  （Newton's  method）。牛顿法基于一个二阶泰勒；展开来近似 x (0) 附近的f( x )：
          關鍵詞：展开来近似, 我们可以使用, 以解决这个问题, 矩阵的信息来指导搜索, 附近的
        - 摘要：接着通过计算，我们可以得到这个函数的临界点：
          關鍵詞：接着通过计算, 我们可以得到这个函数的临界点
        - 摘要：如果f是一个正定二次函数，牛顿法只要应用一次式（4.12）就能直接跳；到函数的最小点。如果f不是一个真正二次但能在局部近似为正定二；次，牛顿法则需要多次迭代应用式（4.12）。迭代地更新近似函数和跳
          關鍵詞：如果, 迭代地更新近似函数和跳, 牛顿法只要应用一次式, 到函数的最小点, 不是一个真正二次但能在局部近似为正定二
        - 摘要：图4.6　梯度下降无法利用包含在Hessian矩阵中的曲率信息。这里我们使用梯度下降来最小化；Hessian矩阵条件数为5的二次函数f( x )。这意味着最大曲率方向具有比最小曲率方向多5倍的曲；率。在这种情况下，最大曲率在
          關鍵詞：矩阵条件数为, 梯度下降无法利用包含在, 矩阵中的曲率信息, 在这种情况下, 最大曲率在
        - 摘要：方向上，最小曲率在
          關鍵詞：方向上, 最小曲率在
        - 摘要：方向上。红线表示梯度
          關鍵詞：红线表示梯度, 方向上
        - 摘要：仅使用梯度信息的优化算法称为一阶优化算法  （first-order  optimization；algorithms），如梯度下降。使用Hessian矩阵的优化算法称为二阶最优；化算法 （second-order optimization algo-rithms）（Nocedal and Wright，
          關鍵詞：矩阵的优化算法称为二阶最优, 化算法, 使用, 如梯度下降, 仅使用梯度信息的优化算法称为一阶优化算法
        - 摘要：本书大多数上下文中使用的优化算法适用于各种各样的函数，但几乎都；没有理论保证。因为在深度学习中使用的函数族是相当复杂的，所以深；度学习算法往往缺乏理论保证。在许多其他领域，优化的主要方法是为
          關鍵詞：但几乎都, 因为在深度学习中使用的函数族是相当复杂的, 没有理论保证, 本书大多数上下文中使用的优化算法适用于各种各样的函数, 在许多其他领域
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；有限的函数族设计优化算法。
          關鍵詞：有限的函数族设计优化算法
        - 摘要：在深度学习的背景下，限制函数满足Lipschitz连续；（Lipschitz；continuous）或其导数Lip-schitz连续可以获得一些保证。Lipschitz连续函
          關鍵詞：连续, 或其导数, 限制函数满足, 连续可以获得一些保证, 连续函
        - 摘要：这个属性允许我们量化自己的假设——梯度下降等算法导致的输入的微；小变化将使输出只产生微小变化，因此是很有用的。Lipschitz连续性也；是相当弱的约束，并且深度学习中很多优化问题经过相对较小的修改后
          關鍵詞：并且深度学习中很多优化问题经过相对较小的修改后, 梯度下降等算法导致的输入的微, 连续性也, 这个属性允许我们量化自己的假设, 因此是很有用的
        - 摘要：最成功的特定优化领域或许是凸优化  （Convex  optimization）。凸优化；通过更强的限制提供更多的保证。凸优化算法只对凸函数适用，即；Hessian处处半正定的函数。因为这些函数没有鞍点而且其所有全局极小
          關鍵詞：处处半正定的函数, 通过更强的限制提供更多的保证, 最成功的特定优化领域或许是凸优化, 凸优化算法只对凸函数适用, 因为这些函数没有鞍点而且其所有全局极小
        - 摘要：4.3.1　梯度之上：；Jacobian和Hessian矩阵
          關鍵詞：矩阵, 梯度之上
    4.4：约束优化
        - 摘要：有时候，在  x  的所有可能值下最大化或最小化一个函数f(x)不是我们所；希望的。相反，我们可能希望在  x  的某些集合   中找f(  x  )的最大值或；最小值。这称为约束优化  （constrained  optimization）。在约束优化术
          關鍵詞：的某些集合, 这称为约束优化, 在约束优化术, 我们可能希望在, 最小值
        - 摘要：我们常常希望找到在某种意义上小的解。针对这种情况下的常见方法是；。；强加一个范数约束，如
          關鍵詞：针对这种情况下的常见方法是, 强加一个范数约束, 我们常常希望找到在某种意义上小的解
        - 摘要：约束优化的一个简单方法是将约束考虑在内后简单地对梯度下降进行修；改。如果使用一个小的恒定步长   ，我们可以先取梯度下降的单步结；果，然后将结果投影回   。如果使用线搜索，我们只能在步长为   范
          關鍵詞：如果使用线搜索, 我们可以先取梯度下降的单步结, 我们只能在步长为, 然后将结果投影回, 约束优化的一个简单方法是将约束考虑在内后简单地对梯度下降进行修
        - 摘要：围内搜索可行的新 x  点，或者可以将线上的每个点投影到约束区域。如；果可能，在梯度下降或线搜索前将梯度投影到可行域的切空间会更高效；（Rosen，1960）。
          關鍵詞：或者可以将线上的每个点投影到约束区域, 围内搜索可行的新, 果可能, 在梯度下降或线搜索前将梯度投影到可行域的切空间会更高效
        - 摘要：)，其中
          關鍵詞：其中
        - 摘要：一个更复杂的方法是设计一个不同的、无约束的优化问题，其解可以转；中最小化f(  x；化成原始约束优化问题的解。例如，我们要在
          關鍵詞：一个更复杂的方法是设计一个不同的, 无约束的优化问题, 我们要在, 例如, 中最小化
        - 摘要：Karush-Kuhn-Tucker （KKT）方法 (2) 是针对约束优化非常通用的解决；方案。为介绍KKT方法，我们引入一个称为广义Lagrangian；（generalized  Lagrangian）或广义Lagrange函数  （generalized  Lagrange
          關鍵詞：我们引入一个称为广义, 函数, 或广义, 是针对约束优化非常通用的解决, 方案
        - 摘要：为了定义Lagrangian，我们先要通过等式和不等式的形式描述   。我们；(j)  描述   ，那么   可以表示为；希望通过m个函数g
          關鍵詞：我们, 希望通过, 那么, 描述, 我们先要通过等式和不等式的形式描述
        - 摘要：(i)  和n个函数h
          關鍵詞：个函数
        - 摘要：。其中涉及g
          關鍵詞：其中涉及
        - 摘要：的等式称为等式约束  （equality  constraint），涉及h  (j)  的不等式称为不；等式约束 （inequality constraint）。
          關鍵詞：的等式称为等式约束, 涉及, 的不等式称为不, 等式约束
        - 摘要：我们为每个约束引入新的变量λ  i  和α  j  ，这些新变量被称为KKT乘子。；广义Lagrangian可以定义为
          關鍵詞：我们为每个约束引入新的变量, 广义, 乘子, 这些新变量被称为, 可以定义为
        - 摘要：现在，我们可以通过优化无约束的广义Lagrangian解决约束最小化问；题。只要存在至少一个可行点且f( x )不允许取∞，那么
          關鍵詞：只要存在至少一个可行点且, 我们可以通过优化无约束的广义, 那么, 解决约束最小化问, 不允许取
        - 摘要：与如下函数有相同的最优目标函数值和最优点集 x
          關鍵詞：与如下函数有相同的最优目标函数值和最优点集
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；这是因为当约束满足时，
          關鍵詞：这是因为当约束满足时
        - 摘要：而违反任意约束时，
          關鍵詞：而违反任意约束时
        - 摘要：这些性质保证不可行点不会是最佳的，并且可行点范围内的最优点不；变。
          關鍵詞：这些性质保证不可行点不会是最佳的, 并且可行点范围内的最优点不
        - 摘要：要解决约束最大化问题，我们可以构造−f(  x  )的广义Lagrange函数，从；而导致以下优化问题：
          關鍵詞：而导致以下优化问题, 函数, 的广义, 要解决约束最大化问题, 我们可以构造
        - 摘要：我们也可将其转换为在外层最大化的问题：
          關鍵詞：我们也可将其转换为在外层最大化的问题
        - 摘要：等式约束对应项的符号并不重要，因为优化可以自由选择每个λ；号，我们可以随意将其定义为加法或减法。
          關鍵詞：因为优化可以自由选择每个, 我们可以随意将其定义为加法或减法, 等式约束对应项的符号并不重要
        - 摘要：i  的符
          關鍵詞：的符
        - 摘要：，我们就说说这个约束h  (i)  (；不等式约束特别有趣。如果；x  )是活跃  （active）的。如果约束不是活跃的，则有该约束的问题的解
          關鍵詞：是活跃, 如果, 如果约束不是活跃的, 不等式约束特别有趣, 我们就说说这个约束
        - 摘要：(i)
          關鍵詞：
        - 摘要：中的α  i  =0。因此，我们
          關鍵詞：因此, 中的, 我们
        - 摘要：可以观察到在该解中
          關鍵詞：可以观察到在该解中
        - 摘要：或
          關鍵詞：
        - 摘要：。换句话说，对于所有的；在收敛时必有一个是活跃的。为了获得
          關鍵詞：为了获得, 对于所有的, 换句话说, 在收敛时必有一个是活跃的
        - 摘要：关于这个想法的一些直观解释，我们可以说这个解是由不等式强加的边；界，我们必须通过对应的KKT乘子影响 x 的解，或者不等式对解没有影；响，我们则归零KKT乘子。
          關鍵詞：或者不等式对解没有影, 乘子影响, 的解, 乘子, 我们必须通过对应的
        - 摘要：我们可以使用一组简单的性质来描述约束优化问题的最优点。这些性质；称为Karush-Kuhn-Tucker  （KKT）条件（Karush，1939；Kuhn  and；Tucker，1951）。这些是确定一个点是最优点的必要条件，但不一定是
          關鍵詞：称为, 这些是确定一个点是最优点的必要条件, 我们可以使用一组简单的性质来描述约束优化问题的最优点, 这些性质, 但不一定是
        - 摘要：广义Lagrangian的梯度为零。；所有关于 x 和KKT乘子的约束都满足。；不等式约束显示的“互补松弛性”：
          關鍵詞：所有关于, 的梯度为零, 广义, 乘子的约束都满足, 不等式约束显示的
        - 摘要：。
          關鍵詞：
        - 摘要：有关KKT方法的详细信息，请参阅Nocedal and Wright（2006）。
          關鍵詞：有关, 请参阅, 方法的详细信息
    4.5：实例：线性最小二乘
        - 摘要：假设我们希望找到最小化下式的 x 值
          關鍵詞：假设我们希望找到最小化下式的
        - 摘要：存在专门的线性代数算法能够高效地解决这个问题，但是我们也可以探；索如何使用基于梯度的优化来解决这个问题，这可以作为这些技术是如；何工作的一个简单例子。
          關鍵詞：但是我们也可以探, 何工作的一个简单例子, 这可以作为这些技术是如, 索如何使用基于梯度的优化来解决这个问题, 存在专门的线性代数算法能够高效地解决这个问题
        - 摘要：首先，我们计算梯度
          關鍵詞：首先, 我们计算梯度
        - 摘要：然后，我们可以采用小的步长，并按照这个梯度下降，见算法4.1中的；详细信息。
          關鍵詞：我们可以采用小的步长, 并按照这个梯度下降, 见算法, 详细信息, 中的
        - 摘要：算法4.1  　从任意点
          關鍵詞：从任意点, 算法
        - 摘要：x  开始，使用梯度下降关于
          關鍵詞：使用梯度下降关于, 开始
        - 摘要：x  最小化
          關鍵詞：最小化
        - 摘要：的算法。
          關鍵詞：的算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；将步长（  ）和容差（δ）设为小的正数。
          關鍵詞：将步长, 和容差, 设为小的正数
        - 摘要：end while
          關鍵詞：
        - 摘要：我们也可以使用牛顿法解决这个问题。因为在这个情况下，真实函数是；二次的，牛顿法所用的二次近似是精确的，该算法会在一步后收敛到全；局最小点。
          關鍵詞：真实函数是, 局最小点, 二次的, 牛顿法所用的二次近似是精确的, 因为在这个情况下
        - 摘要：现在假设我们希望最小化同样的函数，但受；到这一点，我们引入Lagrangian
          關鍵詞：但受, 到这一点, 现在假设我们希望最小化同样的函数, 我们引入
        - 摘要：的约束。要做
          關鍵詞：的约束, 要做
        - 摘要：现在，我们解决以下问题
          關鍵詞：现在, 我们解决以下问题
        - 摘要：找到无约束最小二乘问；我们可以用Moore-Penrose伪逆：；题的最小范数解。如果这一点是可行的，那么这也是约束问题的解。否
          關鍵詞：题的最小范数解, 那么这也是约束问题的解, 找到无约束最小二乘问, 如果这一点是可行的, 伪逆
        - 摘要：这就告诉我们，该解的形式将会是
          關鍵詞：这就告诉我们, 该解的形式将会是
        - 摘要：λ的选择必须使结果服从约束。我们可以关于λ进行梯度上升找到这个；值。为了做到这一点，观察
          關鍵詞：进行梯度上升找到这个, 为了做到这一点, 的选择必须使结果服从约束, 观察, 我们可以关于
        - 摘要：当 x 的范数超过1时，该导数是正的，所以为了跟随导数上坡并相对λ增；的惩罚系数增加了，求解；加Lagrangian，我们需要增加λ。因为
          關鍵詞：因为, 该导数是正的, 的范数超过, 所以为了跟随导数上坡并相对, 我们需要增加
        - 摘要：本章总结了开发机器学习算法所需的数学基础。现在，我们已经为建立；和分析一些成熟的学习系统做好了准备。
          關鍵詞：本章总结了开发机器学习算法所需的数学基础, 我们已经为建立, 现在, 和分析一些成熟的学习系统做好了准备
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)  译者注：与通常的条件数定义有所不同。
          關鍵詞：与通常的条件数定义有所不同, 译者注
        - 摘要：(2)  KKT方法是 Lagrange乘子法 （只允许等式约束）的推广。
          關鍵詞：乘子法, 方法是, 只允许等式约束, 的推广
第5章：机器学习基础
    4.5：实例：线性最小二乘
        - 摘要：深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必；须对机器学习的基本原理有深刻的理解。本章将探讨贯穿本书其余部分；的一些机器学习的重要原理。我们建议新手读者或是希望更全面了解的
          關鍵詞：本章将探讨贯穿本书其余部分, 我们要想充分理解深度学习, 深度学习是机器学习的一个特定分支, 的一些机器学习的重要原理, 须对机器学习的基本原理有深刻的理解
        - 摘要：首先，我们将介绍学习算法的定义，并介绍一个简单的示例：线性回归；算法。接下来，我们会探讨拟合训练数据与寻找能够泛化到新数据的模；式存在哪些不同的挑战。大部分机器学习算法都有超参数（必须在学习
          關鍵詞：接下来, 算法, 我们将介绍学习算法的定义, 我们会探讨拟合训练数据与寻找能够泛化到新数据的模, 并介绍一个简单的示例
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；法。最后在第5.11节，我们会介绍一些限制传统机器学习泛化能力的因；素。这些挑战促进了解决这些问题的深度学习算法的发展。
          關鍵詞：我们会介绍一些限制传统机器学习泛化能力的因, 最后在第, 这些挑战促进了解决这些问题的深度学习算法的发展
    5.1：学习算法
        - 摘要：5.1.1　任务T
          關鍵詞：任务
        - 摘要：5.1.2　性能度量P
          關鍵詞：性能度量
        - 摘要：5.1.3　经验E
          關鍵詞：经验
        - 摘要：5.1.4　示例：线性回归
          關鍵詞：示例, 线性回归
        - 摘要：机器学习算法是一种能够从数据中学习的算法。然而，我们所谓的“学；习”是什么意思呢？Mitchell（1997）提供了一个简洁的定义：“对于某；类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是
          關鍵詞：一个计算机程序被认为可以从经验, 是什么意思呢, 和性能度量, 提供了一个简洁的定义, 然而
        - 摘要：5.1.1　任务T
          關鍵詞：任务
        - 摘要：机器学习可以让我们解决一些人为设计和使用确定性程序很难解决的问；题。从科学和哲学的角度来看，机器学习之所以受到关注，是因为提高；我们对机器学习的认识需要提高我们自身对智能背后原理的理解。
          關鍵詞：机器学习可以让我们解决一些人为设计和使用确定性程序很难解决的问, 我们对机器学习的认识需要提高我们自身对智能背后原理的理解, 是因为提高, 机器学习之所以受到关注, 从科学和哲学的角度来看
        - 摘要：从“任务”的相对正式的定义上说，学习过程本身不能算是任务。学习是；我们所谓的获取完成任务的能力。例如，我们的目标是使机器人能够行；走，那么行走便是任务。我们可以编程让机器人学会如何行走，或者可
          關鍵詞：任务, 我们可以编程让机器人学会如何行走, 我们的目标是使机器人能够行, 学习是, 例如
        - 摘要：通常机器学习任务定义为机器学习系统应该如何处理样本；（example）。样本是指我们从某些希望机器学习系统处理的对象或事；件中收集到的已经量化的特征 （feature）的集合。我们通常会将样本表
          關鍵詞：件中收集到的已经量化的特征, 通常机器学习任务定义为机器学习系统应该如何处理样本, 的集合, 我们通常会将样本表, 样本是指我们从某些希望机器学习系统处理的对象或事
        - 摘要：机器学习可以解决很多类型的任务。一些非常常见的机器学习任务列举；如下。
          關鍵詞：如下, 一些非常常见的机器学习任务列举, 机器学习可以解决很多类型的任务
        - 摘要：分类：  在这类任务中，计算机程序需要指定某些输入属于k类中的；哪一类。为了完成这个任务，学习算法通常会返回一个函数
          關鍵詞：在这类任务中, 学习算法通常会返回一个函数, 类中的, 计算机程序需要指定某些输入属于, 为了完成这个任务
        - 摘要：。当y=f( x )时，模型将向量 x 所代表
          關鍵詞：模型将向量, 所代表
        - 摘要：的输入分类到数字码y所代表的类别。还有一些其他的分类问题，；例如，f输出的是不同类别的概率分布。分类任务中有一个任务是；对象识别，其中输入是图片（通常由一组像素亮度值表示），输出
          關鍵詞：输出的是不同类别的概率分布, 通常由一组像素亮度值表示, 其中输入是图片, 对象识别, 例如
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；的编码。深度学习是现代语音识别系统的重要组成部分，被各大公；司广泛使用，包括微软、IBM和谷歌（Hinton et al. ，2012b）。
          關鍵詞：深度学习是现代语音识别系统的重要组成部分, 的编码, 包括微软, 和谷歌, 被各大公
        - 摘要：。
          關鍵詞：
        - 摘要：去噪：
          關鍵詞：去噪
        - 摘要：经过未知损坏过程后得到的损坏样本
          關鍵詞：经过未知损坏过程后得到的损坏样本
        - 摘要：入并非只有一个正确输出的条件，并且我们明确希望输出有很多变；化，这可以使结果看上去更加自然和真实。；缺失值填补：
          關鍵詞：入并非只有一个正确输出的条件, 缺失值填补, 并且我们明确希望输出有很多变, 这可以使结果看上去更加自然和真实
        - 摘要：在这类任务中，机器学习算法给定一个新样本；， x 中某些元素x i 缺失。算法必须填补这些缺失值。；在这类任务中，机器学习算法的输入是，干净样本
          關鍵詞：在这类任务中, 机器学习算法的输入是, 算法必须填补这些缺失值, 缺失, 机器学习算法给定一个新样本
        - 摘要：当然，还有很多其他同类型或其他类型的任务。这里我们列举的任务类；型只是用来介绍机器学习可以做哪些任务，并非严格地定义机器学习任；务分类。
          關鍵詞：并非严格地定义机器学习任, 型只是用来介绍机器学习可以做哪些任务, 还有很多其他同类型或其他类型的任务, 这里我们列举的任务类, 当然
        - 摘要：5.1.2　性能度量P
          關鍵詞：性能度量
        - 摘要：为了评估机器学习算法的能力，我们必须设计其性能的定量度量。通常；性能度量P是特定于系统执行的任务T而言的。
          關鍵詞：通常, 性能度量, 是特定于系统执行的任务, 为了评估机器学习算法的能力, 而言的
        - 摘要：对于诸如分类、缺失输入分类和转录任务，我们通常度量模型的准确率；（accuracy）。准确率是指该模型输出正确结果的样本比率。我们也可；以通过错误率  （errorrate）得到相同的信息。错误率是指该模型输出错
          關鍵詞：缺失输入分类和转录任务, 准确率是指该模型输出正确结果的样本比率, 得到相同的信息, 我们通常度量模型的准确率, 我们也可
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；误结果的样本比率。我们通常把错误率称为0−1损失的期望。在一个特；定的样本上，如果结果是对的，那么0−1损失是0；否则是1。但是对于
          關鍵詞：定的样本上, 但是对于, 如果结果是对的, 那么, 误结果的样本比率
        - 摘要：通常，我们会更加关注机器学习算法在未观测数据上的性能如何，因为；这将决定其在实际应用中的性能。因此，我们使用测试集 （test set）数；据来评估系统性能，将其与训练机器学习系统的训练集数据分开。
          關鍵詞：因为, 通常, 据来评估系统性能, 我们会更加关注机器学习算法在未观测数据上的性能如何, 因此
        - 摘要：性能度量的选择或许看上去简单且客观，但是选择一个与系统理想表现；对应的性能度量通常是很难的。
          關鍵詞：对应的性能度量通常是很难的, 但是选择一个与系统理想表现, 性能度量的选择或许看上去简单且客观
        - 摘要：在某些情况下，这是因为很难确定应该度量什么。例如，在执行转录任；务时，我们是应该度量系统转录整个序列的准确率，还是应该用一个更；细粒度的指标，对序列中正确的部分元素以正面评价？在执行回归任务
          關鍵詞：这是因为很难确定应该度量什么, 在执行转录任, 细粒度的指标, 例如, 我们是应该度量系统转录整个序列的准确率
        - 摘要：还有一些情况，我们知道应该度量哪些数值，但是度量它们不太现实。；这种情况经常出现在密度估计中。很多最好的概率模型只能隐式地表示；概率分布。在许多这类模型中，计算空间中特定点的概率是不可行的。
          關鍵詞：在许多这类模型中, 很多最好的概率模型只能隐式地表示, 我们知道应该度量哪些数值, 还有一些情况, 概率分布
        - 摘要：5.1.3　经验E
          關鍵詞：经验
        - 摘要：根据学习过程中的不同经验，机器学习算法可以大致分类为无监督；（unsupervised）算法和监督 （supervised）算法。
          關鍵詞：算法和监督, 机器学习算法可以大致分类为无监督, 根据学习过程中的不同经验, 算法
        - 摘要：本书中的大部分学习算法可以被理解为在整个数据集 （dataset）上获取；经验。数据集是指很多样本组成的集合，如第5.1.1节所定义的。有时我；们也将样本称为数据点 （data point）。
          關鍵詞：们也将样本称为数据点, 有时我, 上获取, 经验, 数据集是指很多样本组成的集合
        - 摘要：Iris（鸢尾花卉）数据集（Fisher，1936）是统计学家和机器学习研究者；使用了很久的数据集。它是150个鸢尾花卉植物不同部分测量结果的集
          關鍵詞：使用了很久的数据集, 鸢尾花卉, 是统计学家和机器学习研究者, 个鸢尾花卉植物不同部分测量结果的集, 数据集
        - 摘要：合。每个单独的植物对应一个样本。每个样本的特征是该植物不同部分；的测量结果：萼片长度、萼片宽度、花瓣长度和花瓣宽度。这个数据集；也记录了每个植物属于什么品种，其中共有3个不同的品种。
          關鍵詞：个不同的品种, 萼片长度, 每个样本的特征是该植物不同部分, 这个数据集, 也记录了每个植物属于什么品种
        - 摘要：无监督学习算法  （unsupervised  learning  algorithm）训练含有很多特征；的数据集，然后学习出这个数据集上有用的结构性质。在深度学习中，；我们通常要学习生成数据集的整个概率分布，显式地，比如密度估计，
          關鍵詞：然后学习出这个数据集上有用的结构性质, 比如密度估计, 我们通常要学习生成数据集的整个概率分布, 在深度学习中, 无监督学习算法
        - 摘要：监督学习算法  （supervised  learning  algorithm）训练含有很多特征的数；据集，不过数据集中的样本都有一个标签  （label）或目标  （target）。；例如，Iris数据集注明了每个鸢尾花卉样本属于什么品种。监督学习算
          關鍵詞：数据集注明了每个鸢尾花卉样本属于什么品种, 不过数据集中的样本都有一个标签, 例如, 监督学习算, 或目标
        - 摘要：大致说来，无监督学习涉及观察随机向量x  的好几个样本，试图显式或；隐式地学习出概率分布p（x  ），或者是该分布一些有意思的性质；而；监督学习包含观察随机向量x  及其相关联的值或向量y  ，然后从x  预测y
          關鍵詞：无监督学习涉及观察随机向量, 或者是该分布一些有意思的性质, 试图显式或, 隐式地学习出概率分布, 然后从
        - 摘要：无监督学习和监督学习不是严格定义的术语。它们之间界线通常是模糊；的。很多机器学习技术可以用于这两个任务。例如，概率的链式法则表；明对于随机向量
          關鍵詞：无监督学习和监督学习不是严格定义的术语, 它们之间界线通常是模糊, 例如, 很多机器学习技术可以用于这两个任务, 概率的链式法则表
        - 摘要：，联合分布可以分解成
          關鍵詞：联合分布可以分解成
        - 摘要：该分解意味着我们可以将其拆分成n个监督学习问题，来解决表面上的；无监督学习p(  x  )。另外，我们求解监督学习问题p(y｜x  )时，也可以使；用传统的无监督学习策略学习联合分布p(x ,y)，然后推断
          關鍵詞：也可以使, 来解决表面上的, 然后推断, 我们求解监督学习问题, 用传统的无监督学习策略学习联合分布
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；尽管无监督学习和监督学习并非完全没有交集的正式概念，它们确实有；助于粗略分类我们研究机器学习算法时遇到的问题。传统上，人们将回
          關鍵詞：它们确实有, 尽管无监督学习和监督学习并非完全没有交集的正式概念, 人们将回, 传统上, 助于粗略分类我们研究机器学习算法时遇到的问题
        - 摘要：学习范式的其他变种也是有可能的。例如，半监督学习中，一些样本有；监督目标，但其他样本没有。在多实例学习中，样本的整个集合被标记；为含有或者不含有该类的样本，但是集合中单独的样本是没有标记的。
          關鍵詞：一些样本有, 学习范式的其他变种也是有可能的, 监督目标, 在多实例学习中, 为含有或者不含有该类的样本
        - 摘要：有些机器学习算法并不是训练于一个固定的数据集上。例如，强化学习；（reinforcement learning）算法会和环境进行交互，所以学习系统和它的；训练过程会有反馈回路。这类算法超出了本书的范畴。请参考Sutton
          關鍵詞：算法会和环境进行交互, 请参考, 所以学习系统和它的, 这类算法超出了本书的范畴, 有些机器学习算法并不是训练于一个固定的数据集上
        - 摘要：大部分机器学习算法简单地训练于一个数据集上。数据集可以用很多不；同方式来表示。在所有的情况下，数据集都是样本的集合，而样本是特；征的集合。
          關鍵詞：征的集合, 数据集都是样本的集合, 数据集可以用很多不, 同方式来表示, 而样本是特
        - 摘要：表示数据集的常用方法是设计矩阵  （design  matrix）。设计矩阵的每一；行包含一个不同的样本。每一列对应不同的特征。例如，Iris数据集包；含150个样本，每个样本有4个特征。这意味着我们可以将该数据集表示
          關鍵詞：行包含一个不同的样本, 每一列对应不同的特征, 设计矩阵的每一, 数据集包, 个样本
        - 摘要：当然，每一个样本都能表示成向量，并且这些向量的维度相同，才能将；一个数据集表示成设计矩阵。这一点并非永远可能。例如，你有不同宽；度和高度的照片的集合，那么不同的照片将会包含不同数量的像素。因
          關鍵詞：那么不同的照片将会包含不同数量的像素, 这一点并非永远可能, 你有不同宽, 例如, 每一个样本都能表示成向量
        - 摘要：在监督学习中，样本包含一个标签或目标和一组特征。例如，我们希望；使用学习算法从照片中识别对象。我们需要明确哪些对象会出现在每张；照片中。我们或许会用数字编码表示，如0表示人、1表示车、2表示猫
          關鍵詞：我们或许会用数字编码表示, 表示猫, 使用学习算法从照片中识别对象, 例如, 在监督学习中
        - 摘要：当然，有时标签可能不止一个数。例如，如果我们想要训练语音模型转；录整个句子，那么每个句子样本的标签是一个单词序列。
          關鍵詞：录整个句子, 例如, 那么每个句子样本的标签是一个单词序列, 有时标签可能不止一个数, 当然
        - 摘要：正如监督学习和无监督学习没有正式的定义，数据集或者经验也没有严；格的区分。这里介绍的结构涵盖了大多数情况，但始终有可能为新的应；用设计出新的结构。
          關鍵詞：正如监督学习和无监督学习没有正式的定义, 数据集或者经验也没有严, 这里介绍的结构涵盖了大多数情况, 但始终有可能为新的应, 用设计出新的结构
        - 摘要：5.1.4　示例：线性回归
          關鍵詞：示例, 线性回归
        - 摘要：我们将机器学习算法定义为：通过经验以提高计算机程序在某些任务上；性能的算法。这个定义有点抽象。为了使这个定义更具体点，我们展示；一个简单的机器学习示例：线性回归 （linear regression）。当我们介绍
          關鍵詞：为了使这个定义更具体点, 通过经验以提高计算机程序在某些任务上, 这个定义有点抽象, 我们展示, 性能的算法
        - 摘要：顾名思义，线性回归解决回归问题。换言之，我们的目标是建立一个系；统，将向量；作为输出。线性
          關鍵詞：线性回归解决回归问题, 我们的目标是建立一个系, 将向量, 换言之, 作为输出
        - 摘要：其中
          關鍵詞：其中
        - 摘要：是参数 （parameter）向量。
          關鍵詞：是参数, 向量
        - 摘要：参数是控制系统行为的值。在这种情况下，w  i  是系数，会和特征x  i  相；乘之后全部相加起来。我们可以将 w 看作一组决定每个特征如何影响预；测的权重  （weight）。如果特征x  i  对应的权重w  i  是正的，那么特征的
          關鍵詞：那么特征的, 看作一组决定每个特征如何影响预, 参数是控制系统行为的值, 如果特征, 对应的权重
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；对预测没有影响。
          關鍵詞：对预测没有影响
        - 摘要：因此，我们可以定义任务T：通过输出；来我们需要定义性能度量——P。
          關鍵詞：通过输出, 因此, 我们可以定义任务, 来我们需要定义性能度量
        - 摘要：预测y。接下
          關鍵詞：接下, 预测
        - 摘要：假设我们有m个输入样本组成的设计矩阵，不用它来训练模型，而是评；估模型性能如何。我们也有每个样本对应的正确值y组成的回归目标向；量。因为这个数据集只是用来评估性能，我们称之为测试集
          關鍵詞：我们也有每个样本对应的正确值, 我们称之为测试集, 估模型性能如何, 而是评, 不用它来训练模型
        - 摘要：度量模型性能的一种方法是计算模型在测试集上的均方误差  （mean；squared  error）。如果；表示模型在测试集上的预测值，那么均方
          關鍵詞：如果, 表示模型在测试集上的预测值, 度量模型性能的一种方法是计算模型在测试集上的均方误差, 那么均方
        - 摘要：直观上，当；看到
          關鍵詞：直观上, 看到
        - 摘要：时，我们会发现误差降为0。我们也可以
          關鍵詞：我们也可以, 我们会发现误差降为
        - 摘要：所以当预测值和目标值之间的欧几里得距离增加时，误差也会增加。
          關鍵詞：误差也会增加, 所以当预测值和目标值之间的欧几里得距离增加时
        - 摘要：为了构建一个机器学习算法，我们需要设计一个算法，通过观察训练集；获得经验，减少MSE  test 以改进权重  w  。一种直观；方式（我们将在后续的第5.5.1节说明其合法性）是最小化训练集上的均
          關鍵詞：方式, 获得经验, 减少, 是最小化训练集上的均, 通过观察训练集
        - 摘要：最小化MSE train ，我们可以简单地求解其导数为0的情况：
          關鍵詞：最小化, 的情况, 我们可以简单地求解其导数为
        - 摘要：通过式（5.12）给出解的系统方程被称为正规方程；equation）。计算式（5.12）构成了一个简单的机器学习算法。图5.1展；示了线性回归算法的使用示例。
          關鍵詞：给出解的系统方程被称为正规方程, 构成了一个简单的机器学习算法, 计算式, 示了线性回归算法的使用示例, 通过式
        - 摘要：（normal
          關鍵詞：
        - 摘要：图5.1　一个线性回归问题，其中训练集包括10个数据点，每个数据点包含一个特征。因为只有；一个特征，权重向量 w 也只有一个要学习的参数w 1 。（左）我们可以观察到线性回归学习w 1；，从而使得直线y=w 1 x能够尽量接近穿过所有的训练点。（右）标注的点表示由正规方程学习
          關鍵詞：也只有一个要学习的参数, 因为只有, 从而使得直线, 权重向量, 个数据点
        - 摘要：值得注意的是，术语线性回归 （linear regression）通常用来指稍微复杂；一些，附加额外参数（截距项b）的模型。在这个模型中，
          關鍵詞：一些, 截距项, 通常用来指稍微复杂, 的模型, 值得注意的是
        - 摘要：因此从参数到预测的映射仍是一个线性函数，而从特征到预测的映射是；一个仿射函数。如此扩展到仿射函数意味着模型预测的曲线仍然看起来；像是一条直线，只是这条直线没必要经过原点。除了通过添加偏置参数
          關鍵詞：一个仿射函数, 而从特征到预测的映射是, 只是这条直线没必要经过原点, 如此扩展到仿射函数意味着模型预测的曲线仍然看起来, 除了通过添加偏置参数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；b，我们还可以使用仅含权重的模型，但是 x 需要增加一项永远为1的元；素。对应于额外1的权重起到了偏置参数的作用。当我们在本书中提到
          關鍵詞：但是, 我们还可以使用仅含权重的模型, 的元, 对应于额外, 当我们在本书中提到
        - 摘要：截距项b通常被称为仿射变换的偏置  （bias）参数。这个术语的命名源；自该变换的输出在没有任何输入时会偏移b。它和统计偏差中指代统计；估计算法的某个量的期望估计偏离真实值的意思是不一样的。
          關鍵詞：参数, 截距项, 估计算法的某个量的期望估计偏离真实值的意思是不一样的, 通常被称为仿射变换的偏置, 自该变换的输出在没有任何输入时会偏移
        - 摘要：线性回归当然是一个极其简单且有局限的学习算法，但是它提供了一个；说明学习算法如何工作的例子。在接下来的章节中，我们将会介绍一些；设计学习算法的基本原则，并说明如何使用这些原则来构建更复杂的学
          關鍵詞：并说明如何使用这些原则来构建更复杂的学, 线性回归当然是一个极其简单且有局限的学习算法, 我们将会介绍一些, 说明学习算法如何工作的例子, 在接下来的章节中
        - 摘要：5.1.1　任务T
          關鍵詞：任务
        - 摘要：5.1.2　性能度量P
          關鍵詞：性能度量
        - 摘要：5.1.3　经验E
          關鍵詞：经验
        - 摘要：5.1.4　示例：线性回归
          關鍵詞：示例, 线性回归
    5.2：容量、过拟合和欠拟合
        - 摘要：5.2.1　没有免费午餐定理
          關鍵詞：没有免费午餐定理
        - 摘要：5.2.2　正则化
          關鍵詞：正则化
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；5.3　超参数和验证集
          關鍵詞：超参数和验证集
        - 摘要：5.3.1　交叉验证
          關鍵詞：交叉验证
        - 摘要：机器学习的主要挑战是我们的算法必须能够在先前未观测到的新输入上；表现良好，而不只是在训练集上表现良好。在先前未观测到的输入上表；现良好的能力被称为泛化 （generaliza-tion）。
          關鍵詞：现良好的能力被称为泛化, 在先前未观测到的输入上表, 表现良好, 机器学习的主要挑战是我们的算法必须能够在先前未观测到的新输入上, 而不只是在训练集上表现良好
        - 摘要：通常情况下，训练机器学习模型时，我们可以使用某个训练集，在训练；集上计算一些被称为训练误差  （training  error）的度量误差，目标是降；低训练误差。到目前为止，我们讨论的是一个简单的优化问题。机器学
          關鍵詞：通常情况下, 目标是降, 我们讨论的是一个简单的优化问题, 低训练误差, 在训练
        - 摘要：通常，我们度量模型在训练集中分出来的测试集 （test set）样本上的性；能，来评估机器学习模型的泛化误差。
          關鍵詞：我们度量模型在训练集中分出来的测试集, 通常, 样本上的性, 来评估机器学习模型的泛化误差
        - 摘要：在我们的线性回归示例中，通过最小化训练误差来训练模型，
          關鍵詞：通过最小化训练误差来训练模型, 在我们的线性回归示例中
        - 摘要：但是我们真正关注的是测试误差
          關鍵詞：但是我们真正关注的是测试误差
        - 摘要：。
          關鍵詞：
        - 摘要：当我们只能观测到训练集时，如何才能影响测试集的性能呢？统计学习；理论  （statistical  learning  theory）提供了一些答案。如果训练集和测试；集的数据是任意收集的，那么我们能够做的确实很有限。如果可以对训
          關鍵詞：当我们只能观测到训练集时, 集的数据是任意收集的, 如何才能影响测试集的性能呢, 理论, 提供了一些答案
        - 摘要：训练集和测试集数据通过数据集上被称为数据生成过程；（data；generating  process）的概率分布生成。通常，我们会做一系列被统称为
          關鍵詞：通常, 我们会做一系列被统称为, 训练集和测试集数据通过数据集上被称为数据生成过程, 的概率分布生成
        - 摘要：我们能观察到训练误差和测试误差之间的直接联系是，随机模型训练误；差的期望和该模型测试误差的期望是一样的。假设我们有概率分布p（；x  ，y），从中重复采样生成训练集和测试集。对于某个固定的  w  ，训
          關鍵詞：假设我们有概率分布, 我们能观察到训练误差和测试误差之间的直接联系是, 差的期望和该模型测试误差的期望是一样的, 从中重复采样生成训练集和测试集, 对于某个固定的
        - 摘要：当然，在使用机器学习算法时，我们不会提前固定参数，然后采样得到；两个数据集。我们采样得到训练集，然后挑选参数去降低训练集误差，；然后采样得到测试集。在这个过程中，测试误差期望会大于或等于训练
          關鍵詞：然后采样得到测试集, 我们采样得到训练集, 在这个过程中, 测试误差期望会大于或等于训练, 然后采样得到
        - 摘要：（1）降低训练误差。
          關鍵詞：降低训练误差
        - 摘要：（2）缩小训练误差和测试误差的差距。
          關鍵詞：缩小训练误差和测试误差的差距
        - 摘要：这两个因素对应机器学习的两个主要挑战：欠拟合  （underfitting）和过；拟合  （overfitting）。欠拟合是指模型不能在训练集上获得足够低的误；差，而过拟合是指训练误差和测试误差之间的差距太大。
          關鍵詞：这两个因素对应机器学习的两个主要挑战, 而过拟合是指训练误差和测试误差之间的差距太大, 和过, 欠拟合是指模型不能在训练集上获得足够低的误, 欠拟合
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；通过调整模型的容量  （capacity），我们可以控制模型是否偏向于过拟；合或者欠拟合。通俗来讲，模型的容量是指其拟合各种函数的能力。容
          關鍵詞：通过调整模型的容量, 模型的容量是指其拟合各种函数的能力, 我们可以控制模型是否偏向于过拟, 通俗来讲, 合或者欠拟合
        - 摘要：一种控制训练算法容量的方法是选择假设空间 （hypothesis space），即；学习算法可以选择为解决方案的函数集。例如，线性回归函数将关于其；输入的所有线性函数作为假设空间。广义线性回归的假设空间包括多项
          關鍵詞：一种控制训练算法容量的方法是选择假设空间, 学习算法可以选择为解决方案的函数集, 线性回归函数将关于其, 例如, 广义线性回归的假设空间包括多项
        - 摘要：一次多项式提供了我们已经熟悉的线性回归模型，其预测如下
          關鍵詞：其预测如下, 一次多项式提供了我们已经熟悉的线性回归模型
        - 摘要：通过引入x  2  作为线性回归模型的另一个特征，我们能够学习关于x的二；次函数模型：
          關鍵詞：作为线性回归模型的另一个特征, 我们能够学习关于, 的二, 通过引入, 次函数模型
        - 摘要：尽管该模型是输入的二次函数，但输出仍是参数的线性函数，因此我们；仍然可以用正规方程得到模型的闭解。我们可以继续添加x的更高幂作；为额外特征，例如下面的9次多项式：
          關鍵詞：为额外特征, 因此我们, 仍然可以用正规方程得到模型的闭解, 次多项式, 例如下面的
        - 摘要：当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的；数量时，算法效果通常会最佳。容量不足的模型不能解决复杂任务。容；量高的模型能够解决复杂的任务，但是当其容量高于任务所需时，有可
          關鍵詞：容量不足的模型不能解决复杂任务, 有可, 算法效果通常会最佳, 当机器学习算法的容量适合于所执行任务的复杂度和所提供训练数据的, 数量时
        - 摘要：图5.2展示了这个原理的使用情况。我们比较了线性、二次和9次预测器；拟合真实二次函数的效果。线性函数无法刻画真实函数的曲率，所以欠；拟合。9次函数能够表示正确的函数，但是因为训练参数比训练样本还
          關鍵詞：展示了这个原理的使用情况, 所以欠, 次预测器, 但是因为训练参数比训练样本还, 二次和
        - 摘要：据上。
          關鍵詞：据上
        - 摘要：图5.2　我们用3个模型拟合了这个训练集的样本。训练数据是通过随机抽取x然后用二次函数确；定性地生成y来合成的。（左）用一个线性函数拟合数据会导致欠拟合——它无法捕捉数据中的；曲率信息。（中）用二次函数拟合数据在未观察到的点上泛化得很好，这并不会导致明显的欠
          關鍵詞：来合成的, 这并不会导致明显的欠, 用一个线性函数拟合数据会导致欠拟合, 它无法捕捉数据中的, 个模型拟合了这个训练集的样本
        - 摘要：到目前为止，我们探讨了通过改变输入特征的数目和加入这些特征对应；的参数，改变模型的容量。事实上，还有很多方法可以改变模型的容；量。容量不仅取决于模型的选择。模型规定了调整参数降低训练目标
          關鍵詞：改变模型的容量, 的参数, 事实上, 我们探讨了通过改变输入特征的数目和加入这些特征对应, 到目前为止
        - 摘要：提高机器学习模型泛化的现代思想可以追溯到早在托勒密时期的哲学家；的思想。许多早期的学者提出一个简约原则，现在广泛被称为奥卡姆剃；刀  （Occam's  razor）（c．1287-1387）。该原则指出，在同样能够解释
          關鍵詞：的思想, 现在广泛被称为奥卡姆剃, 该原则指出, 提高机器学习模型泛化的现代思想可以追溯到早在托勒密时期的哲学家, 许多早期的学者提出一个简约原则
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；统计学习理论提供了量化模型容量的不同方法。在这些方法中，最有名；的是Vapnik-Chervonenkis维度  （Vapnik-Chervonenkis
          關鍵詞：的是, 最有名, 在这些方法中, 维度, 统计学习理论提供了量化模型容量的不同方法
        - 摘要：量化模型的容量使得统计学习理论可以进行量化预测。统计学习理论中；最重要的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量；增长而增长，但随着训练样本增多而下降（Vapnik  and  Chervonenkis，
          關鍵詞：统计学习理论中, 增长而增长, 最重要的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量, 量化模型的容量使得统计学习理论可以进行量化预测, 但随着训练样本增多而下降
        - 摘要：我们必须记住虽然更简单的函数更可能泛化（训练误差和测试误差的差；距小），但我们仍然需要选择一个充分复杂的假设以达到低的训练误；差。通常，当模型容量上升时，训练误差会下降，直到其渐近最小可能
          關鍵詞：通常, 训练误差和测试误差的差, 训练误差会下降, 但我们仍然需要选择一个充分复杂的假设以达到低的训练误, 我们必须记住虽然更简单的函数更可能泛化
        - 摘要：图5.3　容量和误差之间的典型关系。训练误差和测试误差表现得非常不同。在图的左端，训练；误差和泛化误差都非常高，这是 欠拟合机制 （underfitting regime）。当我们增加容量时，训练；误差减小，但是训练误差和泛化误差之间的间距却不断扩大。最终，这个间距的大小超过了训
          關鍵詞：训练, 但是训练误差和泛化误差之间的间距却不断扩大, 误差和泛化误差都非常高, 这是, 欠拟合机制
        - 摘要：练误差的下降，我们进入到了 过拟合机制 （overfitting regime），其中容量过大，超过了 最优；容量 （optimal capacity）
          關鍵詞：最优, 练误差的下降, 超过了, 容量, 过拟合机制
        - 摘要：为考虑容量任意高的极端情况，我们介绍非参数  （non-parametric）模；型的概念。至此，我们只探讨过参数模型，例如线性回归。参数模型学；习的函数在观测到新数据前，参数向量的分量个数是有限且固定的。非
          關鍵詞：我们介绍非参数, 参数向量的分量个数是有限且固定的, 习的函数在观测到新数据前, 我们只探讨过参数模型, 型的概念
        - 摘要：有时，非参数模型仅是一些不能实际实现的理论抽象（比如搜索所有可；能概率分布的算法）。然而，我们也可以设计一些实用的非参数模型，；使它们的复杂度和训练集大小有关。这种算法的一个示例是最近邻回归
          關鍵詞：我们也可以设计一些实用的非参数模型, 有时, 然而, 比如搜索所有可, 非参数模型仅是一些不能实际实现的理论抽象
        - 摘要：最后，我们也可以将参数学习算法嵌入另一个增加参数数目的算法来创；建非参数学习算法。例如，我们可以想象这样一个算法，外层循环调整；多项式的次数，内层循环通过线性回归学习模型。
          關鍵詞：我们可以想象这样一个算法, 多项式的次数, 建非参数学习算法, 例如, 内层循环通过线性回归学习模型
        - 摘要：理想模型假设我们能够预先知道生成数据的真实概率分布。然而这样的；模型仍然会在很多问题上发生一些错误，因为分布中仍然会有一些噪；声。在监督学习中，从 x  到y的映射可能内在是随机的，或者y可能是其
          關鍵詞：可能是其, 的映射可能内在是随机的, 因为分布中仍然会有一些噪, 在监督学习中, 然而这样的
        - 摘要：训练误差和泛化误差会随训练集的大小发生变化。泛化误差的期望从不；会因训练样本数目的增加而增加。对于非参数模型而言，更多的数据会；得到更好的泛化能力，直到达到最佳可能的泛化误差。任何模型容量小
          關鍵詞：泛化误差的期望从不, 得到更好的泛化能力, 更多的数据会, 任何模型容量小, 直到达到最佳可能的泛化误差
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；和泛化误差之间存在很大的差距。在这种情况下，我们可以通过收集更；多的训练样本来缩小差距。
          關鍵詞：我们可以通过收集更, 和泛化误差之间存在很大的差距, 在这种情况下, 多的训练样本来缩小差距
        - 摘要：图5.4　训练集大小对训练误差、测试误差以及最优容量的影响。通过给一个5阶多项式添加适；当大小的噪声，我们构造了一个合成的回归问题，生成单个测试集，然后生成一些不同尺寸的；训练集。为了描述95％置信区间的误差条，对于每一个尺寸，我们生成了40个不同的训练集。
          關鍵詞：训练集, 置信区间的误差条, 我们生成了, 测试误差以及最优容量的影响, 当大小的噪声
        - 摘要：5.2.1　没有免费午餐定理
          關鍵詞：没有免费午餐定理
        - 摘要：学习理论表明机器学习算法能够在有限个训练集样本中很好地泛化。这；似乎违背一些基本的逻辑原则。归纳推理，或是从一组有限的样本中推
          關鍵詞：学习理论表明机器学习算法能够在有限个训练集样本中很好地泛化, 或是从一组有限的样本中推, 归纳推理, 似乎违背一些基本的逻辑原则
        - 摘要：断一般的规则，在逻辑上不是很有效。为了逻辑地推断一个规则去描述；集合中的元素，我们必须具有集合中每个元素的信息。
          關鍵詞：断一般的规则, 集合中的元素, 为了逻辑地推断一个规则去描述, 在逻辑上不是很有效, 我们必须具有集合中每个元素的信息
        - 摘要：在一定程度上，机器学习仅通过概率法则就可以避免这个问题，而无须；使用纯逻辑推理整个确定性法则。机器学习保证找到一个在所关注的大；多数样本上可能正确的规则。
          關鍵詞：在一定程度上, 机器学习仅通过概率法则就可以避免这个问题, 多数样本上可能正确的规则, 使用纯逻辑推理整个确定性法则, 而无须
        - 摘要：可惜，即使这样也不能解决整个问题。机器学习的没有免费午餐定理；（no free lunch theorem）表明（Wolpert，1996），在所有可能的数据生；成分布上平均之后，每一个分类算法在未事先观测的点上都有相同的错
          關鍵詞：成分布上平均之后, 每一个分类算法在未事先观测的点上都有相同的错, 机器学习的没有免费午餐定理, 在所有可能的数据生, 表明
        - 摘要：幸运的是，这些结论仅在我们考虑所有可能的数据生成分布时才成立。；在真实世界应用中，如果我们对遇到的概率分布进行假设，那么可以设；计在这些分布上效果良好的学习算法。
          關鍵詞：在真实世界应用中, 幸运的是, 如果我们对遇到的概率分布进行假设, 那么可以设, 计在这些分布上效果良好的学习算法
        - 摘要：这意味着机器学习研究的目标不是找一个通用学习算法或是绝对最好的；学习算法，而是理解什么样的分布与人工智能获取经验的“真实世界”相；关，以及什么样的学习算法在我们关注的数据生成分布上效果最好。
          關鍵詞：这意味着机器学习研究的目标不是找一个通用学习算法或是绝对最好的, 真实世界, 学习算法, 以及什么样的学习算法在我们关注的数据生成分布上效果最好, 而是理解什么样的分布与人工智能获取经验的
        - 摘要：5.2.2　正则化
          關鍵詞：正则化
        - 摘要：没有免费午餐定理暗示我们必须在特定任务上设计性能良好的机器学习；算法。我们建立一组学习算法的偏好来达到这个要求。当这些偏好和我；们希望算法解决的学习问题相吻合时，性能会更好。
          關鍵詞：算法, 当这些偏好和我, 们希望算法解决的学习问题相吻合时, 没有免费午餐定理暗示我们必须在特定任务上设计性能良好的机器学习, 我们建立一组学习算法的偏好来达到这个要求
        - 摘要：至此，我们具体讨论修改学习算法的方法，只有通过增加或减少学习算；法可选假设空间的函数来增加或减少模型的容量。所列举的一个具体示；例是线性回归增加或减少多项式的次数。到目前为止讨论的观点都是过
          關鍵詞：我们具体讨论修改学习算法的方法, 到目前为止讨论的观点都是过, 只有通过增加或减少学习算, 法可选假设空间的函数来增加或减少模型的容量, 所列举的一个具体示
        - 摘要：算法的效果不仅很大程度上受影响于假设空间的函数数量，也取决于这；些函数的具体形式。我们已经讨论的学习算法（线性回归）具有包含其；输入的线性函数集的假设空间。对于输入和输出确实接近线性相关的问
          關鍵詞：我们已经讨论的学习算法, 输入的线性函数集的假设空间, 具有包含其, 些函数的具体形式, 也取决于这
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；例如，我们用线性回归，从x预测sin(x)，效果不会好。因此我们可以通；过两种方式控制算法的性能，一是允许使用的函数种类，二是这些函数
          關鍵詞：过两种方式控制算法的性能, 因此我们可以通, 例如, 我们用线性回归, 二是这些函数
        - 摘要：在假设空间中，相比于某一个学习算法，我们可能更偏好另一个学习算；法。这意味着两个函数都是符合条件的，但是我们更偏好其中一个。只；有非偏好函数比偏好函数在训练数据集上效果明显好很多时，我们才会
          關鍵詞：有非偏好函数比偏好函数在训练数据集上效果明显好很多时, 但是我们更偏好其中一个, 我们可能更偏好另一个学习算, 这意味着两个函数都是符合条件的, 我们才会
        - 摘要：例如，可以加入权重衰减  （weight  decay）来修改线性回归的训练标；准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和；J（ w ），其偏好于平方L 2 范数较小的权重。具体如下
          關鍵詞：具体如下, 其偏好于平方, 范数较小的权重, 可以加入权重衰减, 例如
        - 摘要：其中λ是提前挑选的值，控制我们偏好小范数权重的程度。当λ=0时，我；们没有任何偏好。越大的λ偏好范数越小的权重。最小化J（ w ）可以看；作拟合训练数据和偏好小权重范数之间的权衡。这会使得解决方案的斜
          關鍵詞：们没有任何偏好, 作拟合训练数据和偏好小权重范数之间的权衡, 其中, 偏好范数越小的权重, 这会使得解决方案的斜
        - 摘要：图5.5　我们使用高阶多项式回归模型来拟合图5.2中的训练样本。真实函数是二次的，但是在这；里只使用9阶多项式。我们通过改变权重衰减的量来避免高阶模型的过拟合问题。（左）当λ非；常大时，我们可以强迫模型学习到一个没有斜率的函数。由于它只能表示一个常数函数，所以
          關鍵詞：但是在这, 真实函数是二次的, 我们可以强迫模型学习到一个没有斜率的函数, 我们使用高阶多项式回归模型来拟合图, 我们通过改变权重衰减的量来避免高阶模型的过拟合问题
        - 摘要：问题）时，这个9阶多项式会导致严重的过拟合，这和我们在图5.2中看到的一样
          關鍵詞：中看到的一样, 这和我们在图, 问题, 阶多项式会导致严重的过拟合, 这个
        - 摘要：更一般地，正则化一个学习函数 f（x；θ）  的模型，我们可以给代价函；数添加被称为正则化项  （regularizer）的惩罚。在权重衰减的例子中，；正则化是
          關鍵詞：我们可以给代价函, 更一般地, 的模型, 在权重衰减的例子中, 正则化是
        - 摘要：表示对函数的偏好是比增减假设空间的成员函数更一般地控制模型容量；的方法。我们可以将去掉假设空间中的某个函数看作对不赞成这个函数；的无限偏好。
          關鍵詞：的方法, 表示对函数的偏好是比增减假设空间的成员函数更一般地控制模型容量, 我们可以将去掉假设空间中的某个函数看作对不赞成这个函数, 的无限偏好
        - 摘要：在权重衰减的示例中，通过在最小化的目标中额外增加一项，我们明确；地表示了偏好权重较小的线性函数。有很多其他方法隐式或显式地表示；对不同解的偏好。总而言之，这些不同的方法都被称为正则化
          關鍵詞：总而言之, 地表示了偏好权重较小的线性函数, 对不同解的偏好, 在权重衰减的示例中, 有很多其他方法隐式或显式地表示
        - 摘要：没有免费午餐定理已经清楚地阐述了没有最优的学习算法，特别是没有；最优的正则化形式。反之，我们必须挑选一个非常适合于我们所要解决；的任务的正则形式。深度学习中普遍的（特别是本书中的）理念是大量
          關鍵詞：的任务的正则形式, 深度学习中普遍的, 我们必须挑选一个非常适合于我们所要解决, 特别是没有, 没有免费午餐定理已经清楚地阐述了没有最优的学习算法
        - 摘要：5.2.1　没有免费午餐定理
          關鍵詞：没有免费午餐定理
        - 摘要：5.2.2　正则化
          關鍵詞：正则化
    5.3：超参数和验证集
        - 摘要：大多数机器学习算法都有超参数，可以设置来控制算法行为。超参数的；值不是通过学习算法本身学习出来的（尽管我们可以设计一个嵌套的学；习过程，一个学习算法为另一个学习算法学出最优超参数）。
          關鍵詞：可以设置来控制算法行为, 一个学习算法为另一个学习算法学出最优超参数, 超参数的, 大多数机器学习算法都有超参数, 尽管我们可以设计一个嵌套的学
        - 摘要：在图5.2所示的多项式回归示例中，有一个超参数，即多项式的次数，；作为容量 超参数。控制权重衰减程度的λ是另一个超参数。
          關鍵詞：在图, 即多项式的次数, 有一个超参数, 控制权重衰减程度的, 作为容量
        - 摘要：有时一个选项被设为学习算法不用学习的超参数，是因为它太难优化；了。更多的情况是，该选项必须是超参数，因为它不适合在训练集上学；习。这适用于控制模型容量的所有超参数。如果在训练集上学习超参
          關鍵詞：是因为它太难优化, 该选项必须是超参数, 有时一个选项被设为学习算法不用学习的超参数, 这适用于控制模型容量的所有超参数, 因为它不适合在训练集上学
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；数，这些超参数总是趋向于最大可能的模型容量，导致过拟合（见图；5.3）。例如，相比低次多项式和正的权重衰减设定，更高次的多项式
          關鍵詞：更高次的多项式, 相比低次多项式和正的权重衰减设定, 例如, 见图, 这些超参数总是趋向于最大可能的模型容量
        - 摘要：为了解决这个问题，我们需要一个训练算法观测不到的验证集；（validation set）样本。
          關鍵詞：为了解决这个问题, 我们需要一个训练算法观测不到的验证集, 样本
        - 摘要：早先我们讨论过和训练数据相同分布的样本组成的测试集，它可以用来；估计学习过程完成之后的学习器的泛化误差。其重点在于测试样本不能；以任何形式参与到模型的选择中，包括设定超参数。基于这个原因，测
          關鍵詞：基于这个原因, 以任何形式参与到模型的选择中, 它可以用来, 其重点在于测试样本不能, 估计学习过程完成之后的学习器的泛化误差
        - 摘要：在实际中，当相同的测试集已在很多年中重复地用于评估不同算法的性；能，并且考虑学术界在该测试集上的各种尝试，我们最后可能也会对测；试集有着乐观的估计。基准会因之变得陈旧，而不能反映系统的真实性
          關鍵詞：在实际中, 我们最后可能也会对测, 基准会因之变得陈旧, 试集有着乐观的估计, 而不能反映系统的真实性
        - 摘要：5.3.1　交叉验证
          關鍵詞：交叉验证
        - 摘要：将数据集分成固定的训练集和固定的测试集后，若测试集的误差很小，；这将是有问题的。一个小规模的测试集意味着平均测试误差估计的统计；不确定性，使得很难判断算法A是否比算法B在给定的任务上做得更
          關鍵詞：不确定性, 若测试集的误差很小, 将数据集分成固定的训练集和固定的测试集后, 一个小规模的测试集意味着平均测试误差估计的统计, 是否比算法
        - 摘要：当数据集有十万计或者更多的样本时，这不会是一个严重的问题。当数；据集太小时，也有替代方法允许我们使用所有的样本估计平均测试误；差，代价是增加了计算量。这些过程是基于在原始数据上随机采样或分
          關鍵詞：当数, 这不会是一个严重的问题, 这些过程是基于在原始数据上随机采样或分, 当数据集有十万计或者更多的样本时, 据集太小时
        - 摘要：离出的不同数据集上重复训练和测试的想法。最常见的是k-折交叉验证；过程，如算法5.1所示，将数据集分成k个不重合的子集。测试误差可以；估计为k次计算后的平均测试误差。在第i次测试时，数据的第i个子集用
          關鍵詞：个不重合的子集, 估计为, 最常见的是, 离出的不同数据集上重复训练和测试的想法, 折交叉验证
        - 摘要：算法5.1  　k-折交叉验证算法。当给定数据集   对于简单的训练/测试；或训练/验证分割而言太小难以产生泛化误差的准确估计时（因为在小；的测试集上，L可能具有过高的方差），k-折交叉验证算法可以用于估
          關鍵詞：折交叉验证算法可以用于估, 当给定数据集, 算法, 验证分割而言太小难以产生泛化误差的准确估计时, 对于简单的训练
        - 摘要：Define KFoldXV(
          關鍵詞：
        - 摘要：,A,L,k)：
          關鍵詞：
        - 摘要：Require：
          關鍵詞：
        - 摘要：为给定数据集，其中元素为z (i)
          關鍵詞：其中元素为, 为给定数据集
        - 摘要：Require：  A为学习算法，可视为一个函数（使用数据集作为输入，输；出一个学好的函数）
          關鍵詞：为学习算法, 可视为一个函数, 出一个学好的函数, 使用数据集作为输入
        - 摘要：Require：  L为损失函数，可视为来自学好的函数f，将样本；映射到  中标量的函数
          關鍵詞：将样本, 为损失函数, 中标量的函数, 映射到, 可视为来自学好的函数
        - 摘要：Require： k为折数
          關鍵詞：为折数
        - 摘要：将  分为k个互斥子集  i ，它们的并集为
          關鍵詞：个互斥子集, 它们的并集为, 分为
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；for i from 1 to k do
          關鍵詞：
        - 摘要：for z (j) in
          關鍵詞：
        - 摘要：i do
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：Return e
          關鍵詞：
        - 摘要：5.3.1　交叉验证
          關鍵詞：交叉验证
    5.4：估计、偏差和方差
        - 摘要：5.4.1　点估计
          關鍵詞：点估计
        - 摘要：5.4.2　偏差
          關鍵詞：偏差
        - 摘要：5.4.3　方差和标准差
          關鍵詞：方差和标准差
        - 摘要：5.4.4　权衡偏差和方差以最小化均方误差
          關鍵詞：权衡偏差和方差以最小化均方误差
        - 摘要：5.4.5　一致性
          關鍵詞：一致性
        - 摘要：统计领域为我们提供了很多工具来实现机器学习目标，不仅可以解决训；练集上的任务，还可以泛化。基本的概念，例如参数估计、偏差和方；差，对于正式地刻画泛化、欠拟合和过拟合都非常有帮助。
          關鍵詞：不仅可以解决训, 欠拟合和过拟合都非常有帮助, 例如参数估计, 基本的概念, 统计领域为我们提供了很多工具来实现机器学习目标
        - 摘要：5.4.1　点估计
          關鍵詞：点估计
        - 摘要：点估计试图为一些感兴趣的量提供单个“最优”预测。一般地，感兴趣的；量可以是单个参数，或是某些参数模型中的一个向量参数，例如第5.1.4；节线性回归中的权重，但是也有可能是整个函数。
          關鍵詞：最优, 感兴趣的, 节线性回归中的权重, 但是也有可能是整个函数, 一般地
        - 摘要：为了区分参数估计和真实值，我们习惯将参数 θ 的点估计表示为  。
          關鍵詞：为了区分参数估计和真实值, 的点估计表示为, 我们习惯将参数
        - 摘要：令；（point estimator）或统计量 （statistics）是这些数据的任意函数：
          關鍵詞：是这些数据的任意函数, 或统计量
        - 摘要：是m个独立同分布（i.i.d.）的数据点。点估计
          關鍵詞：点估计, 的数据点, 个独立同分布
        - 摘要：这个定义不要求g返回一个接近真实 θ 的值，或者g的值域恰好是 θ 的允；许取值范围。点估计的定义非常宽泛，给了估计量的设计者极大的灵活
          關鍵詞：许取值范围, 的值域恰好是, 这个定义不要求, 返回一个接近真实, 或者
        - 摘要：性。虽然几乎所有的函数都可以称为估计量，但是一个良好的估计量的；输出会接近生成训练数据的真实参数 θ 。
          關鍵詞：但是一个良好的估计量的, 输出会接近生成训练数据的真实参数, 虽然几乎所有的函数都可以称为估计量
        - 摘要：现在，我们采取频率派在统计上的观点。换言之，我们假设真实参数  θ；是固定但未知的，而点估计   是数据的函数。由于数据是随机过程采样；出来的，数据的任何函数都是随机的，因此  是一个随机变量。
          關鍵詞：是一个随机变量, 我们假设真实参数, 出来的, 因此, 换言之
        - 摘要：点估计也可以指输入和目标变量之间关系的估计，我们将这种类型的点；估计称为函数估计。
          關鍵詞：我们将这种类型的点, 估计称为函数估计, 点估计也可以指输入和目标变量之间关系的估计
        - 摘要：函数估计  　有时我们会关注函数估计（或函数近似）。这时我们试图；从输入向量 x 预测变量 y 。假设有一个函数f( x )表示 y 和 x 之间的近似；关系。例如，我们可能假设
          關鍵詞：有时我们会关注函数估计, 表示, 之间的近似, 从输入向量, 函数估计
        - 摘要：现在我们回顾点估计最常研究的性质，并探讨这些性质说明了估计的哪；些特点。
          關鍵詞：现在我们回顾点估计最常研究的性质, 些特点, 并探讨这些性质说明了估计的哪
        - 摘要：5.4.2　偏差
          關鍵詞：偏差
        - 摘要：估计的偏差被定义为
          關鍵詞：估计的偏差被定义为
        - 摘要：其中期望作用在所有数据（看作从随机变量采样得到的）上， θ 是用于；，那么估计量；定义数据生成分布的  θ  的真实值。如果
          關鍵詞：如果, 看作从随机变量采样得到的, 定义数据生成分布的, 其中期望作用在所有数据, 的真实值
        - 摘要：被称为是无偏  （unbiased），这意味着
          關鍵詞：这意味着, 被称为是无偏
        - 摘要：，那么估计量   被称为是渐近无偏
          關鍵詞：被称为是渐近无偏, 那么估计量
        - 摘要：（asymptotically unbiased），这意味着
          關鍵詞：这意味着
        - 摘要：。
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；示例：伯努利分布  　考虑一组服从均值为θ的伯努利分布的独立同分布；的样本
          關鍵詞：考虑一组服从均值为, 伯努利分布, 的伯努利分布的独立同分布, 示例, 的样本
        - 摘要：：
          關鍵詞：
        - 摘要：这个分布中参数θ的常用估计量是训练样本的均值：
          關鍵詞：这个分布中参数, 的常用估计量是训练样本的均值
        - 摘要：判断这个估计量是否有偏，我们将式（5.22）代入式（5.20）：
          關鍵詞：代入式, 我们将式, 判断这个估计量是否有偏
        - 摘要：因为bias(
          關鍵詞：因为
        - 摘要：)=0，我们称估计  是无偏的。
          關鍵詞：我们称估计, 是无偏的
        - 摘要：示例：均值的高斯分布估计
          關鍵詞：示例, 均值的高斯分布估计
        - 摘要：现在，考虑一组独立同分布的样本；，其中
          關鍵詞：现在, 其中, 考虑一组独立同分布的样本
        - 摘要：服从高斯分布；。回顾高斯概率密度函数如下：
          關鍵詞：服从高斯分布, 回顾高斯概率密度函数如下
        - 摘要：高斯均值参数的常用估计量被称为样本均值 （sample mean）：
          關鍵詞：高斯均值参数的常用估计量被称为样本均值
        - 摘要：判断样本均值是否有偏，我们再次计算它的期望：
          關鍵詞：我们再次计算它的期望, 判断样本均值是否有偏
        - 摘要：因此我们发现样本均值是高斯均值参数的无偏估计量。
          關鍵詞：因此我们发现样本均值是高斯均值参数的无偏估计量
        - 摘要：示例：高斯分布方差估计 　本例中，我们比较高斯分布方差参数σ  2  的；两个不同估计。我们探讨是否有一个是有偏的。
          關鍵詞：示例, 本例中, 我们探讨是否有一个是有偏的, 高斯分布方差估计, 两个不同估计
        - 摘要：我们考虑的第一个方差估计被称为样本方差 （sample variance）：
          關鍵詞：我们考虑的第一个方差估计被称为样本方差
        - 摘要：其中  是样本均值。更形式化地，我们对计算感兴趣
          關鍵詞：是样本均值, 我们对计算感兴趣, 更形式化地, 其中
        - 摘要：我们首先估计项
          關鍵詞：我们首先估计项
        - 摘要：：
          關鍵詞：
        - 摘要：回到式（5.37），我们可以得出   的偏差是
          關鍵詞：我们可以得出, 回到式, 的偏差是
        - 摘要：。因此样本方差
          關鍵詞：因此样本方差
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；是有偏估计。
          關鍵詞：是有偏估计
        - 摘要：无偏样本方差 （unbiased sample variance）估计：
          關鍵詞：估计, 无偏样本方差
        - 摘要：提供了另一种可选方法。正如名字所言，这个估计是无偏的。换言之，；我们会发现
          關鍵詞：我们会发现, 换言之, 这个估计是无偏的, 提供了另一种可选方法, 正如名字所言
        - 摘要：：
          關鍵詞：
        - 摘要：我们有两个估计量：一个是有偏的，另一个是无偏的。尽管无偏估计显；然是令人满意的，但它并不总是“最好”的估计。我们将看到，经常会使；用其他具有重要性质的有偏估计。
          關鍵詞：的估计, 用其他具有重要性质的有偏估计, 我们将看到, 经常会使, 但它并不总是
        - 摘要：5.4.3　方差和标准差
          關鍵詞：方差和标准差
        - 摘要：我们有时会考虑估计量的另一个性质是它作为数据样本的函数，期望的；变化程度是多少。正如我们可以计算估计量的期望来决定它的偏差，我；们也可以计算它的方差。估计量的方差 （variance）就是一个方差：
          關鍵詞：变化程度是多少, 正如我们可以计算估计量的期望来决定它的偏差, 期望的, 估计量的方差, 我们有时会考虑估计量的另一个性质是它作为数据样本的函数
        - 摘要：其中随机变量是训练集。另外，方差的平方根被称为标准差  （standard；error），记作SE（  ）。
          關鍵詞：其中随机变量是训练集, 记作, 另外, 方差的平方根被称为标准差
        - 摘要：估计量的方差或标准差告诉我们，当独立地从潜在的数据生成过程中重；采样数据集时，如何期望估计的变化。正如我们希望估计的偏差较小，；我们也希望其方差较小。
          關鍵詞：我们也希望其方差较小, 正如我们希望估计的偏差较小, 采样数据集时, 当独立地从潜在的数据生成过程中重, 如何期望估计的变化
        - 摘要：当我们使用有限的样本计算任何统计量时，真实参数的估计都是不确定；的，在这个意义下，从相同的分布得到其他样本时，它们的统计量也会；不一样。任何方差估计量的期望程度是我们想量化的误差的来源。
          關鍵詞：它们的统计量也会, 任何方差估计量的期望程度是我们想量化的误差的来源, 不一样, 在这个意义下, 真实参数的估计都是不确定
        - 摘要：均值的标准差被记作
          關鍵詞：均值的标准差被记作
        - 摘要：其中σ  2  是样本 x  (i) 的真实方差。标准差通常被记作σ。可惜，样本方差；的平方根和方差无偏估计的平方根都不是标准差的无偏估计。这两种计；算方法都倾向于低估真实的标准差，但仍用于实际中。相较而言，方差
          關鍵詞：是样本, 的真实方差, 标准差通常被记作, 相较而言, 但仍用于实际中
        - 摘要：均值的标准差在机器学习实验中非常有用。我们通常用测试集样本的误；差均值来估计泛化误差。测试集中样本的数量决定了这个估计的精确；度。中心极限定理告诉我们均值会接近一个高斯分布，我们可以用标准
          關鍵詞：中心极限定理告诉我们均值会接近一个高斯分布, 我们可以用标准, 均值的标准差在机器学习实验中非常有用, 差均值来估计泛化误差, 我们通常用测试集样本的误
        - 摘要：的高斯分布。在机器学习；以上区间是基于均值   和方差；实验中，我们通常说算法A比算法B好，是指算法A的误差的95％置信区
          關鍵詞：和方差, 比算法, 实验中, 的误差的, 置信区
        - 摘要：示例：伯努利分布
          關鍵詞：示例, 伯努利分布
        - 摘要：我们再次考虑从伯努利分布（回顾；中独立同分布采样出来的一组样本
          關鍵詞：回顾, 中独立同分布采样出来的一组样本, 我们再次考虑从伯努利分布
        - 摘要：。这次我们关注估计
          關鍵詞：这次我们关注估计
        - 摘要：的方差：
          關鍵詞：的方差
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；估计量方差的下降速率是关于数据集样本数目m的函数。这是常见估计；量的普遍性质，在探讨一致性（参见第5.4.5节）时，我们会继续讨论。
          關鍵詞：这是常见估计, 的函数, 参见第, 估计量方差的下降速率是关于数据集样本数目, 在探讨一致性
        - 摘要：5.4.4　权衡偏差和方差以最小化均方误差
          關鍵詞：权衡偏差和方差以最小化均方误差
        - 摘要：偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函；数或参数的误差期望，而方差度量着数据上任意特定采样可能导致的估；计期望的偏差。
          關鍵詞：数或参数的误差期望, 计期望的偏差, 偏差和方差度量着估计量的两个不同误差来源, 而方差度量着数据上任意特定采样可能导致的估, 偏差度量着偏离真实函
        - 摘要：当我们可以在一个偏差更大的估计和一个方差更大的估计中进行选择；时，会发生什么呢？我们该如何选择？例如，想象我们希望近似图5.2；中的函数，如果只可以选择一个偏差较大的估计或一个方差较大的估
          關鍵詞：我们该如何选择, 例如, 如果只可以选择一个偏差较大的估计或一个方差较大的估, 中的函数, 想象我们希望近似图
        - 摘要：判断这种权衡最常用的方法是交叉验证。经验上，交叉验证在真实世界；的许多任务中都非常成功。另外，我们也可以比较这些估计的均方误差；（mean squared error，MSE）：
          關鍵詞：经验上, 的许多任务中都非常成功, 我们也可以比较这些估计的均方误差, 判断这种权衡最常用的方法是交叉验证, 交叉验证在真实世界
        - 摘要：MSE度量着估计和真实参数θ之间平方误差的总体期望偏差。如式；（5.54）所示，MSE估计包含了偏差和方差。理想的估计具有较小的；MSE或是在检查中会稍微约束它们的偏差和方差。
          關鍵詞：之间平方误差的总体期望偏差, 度量着估计和真实参数, 如式, 估计包含了偏差和方差, 或是在检查中会稍微约束它们的偏差和方差
        - 摘要：偏差和方差的关系与机器学习容量、欠拟合和过拟合的概念紧密相联。；用MSE度量泛化误差（偏差和方差对于泛化误差都是有意义的）时，增；加容量会增加方差，降低偏差。如图5.6所示，我们再次在关于容量的
          關鍵詞：欠拟合和过拟合的概念紧密相联, 加容量会增加方差, 偏差和方差的关系与机器学习容量, 我们再次在关于容量的, 所示
        - 摘要：图5.6　当容量增大（x轴）时，偏差（用点表示）随之减小，而方差（虚线）随之增大，使得；泛化误差（加粗曲线）产生了另一种U形。如果我们沿着轴改变容量，会发现最佳容量，当容；量小于最佳容量会呈现欠拟合，大于时导致过拟合。这种关系与第5.2节以及图5.3中讨论的容
          關鍵詞：泛化误差, 量小于最佳容量会呈现欠拟合, 随之减小, 中讨论的容, 当容
        - 摘要：5.4.5　一致性
          關鍵詞：一致性
        - 摘要：目前我们已经探讨了固定大小训练集下不同估计量的性质。通常，我们；也会关注训练数据增多后估计量的效果。特别地，我们希望当数据集中；数据点的数量m增加时，点估计会收敛到对应参数的真实值。更形式化
          關鍵詞：通常, 我们希望当数据集中, 特别地, 增加时, 我们
        - 摘要：符号plim表示依概率收敛，即对于任意的
          關鍵詞：表示依概率收敛, 符号, 即对于任意的
        - 摘要：＞0，当m→∞时，有；。式（5.55）表示的条件被称为一致性
          關鍵詞：表示的条件被称为一致性
        - 摘要：（consistency）。有时它是指弱一致性，强一致性是指几乎必然；（almost；sure）从   收敛到θ。几乎必然收敛  （almost
          關鍵詞：有时它是指弱一致性, 几乎必然收敛, 收敛到, 强一致性是指几乎必然
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；一致性保证了估计量的偏差会随数据样本数目的增多而减少。然而，反；过来是不正确的——渐近无偏并不意味着一致性。例如，考虑用包含m
          關鍵詞：过来是不正确的, 然而, 例如, 一致性保证了估计量的偏差会随数据样本数目的增多而减少, 考虑用包含
        - 摘要：。
          關鍵詞：
        - 摘要：5.4.1　点估计
          關鍵詞：点估计
        - 摘要：5.4.2　偏差
          關鍵詞：偏差
        - 摘要：5.4.3　方差和标准差
          關鍵詞：方差和标准差
        - 摘要：5.4.4　权衡偏差和方差以；最小化均方误差
          關鍵詞：最小化均方误差, 权衡偏差和方差以
        - 摘要：5.4.5　一致性
          關鍵詞：一致性
    5.5：最大似然估计
        - 摘要：5.5.1　条件对数似然和均方误差
          關鍵詞：条件对数似然和均方误差
        - 摘要：5.5.2　最大似然的性质
          關鍵詞：最大似然的性质
        - 摘要：之前，我们已经看过常用估计的定义，并分析了它们的性质。但是这些；估计是从哪里来的呢？我们希望有些准则可以让我们从不同模型中得到；特定函数作为好的估计，而不是猜测某些函数可能是好的估计，然后分
          關鍵詞：而不是猜测某些函数可能是好的估计, 然后分, 但是这些, 我们已经看过常用估计的定义, 特定函数作为好的估计
        - 摘要：最常用的准则是最大似然估计。
          關鍵詞：最常用的准则是最大似然估计
        - 摘要：考虑一组含有m个样本的数据集；未知的真实数据生成分布p data (x)生成。
          關鍵詞：未知的真实数据生成分布, 考虑一组含有, 生成, 个样本的数据集
        - 摘要：，独立地由
          關鍵詞：独立地由
        - 摘要：令p model (x ； θ  )是一族由 θ 确定在相同空间上的概率分布。换言之，p；model ( x ; θ )将任意输入 x 映射到实数来估计真实概率p data ( x )。
          關鍵詞：将任意输入, 确定在相同空间上的概率分布, 换言之, 映射到实数来估计真实概率, 是一族由
        - 摘要：对 θ 的最大似然估计被定义为
          關鍵詞：的最大似然估计被定义为
        - 摘要：多个概率的乘积会因很多原因不便于计算。例如，计算中很可能会出现；数值下溢。为了得到一个便于计算的等价优化问题，我们观察到似然对；数不会改变其arg max，但是将乘积转化成了便于计算的求和形式：
          關鍵詞：多个概率的乘积会因很多原因不便于计算, 但是将乘积转化成了便于计算的求和形式, 计算中很可能会出现, 为了得到一个便于计算的等价优化问题, 数值下溢
        - 摘要：因为当重新缩放代价函数时arg  max不会改变，我们可以除以m得到和训；练数据经验分布ˆp data 相关的期望作为准则：
          關鍵詞：因为当重新缩放代价函数时, 得到和训, 相关的期望作为准则, 我们可以除以, 不会改变
        - 摘要：一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布ˆp；data  和模型分布之间的差异，两者之间的差异程度可以通过KL散度度；量。KL散度被定义为
          關鍵詞：两者之间的差异程度可以通过, 一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布, 散度被定义为, 和模型分布之间的差异, 散度度
        - 摘要：左边一项仅涉及数据生成过程，和模型无关。这意味着当训练模型最小；化KL散度时，我们只需要最小化
          關鍵詞：我们只需要最小化, 左边一项仅涉及数据生成过程, 和模型无关, 散度时, 这意味着当训练模型最小
        - 摘要：当然，这和式（5.59）中最大化是相同的。
          關鍵詞：当然, 中最大化是相同的, 这和式
        - 摘要：最小化KL散度其实就是在最小化分布之间的交叉熵。许多作者使用术；语“交叉熵”特定表示伯努利或softmax分布的负对数似然，但那是用词不；当的。任何一个由负对数似然组成的损失都是定义在训练集上的经验分
          關鍵詞：交叉熵, 散度其实就是在最小化分布之间的交叉熵, 分布的负对数似然, 许多作者使用术, 当的
        - 摘要：我们可以将最大似然看作使模型分布尽可能地和经验分布ˆp  data  相匹配；的尝试。理想情况下，我们希望匹配真实的数据生成分布p  data  ，但我；们无法直接知道这个分布。
          關鍵詞：但我, 我们可以将最大似然看作使模型分布尽可能地和经验分布, 相匹配, 们无法直接知道这个分布, 我们希望匹配真实的数据生成分布
        - 摘要：虽然最优  θ  在最大化似然或是最小化KL散度时是相同的，但目标函数；值是不一样的。在软件中，我们通常将两者都称为最小化代价函数。因；此最大化似然变成了最小化负对数似然（NLL），或者等价的是最小化
          關鍵詞：或者等价的是最小化, 值是不一样的, 在软件中, 在最大化似然或是最小化, 我们通常将两者都称为最小化代价函数
        - 摘要：5.5.1　条件对数似然和均方误差
          關鍵詞：条件对数似然和均方误差
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；最大似然估计很容易扩展到估计条件概率P (y |x  ;θ  )，从而给定x  预测y；。实际上这是最常见的情况，因为这构成了大多数监督学习的基础。如
          關鍵詞：最大似然估计很容易扩展到估计条件概率, 从而给定, 预测, 实际上这是最常见的情况, 因为这构成了大多数监督学习的基础
        - 摘要：如果假设样本是独立同分布的，那么式（5.62）可以分解成
          關鍵詞：那么式, 可以分解成, 如果假设样本是独立同分布的
        - 摘要：示例：线性回归作为最大似然  　第5.1.4节介绍的线性回归，可以被看；作最大似然过程。之前，我们将线性回归作为学习从输入 x 映射到输出；的算法。从x到   的映射选自最小化均方误差（我们或多或少介绍的
          關鍵詞：可以被看, 作最大似然过程, 的映射选自最小化均方误差, 的算法, 节介绍的线性回归
        - 摘要：。函数
          關鍵詞：函数
        - 摘要：其中  是线性回归在第i个输入 x  (i)  上的输出，m是训练样本的数目。；对比均方误差和对数似然，
          關鍵詞：上的输出, 是线性回归在第, 其中, 个输入, 对比均方误差和对数似然
        - 摘要：我们立刻可以看出，最大化关于 w 的对数似然和最小化均方误差会得到；相同的参数估计  w  。但是对于相同的最优  w  ，这两个准则有着不同的；值。这验证了MSE可以用于最大似然估计。正如我们将看到的，最大似
          關鍵詞：正如我们将看到的, 相同的参数估计, 可以用于最大似然估计, 的对数似然和最小化均方误差会得到, 但是对于相同的最优
        - 摘要：5.5.2　最大似然的性质
          關鍵詞：最大似然的性质
        - 摘要：最大似然估计最吸引人的地方在于，它被证明当样本数目m→∞时，就；收敛率而言是最好的渐近估计。
          關鍵詞：收敛率而言是最好的渐近估计, 最大似然估计最吸引人的地方在于, 它被证明当样本数目
        - 摘要：在合适的条件下，最大似然估计具有一致性（参考第5.4.5节），意味着；训练样本数目趋向于无穷大时，参数的最大似然估计会收敛到参数的真；实值。这些条件是：
          關鍵詞：在合适的条件下, 参数的最大似然估计会收敛到参数的真, 参考第, 意味着, 实值
        - 摘要：真实分布p  data  必须在模型族p  model  (·;  θ  )中。否则，没有估计可以；还原p data 。；真实分布p  data  必须刚好对应一个  θ  值。否则，最大似然估计恢复
          關鍵詞：最大似然估计恢复, 否则, 必须刚好对应一个, 还原, 没有估计可以
        - 摘要：除了最大似然估计，还有其他的归纳准则，其中许多共享一致估计的性；质。然而，一致估计的统计效率  （statistic  efficiency）可能区别很大。；某些一致估计可能会在固定数目的样本上获得一个较低的泛化误差，或
          關鍵詞：某些一致估计可能会在固定数目的样本上获得一个较低的泛化误差, 然而, 其中许多共享一致估计的性, 可能区别很大, 还有其他的归纳准则
        - 摘要：统计效率通常用于有参情况  （parametric  case）的研究中（例如线性回；归）。在有参情况中，我们的目标是估计参数值（假设有可能确定真实；参数），而不是函数值。一种度量和真实参数相差多少的方法是计算均
          關鍵詞：参数, 假设有可能确定真实, 我们的目标是估计参数值, 在有参情况中, 一种度量和真实参数相差多少的方法是计算均
        - 摘要：因为这些原因（一致性和统计效率），最大似然通常是机器学习中的首；选估计方法。当样本数目小到会发生过拟合时，正则化策略如权重衰减；可用于获得训练数据有限时方差较小的最大似然有偏版本。
          關鍵詞：选估计方法, 正则化策略如权重衰减, 最大似然通常是机器学习中的首, 当样本数目小到会发生过拟合时, 因为这些原因
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；5.6　贝叶斯统计
          關鍵詞：贝叶斯统计
        - 摘要：至此我们已经讨论了频率派统计  （frequentist  statistics）方法和基于估；计单一值 θ 的方法，然后基于该估计作所有的预测。另一种方法是在做；预测时会考虑所有可能的
          關鍵詞：至此我们已经讨论了频率派统计, 另一种方法是在做, 然后基于该估计作所有的预测, 预测时会考虑所有可能的, 的方法
        - 摘要：正如第5.4.1节中讨论的，频率派的视角是真实参数 θ 是未知的定值，而；点估计  是考虑数据集上函数（可以看作随机的）的随机变量。
          關鍵詞：可以看作随机的, 正如第, 点估计, 是未知的定值, 节中讨论的
        - 摘要：贝叶斯统计的视角完全不同。贝叶斯统计用概率反映知识状态的确定性；程度。数据集能够被直接观测到，因此不是随机的。另一方面，真实参；数 θ 是未知或不确定的，因此可以表示成随机变量。
          關鍵詞：是未知或不确定的, 贝叶斯统计用概率反映知识状态的确定性, 数据集能够被直接观测到, 因此可以表示成随机变量, 另一方面
        - 摘要：在观察到数据前，我们将；θ  的已知知识表示成先验概率分布（prior；probability  distribu-tion），p(  θ  )（有时简单地称为“先验”）。一般而
          關鍵詞：的已知知识表示成先验概率分布, 一般而, 我们将, 在观察到数据前, 先验
        - 摘要：现在假设我们有一组数据样本；，通过贝叶斯规则结；合数据似然p(x (1) ，···，x (m) ｜ θ )和先验，可以恢复数据对我们关于 θ
          關鍵詞：合数据似然, 和先验, 通过贝叶斯规则结, 现在假设我们有一组数据样本, 可以恢复数据对我们关于
        - 摘要：在贝叶斯估计常用的情景下，先验开始是相对均匀的分布或高熵的高斯；分布，观测数据通常会使后验的熵下降，并集中在参数的几个可能性很；高的值。
          關鍵詞：高的值, 先验开始是相对均匀的分布或高熵的高斯, 并集中在参数的几个可能性很, 观测数据通常会使后验的熵下降, 分布
        - 摘要：相对于最大似然估计，贝叶斯估计有两个重要区别。第一，不像最大似；然方法预测时使用 θ 的点估计，贝叶斯方法使用 θ 的全分布。例如，在；观测到m个样本后，下一个数据样本x (m+1) 的预测分布如下：
          關鍵詞：贝叶斯方法使用, 第一, 不像最大似, 观测到, 的预测分布如下
        - 摘要：这里，每个具有正概率密度的 θ  的值有助于下一个样本的预测，其中贡；献由后验密度本身加权。在观测到数据集{x (1) ,···,x (m) }之后，如果我们；仍然非常不确定 θ 的值，那么这个不确定性会直接包含在我们所做的任
          關鍵詞：的值有助于下一个样本的预测, 之后, 其中贡, 如果我们, 在观测到数据集
        - 摘要：在第5.4节中，我们已经探讨频率派方法解决给定点估计  θ  的不确定性；的方法是评估方差，估计的方差评估了观测数据重新从观测数据中采样；后，估计可能如何变化。对于如何处理估计不确定性的这个问题，贝叶
          關鍵詞：估计的方差评估了观测数据重新从观测数据中采样, 的不确定性, 的方法是评估方差, 节中, 贝叶
        - 摘要：贝叶斯方法和最大似然方法的第二个最大区别是由贝叶斯先验分布造成；的。先验能够影响概率质量密度朝参数空间中偏好先验的区域偏移。实；践中，先验通常表现为偏好更简单或更光滑的模型。对贝叶斯方法的批
          關鍵詞：对贝叶斯方法的批, 先验能够影响概率质量密度朝参数空间中偏好先验的区域偏移, 践中, 贝叶斯方法和最大似然方法的第二个最大区别是由贝叶斯先验分布造成, 先验通常表现为偏好更简单或更光滑的模型
        - 摘要：当训练数据很有限时，贝叶斯方法通常泛化得更好，但是当训练样本数；目很大时，通常会有很大的计算代价。
          關鍵詞：通常会有很大的计算代价, 贝叶斯方法通常泛化得更好, 当训练数据很有限时, 目很大时, 但是当训练样本数
        - 摘要：示例：贝叶斯线性回归  　我们使用贝叶斯估计方法学习线性回归的参；预测标量；数。在线性回归中，我们学习从输入向量
          關鍵詞：在线性回归中, 预测标量, 我们学习从输入向量, 我们使用贝叶斯估计方法学习线性回归的参, 示例
        - 摘要：的线性映射。该预测由向量
          關鍵詞：的线性映射, 该预测由向量
        - 摘要：参数化：
          關鍵詞：参数化
        - 摘要：给定一组m个训练样本（ X （train） ， y （train） ），我们可以表示整个训；练集对y的预测：
          關鍵詞：我们可以表示整个训, 练集对, 个训练样本, 给定一组, 的预测
        - 摘要：表示为 y （train） 上的高斯条件分布，我们得到
          關鍵詞：上的高斯条件分布, 表示为, 我们得到
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；其中，我们根据标准的MSE公式假设y上的高斯方差为1。在下文中，为；减少符号负担，我们将（  X  （train）  ，  y  （train）  ）简单表示为（  X，y
          關鍵詞：上的高斯方差为, 我们将, 其中, 公式假设, 简单表示为
        - 摘要：为确定模型参数向量 w 的后验分布，我们首先需要指定一个先验分布。；先验应该反映我们对这些参数取值的信念。虽然有时将我们的先验信念；表示为模型的参数很难或很不自然，但在实践中我们通常假设一个相当
          關鍵詞：我们首先需要指定一个先验分布, 为确定模型参数向量, 先验应该反映我们对这些参数取值的信念, 虽然有时将我们的先验信念, 的后验分布
        - 摘要：其中， µ 0 和Λ 0 分别是先验分布的均值向量和协方差矩阵。 (1)
          關鍵詞：分别是先验分布的均值向量和协方差矩阵, 其中
        - 摘要：确定好先验后，我们现在可以继续确定模型参数的后验 分布。
          關鍵詞：确定好先验后, 我们现在可以继续确定模型参数的后验, 分布
        - 摘要：现在我们定义
          關鍵詞：现在我们定义
        - 摘要：这些新的变量，我们发现后验可改写为高斯分布：
          關鍵詞：我们发现后验可改写为高斯分布, 这些新的变量
        - 摘要：。使用
          關鍵詞：使用
        - 摘要：分布的积分必须归一这个事实意味着要删去所有不包括参数向量w的；项。式（3.23）显示了如何标准化多元高斯分布。
          關鍵詞：分布的积分必须归一这个事实意味着要删去所有不包括参数向量, 显示了如何标准化多元高斯分布
        - 摘要：检查此后验分布可以让我们获得贝叶斯推断效果的一些直觉。大多数情
          關鍵詞：大多数情, 检查此后验分布可以让我们获得贝叶斯推断效果的一些直觉
        - 摘要：况下，我们设置 µ 0 =0。如果我们设置
          關鍵詞：我们设置, 如果我们设置, 况下
        - 摘要：，那么µ  m
          關鍵詞：那么
        - 摘要：的线性回归的估计是；对 w 的估计就和频率派带权重衰减惩罚；一样的。一个区别是若α设为0，则贝叶斯估计是未定义的——我们不能
          關鍵詞：一个区别是若, 一样的, 的估计就和频率派带权重衰减惩罚, 则贝叶斯估计是未定义的, 设为
        - 摘要：5.6.1　最大后验（MAP）估计
          關鍵詞：最大后验, 估计
        - 摘要：原则上，我们应该使用参数 θ  的完整贝叶斯后验分布进行预测，但单点；估计常常也是需要的。希望使用点估计的一个常见原因是，对于大多数；有意义的模型而言，大多数涉及贝叶斯后验的计算是非常棘手的，点估
          關鍵詞：但单点, 有意义的模型而言, 点估, 希望使用点估计的一个常见原因是, 我们应该使用参数
        - 摘要：我们可以认出式（5.79）右边的log  p(  x  ｜  θ  )对应着标准的对数似然；项，log p( θ )对应着先验分布。
          關鍵詞：对应着标准的对数似然, 我们可以认出式, 对应着先验分布, 右边的
        - 摘要：例如，考虑具有高斯先验权重
          關鍵詞：考虑具有高斯先验权重, 例如
        - 摘要：w
          關鍵詞：
        - 摘要：的线性回归模型。如果先验是
          關鍵詞：的线性回归模型, 如果先验是
        - 摘要：，那么式（5.79）的对数先验项正比于
          關鍵詞：那么式, 的对数先验项正比于
        - 摘要：熟悉的权重衰减惩罚；过程的项。因此，具有高斯先验权重的MAP贝叶斯推断对应着权重衰；减。
          關鍵詞：贝叶斯推断对应着权重衰, 因此, 熟悉的权重衰减惩罚, 具有高斯先验权重的, 过程的项
        - 摘要：，加上一个不依赖于 w 也不会影响学习
          關鍵詞：加上一个不依赖于, 也不会影响学习
        - 摘要：正如全贝叶斯推断，MAP贝叶斯推断的优势是能够利用来自先验的信；息，这些信息无法从训练数据中获得。该附加信息有助于减少最大后验；点估计的方差（相比于ML估计）。然而，这个优点的代价是增加了偏
          關鍵詞：估计, 正如全贝叶斯推断, 这些信息无法从训练数据中获得, 该附加信息有助于减少最大后验, 这个优点的代价是增加了偏
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；许多正规化估计方法，例如权重衰减正则化的最大似然学习，可以被解；释为贝叶斯推断的MAP近似。这个适应于正则化时加到目标函数的附加
          關鍵詞：许多正规化估计方法, 可以被解, 释为贝叶斯推断的, 例如权重衰减正则化的最大似然学习, 这个适应于正则化时加到目标函数的附加
        - 摘要：MAP贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化。；例如，更复杂的惩罚项可以通过混合高斯分布作为先验得到，而不是一；个单独的高斯分布（Nowlan and Hinton，1992）。
          關鍵詞：更复杂的惩罚项可以通过混合高斯分布作为先验得到, 例如, 贝叶斯推断提供了一个直观的方法来设计复杂但可解释的正则化, 而不是一, 个单独的高斯分布
        - 摘要：5.5.1　条件对数似然和均；方误差
          關鍵詞：条件对数似然和均, 方误差
        - 摘要：5.5.2　最大似然的性质
          關鍵詞：最大似然的性质
    5.6：贝叶斯统计
        - 摘要：5.6.1　最大后验（MAP）估计
          關鍵詞：最大后验, 估计
        - 摘要：5.6.1　最大后验（MAP）；估计
          關鍵詞：最大后验, 估计
    5.7：监督学习算法
        - 摘要：5.7.1　概率监督学习
          關鍵詞：概率监督学习
        - 摘要：5.7.2　支持向量机
          關鍵詞：支持向量机
        - 摘要：5.7.3　其他简单的监督学习算法
          關鍵詞：其他简单的监督学习算法
        - 摘要：回顾第5.1.3节，粗略地说，监督学习算法是给定一组输入x和输出y的训；练集，学习如何关联输入和输出。在许多情况下，输出y很难自动收；集，必须由人来提供“监督”，不过该术语仍然适用于训练集目标可以被
          關鍵詞：输出, 的训, 在许多情况下, 回顾第, 不过该术语仍然适用于训练集目标可以被
        - 摘要：5.7.1　概率监督学习
          關鍵詞：概率监督学习
        - 摘要：本书的大部分监督学习算法都是基于估计概率分布p(y｜  x  )的。我们可；以使用最大似然估计找到对于有参分布族p(y｜ x ; θ )最好的参数向量 θ；。
          關鍵詞：以使用最大似然估计找到对于有参分布族, 本书的大部分监督学习算法都是基于估计概率分布, 我们可, 最好的参数向量
        - 摘要：我们已经看到，线性回归对应于分布族
          關鍵詞：线性回归对应于分布族, 我们已经看到
        - 摘要：通过定义一族不同的概率分布，我们可以将线性回归扩展到分类情况；中。如果我们有两个类，类0和类1，那么只需要指定这两类之一的概；率。类1的概率决定了类0的概率，因为这两个值加起来必须等于1。
          關鍵詞：通过定义一族不同的概率分布, 那么只需要指定这两类之一的概, 的概率决定了类, 和类, 的概率
        - 摘要：我们用于线性回归的实数正态分布是用均值参数化的。我们提供这个均；值的任何值都是有效的。二元变量上的的分布稍微复杂些，因为它的均；值必须始终在0和1之间。解决这个问题的一种方法是使用logistic
          關鍵詞：值的任何值都是有效的, 我们用于线性回归的实数正态分布是用均值参数化的, 解决这个问题的一种方法是使用, 我们提供这个均, 因为它的均
        - 摘要：这个方法被称为逻辑回归  （logistic  regression），这个名字有点奇怪，；因为该模型用于分类而非回归。
          關鍵詞：这个方法被称为逻辑回归, 因为该模型用于分类而非回归, 这个名字有点奇怪
        - 摘要：线性回归中，我们能够通过求解正规方程以找到最佳权重。相比而言，；逻辑回归会更困难些。其最佳权重没有闭解。反之，我们必须最大化对；数似然来搜索最优解。我们可以通过梯度下降算法最小化负对数似然来
          關鍵詞：相比而言, 反之, 我们可以通过梯度下降算法最小化负对数似然来, 线性回归中, 其最佳权重没有闭解
        - 摘要：通过确定正确的输入和输出变量上的有参条件概率分布族，相同的策略；基本上可以用于任何监督学习问题。
          關鍵詞：通过确定正确的输入和输出变量上的有参条件概率分布族, 相同的策略, 基本上可以用于任何监督学习问题
        - 摘要：5.7.2　支持向量机
          關鍵詞：支持向量机
        - 摘要：支持向量机 （support vector machine，SVM）是监督学习中最有影响力；的方法之一（Boser  et  al.  ，1992；Cortes  and  Vapnik，1995）。类似于；的。不同于逻辑回
          關鍵詞：不同于逻辑回, 是监督学习中最有影响力, 支持向量机, 类似于, 的方法之一
        - 摘要：支持向量机的一个重要创新是核技巧  （kernel  trick）。核技巧观察到许；多机器学习算法都可以写成样本间点积的形式。例如，支持向量机中的；线性函数可以重写为
          關鍵詞：支持向量机的一个重要创新是核技巧, 核技巧观察到许, 多机器学习算法都可以写成样本间点积的形式, 例如, 支持向量机中的
        - 摘要：其中， x (i) 是训练样本， α  是系数向量。学习算法重写为这种形式允许；我们将  x  替换为特征函数φ(  x；)的输出，点积替换为被称为核函数
          關鍵詞：是系数向量, 的输出, 我们将, 其中, 学习算法重写为这种形式允许
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；使用核估计替换点积之后，我们可以使用如下函数进行预测
          關鍵詞：使用核估计替换点积之后, 我们可以使用如下函数进行预测
        - 摘要：这个函数关于 x  是非线性的，关于φ( x )是线性的。 α  和f(  x  )之间的关；系也是线性的。核函数完全等价于用φ(  x  )预处理所有的输入，然后在；新的转换空间学习线性模型。
          關鍵詞：核函数完全等价于用, 之间的关, 系也是线性的, 是非线性的, 然后在
        - 摘要：核技巧十分强大有两个原因：其一，它使我们能够使用保证有效收敛的；凸优化技术来学习非线性模型（关于 x 的函数）。这是可能的，因为我；们可以认为φ是固定的，仅优化α，即优化算法可以将决策函数视为不同
          關鍵詞：们可以认为, 是固定的, 即优化算法可以将决策函数视为不同, 凸优化技术来学习非线性模型, 的函数
        - 摘要：)甚至可以是无限维的，对于普通的显式方法而；在某些情况下，φ(  x；言，这将是无限的计算代价。在很多情况下，即使φ( x )是难算的，k(  x
          關鍵詞：甚至可以是无限维的, 这将是无限的计算代价, 对于普通的显式方法而, 即使, 在某些情况下
        - 摘要：最常用的核函数是高斯核 （Gaussian kernel），
          關鍵詞：最常用的核函数是高斯核
        - 摘要：是标准正态密度。这个核也被称为径向基函数；其中；（radial basis function，RBF）核，因为其值沿 ν 中从 u 向外辐射的方向
          關鍵詞：中从, 是标准正态密度, 其中, 向外辐射的方向, 因为其值沿
        - 摘要：我们可以认为高斯核在执行一种模板匹配  （template  matching）。训练；标签y相关的训练样本  x  变成了类别y的模板。当测试点  x/  到  x  的欧几；里得距离很小，对应的高斯核响应很大时，表明  x'  和模板  x  非常相
          關鍵詞：训练, 对应的高斯核响应很大时, 里得距离很小, 当测试点, 和模板
        - 摘要：支持向量机不是唯一可以使用核技巧来增强的算法。许多其他的线性模；型也可以通过这种方式来增强。使用核技巧的算法类别被称为核机器；（kernel  machine）或核方法  （kernel  method）（Williams
          關鍵詞：许多其他的线性模, 支持向量机不是唯一可以使用核技巧来增强的算法, 或核方法, 型也可以通过这种方式来增强, 使用核技巧的算法类别被称为核机器
        - 摘要：and
          關鍵詞：
        - 摘要：核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线；性的。因为第i个样本贡献α i k( x , x  (i)  )到决策函数。支持向量机能够通；过学习主要包含零的向量 α ，以缓和这个缺点。那么判断新样本的类别
          關鍵詞：核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线, 那么判断新样本的类别, 过学习主要包含零的向量, 性的, 个样本贡献
        - 摘要：当数据集很大时，核机器的计算量也会很大。我们将会在第5.9节回顾；这个想法。带通用核的核机器致力于泛化得更好。我们将在第5.11节解；释原因。现代深度学习的设计旨在克服核机器的这些限制。当前深度学
          關鍵詞：释原因, 节回顾, 节解, 现代深度学习的设计旨在克服核机器的这些限制, 当前深度学
        - 摘要：5.7.3　其他简单的监督学习算法
          關鍵詞：其他简单的监督学习算法
        - 摘要：我们已经简要介绍过另一个非概率监督学习算法，最近邻回归。通常，；k-最近邻是一类可用于分类或回归的技术。作为一个非参数学习算法，；k-最近邻并不局限于固定数目的参数。我们通常认为k-最近邻算法没有
          關鍵詞：通常, 我们已经简要介绍过另一个非概率监督学习算法, 最近邻是一类可用于分类或回归的技术, 作为一个非参数学习算法, 最近邻并不局限于固定数目的参数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；高容量使其在训练样本数目大时能够获取较高的精度。然而，它的计算；成本很高，另外在训练集较小时泛化能力很差。k-最近邻的一个弱点是
          關鍵詞：最近邻的一个弱点是, 成本很高, 另外在训练集较小时泛化能力很差, 然而, 高容量使其在训练样本数目大时能够获取较高的精度
        - 摘要：决策树  （decision；tree）及其变种是另一类将输入空间分成不同的区；域，每个区域有独立参数的算法（Breiman  et  al.  ，1984）。如图5.7所
          關鍵詞：每个区域有独立参数的算法, 决策树, 如图, 及其变种是另一类将输入空间分成不同的区
        - 摘要：图5.7　描述一个决策树如何工作的示意图。（上）树中每个节点都选择将输入样本送到左子节；点（0）或者右子节点（1）。内部的节点用圆圈表示，叶节点用方块表示。每一个节点可以用；一个二值的字符串识别并对应树中的位置，这个字符串是通过给起父亲节点的字符串添加一个
          關鍵詞：内部的节点用圆圈表示, 叶节点用方块表示, 或者右子节点, 树中每个节点都选择将输入样本送到左子节, 这个字符串是通过给起父亲节点的字符串添加一个
        - 摘要：。这个平面中画出了树的节点，每个内部点穿过分割线
          關鍵詞：每个内部点穿过分割线, 这个平面中画出了树的节点
        - 摘要：正如我们已经看到的，最近邻预测和决策树都有很多的局限性。尽管如
          關鍵詞：尽管如, 正如我们已经看到的, 最近邻预测和决策树都有很多的局限性
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；此，在计算资源受限制时，它们都是很有用的学习算法。通过思考复杂；算法和k-最近邻或决策树之间的相似性和差异，我们可以建立对更复杂
          關鍵詞：通过思考复杂, 它们都是很有用的学习算法, 我们可以建立对更复杂, 最近邻或决策树之间的相似性和差异, 算法和
        - 摘要：读者可以参考Murphy（2012）；Bishop（2006）；Hastie et al. （2001）；或其他机器学习教科书了解更多的传统监督学习算法。
          關鍵詞：或其他机器学习教科书了解更多的传统监督学习算法, 读者可以参考
        - 摘要：5.7.1　概率监督学习
          關鍵詞：概率监督学习
        - 摘要：5.7.2　支持向量机
          關鍵詞：支持向量机
        - 摘要：5.7.3　其他简单的监督学；习算法
          關鍵詞：其他简单的监督学, 习算法
    5.8：无监督学习算法
        - 摘要：5.8.1　主成分分析
          關鍵詞：主成分分析
        - 摘要：5.8.2　k-均值聚类
          關鍵詞：均值聚类
        - 摘要：回顾第5.1.3节，无监督算法只处理“特征”，不操作监督信号。监督和无；监督算法之间的区别没有规范严格的定义，因为没有客观的判断来区分；监督者提供的值是特征还是目标。通俗地说，无监督学习的大多数尝试
          關鍵詞：监督和无, 通俗地说, 回顾第, 监督者提供的值是特征还是目标, 监督算法之间的区别没有规范严格的定义
        - 摘要：一个经典的无监督学习任务是找到数据的“最佳”表示。“最佳”可以是不；同的表示，但是一般来说，是指该表示在比本身表示的信息更简单或更；易访问而受到一些惩罚或限制的情况下，尽可能地保存关于 x 更多的信
          關鍵詞：表示, 最佳, 是指该表示在比本身表示的信息更简单或更, 可以是不, 易访问而受到一些惩罚或限制的情况下
        - 摘要：有很多方式定义较简单的表示。最常见的3种包括低维表示、稀疏表示；和独立表示。低维表示尝试将 x 中的信息尽可能压缩在一个较小的表示；中。稀疏表示将数据集嵌入到输入项大多数为零的表示中（Barlow，
          關鍵詞：最常见的, 低维表示尝试将, 稀疏表示将数据集嵌入到输入项大多数为零的表示中, 有很多方式定义较简单的表示, 稀疏表示
        - 摘要：当然，这3个标准并非相互排斥的。低维表示通常会产生比原始的高维；数据具有较少或较弱依赖关系的元素。这是因为减少表示大小的一种方；式是找到并消除冗余。识别并去除更多的冗余使得降维算法在丢失更少
          關鍵詞：这是因为减少表示大小的一种方, 式是找到并消除冗余, 低维表示通常会产生比原始的高维, 识别并去除更多的冗余使得降维算法在丢失更少, 当然
        - 摘要：表示的概念是深度学习核心主题之一，因此也是本书的核心主题之一。
          關鍵詞：因此也是本书的核心主题之一, 表示的概念是深度学习核心主题之一
        - 摘要：本节会介绍表示学习算法中的一些简单示例。总的来说，这些示例算法；会说明如何实施上面的3个标准。剩余的大部分章节会介绍额外的表示；学习算法，它们以不同方式处理这3个标准或是引入其他标准。
          關鍵詞：剩余的大部分章节会介绍额外的表示, 个标准或是引入其他标准, 学习算法, 这些示例算法, 个标准
        - 摘要：5.8.1　主成分分析
          關鍵詞：主成分分析
        - 摘要：在第2.12节中，我们看到PCA算法提供了一种压缩数据的方式。我们也；可以将PCA视为学习数据表示的无监督学习算法。这种表示基于上述简；单表示的两个标准。PCA学习一种比原始输入维数更低的表示。它也学
          關鍵詞：单表示的两个标准, 它也学, 可以将, 学习一种比原始输入维数更低的表示, 节中
        - 摘要：如图5.8所示，PCA将输入 x 投影表示成 z ，学习数据的正交线性变换。；在第2.12节中，我们看到了如何学习重建原始数据的最佳一维表示（就；均方误差而言），这种表示其实对应着数据的第一个主要成分。因此，
          關鍵詞：学习数据的正交线性变换, 投影表示成, 所示, 这种表示其实对应着数据的第一个主要成分, 因此
        - 摘要：图5.8　PCA学习一种线性投影，使最大方差的方向和新空间的轴对齐。（左）原始数据包含了；x 的样本。在这个空间中，方差的方向与轴的方向并不是对齐的。（右）变换过的数据；在轴z 1 的方向上有最大的变化。第二大变化方差的方向沿着轴z 2
          關鍵詞：使最大方差的方向和新空间的轴对齐, 原始数据包含了, 学习一种线性投影, 第二大变化方差的方向沿着轴, 的方向上有最大的变化
        - 摘要：假设有一个m×n的设计矩阵 X  ，数据的均值为零，；如此，通过预处理步骤使所有样本减去均值，数据可以很容易地中心；化。
          關鍵詞：如此, 数据的均值为零, 的设计矩阵, 通过预处理步骤使所有样本减去均值, 数据可以很容易地中心
        - 摘要：。若非
          關鍵詞：若非
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；X 对应的无偏样本协方差矩阵给定如下
          關鍵詞：对应的无偏样本协方差矩阵给定如下
        - 摘要：PCA通过线性变换找到一个Var［ z  ］是对角矩阵的表示；。
          關鍵詞：是对角矩阵的表示, 通过线性变换找到一个
        - 摘要：在第2.12节，我们已知设计矩阵  X  的主成分由；定。从这个角度，我们有
          關鍵詞：我们有, 从这个角度, 的主成分由, 在第, 我们已知设计矩阵
        - 摘要：的特征向量给
          關鍵詞：的特征向量给
        - 摘要：本节中，我们会探索主成分的另一种推导。主成分也可以通过奇异值分；解（SVD）得到。具体来说，它们是 X 的右奇异向量。为了说明这点，；假设 W 是奇异值分解
          關鍵詞：本节中, 它们是, 是奇异值分解, 得到, 具体来说
        - 摘要：SVD有助于说明PCA后的Var［  z  ］是对角的。使用  X  的SVD分解，  X；的方差可以表示为
          關鍵詞：有助于说明, 后的, 分解, 是对角的, 的方差可以表示为
        - 摘要：其中，我们使用；的。这表明 z 的协方差满足对角的要求：
          關鍵詞：的协方差满足对角的要求, 这表明, 其中, 我们使用
        - 摘要：，因为根据奇异值的定义矩阵 U 是正交
          關鍵詞：是正交, 因为根据奇异值的定义矩阵
        - 摘要：其中，再次使用SVD的定义有
          關鍵詞：的定义有, 其中, 再次使用
        - 摘要：。
          關鍵詞：
        - 摘要：以上分析指明当我们通过线性变换  W  将数据  x  投影到  z  时，得到的数；据表示的协方差矩阵是对角的（即Σ  2  ），立刻可得  z  中的元素是彼此；无关的。
          關鍵詞：无关的, 中的元素是彼此, 得到的数, 据表示的协方差矩阵是对角的, 立刻可得
        - 摘要：PCA这种将数据变换为元素之间彼此不相关表示的能力是PCA的一个重；要性质。它是消除数据中未知变化因素的简单表示示例。在PCA中，这；个消除是通过寻找输入空间的一个旋转（由  W  确定），使得方差的主
          關鍵詞：它是消除数据中未知变化因素的简单表示示例, 使得方差的主, 这种将数据变换为元素之间彼此不相关表示的能力是, 确定, 要性质
        - 摘要：虽然相关性是数据元素间依赖关系的一个重要范畴，但我们对于能够消；除更复杂形式的特征依赖的表示学习也很感兴趣。对此，我们需要比简；单线性变换更强的工具。
          關鍵詞：虽然相关性是数据元素间依赖关系的一个重要范畴, 单线性变换更强的工具, 我们需要比简, 除更复杂形式的特征依赖的表示学习也很感兴趣, 对此
        - 摘要：5.8.2　k-均值聚类
          關鍵詞：均值聚类
        - 摘要：另外一个简单的表示学习算法是k-均值聚类。k-均值聚类算法将训练集；分成k个靠近彼此的不同样本聚类。因此我们可以认为该算法提供了k-；维的one-hot编码向量h以表示输入  x 。当 x 属于聚类i时，有h  i  =1，h的
          關鍵詞：另外一个简单的表示学习算法是, 均值聚类, 因此我们可以认为该算法提供了, 个靠近彼此的不同样本聚类, 分成
        - 摘要：k-均值聚类提供的one-hot编码也是一种稀疏表示，因为每个输入的表示；中大部分元素为零。之后，我们会介绍能够学习更灵活的稀疏表示的一；些其他算法（表示中每个输入 x 不只一个非零项）。one-hot编码是稀疏
          關鍵詞：之后, 编码是稀疏, 中大部分元素为零, 不只一个非零项, 些其他算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；k-均值聚类初始化k个不同的中心点{µ (1) ,...,µ (k) }，然后迭代交换两个不；同的步骤直到收敛。步骤一，每个训练样本分配到最近的中心点µ  (i)  所
          關鍵詞：步骤一, 个不同的中心点, 每个训练样本分配到最近的中心点, 同的步骤直到收敛, 均值聚类初始化
        - 摘要：关于聚类的一个问题是，聚类问题本身是病态的。这是说没有单一的标；准去度量聚类的数据在真实世界中效果如何。我们可以度量聚类的性；质，例如类中元素到类中心点的欧几里得距离的均值。这使我们可以判
          關鍵詞：准去度量聚类的数据在真实世界中效果如何, 关于聚类的一个问题是, 聚类问题本身是病态的, 这使我们可以判, 例如类中元素到类中心点的欧几里得距离的均值
        - 摘要：这些问题说明了一些我们可能更偏好于分布式表示（相对于one-hot表示；而言）的原因。分布式表示可以对每个车辆赋予两个属性——一个表示；它的颜色，一个表示它是汽车还是卡车。目前仍然不清楚什么是最优的
          關鍵詞：表示, 的原因, 相对于, 而言, 它的颜色
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；5.8.1　主成分分析
          關鍵詞：主成分分析
        - 摘要：5.8.2　k-均值聚类
          關鍵詞：均值聚类
    5.9：随机梯度下降
        - 摘要：几乎所有的深度学习算法都用到了一个非常重要的算法：随机梯度下降；（stochastic  gradi-ent  descent，SGD）。随机梯度下降是第4.3节介绍的；梯度下降算法的一个扩展。
          關鍵詞：节介绍的, 梯度下降算法的一个扩展, 随机梯度下降是第, 随机梯度下降, 几乎所有的深度学习算法都用到了一个非常重要的算法
        - 摘要：机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的训；练集的计算代价也更大。
          關鍵詞：但大的训, 练集的计算代价也更大, 机器学习中反复出现的一个问题是好的泛化需要大的训练集
        - 摘要：机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总；和。例如，训练数据的负条件对数似然可以写成
          關鍵詞：机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总, 例如, 训练数据的负条件对数似然可以写成
        - 摘要：其中L是每个样本的损失
          關鍵詞：是每个样本的损失, 其中
        - 摘要：。
          關鍵詞：
        - 摘要：对于这些相加的代价函数，梯度下降需要计算
          關鍵詞：对于这些相加的代价函数, 梯度下降需要计算
        - 摘要：这个运算的计算代价是O(m)。随着训练集规模增长为数十亿的样本，计；算一步梯度也会消耗相当长的时间。
          關鍵詞：算一步梯度也会消耗相当长的时间, 随着训练集规模增长为数十亿的样本, 这个运算的计算代价是
        - 摘要：随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估；计。具体而言，在算法的每一步，我们从训练集中均匀抽出一小批量；。小批量的数目m'通常是
          關鍵詞：具体而言, 梯度是期望, 期望可使用小规模的样本近似估, 在算法的每一步, 随机梯度下降的核心是
        - 摘要：梯度的估计可以表示成
          關鍵詞：梯度的估计可以表示成
        - 摘要：使用来自小批量B的样本。然后，随机梯度下降算法使用如下的梯度下；降估计：
          關鍵詞：降估计, 使用来自小批量, 的样本, 随机梯度下降算法使用如下的梯度下, 然后
        - 摘要：其中，  是学习率。
          關鍵詞：是学习率, 其中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；梯度下降往往被认为很慢或不可靠。以前，将梯度下降应用到非凸优化；问题被认为很鲁莽或没有原则。现在，我们知道梯度下降用于本书第2
          關鍵詞：问题被认为很鲁莽或没有原则, 以前, 将梯度下降应用到非凸优化, 现在, 我们知道梯度下降用于本书第
        - 摘要：随机梯度下降在深度学习之外有很多重要的应用。它是在大规模数据上；训练大型线性模型的主要方法。对于固定大小的模型，每一步随机梯度；下降更新的计算量不取决于训练集的大小m。在实践中，当训练集大小
          關鍵詞：训练大型线性模型的主要方法, 每一步随机梯度, 下降更新的计算量不取决于训练集的大小, 当训练集大小, 它是在大规模数据上
        - 摘要：在深度学习兴起之前，学习非线性模型的主要方法是结合核技巧的线性；模型。很多核学习算法需要构建一个m×m的矩阵；。构建这个矩阵的计算量是O(m  2  )。当数据集是几十亿个样本时，这个
          關鍵詞：当数据集是几十亿个样本时, 很多核学习算法需要构建一个, 在深度学习兴起之前, 的矩阵, 学习非线性模型的主要方法是结合核技巧的线性
        - 摘要：我们将会在第8章继续探讨随机梯度下降及其很多改进方法。
          關鍵詞：我们将会在第, 章继续探讨随机梯度下降及其很多改进方法
    5.10：构建机器学习算法
        - 摘要：几乎所有的深度学习算法都可以被描述为一个相当简单的配方：特定的；数据集、代价函数、优化过程和模型。
          關鍵詞：几乎所有的深度学习算法都可以被描述为一个相当简单的配方, 优化过程和模型, 特定的, 数据集, 代价函数
        - 摘要：例如，线性回归算法由以下部分组成： X 和 y 构成的数据集，代价函数
          關鍵詞：线性回归算法由以下部分组成, 构成的数据集, 代价函数, 例如
        - 摘要：模型是
          關鍵詞：模型是
        - 摘要：，在大多数情况下，
          關鍵詞：在大多数情况下
        - 摘要：优化算法可以定义为求解代价函数梯度为零的正规方程。
          關鍵詞：优化算法可以定义为求解代价函数梯度为零的正规方程
        - 摘要：意识到可以替换独立于其他组件的大多数组件，因此我们能得到很多不；同的算法。
          關鍵詞：意识到可以替换独立于其他组件的大多数组件, 因此我们能得到很多不, 同的算法
        - 摘要：通常代价函数至少含有一项使学习过程进行统计估计的成分。最常见的；代价函数是负对数似然，最小化代价函数导致的最大似然估计。
          關鍵詞：通常代价函数至少含有一项使学习过程进行统计估计的成分, 最小化代价函数导致的最大似然估计, 代价函数是负对数似然, 最常见的
        - 摘要：代价函数也可能含有附加项，如正则化。例如，我们可以将权重衰减加；到线性回归的代价函数中
          關鍵詞：我们可以将权重衰减加, 代价函数也可能含有附加项, 例如, 到线性回归的代价函数中, 如正则化
        - 摘要：该优化仍然有闭解。
          關鍵詞：该优化仍然有闭解
        - 摘要：如果我们将该模型变成非线性的，那么大多数代价函数不再能通过闭解；优化。这就要求我们选择一个迭代数值优化过程，如梯度下降等。
          關鍵詞：这就要求我们选择一个迭代数值优化过程, 如梯度下降等, 如果我们将该模型变成非线性的, 那么大多数代价函数不再能通过闭解, 优化
        - 摘要：组合模型、代价和优化算法来构建学习算法的配方同时适用于监督学习；和无监督学习。线性回归示例说明了如何适用于监督学习的。无监督学；习时，我们需要定义一个只包含 X 的数据集、一个合适的无监督代价和
          關鍵詞：线性回归示例说明了如何适用于监督学习的, 无监督学, 代价和优化算法来构建学习算法的配方同时适用于监督学习, 和无监督学习, 一个合适的无监督代价和
        - 摘要：模型定义为重构函数
          關鍵詞：模型定义为重构函数
        - 摘要：，并且 w 有范数为1的限制。
          關鍵詞：并且, 有范数为, 的限制
        - 摘要：在某些情况下，由于计算原因，我们不能实际计算代价函数。在这种情；况下，只要有近似其梯度的方法，那么我们仍然可以使用迭代数值优化；近似最小化目标。
          關鍵詞：在这种情, 只要有近似其梯度的方法, 况下, 我们不能实际计算代价函数, 那么我们仍然可以使用迭代数值优化
        - 摘要：尽管有时候不明显，但大多数学习算法都用到了上述配方。如果一个机；器学习算法看上去特别独特或是手动设计的，那么通常需要使用特殊的；优化方法进行求解。有些模型，如决策树或k-均值，需要特殊的优化，
          關鍵詞：需要特殊的优化, 尽管有时候不明显, 那么通常需要使用特殊的, 但大多数学习算法都用到了上述配方, 优化方法进行求解
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；一长串各个不同的算法。
          關鍵詞：一长串各个不同的算法
    5.11：促使深度学习发展的挑战
        - 摘要：5.11.1　维数灾难
          關鍵詞：维数灾难
        - 摘要：5.11.2　局部不变性和平滑正则化
          關鍵詞：局部不变性和平滑正则化
        - 摘要：5.11.3　流形学习
          關鍵詞：流形学习
        - 摘要：第2部分　深度网络：现代实践
          關鍵詞：深度网络, 现代实践, 部分
        - 摘要：本章描述的简单机器学习算法在很多不同的重要问题上效果都良好，但；是它们不能成功解决人工智能中的核心问题，如语音识别或者对象识；别。
          關鍵詞：如语音识别或者对象识, 是它们不能成功解决人工智能中的核心问题, 本章描述的简单机器学习算法在很多不同的重要问题上效果都良好
        - 摘要：促使深度学习发展的一部分原因是传统学习算法在这类人工智能问题上；泛化能力不足。
          關鍵詞：促使深度学习发展的一部分原因是传统学习算法在这类人工智能问题上, 泛化能力不足
        - 摘要：本节介绍为何处理高维数据时在新样本上泛化特别困难，以及为何在传；统机器学习中实现泛化的机制不适合学习高维空间中复杂的函数。这些；空间经常涉及巨大的计算代价，深度学习旨在克服这些以及其他一些难
          關鍵詞：统机器学习中实现泛化的机制不适合学习高维空间中复杂的函数, 本节介绍为何处理高维数据时在新样本上泛化特别困难, 深度学习旨在克服这些以及其他一些难, 这些, 空间经常涉及巨大的计算代价
        - 摘要：5.11.1　维数灾难
          關鍵詞：维数灾难
        - 摘要：当数据的维数很高时，很多机器学习问题变得相当困难。这种现象被称；为维数灾难  （curse  of  dimensionality）。特别值得注意的是，一组变量；不同的可能配置数量会随着变量数目的增加而指数级增长。
          關鍵詞：为维数灾难, 这种现象被称, 不同的可能配置数量会随着变量数目的增加而指数级增长, 很多机器学习问题变得相当困难, 特别值得注意的是
        - 摘要：维数灾难发生在计算机科学的许多地方，在机器学习中尤其如此。
          關鍵詞：维数灾难发生在计算机科学的许多地方, 在机器学习中尤其如此
        - 摘要：由维数灾难带来的一个挑战是统计挑战。如图5.9所示，统计挑战产生；于 x 的可能配置数目远大于训练样本的数目。为了充分理解这个问题，；我们假设输入空间如图所示被分成网格。低维时，我们可以用由数据占
          關鍵詞：统计挑战产生, 我们可以用由数据占, 所示, 由维数灾难带来的一个挑战是统计挑战, 我们假设输入空间如图所示被分成网格
        - 摘要：最接近的训练点的输出相同。
          關鍵詞：最接近的训练点的输出相同
        - 摘要：图5.9　当数据的相关维度增大时（从左向右），我们感兴趣的配置数目会随之指数级增长。；（左）在这个一维的例子中，我们用一个变量来区分所感兴趣的仅仅10个区域。当每个区域都；有足够的样本数时（图中每个样本对应了一个细胞），学习算法能够轻易地泛化得很好。泛化
          關鍵詞：我们用一个变量来区分所感兴趣的仅仅, 图中每个样本对应了一个细胞, 当数据的相关维度增大时, 个区域, 有足够的样本数时
        - 摘要：5.11.2　局部不变性和平滑正则化
          關鍵詞：局部不变性和平滑正则化
        - 摘要：为了更好地泛化，机器学习算法需要由先验信念引导应该学习什么类型；的函数。此前，我们已经看到过由模型参数的概率分布形成的先验。通；俗地讲，我们也可以说先验信念直接影响函数本身，而仅仅通过它们对
          關鍵詞：机器学习算法需要由先验信念引导应该学习什么类型, 而仅仅通过它们对, 的函数, 我们也可以说先验信念直接影响函数本身, 此前
        - 摘要：其中使用最广泛的隐式“先验”是平滑先验 （smoothness prior），或局部；不变性先验  （local  constancy  prior）。这个先验表明我们学习的函数不；应在小区域内发生很大的变化。
          關鍵詞：应在小区域内发生很大的变化, 或局部, 其中使用最广泛的隐式, 不变性先验, 这个先验表明我们学习的函数不
        - 摘要：许多简单算法完全依赖于此先验达到良好的泛化，其结果是不能推广去；解决人工智能级别任务中的统计挑战。本书中，我们将介绍深度学习如；何引入额外的（显式或隐式的）先验去降低复杂任务中的泛化误差。这
          關鍵詞：显式或隐式的, 解决人工智能级别任务中的统计挑战, 许多简单算法完全依赖于此先验达到良好的泛化, 其结果是不能推广去, 何引入额外的
        - 摘要：有许多不同的方法来显式或隐式地表示学习函数应该具有光滑或局部不
          關鍵詞：有许多不同的方法来显式或隐式地表示学习函数应该具有光滑或局部不
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；变的先验。所有这些不同的方法都旨在鼓励学习过程能够学习出函数f  ∗；对于大多数设置 x 和小变动  ，都满足条件
          關鍵詞：变的先验, 所有这些不同的方法都旨在鼓励学习过程能够学习出函数, 对于大多数设置, 都满足条件, 和小变动
        - 摘要：换言之，如果我们知道对应输入 x 的答案（例如， x 是个有标签的训练；样本），那么该答案对于 x 的邻域应该也适用。如果在有些邻域中我们；有几个好答案，那么我们可以组合它们（通过某种形式的平均或插值
          關鍵詞：的邻域应该也适用, 有几个好答案, 样本, 换言之, 例如
        - 摘要：局部不变方法的一个极端例子是k-最近邻系列的学习算法。当一个区域；里的所有点  x  在训练集中的k个最近邻是一样的，那么对这些点的预测；也是一样的。当k=1时，不同区域的数目不会比训练样本还多。
          關鍵詞：个最近邻是一样的, 在训练集中的, 当一个区域, 局部不变方法的一个极端例子是, 最近邻系列的学习算法
        - 摘要：虽然k-最近邻算法复制了附近训练样本的输出，大部分核机器也是在和；附近训练样本相关的训练集输出上插值。一类重要的核函数是局部核；（local kernel），其核函数k（ u，ν ）在  u=ν 时很大，当u和ν距离拉大
          關鍵詞：最近邻算法复制了附近训练样本的输出, 大部分核机器也是在和, 距离拉大, 虽然, 附近训练样本相关的训练集输出上插值
        - 摘要：决策树也有平滑学习的局限性，因为它将输入空间分成和叶节点一样多；的区间，并在每个区间使用单独的参数（或者有些决策树的拓展有多个；参数）。如果目标函数需要至少拥有n个叶节点的树才能精确表示，那
          關鍵詞：或者有些决策树的拓展有多个, 参数, 如果目标函数需要至少拥有, 个叶节点的树才能精确表示, 并在每个区间使用单独的参数
        - 摘要：总的来说，区分输入空间中O(k)个区间，所有的这些方法需要O(k)个样；本。通常会有O(k)个参数，O(1)参数对应于O(k)区间之一。最近邻算法；中，每个训练样本至多用于定义一个区间，如图5.10所示。
          關鍵詞：最近邻算法, 所示, 参数对应于, 个区间, 个参数
        - 摘要：图5.10　最近邻算法如何划分输入空间的示例。每个区域内的一个样本（这里用圆圈表示）定；义了区域边界（这里用线表示）。每个样本相关的y值定义了对应区域内所有数据点的输出。由；最近邻定义并且匹配几何模式的区域被称为Voronoi图。这些连续区域的数量不会比训练样本的
          關鍵詞：值定义了对应区域内所有数据点的输出, 这里用圆圈表示, 这些连续区域的数量不会比训练样本的, 最近邻算法如何划分输入空间的示例, 这里用线表示
        - 摘要：有没有什么方法能表示区间数目比训练样本数目还多的复杂函数？显；然，只是假设函数的平滑性不能做到这点。例如，想象目标函数作用在；西洋跳棋盘上。棋盘包含许多变化，但只有一个简单的结构。想象一
          關鍵詞：棋盘包含许多变化, 只是假设函数的平滑性不能做到这点, 但只有一个简单的结构, 有没有什么方法能表示区间数目比训练样本数目还多的复杂函数, 例如
        - 摘要：只要在要学习的真实函数的峰值和谷值处有足够多的样本，那么平滑性；假设和相关的无参数学习算法的效果都非常好。当要学习的函数足够平；滑，并且只在少数几维变化时，这样做一般没问题。在高维空间中，即
          關鍵詞：这样做一般没问题, 并且只在少数几维变化时, 假设和相关的无参数学习算法的效果都非常好, 只要在要学习的真实函数的峰值和谷值处有足够多的样本, 当要学习的函数足够平
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；这些问题，即是否可以有效地表示复杂的函数以及所估计的函数是否可；以很好地泛化到新的输入，答案是有的。关键观点是，只要我们通过额
          關鍵詞：这些问题, 只要我们通过额, 即是否可以有效地表示复杂的函数以及所估计的函数是否可, 答案是有的, 关键观点是
        - 摘要：一些其他的机器学习方法往往会提出更强的、针对特定问题的假设。例；如，假设目标函数是周期性的，我们很容易解决棋盘问题。通常，神经；网络不会包含这些很强的（针对特定任务的）假设，因此神经网络可以
          關鍵詞：针对特定问题的假设, 一些其他的机器学习方法往往会提出更强的, 通常, 针对特定任务的, 神经
        - 摘要：5.11.3　流形学习
          關鍵詞：流形学习
        - 摘要：流形是一个机器学习中很多想法内在的重要概念。
          關鍵詞：流形是一个机器学习中很多想法内在的重要概念
        - 摘要：流形  （manifold）指连接在一起的区域。数学上，它是指一组点，且每；个点都有其邻域。给定一个任意的点，其流形局部看起来像是欧几里得；空间。日常生活中，我们将地球视为二维平面，但实际上它是三维空间
          關鍵詞：它是指一组点, 但实际上它是三维空间, 数学上, 给定一个任意的点, 其流形局部看起来像是欧几里得
        - 摘要：每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位；置。例如在地球表面这个流形中，我们可以朝东南西北走。
          關鍵詞：例如在地球表面这个流形中, 每个点周围邻域的定义暗示着存在变换能够从一个位置移动到其邻域位, 我们可以朝东南西北走
        - 摘要：尽管术语“流形”有正式的数学定义，但是机器学习倾向于更松散地定义；一组点，只需要考虑少数嵌入在高维空间中的自由度或维数就能很好地；近似。每一维都对应着局部的变化方向。如图5.11所示，训练数据位于
          關鍵詞：有正式的数学定义, 所示, 每一维都对应着局部的变化方向, 训练数据位于, 如图
        - 摘要：点到另一个点有所变化。这经常发生于流形和自身相交的情况中。例；如，数字“8”形状的流形在大多数位置只有一维，但在中心的相交处有；两维。
          關鍵詞：但在中心的相交处有, 点到另一个点有所变化, 这经常发生于流形和自身相交的情况中, 两维, 数字
        - 摘要：图5.11　从一个二维空间的分布中抽取的数据样本，这些样本实际上聚集在一维流形附近，像；一个缠绕的带子。实线代表学习器应该推断的隐式流形
          關鍵詞：一个缠绕的带子, 这些样本实际上聚集在一维流形附近, 实线代表学习器应该推断的隐式流形, 从一个二维空间的分布中抽取的数据样本
        - 摘要：如果我们希望机器学习算法学习整个；上有趣变化的函数，那么很；多机器学习问题看上去都是无望的。流形学习  （manifold  learning）算
          關鍵詞：多机器学习问题看上去都是无望的, 流形学习, 如果我们希望机器学习算法学习整个, 那么很, 上有趣变化的函数
        - 摘要：数据位于低维流形的假设并不总是对的或者有用的。我们认为在人工智；能的一些场景中，如涉及处理图像、声音或者文本时，流形假设至少是；近似对的。这个假设的支持证据包含两类观察结果。
          關鍵詞：数据位于低维流形的假设并不总是对的或者有用的, 能的一些场景中, 流形假设至少是, 近似对的, 声音或者文本时
        - 摘要：第一个支持流形假设  （manifold  hypothesis）的观察是现实生活中的图；像、文本、声音的概率分布都是高度集中的。均匀的噪声从来不会与这；类领域的结构化输入类似。图5.12显示均匀采样的点看上去像是没有信
          關鍵詞：的观察是现实生活中的图, 类领域的结构化输入类似, 显示均匀采样的点看上去像是没有信, 第一个支持流形假设, 均匀的噪声从来不会与这
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；大部分字母长序列不对应着自然语言序列：自然语言序列的分布只占了；字母序列的总空间里非常小的一部分。
          關鍵詞：大部分字母长序列不对应着自然语言序列, 字母序列的总空间里非常小的一部分, 自然语言序列的分布只占了
        - 摘要：图5.12　随机地均匀抽取图像（根据均匀分布随机地选择每一个像素）会得到噪声图像。尽管；在人工智能应用中以这种方式生成一个脸或者其他物体的图像是非零概率的，但是实际上我们；从来没有观察到这种现象。这也意味着人工智能应用中遇到的图像在所有图像空间中的占比可
          關鍵詞：但是实际上我们, 从来没有观察到这种现象, 随机地均匀抽取图像, 根据均匀分布随机地选择每一个像素, 会得到噪声图像
        - 摘要：当然，集中的概率分布不足以说明数据位于一个相当小的流形中。我们；还必须确保，所遇到的样本和其他样本相互连接，每个样本被其他高度；相似的样本包围，而这些高度相似的样本可以通过变换来遍历该流形得
          關鍵詞：而这些高度相似的样本可以通过变换来遍历该流形得, 还必须确保, 我们, 相似的样本包围, 所遇到的样本和其他样本相互连接
        - 摘要：到。支持流形假设的第二个论点是，我们至少能够非正式地想象这些邻；域和变换。在图像中，我们当然会认为有很多可能的变换仍然允许我们；描绘出图片空间的流形：我们可以逐渐变暗或变亮光泽、逐步移动或旋
          關鍵詞：我们至少能够非正式地想象这些邻, 我们可以逐渐变暗或变亮光泽, 逐步移动或旋, 在图像中, 我们当然会认为有很多可能的变换仍然允许我们
        - 摘要：这些支持流形假设的思维实验传递了一些支持它的直观理由。更严格的；实验（Cayton，2005；Narayanan  and  Mitter，2010；Schölkopf  et  al.  ，；1998a；Roweis  and  Saul，2000；Tenenbaum  et  al.  ，2000；Brand，
          關鍵詞：这些支持流形假设的思维实验传递了一些支持它的直观理由, 更严格的, 实验
        - 摘要：and  Niyogi，2003b；Donoho
          關鍵詞：
        - 摘要：中的坐标表示；当数据位于低维流形中时，使用流形中的坐标而非；机器学习数据更为自然。日常生活中，我们可以认为道路是嵌入在三维
          關鍵詞：机器学习数据更为自然, 中的坐标表示, 当数据位于低维流形中时, 使用流形中的坐标而非, 我们可以认为道路是嵌入在三维
        - 摘要：图5.13　QMUL Multiview Face数据集中的训练样本（Gong et al. ，2000），其中的物体是移动；的，从而覆盖对应两个旋转角度的二维流形。我们希望学习算法能够发现并且理出这些流形坐；标。图20.6提供了这样一个示例
          關鍵詞：提供了这样一个示例, 我们希望学习算法能够发现并且理出这些流形坐, 其中的物体是移动, 数据集中的训练样本, 从而覆盖对应两个旋转角度的二维流形
        - 摘要：本书第1部分介绍了数学和机器学习中的基本概念，这将用于本书其他
          關鍵詞：本书第, 这将用于本书其他, 部分介绍了数学和机器学习中的基本概念
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；章节中。至此，我们已经做好了研究深度学习的准备。
          關鍵詞：章节中, 我们已经做好了研究深度学习的准备, 至此
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)
          關鍵詞：
        - 摘要：除非有理由使用协方差矩阵的特定结构，我们通常假设其为对角协方差矩阵
          關鍵詞：除非有理由使用协方差矩阵的特定结构, 我们通常假设其为对角协方差矩阵
        - 摘要：。
          關鍵詞：
        - 摘要：第2部分　深度网络：现代实践
          關鍵詞：深度网络, 现代实践, 部分
        - 摘要：本书这一部分总结了现代深度学习用于解决实际应用的现状。
          關鍵詞：本书这一部分总结了现代深度学习用于解决实际应用的现状
        - 摘要：深度学习有着悠久的历史和许多愿景。数种提出的方法尚未完全结出果；实，数个雄心勃勃的目标尚未实现。这些较不发达的深度学习分支将出；现在本书的最后一部分。
          關鍵詞：数种提出的方法尚未完全结出果, 数个雄心勃勃的目标尚未实现, 深度学习有着悠久的历史和许多愿景, 这些较不发达的深度学习分支将出, 现在本书的最后一部分
        - 摘要：本书的第2部分仅关注那些基本上已在工业中大量使用的技术方法。
          關鍵詞：本书的第, 部分仅关注那些基本上已在工业中大量使用的技术方法
        - 摘要：现代深度学习为监督学习提供了一个强大的框架。通过添加更多层以及；向层内添加更多单元，深度网络可以表示复杂性不断增加的函数。给定；足够大的模型和足够大的标注训练数据集，我们可以通过深度学习将输
          關鍵詞：足够大的模型和足够大的标注训练数据集, 通过添加更多层以及, 深度网络可以表示复杂性不断增加的函数, 现代深度学习为监督学习提供了一个强大的框架, 向层内添加更多单元
        - 摘要：本书这一部分描述参数化函数近似技术的核心，几乎所有现代实际应用；的深度学习背后都用到了这一技术。首先，我们描述用于表示这些函数；的前馈深度网络模型。其次，我们提出正则化和优化这种模型的高级技
          關鍵詞：其次, 我们描述用于表示这些函数, 几乎所有现代实际应用, 的前馈深度网络模型, 的深度学习背后都用到了这一技术
        - 摘要：这些章节对于从业者来说是最重要的，也就是说，现在想开始实现和使；用深度学习算法解决现实问题的人需要阅读这些章节。
          關鍵詞：这些章节对于从业者来说是最重要的, 现在想开始实现和使, 也就是说, 用深度学习算法解决现实问题的人需要阅读这些章节
        - 摘要：5.11.1　维数灾难
          關鍵詞：维数灾难
        - 摘要：5.11.2　局部不变性和平；滑正则化
          關鍵詞：滑正则化, 局部不变性和平
        - 摘要：5.11.3　流形学习
          關鍵詞：流形学习
        - 摘要：第2部分　深度网络：现代实践
          關鍵詞：深度网络, 现代实践, 部分
第6章：深度前馈网络
    5.11：促使深度学习发展的挑战
        - 摘要：feedforward  network），也叫作前馈神经网络；net-work）或者多层感知机  （multilayer
          關鍵詞：或者多层感知机, 也叫作前馈神经网络
        - 摘要：深度前馈网络  （deep；（feedforward；neural
          關鍵詞：深度前馈网络
        - 摘要：将输入  x  映射到一个；，并且学习参数  θ  的
          關鍵詞：将输入, 映射到一个, 并且学习参数
        - 摘要：这种模型被称为前向 （feedforward）的，是因为信息流过 x 的函数，流；经用于定义f的中间计算过程，最终到达输出  y  。在模型的输出和模型；本身之间没有反馈  （feedback）连接。当前馈神经网络被扩展成包含反
          關鍵詞：最终到达输出, 经用于定义, 连接, 当前馈神经网络被扩展成包含反, 的函数
        - 摘要：前馈网络对于机器学习的从业者是极其重要的。它们是许多重要商业应；用的基础。例如，用于对照片中的对象进行识别的卷积神经网络就是一；种专门的前馈网络。前馈网络是通往循环网络之路的概念基石，后者在
          關鍵詞：用于对照片中的对象进行识别的卷积神经网络就是一, 后者在, 例如, 前馈网络是通往循环网络之路的概念基石, 种专门的前馈网络
        - 摘要：前馈神经网络之所以被称作网络  （network），是因为它们通常用许多；不同函数复合在一起来表示。该模型与一个有向无环图相关联，而图描；述了函数是如何复合在一起的。例如，我们有三个函数f  (1) 、f  (2)  和f  (3)
          關鍵詞：而图描, 该模型与一个有向无环图相关联, 前馈神经网络之所以被称作网络, 述了函数是如何复合在一起的, 例如
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；必须决定如何使用这些层来最好地实现f * 的近似。因为训练数据并没有；给出这些层中的每一层所需的输出，所以这些层被称为隐藏层  （hidden
          關鍵詞：的近似, 因为训练数据并没有, 给出这些层中的每一层所需的输出, 所以这些层被称为隐藏层, 必须决定如何使用这些层来最好地实现
        - 摘要：最后，这些网络之所以被称为神经网络，是因为它们或多或少地受到神；经科学的启发。网络中的每个隐藏层通常都是向量值的。这些隐藏层的；维数决定了模型的宽度  （width）。向量的每个元素都可以被视为起到
          關鍵詞：向量的每个元素都可以被视为起到, 这些网络之所以被称为神经网络, 经科学的启发, 维数决定了模型的宽度, 是因为它们或多或少地受到神
        - 摘要：一种理解前馈网络的方式是从线性模型开始，并考虑如何克服它的局限；性。线性模型，例如逻辑回归和线性回归，是非常吸引人的，因为无论；是通过闭解形式还是使用凸优化，它们都能高效且可靠地拟合。线性模
          關鍵詞：线性模型, 线性模, 例如逻辑回归和线性回归, 是通过闭解形式还是使用凸优化, 是非常吸引人的
        - 摘要：为了扩展线性模型来表示 x 的非线性函数，我们可以不把线性模型用于；x  本身，而是用在一个变换后的输入φ(  x  )上，这里φ是一个非线性变；换。同样，我们可以使用第5.7.2节中描述的核技巧，来得到一个基于隐
          關鍵詞：本身, 的非线性函数, 节中描述的核技巧, 是一个非线性变, 这里
        - 摘要：剩下的问题就是如何选择映射φ。
          關鍵詞：剩下的问题就是如何选择映射
        - 摘要：（1）其中一种选择是使用一个通用的φ，例如无限维的φ，它隐含地用；在基于RBF核的核机器上。如果φ( x )具有足够高的维数，我们总是有足；够的能力来拟合训练集，但是对于测试集的泛化往往不佳。非常通用的
          關鍵詞：如果, 非常通用的, 它隐含地用, 其中一种选择是使用一个通用的, 在基于
        - 摘要：编码来解决高级问题。
          關鍵詞：编码来解决高级问题
        - 摘要：（2）另一种选择是手动地设计φ。在深度学习出现以前，这一直是主流；的方法。这种方法对于每个单独的任务都需要人们数十年的努力，从业；者各自擅长特定的领域（如语音识别或计算机视觉），并且不同领域之
          關鍵詞：者各自擅长特定的领域, 另一种选择是手动地设计, 如语音识别或计算机视觉, 这种方法对于每个单独的任务都需要人们数十年的努力, 这一直是主流
        - 摘要：（3）深度学习的策略是去学习φ。在这种方法中，我们有一个模型
          關鍵詞：在这种方法中, 深度学习的策略是去学习, 我们有一个模型
        - 摘要：。我们现在有两种参数：用于；从一大类函数中学习φ的参数 θ  ，以及用于将φ( x )映射到所需的输出的；参数  w  。这是深度前馈网络的一个例子，其中φ定义了一个隐藏层。这
          關鍵詞：的参数, 参数, 以及用于将, 从一大类函数中学习, 映射到所需的输出的
        - 摘要：这种通过学习特征来改善模型的一般化原则不仅仅适用于本章描述的前；馈神经网络。它是深度学习中反复出现的主题，适用于本书描述的所有；种类的模型。前馈神经网络是这个原则的应用，它学习从 x 到 y 的确定
          關鍵詞：它学习从, 的确定, 适用于本书描述的所有, 这种通过学习特征来改善模型的一般化原则不仅仅适用于本章描述的前, 前馈神经网络是这个原则的应用
        - 摘要：本章我们先从前馈网络的一个简单例子说起。接着，我们讨论部署一个；前馈网络所需的每个设计决策。首先，训练一个前馈网络至少需要做和；线性模型同样多的设计决策：选择一个优化模型、代价函数以及输出单
          關鍵詞：训练一个前馈网络至少需要做和, 前馈网络所需的每个设计决策, 接着, 我们讨论部署一个, 本章我们先从前馈网络的一个简单例子说起
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；习中需要计算复杂函数的梯度。我们给出反向传播；propagation）算法和它的现代推广，它们可以用来高效地计算这些梯
          關鍵詞：算法和它的现代推广, 习中需要计算复杂函数的梯度, 我们给出反向传播, 它们可以用来高效地计算这些梯
        - 摘要：（back
          關鍵詞：
    6.1：实例：学习XOR
        - 摘要：为了使前馈网络的想法更加具体，我们首先从一个可以完整工作的前馈；网络说起。这个例子解决一个非常简单的任务：学习XOR函数。
          關鍵詞：这个例子解决一个非常简单的任务, 函数, 学习, 我们首先从一个可以完整工作的前馈, 网络说起
        - 摘要：XOR函数（“异或”逻辑）是两个二进制值x  1  和x  2  的运算。当这些二进；制值中恰好有一个为1时，XOR函数返回值为1。其余情况下返回值为；0。XOR函数提供了我们想要学习的目标函数
          關鍵詞：其余情况下返回值为, 当这些二进, 函数, 制值中恰好有一个为, 异或
        - 摘要：。我们的模；，并且我们的学习算法会不断调整参
          關鍵詞：并且我们的学习算法会不断调整参, 我们的模
        - 摘要：在这个简单的例子中，我们不会关心统计泛化。我们希望网络在这4个；点；部这4个点来训练我们的网络，唯一的挑战是拟合训练集。
          關鍵詞：部这, 唯一的挑战是拟合训练集, 我们希望网络在这, 在这个简单的例子中, 我们不会关心统计泛化
        - 摘要：上表现正确。我们会用全
          關鍵詞：我们会用全, 上表现正确
        - 摘要：我们可以把这个问题当作回归问题，并使用均方误差损失函数。选择这；个损失函数是为了尽可能地简化本例中用到的数学知识。在应用领域，；对于二进制数据建模时，MSE通常并不是一个合适的代价函数。更加合
          關鍵詞：通常并不是一个合适的代价函数, 在应用领域, 更加合, 对于二进制数据建模时, 我们可以把这个问题当作回归问题
        - 摘要：评估整个训练集上表现的MSE代价函数为
          關鍵詞：评估整个训练集上表现的, 代价函数为
        - 摘要：我们现在必须要选择模型f  (  x  ;  θ  )的形式。假设选择一个线性模型，  θ；包含 w 和b，那么模型被定义成
          關鍵詞：包含, 假设选择一个线性模型, 那么模型被定义成, 我们现在必须要选择模型, 的形式
        - 摘要：我们可以使用正规方程关于 w 和b最小化J( θ )，来得到一个闭式解。
          關鍵詞：最小化, 来得到一个闭式解, 我们可以使用正规方程关于
        - 摘要：解正规方程以后，我们得到  w  ＝0以及；一点都输出0.5。为什么会发生这种事？图6.1演示了线性模型为什么不；能用来表示XOR函数。解决这个问题的其中一种方法是使用一个模型来
          關鍵詞：解决这个问题的其中一种方法是使用一个模型来, 我们得到, 为什么会发生这种事, 能用来表示, 函数
        - 摘要：。线性模型仅仅是在任意
          關鍵詞：线性模型仅仅是在任意
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；学习一个不同的特征空间，在这个空间上线性模型能够表示这个解。
          關鍵詞：学习一个不同的特征空间, 在这个空间上线性模型能够表示这个解
        - 摘要：图6.1　通过学习一个表示来解决XOR问题。图上的粗体数字标明了学得的函数必须在每个点输；出的值。（左）直接应用于原始输入的线性模型不能实现XOR函数。当x 1 ＝0时，模型的输出；必须随着x 2 的增大而增大。当x 1 ＝1时，模型的输出必须随着x 2 的增大而减小。线性模型必
          關鍵詞：的增大而增大, 图上的粗体数字标明了学得的函数必须在每个点输, 通过学习一个表示来解决, 函数, 模型的输出
        - 摘要：都映射到了特征空间中的单个点
          關鍵詞：都映射到了特征空间中的单个点
        - 摘要：。线性模型现在可以将函数描述为h 1 增大和h 2 减小。在该示例中，学习特征空；间的动机仅仅是使得模型的能力更大，使得它可以拟合训练集。在更现实的应用中，学习的表；示也可以帮助模型泛化
          關鍵詞：使得它可以拟合训练集, 在该示例中, 在更现实的应用中, 间的动机仅仅是使得模型的能力更大, 学习特征空
        - 摘要：具体来说，我们这里引入一个非常简单的前馈神经网络，它有一层隐藏；层并且隐藏层中包含两个单元，见图6.2中对该模型的解释。这个前馈；网络有一个通过函数
          關鍵詞：中对该模型的解释, 它有一层隐藏, 见图, 具体来说, 网络有一个通过函数
        - 摘要：x
          關鍵詞：
        - 摘要：。
          關鍵詞：
        - 摘要：图6.2　使用两种不同样式绘制的前馈网络的示例。具体来说，这是我们用来解决XOR问题的前；馈网络。它有单个隐藏层，包含两个单元。（左）在这种样式中，我们将每个单元绘制为图中；的一个节点。这种风格是清楚而明确的，但对于比这个例子更大的网络，它可能会消耗太多的
          關鍵詞：的一个节点, 使用两种不同样式绘制的前馈网络的示例, 馈网络, 它可能会消耗太多的, 这种风格是清楚而明确的
        - 摘要：描述从 x 到 h 的映射，用向量 w 描述从 h 到y的映射。当标记这
          關鍵詞：用向量, 当标记这, 的映射, 描述从
        - 摘要：f (1) 应该是哪种函数？线性模型到目前为止都表现不错，让f (1) 也是线性；的似乎很有诱惑力。可惜的是，如果f  (1)  是线性的，那么前馈网络作为；一个整体对于输入仍然是线性的。暂时忽略截距项，假设
          關鍵詞：的似乎很有诱惑力, 如果, 一个整体对于输入仍然是线性的, 那么前馈网络作为, 是线性的
        - 摘要：。我们可以将这个函数重新表示成
          關鍵詞：我们可以将这个函数重新表示成
        - 摘要：。
          關鍵詞：
        - 摘要：显然，我们必须用非线性函数来描述这些特征。大多数神经网络通过仿；射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现这个目；标，其中仿射变换由学得的参数控制。我们这里使用这种策略，定义
          關鍵詞：显然, 定义, 射变换之后紧跟着一个被称为激活函数的固定非线性函数来实现这个目, 其中仿射变换由学得的参数控制, 我们这里使用这种策略
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；，其中
          關鍵詞：其中
        - 摘要：是线性变换的权重矩阵，  c  是偏；置。此前，为了描述线性回归模型，我们使用权重向量和一个标量的偏；置参数来描述从输入向量到输出标量的仿射变换。现在，因为描述的是
          關鍵詞：是偏, 因为描述的是, 我们使用权重向量和一个标量的偏, 是线性变换的权重矩阵, 现在
        - 摘要：。在现代神经网络中，默认的推荐是使用由激；活函数g(z)＝max{0,z}定义的整流线性单元  （rectified  linear  unit）或者；称为ReLU（Jarrett et al. ，2009b；Nair and Hinton，2010a；Glorot et al.
          關鍵詞：称为, 活函数, 定义的整流线性单元, 默认的推荐是使用由激, 或者
        - 摘要：图6.3　整流线性激活函数。该激活函数是被推荐用于大多数前馈神经网络的默认激活函数。将；此函数用于线性变换的输出将产生非线性变换。然而，函数仍然非常接近线性，在这种意义上；它是具有两个线性部分的分段线性函数。由于整流线性单元几乎是线性的，因此它们保留了许
          關鍵詞：整流线性激活函数, 它是具有两个线性部分的分段线性函数, 函数仍然非常接近线性, 由于整流线性单元几乎是线性的, 该激活函数是被推荐用于大多数前馈神经网络的默认激活函数
        - 摘要：现在可以指明我们的整个网络是
          關鍵詞：现在可以指明我们的整个网络是
        - 摘要：我们现在可以给出XOR问题的一个解。令
          關鍵詞：我们现在可以给出, 问题的一个解
        - 摘要：以及b＝0。
          關鍵詞：以及
        - 摘要：我们现在可以了解这个模型如何处理一批输入。令   表示设计矩阵，；它包含二进制输入空间中全部的四个点，每个样本占一行，那么矩阵表；示为
          關鍵詞：那么矩阵表, 我们现在可以了解这个模型如何处理一批输入, 示为, 表示设计矩阵, 它包含二进制输入空间中全部的四个点
        - 摘要：神经网络的第一步是将输入矩阵乘以第一层的权重矩阵：
          關鍵詞：神经网络的第一步是将输入矩阵乘以第一层的权重矩阵
        - 摘要：然后，我们加上偏置向量 c ，得到
          關鍵詞：我们加上偏置向量, 得到, 然后
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在这个空间中，所有的样本都处在一条斜率为1的直线上。当我们沿着；这条直线移动时，输出需要从0升到1，然后再降回0。线性模型不能实
          關鍵詞：升到, 然后再降回, 这条直线移动时, 输出需要从, 当我们沿着
        - 摘要：这个变换改变了样本间的关系。它们不再处于同一条直线上了。如图；6.1所示，它们现在处在一个可以用线性模型解决的空间上。
          關鍵詞：它们现在处在一个可以用线性模型解决的空间上, 所示, 它们不再处于同一条直线上了, 这个变换改变了样本间的关系, 如图
        - 摘要：我们最后乘以一个权重向量 w ：
          關鍵詞：我们最后乘以一个权重向量
        - 摘要：神经网络对这一批次中的每个样本都给出了正确的结果。
          關鍵詞：神经网络对这一批次中的每个样本都给出了正确的结果
        - 摘要：在这个例子中，我们简单地指定了解决方案，然后说明它得到的误差为；零。在实际情况中，可能会有数十亿的模型参数以及数十亿的训练样；本，所以不能像我们这里做的那样进行简单地猜解。与之相对的，基于
          關鍵詞：与之相对的, 我们简单地指定了解决方案, 基于, 所以不能像我们这里做的那样进行简单地猜解, 在这个例子中
    6.2：基于梯度的学习
        - 摘要：6.2.1　代价函数
          關鍵詞：代价函数
        - 摘要：6.2.2　输出单元
          關鍵詞：输出单元
        - 摘要：设计和训练神经网络与使用梯度下降训练其他任何机器学习模型并没有
          關鍵詞：设计和训练神经网络与使用梯度下降训练其他任何机器学习模型并没有
        - 摘要：太大不同。在第5.10节中，我们描述了如何通过指定一个优化过程、代；价函数和一个模型族来构建一个机器学习算法。
          關鍵詞：太大不同, 节中, 在第, 价函数和一个模型族来构建一个机器学习算法, 我们描述了如何通过指定一个优化过程
        - 摘要：我们到目前为止看到的线性模型和神经网络的最大区别，在于神经网络；的非线性导致大多数我们感兴趣的代价函数都变得非凸。这意味着神经；网络的训练通常使用迭代的、基于梯度的优化，仅仅使得代价函数达到
          關鍵詞：在于神经网络, 基于梯度的优化, 网络的训练通常使用迭代的, 仅仅使得代价函数达到, 这意味着神经
        - 摘要：我们当然也可以用梯度下降来训练诸如线性回归和支持向量机之类的模；型，并且事实上当训练集相当大时这是很常用的。从这一点来看，训练；神经网络和训练其他任何模型并没有太大区别。计算梯度对于神经网络
          關鍵詞：训练, 神经网络和训练其他任何模型并没有太大区别, 从这一点来看, 我们当然也可以用梯度下降来训练诸如线性回归和支持向量机之类的模, 并且事实上当训练集相当大时这是很常用的
        - 摘要：和其他的机器学习模型一样，为了使用基于梯度的学习方法，我们必须；选择一个代价函数，并且必须选择如何表示模型的输出。现在，我们重；温这些设计上的考虑，并且特别强调神经网络的情景。
          關鍵詞：我们必须, 选择一个代价函数, 和其他的机器学习模型一样, 我们重, 温这些设计上的考虑
        - 摘要：6.2.1　代价函数
          關鍵詞：代价函数
        - 摘要：深度神经网络设计中的一个重要方面是代价函数的选择。幸运的是，神；经网络的代价函数或多或少是和其他的参数模型（例如线性模型的代价；函数）相同的。
          關鍵詞：函数, 深度神经网络设计中的一个重要方面是代价函数的选择, 幸运的是, 经网络的代价函数或多或少是和其他的参数模型, 例如线性模型的代价
        - 摘要：在大多数情况下，参数模型定义了一个分布p ( y | x ; θ )并且简单地使用
          關鍵詞：参数模型定义了一个分布, 并且简单地使用, 在大多数情况下
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；最大似然原理。这意味着我们使用训练数据和模型预测间的交叉熵作为；代价函数。
          關鍵詞：最大似然原理, 这意味着我们使用训练数据和模型预测间的交叉熵作为, 代价函数
        - 摘要：有时，我们使用一个更简单的方法，不是预测 y  的完整概率分布，而是；仅仅预测在给定 x 的条件下 y 的某种统计量。某些专门的损失函数允许；我们来训练这些估计量的预测器。
          關鍵詞：不是预测, 而是, 的条件下, 某些专门的损失函数允许, 有时
        - 摘要：用于训练神经网络的完整的代价函数，通常在我们这里描述的基本代价；函数的基础上结合一个正则项。我们已经在第5.2.2节中看到正则化应用；到线性模型中的一些简单的例子。用于线性模型的权重衰减方法也直接
          關鍵詞：节中看到正则化应用, 用于线性模型的权重衰减方法也直接, 用于训练神经网络的完整的代价函数, 到线性模型中的一些简单的例子, 通常在我们这里描述的基本代价
        - 摘要：6.2.1.1　使用最大似然学习条件分布
          關鍵詞：使用最大似然学习条件分布
        - 摘要：大多数现代的神经网络使用最大似然来训练。这意味着代价函数就是负；的对数似然，它与训练数据和模型分布间的交叉熵等价。这个代价函数；表示为
          關鍵詞：的对数似然, 大多数现代的神经网络使用最大似然来训练, 这个代价函数, 表示为, 这意味着代价函数就是负
        - 摘要：代价函数的具体形式随着模型而改变，取决于；的具体形；式。上述方程的展开形式通常会有一些项不依赖于模型的参数，我们可
          關鍵詞：上述方程的展开形式通常会有一些项不依赖于模型的参数, 取决于, 我们可, 代价函数的具体形式随着模型而改变, 的具体形
        - 摘要：，那么我们就重新得到了
          關鍵詞：那么我们就重新得到了
        - 摘要：均方误差代价：
          關鍵詞：均方误差代价
        - 摘要：至少系数   和常数项不依赖于  θ  。舍弃的常数是基于高斯分布的方
          關鍵詞：和常数项不依赖于, 舍弃的常数是基于高斯分布的方, 至少系数
        - 摘要：差，在这种情况下，我们选择不把它参数化。之前，我们看到了对输出；分布的最大似然估计和对线性模型均方误差的最小化之间的等价性，但；事实上，这种等价性并不要求f ( x ; θ )用于预测高斯分布的均值。
          關鍵詞：我们看到了对输出, 事实上, 在这种情况下, 用于预测高斯分布的均值, 分布的最大似然估计和对线性模型均方误差的最小化之间的等价性
        - 摘要：使用最大似然来导出代价函数的方法的一个优势是，它减轻了为每个模
          關鍵詞：使用最大似然来导出代价函数的方法的一个优势是, 它减轻了为每个模
        - 摘要：型设计代价函数的负担。明确一个模型p ( y |  x  )则自动地确定了一个代；价函数logp ( y | x )。
          關鍵詞：型设计代价函数的负担, 价函数, 明确一个模型, 则自动地确定了一个代
        - 摘要：贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的；大和具有足够的预测性，来为学习算法提供一个好的指引。饱和（变得；非常平）的函数破坏了这一目标，因为它们把梯度变得非常小。这在很
          關鍵詞：贯穿神经网络设计的一个反复出现的主题是代价函数的梯度必须足够的, 变得, 的函数破坏了这一目标, 因为它们把梯度变得非常小, 这在很
        - 摘要：用于实现最大似然估计的交叉熵代价函数有一个不同寻常的特性，那就；是当它被应用于实践中经常遇到的模型时，它通常没有最小值。对于离；散型输出变量，大多数模型以一种特殊的形式来参数化，即它们不能表
          關鍵詞：它通常没有最小值, 散型输出变量, 用于实现最大似然估计的交叉熵代价函数有一个不同寻常的特性, 那就, 是当它被应用于实践中经常遇到的模型时
        - 摘要：6.2.1.2　学习条件统计量
          關鍵詞：学习条件统计量
        - 摘要：有时我们并不是想学习一个完整的概率分布p ( y | x ; θ )，而仅仅是想学；习在给定 x 时 y 的某个条件统计量。
          關鍵詞：有时我们并不是想学习一个完整的概率分布, 的某个条件统计量, 习在给定, 而仅仅是想学
        - 摘要：例如，我们可能有一个预测器f ( x ; θ )，想用它来预测 y  的均值。如果；使用一个足够强大的神经网络，我们可以认为这个神经网络能够表示一；大类函数中的任何一个函数f，这个类仅仅被一些特征所限制，例如连
          關鍵詞：如果, 例如连, 这个类仅仅被一些特征所限制, 使用一个足够强大的神经网络, 例如
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；特殊的函数上，这个函数将 x 映射到给定 x 时 y 的期望值。对函数求解；优化问题需要用到变分法 （calculus of variations）这个数学工具，我们
          關鍵詞：这个函数将, 映射到给定, 优化问题需要用到变分法, 特殊的函数上, 我们
        - 摘要：我们使用变分法导出的第一个结果是解优化问题：
          關鍵詞：我们使用变分法导出的第一个结果是解优化问题
        - 摘要：得到
          關鍵詞：得到
        - 摘要：要求这个函数处在我们要优化的类里。换句话说，如果我们能够用无穷；多的、来源于真实的数据生成分布的样本进行训练，最小化均方误差代；价函数将得到一个函数，它可以用来对每个 x 的值预测出 y 的均值。
          關鍵詞：的值预测出, 来源于真实的数据生成分布的样本进行训练, 最小化均方误差代, 价函数将得到一个函数, 它可以用来对每个
        - 摘要：不同的代价函数给出不同的统计量。第二个使用变分法得到的结果是
          關鍵詞：不同的代价函数给出不同的统计量, 第二个使用变分法得到的结果是
        - 摘要：将得到一个函数可以对每个 x 预测 y 取值的中位数，只要这个函数在我；们要优化的函数族里。这个代价函数通常被称为平均绝对误差  （mean；absolute error）。
          關鍵詞：这个代价函数通常被称为平均绝对误差, 取值的中位数, 们要优化的函数族里, 预测, 只要这个函数在我
        - 摘要：可惜的是，均方误差和平均绝对误差在使用基于梯度的优化方法时往往；成效不佳。一些饱和的输出单元当结合这些代价函数时会产生非常小的；梯度。这就是交叉熵代价函数比均方误差或者平均绝对误差更受欢迎的
          關鍵詞：梯度, 均方误差和平均绝对误差在使用基于梯度的优化方法时往往, 一些饱和的输出单元当结合这些代价函数时会产生非常小的, 可惜的是, 成效不佳
        - 摘要：6.2.2　输出单元
          關鍵詞：输出单元
        - 摘要：代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地；使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵；函数的形式。
          關鍵詞：代价函数的选择与输出单元的选择紧密相关, 使用数据分布和模型分布间的交叉熵, 我们简单地, 函数的形式, 大多数时候
        - 摘要：任何可用作输出的神经网络单元，也可以被用作隐藏单元。这里，我们
          關鍵詞：这里, 任何可用作输出的神经网络单元, 也可以被用作隐藏单元, 我们
        - 摘要：着重讨论将这些单元用作模型输出时的情况，不过原则上它们也可以在；内部使用。我们将在第6.3节中重温这些单元，并且给出当它们被用作；隐藏单元时一些额外的细节。
          關鍵詞：隐藏单元时一些额外的细节, 内部使用, 节中重温这些单元, 我们将在第, 着重讨论将这些单元用作模型输出时的情况
        - 摘要：在本节中，我们假设前馈网络提供了一组定义为h  =f  (  x  ;  θ  )的隐藏特；征。输出层的作用是随后对这些特征进行一些额外的变换来完成整个网；络必须完成的任务。
          關鍵詞：的隐藏特, 在本节中, 输出层的作用是随后对这些特征进行一些额外的变换来完成整个网, 我们假设前馈网络提供了一组定义为, 络必须完成的任务
        - 摘要：6.2.2.1　用于高斯输出分布的线性单元
          關鍵詞：用于高斯输出分布的线性单元
        - 摘要：一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线；性。这些单元往往被直接称为线性单元。
          關鍵詞：这些单元往往被直接称为线性单元, 仿射变换不具有非线, 一种简单的输出单元是基于仿射变换的输出单元
        - 摘要：给定特征 h ，线性输出单元层产生一个向量
          關鍵詞：给定特征, 线性输出单元层产生一个向量
        - 摘要：线性输出层经常被用来产生条件高斯分布的均值：
          關鍵詞：线性输出层经常被用来产生条件高斯分布的均值
        - 摘要：最大化其对数似然此时等价于最小化均方误差。
          關鍵詞：最大化其对数似然此时等价于最小化均方误差
        - 摘要：最大似然框架也使得学习高斯分布的协方差矩阵更加容易，或更容易地；使高斯分布的协方差矩阵作为输入的函数。然而，对于所有输入，协方；差矩阵都必须被限定成一个正定矩阵。线性输出层很难满足这种限定，
          關鍵詞：差矩阵都必须被限定成一个正定矩阵, 最大似然框架也使得学习高斯分布的协方差矩阵更加容易, 然而, 对于所有输入, 使高斯分布的协方差矩阵作为输入的函数
        - 摘要：因为线性单元不会饱和，所以它们易于采用基于梯度的优化算法，甚至；可以使用其他多种优化算法。
          關鍵詞：因为线性单元不会饱和, 甚至, 可以使用其他多种优化算法, 所以它们易于采用基于梯度的优化算法
        - 摘要：6.2.2.2　用于Bernoulli输出分布的sigmoid单元
          關鍵詞：用于, 单元, 输出分布的
        - 摘要：许多任务需要预测二值型变量y的值。具有两个类的分类问题可以归结；为这种形式。
          關鍵詞：的值, 许多任务需要预测二值型变量, 具有两个类的分类问题可以归结, 为这种形式
        - 摘要：此时最大似然的方法是定义y在 x 条件下的Bernoulli分布。
          關鍵詞：条件下的, 此时最大似然的方法是定义, 分布
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Bernoulli分布仅需单个参数来定义。神经网络只需要预测
          關鍵詞：分布仅需单个参数来定义, 神经网络只需要预测
        - 摘要：即可。为了使这个数是有效的概率，它必须处在区间
          關鍵詞：为了使这个数是有效的概率, 它必须处在区间, 即可
        - 摘要：［0,1］中。
          關鍵詞：
        - 摘要：为满足该约束条件需要一些细致的设计工作。假设我们打算使用线性单；元，并且通过阈值来限制它成为一个有效的概率：
          關鍵詞：并且通过阈值来限制它成为一个有效的概率, 为满足该约束条件需要一些细致的设计工作, 假设我们打算使用线性单
        - 摘要：这的确定义了一个有效的条件概率分布，但我们无法使用梯度下降来高；效地训练它。当；b处于单位区间外时，模型的输出对其参数的
          關鍵詞：处于单位区间外时, 效地训练它, 但我们无法使用梯度下降来高, 模型的输出对其参数的, 这的确定义了一个有效的条件概率分布
        - 摘要：相反，最好是使用一种新的方法来保证无论何时模型给出了错误的答案；时，总能有一个较大的梯度。这种方法是基于使用sigmoid输出单元结；合最大似然来实现的。
          關鍵詞：这种方法是基于使用, 输出单元结, 总能有一个较大的梯度, 最好是使用一种新的方法来保证无论何时模型给出了错误的答案, 相反
        - 摘要：sigmoid输出单元定义为
          關鍵詞：输出单元定义为
        - 摘要：这里σ是第3.10节中介绍的logistic sigmoid函数。
          關鍵詞：函数, 这里, 是第, 节中介绍的
        - 摘要：我们可以认为sigmoid输出单元具有两个部分。首先，它使用一个线性；层来计算z＝；率。
          關鍵詞：输出单元具有两个部分, 它使用一个线性, 层来计算, 首先, 我们可以认为
        - 摘要：。其次，它使用sigmoid激活函数将z转化成概
          關鍵詞：其次, 它使用, 转化成概, 激活函数将
        - 摘要：我们暂时忽略对于  x  的依赖性，只讨论如何用z的值来定义y的概率分；布。sigmoid可以通过构造一个非归一化（和不为1）的概率分布；来得到。我们可以随后除以一个合适的常数来得到有效的概率分布。如
          關鍵詞：的概率分布, 来得到, 只讨论如何用, 的依赖性, 的概率分
        - 摘要：基于指数和归一化的概率分布在统计建模的文献中很常见。用于定义这；种二值型变量分布的变量z被称为分对数 （logit）。
          關鍵詞：被称为分对数, 种二值型变量分布的变量, 用于定义这, 基于指数和归一化的概率分布在统计建模的文献中很常见
        - 摘要：这种在对数空间里预测概率的方法可以很自然地使用最大似然学习。因；，代价函数中的log抵；为用于最大似然的代价函数是
          關鍵詞：这种在对数空间里预测概率的方法可以很自然地使用最大似然学习, 为用于最大似然的代价函数是, 代价函数中的
        - 摘要：这个推导使用了第3.10节中的一些性质。通过将损失函数写成softplus函；数的形式，我们可以看到它仅仅在(1−2y)z取绝对值非常大的负值时才会；饱和。因此饱和只会出现在模型已经得到正确答案时——当y＝1且z取
          關鍵詞：取绝对值非常大的负值时才会, 通过将损失函数写成, 数的形式, 因此饱和只会出现在模型已经得到正确答案时, 这个推导使用了第
        - 摘要：当我们使用其他的损失函数，例如均方误差之类的，损失函数就会在；σ(z)饱和时饱和。sigmoid激活函数在z取非常小的负值时会饱和到0，当；z取非常大的正值时会饱和到1。这种情况一旦发生，梯度会变得非常小
          關鍵詞：当我们使用其他的损失函数, 梯度会变得非常小, 这种情况一旦发生, 激活函数在, 取非常小的负值时会饱和到
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；理论上，sigmoid的对数总是确定和有限的，因为sigmoid的返回值总是；被限制在开区间（0，1）上，而不是使用整个闭区间［0，1］的有效概
          關鍵詞：因为, 的有效概, 被限制在开区间, 的返回值总是, 的对数总是确定和有限的
        - 摘要：的函数。如果sigmoid函数下溢到零，那么之
          關鍵詞：那么之, 如果, 函数下溢到零, 的函数
        - 摘要：6.2.2.3　用于Multinoulli输出分布的softmax单元
          關鍵詞：用于, 单元, 输出分布的
        - 摘要：任何时候，当我们想要表示一个具有n个可能取值的离散型随机变量的；分布时，都可以使用softmax函数。它可以看作sigmoid函数的扩展，其；中sigmoid函数用来表示二值型变量的分布。
          關鍵詞：当我们想要表示一个具有, 都可以使用, 函数用来表示二值型变量的分布, 任何时候, 函数
        - 摘要：softmax函数最常用作分类器的输出，来表示n个不同类上的概率分布。；比较少见的是，softmax函数可以在模型内部使用，例如，如果我们想；要在某个内部变量的n个不同选项中进行选择。
          關鍵詞：个不同类上的概率分布, 要在某个内部变量的, 函数可以在模型内部使用, 例如, 个不同选项中进行选择
        - 摘要：在二值型变量的情况下，我们希望计算一个单独的数
          關鍵詞：在二值型变量的情况下, 我们希望计算一个单独的数
        - 摘要：因为这个数需要处在0和1之间，并且我们想要让这个数的对数可以很好；地用于对数似然的基于梯度的优化，因而我们选择去预测另外一个数
          關鍵詞：地用于对数似然的基于梯度的优化, 之间, 并且我们想要让这个数的对数可以很好, 因为这个数需要处在, 因而我们选择去预测另外一个数
        - 摘要：。对其指数化和归一化，就得到了一个由
          關鍵詞：就得到了一个由, 对其指数化和归一化
        - 摘要：sigmoid函数控制的Bernoulli分布。
          關鍵詞：分布, 函数控制的
        - 摘要：为了推广到具有n个值的离散型变量的情况，现在需要创造一个向量；，它的每个元素是；介于0和1之间，还要使得整个向量的和为1，使得它表示一个有效的概
          關鍵詞：为了推广到具有, 使得它表示一个有效的概, 还要使得整个向量的和为, 个值的离散型变量的情况, 它的每个元素是
        - 摘要：。我们不仅要求每个  元素
          關鍵詞：我们不仅要求每个, 元素
        - 摘要：其中；化来获得需要的  。最终，softmax函数的形式为
          關鍵詞：函数的形式为, 化来获得需要的, 其中, 最终
        - 摘要：。softmax函数然后可以对z指数化和归一
          關鍵詞：指数化和归一, 函数然后可以对
        - 摘要：和logistic  sigmoid一样，当使用最大化对数似然训练softmax来输出目标；值y时，使用指数函数工作地非常好。这种情况下，我们想要最大化
          關鍵詞：这种情况下, 我们想要最大化, 一样, 来输出目标, 当使用最大化对数似然训练
        - 摘要：。将softmax定义成指数
          關鍵詞：定义成指数
        - 摘要：的形式是很自然的，因为对数似然中的log可以抵消softmax中的exp：
          關鍵詞：中的, 的形式是很自然的, 可以抵消, 因为对数似然中的
        - 摘要：式（6.30）中的第一项表示输入z  i  总是对代价函数有直接的贡献。因为；这一项不会饱和，所以即使z  i  对式（6.30）的第二项的贡献很小，学习；依然可以进行。当最大化对数似然时，第一项鼓励z i 被推高，而第二项
          關鍵詞：因为, 总是对代价函数有直接的贡献, 第一项鼓励, 依然可以进行, 被推高
        - 摘要：则鼓励所有的z被压低。为了对第二项
          關鍵詞：则鼓励所有的, 被压低, 为了对第二项
        - 摘要：有一个直观的理解，注意到这一项可以大致近似为max  j  z  j  。这种近似；是基于对任何明显小于max j z j 的z k ，exp(z k )都是不重要的。我们能从；这种近似中得到的直觉是，负对数似然代价函数总是强烈地惩罚最活跃
          關鍵詞：注意到这一项可以大致近似为, 这种近似中得到的直觉是, 负对数似然代价函数总是强烈地惩罚最活跃, 我们能从, 都是不重要的
        - 摘要：i
          關鍵詞：
        - 摘要：项将大致抵消。这个样本对于整体训练代价贡献很小，这个代价主要由；其他未被正确分类的样本产生。
          關鍵詞：项将大致抵消, 其他未被正确分类的样本产生, 这个样本对于整体训练代价贡献很小, 这个代价主要由
        - 摘要：到目前为止，我们只讨论了一个例子。总体来说，未正则化的最大似然；会驱动模型去学习一些参数，而这些参数会驱动softmax函数来预测在；训练集中观察到的每个结果的比率：
          關鍵詞：未正则化的最大似然, 总体来说, 会驱动模型去学习一些参数, 到目前为止, 而这些参数会驱动
        - 摘要：因为最大似然是一致的估计量，所以只要模型族能够表示训练的分布，；这就能保证发生。在实践中，有限的模型能力和不完美的优化将意味着
          關鍵詞：所以只要模型族能够表示训练的分布, 因为最大似然是一致的估计量, 这就能保证发生, 有限的模型能力和不完美的优化将意味着, 在实践中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；模型只能近似这些比率。
          關鍵詞：模型只能近似这些比率
        - 摘要：对数似然之外的许多目标函数对softmax函数不起作用。具体来说，那；些不使用对数来抵消softmax中的指数的目标函数，当指数函数的变量；取非常小的负值时会造成梯度消失，从而无法学习。特别是平方误差，
          關鍵詞：中的指数的目标函数, 对数似然之外的许多目标函数对, 当指数函数的变量, 特别是平方误差, 具体来说
        - 摘要：像sigmoid一样，softmax激活函数可能会饱和。sigmoid函数具有单个输；出，当它的输入极端负或者极端正时会饱和。对于softmax的情况，它；有多个输出值。当输入值之间的差异变得极端时，这些输出值可能饱
          關鍵詞：激活函数可能会饱和, 这些输出值可能饱, 有多个输出值, 函数具有单个输, 的情况
        - 摘要：为了说明softmax函数对于输入之间差异的响应，观察到当对所有的输；入都加上一个相同常数时softmax的输出不变：
          關鍵詞：函数对于输入之间差异的响应, 入都加上一个相同常数时, 观察到当对所有的输, 为了说明, 的输出不变
        - 摘要：使用这个性质，我们可以导出一个数值方法稳定的softmax函数的变；体：
          關鍵詞：函数的变, 使用这个性质, 我们可以导出一个数值方法稳定的
        - 摘要：变换后的形式允许我们在对softmax函数求值时只有很小的数值误差，；即使是当z包含极正或者极负的数时。观察softmax数值稳定的变体，可；以看到softmax函数由它的变量偏离max i z i 的量来驱动。
          關鍵詞：即使是当, 函数由它的变量偏离, 包含极正或者极负的数时, 数值稳定的变体, 观察
        - 摘要：当其中一个输入是最大(z i ＝max i z i )并且z i 远大于其他的输入时，相应；的输出softmax(z)  i  会饱和到1。当z  i  不是最大值并且最大值非常大时，；相应的输出softmax(z)  i  也会饱和到0。这是sigmoid单元饱和方式的一般
          關鍵詞：相应的输出, 会饱和到, 当其中一个输入是最大, 的输出, 单元饱和方式的一般
        - 摘要：softmax函数的变量z可以通过两种方式产生。最常见的是简单地使神经；网络较早的层输出z的每个元素，就像先前描述的使用线性层
          關鍵詞：的每个元素, 网络较早的层输出, 函数的变量, 可以通过两种方式产生, 就像先前描述的使用线性层
        - 摘要：。虽然很直观，但这种方法是对分布的过度参数；化。n个输出总和必须为1的约束意味着只有n−1个参数是必要的；第n个；概率值可以通过1减去前面n−1个概率来获得。因此，我们可以强制要求
          關鍵詞：虽然很直观, 概率值可以通过, 减去前面, 个概率来获得, 因此
        - 摘要：从神经科学的角度看，有趣的是认为softmax是一种在参与其中的单元；之间形成竞争的方式：softmax输出总是和为1，所以一个单元的值增加；必然对应着其他单元值的减少。这与被认为存在于皮质中相邻神经元间
          關鍵詞：必然对应着其他单元值的减少, 输出总是和为, 是一种在参与其中的单元, 这与被认为存在于皮质中相邻神经元间, 之间形成竞争的方式
        - 摘要：“softmax”的名称可能会让人产生困惑。这个函数更接近于argmax函数而；不是max函数。“soft”这个术语来源于softmax函数是连续可微；的。“argmax”函数的结果表示为一个one-hot向量（只有一个元素为1，
          關鍵詞：函数, 这个术语来源于, 的名称可能会让人产生困惑, 函数而, 向量
        - 摘要：6.2.2.4　其他的输出类型
          關鍵詞：其他的输出类型
        - 摘要：之前描述的线性、sigmoid和softmax输出单元是最常见的。神经网络可；以推广到我们希望的几乎任何种类的输出层。最大似然原则给如何为几；乎任何种类的输出层设计一个好的代价函数提供了指导。
          關鍵詞：乎任何种类的输出层设计一个好的代价函数提供了指导, 之前描述的线性, 最大似然原则给如何为几, 神经网络可, 以推广到我们希望的几乎任何种类的输出层
        - 摘要：一般而言，如果我们定义了一个条件分布p ( y | x ; θ )，最大似然原则建；议我们使用-logp ( y | x ; θ ) 作为代价函数。
          關鍵詞：最大似然原则建, 一般而言, 如果我们定义了一个条件分布, 议我们使用, 作为代价函数
        - 摘要：一般来说，我们可以认为神经网络表示函数f ( x ; θ ) 。这个函数的输出
          關鍵詞：这个函数的输出, 我们可以认为神经网络表示函数, 一般来说
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；不是对 y 值的直接预测。相反，；我们的损失函数就可以表示成
          關鍵詞：我们的损失函数就可以表示成, 不是对, 值的直接预测, 相反
        - 摘要：提供了y分布的参数。
          關鍵詞：分布的参数, 提供了
        - 摘要：。
          關鍵詞：
        - 摘要：例如，我们想要学习在给定 x 时， y 的条件高斯分布的方差。简单情况；下，方差σ  2  是一个常数，此时有一个解析表达式，这是因为方差的最；大似然估计量仅仅是观测值y与它们的期望值的差值的平方平均。一种
          關鍵詞：的条件高斯分布的方差, 这是因为方差的最, 与它们的期望值的差值的平方平均, 例如, 我们想要学习在给定
        - 摘要：数可以是σ本身，或者可以是表示σ  2  的参数ν，或者可以是表示
          關鍵詞：本身, 的参数, 或者可以是表示, 数可以是
        - 摘要：的
          關鍵詞：
        - 摘要：参数β，取决于我们怎样对分布参数化。我们可能希望模型对不同的  x；值预测出  y  不同的方差。这被称为异方差  （heteroscedastic）模型。在；异方差情况下，我们简单地把方差指定为f (  x  ;  θ  )其中一个输出值。实
          關鍵詞：参数, 我们简单地把方差指定为, 其中一个输出值, 模型, 我们可能希望模型对不同的
        - 摘要：这个公式适用于梯度下降，因为由 β 参数化的高斯分布的对数似然的公；式仅涉及   的乘法和；的加法。乘法、加法和对数运算的梯度
          關鍵詞：参数化的高斯分布的对数似然的公, 乘法, 因为由, 式仅涉及, 的乘法和
        - 摘要：获得正的精度向量：；同样适用，也适用于常数乘以单位阵的情况。
          關鍵詞：获得正的精度向量, 也适用于常数乘以单位阵的情况, 同样适用
        - 摘要：。这种相同的策略对于方差或标准差
          關鍵詞：这种相同的策略对于方差或标准差
        - 摘要：学习一个比对角矩阵具有更丰富结构的协方差或者精度矩阵是很少见；的。如果协方差矩阵是“满的”和有条件的，那么参数化的选择就必须要；保证预测的协方差矩阵是正定的。这可以通过写成
          關鍵詞：和有条件的, 满的, 学习一个比对角矩阵具有更丰富结构的协方差或者精度矩阵是很少见, 这可以通过写成, 如果协方差矩阵是
        - 摘要：来实现，这里   是一个无约束的方阵。如果
          關鍵詞：如果, 这里, 是一个无约束的方阵, 来实现
        - 摘要：矩阵是满秩的，那么一个实际问题是计算似然的代价很高，计算一个；d×d的矩阵的行列式或者；特征值分解或者
          關鍵詞：计算一个, 的矩阵的行列式或者, 矩阵是满秩的, 那么一个实际问题是计算似然的代价很高, 特征值分解或者
        - 摘要：的特征值分解）需要O(d 3 )的计算量。
          關鍵詞：的特征值分解, 需要, 的计算量
        - 摘要：的逆（或者等价地并且更常用地，对它
          關鍵詞：对它, 或者等价地并且更常用地, 的逆
        - 摘要：我们经常想要执行多峰回归（multimodal；regression），即预测条件分；布p ( y | x )的实值，该条件分布对于相同的  x 值在 y 空间中有多个不同
          關鍵詞：该条件分布对于相同的, 我们经常想要执行多峰回归, 即预测条件分, 值在, 的实值
        - 摘要：神经网络必须有3个输出：定义
          關鍵詞：定义, 个输出, 神经网络必须有
        - 摘要：的矩阵，以及对所有的i给出
          關鍵詞：以及对所有的, 的矩阵, 给出
        - 摘要：的向量，对所有的i给出；的张量。这些输出必须
          關鍵詞：的张量, 对所有的, 这些输出必须, 给出, 的向量
        - 摘要：满足不同的约束：
          關鍵詞：满足不同的约束
        - 摘要：（1）混合组件；组件上形成Multinoulli分布。这个分布通常可以由n维向量的softmax来；获得，以确保这些输出是正的并且和为1。
          關鍵詞：获得, 组件上形成, 混合组件, 以确保这些输出是正的并且和为, 分布
        - 摘要：：它们由潜变量  (2)  c关联着，在n个不同
          關鍵詞：个不同, 它们由潜变量, 关联着
        - 摘要：（2）均值；：它们指明了与第i个高斯组件相关联的中心或者均；值，并且是无约束的（通常对于这些输出单元完全没有非线性）。如果
          關鍵詞：如果, 个高斯组件相关联的中心或者均, 它们指明了与第, 并且是无约束的, 均值
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；表达式将每个样本对每个组件的贡献进行赋权，权重的大小由相应的组；件产生这个样本的概率来决定。
          關鍵詞：权重的大小由相应的组, 表达式将每个样本对每个组件的贡献进行赋权, 件产生这个样本的概率来决定
        - 摘要：：它们指明了每个组件i的协方差矩阵。和学习单；（3）协方差；个高斯组件时一样，我们通常使用对角矩阵来避免计算行列式。和学习
          關鍵詞：的协方差矩阵, 和学习单, 和学习, 我们通常使用对角矩阵来避免计算行列式, 协方差
        - 摘要：有报告说，基于梯度的优化方法对于混合条件高斯（作为神经网络的输；出）可能是不可靠的，部分是因为涉及除法（除以方差）可能是数值不；稳定的（当某个方差对于特定的实例变得非常小时，会导致非常大的梯
          關鍵詞：可能是数值不, 基于梯度的优化方法对于混合条件高斯, 稳定的, 作为神经网络的输, 除以方差
        - 摘要：高斯混合输出在语音生成模型（Schuster，1999）和物理运动；（Graves，2013）中特别有效。混合密度策略为网络提供了一种方法来；表示多种输出模式，并且控制输出的方差，这对于在这些实数域中获得
          關鍵詞：混合密度策略为网络提供了一种方法来, 中特别有效, 这对于在这些实数域中获得, 表示多种输出模式, 高斯混合输出在语音生成模型
        - 摘要：一般地，我们可能希望继续对包含更多变量的、更大的向量 y  来建模，；并在这些输出变量上施加更多更丰富的结构。例如，可能希望神经网络；输出字符序列形成一个句子。在这些情况下，我们可以继续使用最大似
          關鍵詞：输出字符序列形成一个句子, 更大的向量, 来建模, 例如, 我们可以继续使用最大似
        - 摘要：图6.4　从具有混合密度输出层的神经网络中抽取的样本。输入 x 从均匀分布中采样，输出y从 p；model ( y |x）中采样。神经网络能够学习从输入到输出分布的参数的非线性映射。这些参数包括；控制3个组件中的哪一个将产生输出的概率，以及每个组件各自的参数。每个混合组件都是高斯
          關鍵詞：每个混合组件都是高斯, 这些参数包括, 输入, 从具有混合密度输出层的神经网络中抽取的样本, 从均匀分布中采样
        - 摘要：6.2.1　代价函数
          關鍵詞：代价函数
        - 摘要：6.2.2　输出单元
          關鍵詞：输出单元
    6.3：隐藏单元
        - 摘要：6.3.1　整流线性单元及其扩展
          關鍵詞：整流线性单元及其扩展
        - 摘要：6.3.2　logistic sigmoid与双曲正切函数
          關鍵詞：与双曲正切函数
        - 摘要：6.3.3　其他隐藏单元
          關鍵詞：其他隐藏单元
        - 摘要：到目前为止，我们集中讨论了神经网络的设计选择，这对于使用基于梯；度的优化方法来训练的大多数参数化机器学习模型都是通用的。现在我；们转向一个前馈神经网络独有的问题：该如何选择隐藏单元的类型，这
          關鍵詞：现在我, 这对于使用基于梯, 度的优化方法来训练的大多数参数化机器学习模型都是通用的, 到目前为止, 我们集中讨论了神经网络的设计选择
        - 摘要：隐藏单元的设计是一个非常活跃的研究领域，并且还没有许多明确的指；导性理论原则。
          關鍵詞：导性理论原则, 隐藏单元的设计是一个非常活跃的研究领域, 并且还没有许多明确的指
        - 摘要：整流线性单元是隐藏单元极好的默认选择。许多其他类型的隐藏单元也；是可用的。决定何时使用哪种类型的隐藏单元是困难的事（尽管整流线；性单元通常是一个可接受的选择）。我们这里描述对于每种隐藏单元的
          關鍵詞：尽管整流线, 许多其他类型的隐藏单元也, 性单元通常是一个可接受的选择, 整流线性单元是隐藏单元极好的默认选择, 我们这里描述对于每种隐藏单元的
        - 摘要：这里列出的一些隐藏单元可能并不是在所有的输入点上都是可微的。例；如，整流线性单元g(z)＝max{0,z}在z＝0处不可微。这似乎使得g对于基
          關鍵詞：处不可微, 这似乎使得, 对于基, 这里列出的一些隐藏单元可能并不是在所有的输入点上都是可微的, 整流线性单元
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；于梯度的学习算法无效。在实践中，梯度下降对这些机器学习模型仍然；表现得足够好。部分原因是神经网络训练算法通常不会达到代价函数的
          關鍵詞：表现得足够好, 部分原因是神经网络训练算法通常不会达到代价函数的, 梯度下降对这些机器学习模型仍然, 在实践中, 于梯度的学习算法无效
        - 摘要：除非另有说明，大多数的隐藏单元都可以描述为接受输入向量 x ，计算；仿射变换z＝；，然后使用一个逐元素的非线性函数g(z)。大
          關鍵詞：然后使用一个逐元素的非线性函数, 仿射变换, 除非另有说明, 计算, 大多数的隐藏单元都可以描述为接受输入向量
        - 摘要：6.3.1　整流线性单元及其扩展
          關鍵詞：整流线性单元及其扩展
        - 摘要：整流线性单元使用激活函数g(z)＝max{0,z}。
          關鍵詞：整流线性单元使用激活函数
        - 摘要：整流线性单元易于优化，因为它们和线性单元非常类似。线性单元和整；流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为；零。这使得只要整流线性单元处于激活状态，它的导数都能保持较大。
          關鍵詞：整流线性单元易于优化, 线性单元和整, 这使得只要整流线性单元处于激活状态, 它的导数都能保持较大, 流线性单元的唯一区别在于整流线性单元在其一半的定义域上输出为
        - 摘要：整流线性单元通常作用于仿射变换之上：
          關鍵詞：整流线性单元通常作用于仿射变换之上
        - 摘要：当初始化仿射变换的参数时，可以将  b  的所有元素设置成一个小的正；值，例如0.1。这使得整流线性单元很可能初始时就对训练集中的大多；数输入呈现激活状态，并且允许导数通过。
          關鍵詞：当初始化仿射变换的参数时, 可以将, 例如, 的所有元素设置成一个小的正, 并且允许导数通过
        - 摘要：有很多整流线性单元的扩展存在。大多数这些扩展的表现比得上整流线；性单元，并且偶尔表现得更好。
          關鍵詞：有很多整流线性单元的扩展存在, 性单元, 大多数这些扩展的表现比得上整流线, 并且偶尔表现得更好
        - 摘要：整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它；们激活为零的样本。整流线性单元的各种扩展保证了它们能在各个位置；都接收到梯度。
          關鍵詞：们激活为零的样本, 都接收到梯度, 整流线性单元的各种扩展保证了它们能在各个位置, 整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它
        - 摘要：i
          關鍵詞：
        - 摘要：整流线性单元的3个扩展基于当z
          關鍵詞：个扩展基于当, 整流线性单元的
        - 摘要：＜0时使用一个非零的斜率；。绝；对值整流  （absolute  value  rectification）固定α  i  ＝−1来得到g(z)＝｜z
          關鍵詞：时使用一个非零的斜率, 对值整流, 固定, 来得到
        - 摘要：i
          關鍵詞：
        - 摘要：maxout单元 （maxout unit）（Goodfellow et al. ，2013a）进一步扩展了；整流线性单元。maxout单元将z划分为每组具有k个值的组，而不是使用；作用于每个元素的函数g(z)。每个maxout单元则输出每组中的最大元
          關鍵詞：进一步扩展了, 作用于每个元素的函数, 而不是使用, 单元将, 单元则输出每组中的最大元
        - 摘要：这里；了一种方法来学习对输入 x 空间中多个方向响应的分段线性函数。
          關鍵詞：这里, 空间中多个方向响应的分段线性函数, 了一种方法来学习对输入
        - 摘要：是组i的输入索引集
          關鍵詞：是组, 的输入索引集
        - 摘要：。这提供
          關鍵詞：这提供
        - 摘要：maxout单元可以学习具有多达k段的分段线性的凸函数。maxout单元因；此可以视为学习激活函数本身，而不仅仅是单元之间的关系。使用足够；大的k，maxout单元可以以任意的精确度来近似任何凸函数。特别地，
          關鍵詞：段的分段线性的凸函数, 单元可以学习具有多达, 特别地, 大的, 而不仅仅是单元之间的关系
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；具有两块的maxout层可以学习实现和传统层相同的输入 x 的函数，这些；传统层可以使用整流线性激活函数、绝对值整流、渗漏整流线性单元或
          關鍵詞：绝对值整流, 层可以学习实现和传统层相同的输入, 渗漏整流线性单元或, 这些, 的函数
        - 摘要：每个maxout单元现在由k个权重向量来参数化，而不仅仅是一个，所以；maxout单元通常比整流线性单元需要更多的正则化。如果训练集很大并；且每个单元的块数保持很低的话，它们可以在没有正则化的情况下工作
          關鍵詞：个权重向量来参数化, 且每个单元的块数保持很低的话, 而不仅仅是一个, 它们可以在没有正则化的情况下工作, 所以
        - 摘要：maxout单元还有一些其他的优点。在某些情况下，要求更少的参数可以；获得一些统计和计算上的优点。具体来说，如果由n个不同的线性过滤；器描述的特征可以在不损失信息的情况下，用每一组k个特征的最大值
          關鍵詞：用每一组, 器描述的特征可以在不损失信息的情况下, 个不同的线性过滤, 单元还有一些其他的优点, 具体来说
        - 摘要：因为每个单元由多个过滤器驱动，maxout单元具有一些冗余来帮助它们；抵抗一种被称为灾难遗忘  （catastrophic  forgetting）的现象，这个现象；是说神经网络忘记了如何执行它们过去训练的任务（Goodfellow  et  al.
          關鍵詞：单元具有一些冗余来帮助它们, 是说神经网络忘记了如何执行它们过去训练的任务, 因为每个单元由多个过滤器驱动, 的现象, 这个现象
        - 摘要：整流线性单元和它们的这些扩展都是基于一个原则，那就是如果它们的；行为更接近线性，那么模型更容易优化。使用线性行为更容易优化的一；般性原则同样也适用于除深度线性网络以外的情景。循环网络可以从序
          關鍵詞：循环网络可以从序, 般性原则同样也适用于除深度线性网络以外的情景, 那么模型更容易优化, 使用线性行为更容易优化的一, 行为更接近线性
        - 摘要：6.3.2　logistic sigmoid与双曲正切函数
          關鍵詞：与双曲正切函数
        - 摘要：在引入整流线性单元之前，大多数神经网络使用logistic  sigmoid激活函；数
          關鍵詞：在引入整流线性单元之前, 大多数神经网络使用, 激活函
        - 摘要：或者是双曲正切激活函数
          關鍵詞：或者是双曲正切激活函数
        - 摘要：这些激活函数紧密相关，因为
          關鍵詞：因为, 这些激活函数紧密相关
        - 摘要：。
          關鍵詞：
        - 摘要：我们已经看过sigmoid单元作为输出单元用来预测二值型变量取值为1的；概率。与分段线性单元不同，sigmoid单元在其大部分定义域内都饱和；——当z取绝对值很大的正值时，它们饱和到一个高值，当z取绝对值很
          關鍵詞：与分段线性单元不同, 取绝对值很大的正值时, 我们已经看过, 取绝对值很, 概率
        - 摘要：当必须要使用sigmoid激活函数时，双曲正切激活函数通常要比logistic；sigmoid函数表现更好。在
          關鍵詞：当必须要使用, 激活函数时, 双曲正切激活函数通常要比, 函数表现更好
        - 摘要：的意义上，
          關鍵詞：的意义上
        - 摘要：它更像是单位函数。因为tanh在0附近与单位函数类似，训练深层神经；网络
          關鍵詞：因为, 网络, 训练深层神经, 它更像是单位函数, 附近与单位函数类似
        - 摘要：类似于训练一个线性模型；，只要网络的激活能够被保持地很小。这使得训
          關鍵詞：只要网络的激活能够被保持地很小, 这使得训, 类似于训练一个线性模型
        - 摘要：练tanh网络更加容易。
          關鍵詞：网络更加容易
        - 摘要：sigmoid激活函数在除了前馈网络以外的情景中更为常见。循环网络、；许多概率模型以及一些自编码器有一些额外的要求使得它们不能使用分；段线性激活函数，并且使得sigmoid单元更具有吸引力，尽管它存在饱
          關鍵詞：单元更具有吸引力, 尽管它存在饱, 循环网络, 激活函数在除了前馈网络以外的情景中更为常见, 并且使得
        - 摘要：6.3.3　其他隐藏单元
          關鍵詞：其他隐藏单元
        - 摘要：也存在许多其他种类的隐藏单元，但它们并不常用。
          關鍵詞：也存在许多其他种类的隐藏单元, 但它们并不常用
        - 摘要：一般来说，很多种类的可微函数都表现得很好。许多未发布的激活函数；与流行的激活函数表现得一样好。为了提供一个具体的例子，作者在；MNIST数据集上使用
          關鍵詞：很多种类的可微函数都表现得很好, 与流行的激活函数表现得一样好, 为了提供一个具体的例子, 一般来说, 作者在
        - 摘要：测试了一个前馈网络，
          關鍵詞：测试了一个前馈网络
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；并获得了小于1％的误差率，这可以与更为传统的激活函数获得的结果；相媲美。在新技术的研究和开发期间，通常会测试许多不同的激活函
          關鍵詞：的误差率, 在新技术的研究和开发期间, 通常会测试许多不同的激活函, 并获得了小于, 相媲美
        - 摘要：列出文献中出现的所有隐藏单元类型是不切实际的。我们只对一些特别；有用和独特的类型进行强调。
          關鍵詞：我们只对一些特别, 列出文献中出现的所有隐藏单元类型是不切实际的, 有用和独特的类型进行强调
        - 摘要：其中一种是完全没有激活函数g(z)。也可以认为这是使用单位函数作为；激活函数的情况。我们已经看过线性单元可以用作神经网络的输出。它；也可以用作隐藏单元。如果神经网络的每一层都仅由线性变换组成，那
          關鍵詞：其中一种是完全没有激活函数, 激活函数的情况, 我们已经看过线性单元可以用作神经网络的输出, 如果神经网络的每一层都仅由线性变换组成, 也可以用作隐藏单元
        - 摘要：。我们可以用两层来代替它，一层使用权重矩阵 U；，另一层使用权重矩阵 V 。如果第一层没有激活函数，那么我们对基于；的原始层的权重矩阵进行因式分解。分解方法是计算
          關鍵詞：的原始层的权重矩阵进行因式分解, 如果第一层没有激活函数, 我们可以用两层来代替它, 一层使用权重矩阵, 另一层使用权重矩阵
        - 摘要：softmax单元是另外一种经常用作输出的单元（如第6.2.2.3节中所描述；的），但有时也可以用作隐藏单元。softmax单元很自然地表示具有k个；可能值的离散型随机变量的概率分布，所以它们可以用作一种开关。这
          關鍵詞：但有时也可以用作隐藏单元, 单元是另外一种经常用作输出的单元, 单元很自然地表示具有, 所以它们可以用作一种开关, 如第
        - 摘要：其他一些常见的隐藏单元类型包括：
          關鍵詞：其他一些常见的隐藏单元类型包括
        - 摘要：径向基函数
          關鍵詞：径向基函数
        - 摘要：（radial
          關鍵詞：
        - 摘要：basis
          關鍵詞：
        - 摘要：function，RBF）：
          關鍵詞：
        - 摘要：。这个函数在  x  接近
          關鍵詞：这个函数在, 接近
        - 摘要：模板 W :, i 时更加活跃。因为它对大部分 x 都饱和到0，因此很难优；化。；。这是整流线
          關鍵詞：时更加活跃, 模板, 因此很难优, 都饱和到, 因为它对大部分
        - 摘要：隐藏单元的设计仍然是一个活跃的研究领域，许多有用的隐藏单元类型；仍有待发现。
          關鍵詞：许多有用的隐藏单元类型, 仍有待发现, 隐藏单元的设计仍然是一个活跃的研究领域
        - 摘要：6.3.1　整流线性单元及其；扩展
          關鍵詞：扩展, 整流线性单元及其
        - 摘要：6.3.2　logistic sigmoid与；双曲正切函数
          關鍵詞：双曲正切函数
        - 摘要：6.3.3　其他隐藏单元
          關鍵詞：其他隐藏单元
    6.4：架构设计
        - 摘要：6.4.1　万能近似性质和深度
          關鍵詞：万能近似性质和深度
        - 摘要：6.4.2　其他架构上的考虑
          關鍵詞：其他架构上的考虑
        - 摘要：神经网络设计的另一个关键点是确定它的架构。架构  （architecture）一；词是指网络的整体结构：它应该具有多少单元，以及这些单元应该如何；连接。
          關鍵詞：连接, 神经网络设计的另一个关键点是确定它的架构, 它应该具有多少单元, 以及这些单元应该如何, 架构
        - 摘要：大多数神经网络被组织成称为层的单元组。大多数神经网络架构将这些；层布置成链式结构，其中每一层都是前一层的函数。在这种结构中，第；一层由下式给出：
          關鍵詞：大多数神经网络被组织成称为层的单元组, 大多数神经网络架构将这些, 其中每一层都是前一层的函数, 一层由下式给出, 在这种结构中
        - 摘要：第二层由
          關鍵詞：第二层由
        - 摘要：给出，以此类推。
          關鍵詞：给出, 以此类推
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在这些链式架构中，主要的架构考虑是选择网络的深度和每一层的宽；度。我们将会看到，即使只有一个隐藏层的网络也足够适应训练集。更
          關鍵詞：在这些链式架构中, 即使只有一个隐藏层的网络也足够适应训练集, 我们将会看到, 主要的架构考虑是选择网络的深度和每一层的宽
        - 摘要：6.4.1　万能近似性质和深度
          關鍵詞：万能近似性质和深度
        - 摘要：线性模型，通过矩阵乘法将特征映射到输出，顾名思义，仅能表示线性；函数。它具有易于训练的优点，因为当使用线性模型时，许多损失函数；会导出凸优化问题。可惜的是，我们经常希望我们的系统学习非线性函
          關鍵詞：会导出凸优化问题, 线性模型, 我们经常希望我们的系统学习非线性函, 因为当使用线性模型时, 它具有易于训练的优点
        - 摘要：（universal
          關鍵詞：
        - 摘要：乍一看，可能认为学习非线性函数需要为我们想要学习的那种非线性专；门设计一类模型族。幸运的是，具有隐藏层的前馈网络提供了一种万能；近似框架。具体来说，万能近似定理
          關鍵詞：乍一看, 万能近似定理, 幸运的是, 门设计一类模型族, 具体来说
        - 摘要：万能近似定理意味着无论我们试图学习什么函数，我们知道一个大的；MLP一定能够表示这个函数。然而，我们不能保证训练算法能够学得这；个函数。即使MLP能够表示该函数，学习也可能因两个不同的原因而失
          關鍵詞：一定能够表示这个函数, 学习也可能因两个不同的原因而失, 我们知道一个大的, 即使, 然而
        - 摘要：网络提供了表示函数的万能系统，在这种意义上，给定一个函数，存在；一个前馈网络能够近似该函数。不存在万能的过程既能够验证训练集上；的特殊样本，又能够选择一个函数来扩展到训练集上没有的点。
          關鍵詞：不存在万能的过程既能够验证训练集上, 网络提供了表示函数的万能系统, 在这种意义上, 又能够选择一个函数来扩展到训练集上没有的点, 存在
        - 摘要：万能近似定理说明，存在一个足够大的网络能够达到我们所希望的任意；精度，但是定理并没有说这个网络有多大。Barron（1993）提供了单层；网络近似一大类函数所需大小的一些界。不幸的是，在最坏情况下，可
          關鍵詞：精度, 网络近似一大类函数所需大小的一些界, 存在一个足够大的网络能够达到我们所希望的任意, 万能近似定理说明, 但是定理并没有说这个网络有多大
        - 摘要：，并且选择一个这样的函数需要2
          關鍵詞：并且选择一个这样的函数需要
        - 摘要：总之，具有单层的前馈网络足以表示任何函数，但是网络层可能大得不；可实现，并且可能无法正确地学习和泛化。在很多情况下，使用更深的；模型能够减少表示期望函数所需的单元的数量，并且可以减少泛化误
          關鍵詞：并且可能无法正确地学习和泛化, 并且可以减少泛化误, 具有单层的前馈网络足以表示任何函数, 可实现, 使用更深的
        - 摘要：存在一些函数族能够在网络的深度大于某个值d时被高效地近似，而当；深度被限制到小于或等于d时需要一个远远大于之前的模型。在很多情；况下，浅层模型所需的隐藏单元的数量是n的指数级。这个结果最初被
          關鍵詞：而当, 况下, 的指数级, 深度被限制到小于或等于, 这个结果最初被
        - 摘要：et
          關鍵詞：
        - 摘要：et
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；数级的分段线性区域，它们可以概括所有种类的规则模式（例如，重；复）。
          關鍵詞：它们可以概括所有种类的规则模式, 数级的分段线性区域, 例如
        - 摘要：图6.5　关于更深的整流网络具有指数优势的一个直观的几何解释，来自Montufar et al.；（2014）。（左）绝对值整流单元对其输入中的每对镜像点有相同的输出。镜像的对称轴由单；元的权重和偏置定义的超平面给出。在该单元顶部计算的函数（绿色决策面）将是横跨该对称
          關鍵詞：绿色决策面, 将是横跨该对称, 绝对值整流单元对其输入中的每对镜像点有相同的输出, 在该单元顶部计算的函数, 元的权重和偏置定义的超平面给出
        - 摘要：Montufar et al. （2014）的主要定理指出，具有d个输入、深度为l、每个；隐藏层具有n个单元的深度整流网络可以描述的线性区域的数量是
          關鍵詞：个单元的深度整流网络可以描述的线性区域的数量是, 的主要定理指出, 个输入, 深度为, 具有
        - 摘要：意味着，这是深度l的指数级。在每个单元具有k个过滤器的maxout网络；中，线性区域的数量是
          關鍵詞：网络, 的指数级, 个过滤器的, 在每个单元具有, 意味着
        - 摘要：当然，我们不能保证在机器学习（特别是AI）的应用中想要学得的函数；类型享有这样的属性。
          關鍵詞：特别是, 类型享有这样的属性, 我们不能保证在机器学习, 当然, 的应用中想要学得的函数
        - 摘要：还可能出于统计原因来选择深度模型。任何时候，当选择一个特定的机；器学习算法时，我们隐含地陈述了一些先验，这些先验是关于算法应该；学得什么样的函数的。选择深度模型默许了一个非常普遍的信念，那就
          關鍵詞：任何时候, 那就, 选择深度模型默许了一个非常普遍的信念, 学得什么样的函数的, 这些先验是关于算法应该
        - 摘要：是包含多个步骤的计算机程序，其中每个步骤使用前一步骤的输出。这；些中间输出不一定是变差因素，而是可以类似于网络用来组织其内部处；理的计数器或指针。根据经验，更深的模型似乎确实在广泛的任务中泛
          關鍵詞：是包含多个步骤的计算机程序, 其中每个步骤使用前一步骤的输出, 根据经验, 些中间输出不一定是变差因素, 理的计数器或指针
        - 摘要：图6.6　深度的影响。实验结果表明，当从地址照片转录多位数字时，更深层的网络能够更好地；泛化。数据来自Goodfellow et al. （2014d）。测试集上的准确率随着深度的增加而不断增加。图；6.7给出了一个对照实验，它说明了对模型尺寸其他方面的增加并不能产生相同的效果
          關鍵詞：测试集上的准确率随着深度的增加而不断增加, 实验结果表明, 当从地址照片转录多位数字时, 给出了一个对照实验, 更深层的网络能够更好地
        - 摘要：图6.7　参数数量的影响。更深的模型往往表现更好。这不仅仅是因为模型更大。Goodfellow et；al. （2014d）的这项实验表明，增加卷积网络层中参数的数量，但是不增加它们的深度，在提
          關鍵詞：在提, 但是不增加它们的深度, 的这项实验表明, 这不仅仅是因为模型更大, 增加卷积网络层中参数的数量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；升测试集性能方面几乎没有效果。图例标明了用于画出每条曲线的网络深度，以及曲线表示的；是卷积层还是全连接层的大小变化。我们可以观察到，在这种情况下，浅层模型在参数数量达
          關鍵詞：以及曲线表示的, 升测试集性能方面几乎没有效果, 图例标明了用于画出每条曲线的网络深度, 浅层模型在参数数量达, 在这种情况下
        - 摘要：6.4.2　其他架构上的考虑
          關鍵詞：其他架构上的考虑
        - 摘要：到目前为止，我们都将神经网络描述成层的简单链式结构，主要的考虑；因素是网络的深度和每层的宽度。在实践中，神经网络显示出相当的多；样性。
          關鍵詞：神经网络显示出相当的多, 样性, 因素是网络的深度和每层的宽度, 主要的考虑, 到目前为止
        - 摘要：许多神经网络架构已经被开发用于特定的任务。用于计算机视觉的卷积；神经网络的特殊架构将在第9章中介绍。前馈网络也可以推广到用于序；列处理的循环神经网络，但有它们自己的架构考虑，这将在第10章中介
          關鍵詞：许多神经网络架构已经被开发用于特定的任务, 章中介, 章中介绍, 这将在第, 神经网络的特殊架构将在第
        - 摘要：一般来说，层不需要连接在链中，尽管这是最常见的做法。许多架构构；建了一个主链，但随后又添加了额外的架构特性，例如从层i到层i＋2或；者更高层的跳跃连接。这些跳跃连接使得梯度更容易从输出层流向更接
          關鍵詞：尽管这是最常见的做法, 例如从层, 到层, 者更高层的跳跃连接, 许多架构构
        - 摘要：架构设计考虑的另外一个关键点是如何将层与层之间连接起来。默认的；神经网络层采用矩阵  W  描述的线性变换，每个输入单元连接到每个输；出单元。在之后章节中的许多专用网络具有较少的连接，使得输入层中
          關鍵詞：使得输入层中, 描述的线性变换, 神经网络层采用矩阵, 架构设计考虑的另外一个关键点是如何将层与层之间连接起来, 每个输入单元连接到每个输
        - 摘要：6.4.1　万能近似性质和深；度
          關鍵詞：万能近似性质和深
        - 摘要：6.4.2　其他架构上的考虑
          關鍵詞：其他架构上的考虑
    6.5：反向传播和其他的微分算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；6.5.1　计算图
          關鍵詞：计算图
        - 摘要：6.5.2　微积分中的链式法则
          關鍵詞：微积分中的链式法则
        - 摘要：6.5.3　递归地使用链式法则来实现反向传播
          關鍵詞：递归地使用链式法则来实现反向传播
        - 摘要：6.5.4　全连接MLP中的反向传播计算
          關鍵詞：全连接, 中的反向传播计算
        - 摘要：6.5.5　符号到符号的导数
          關鍵詞：符号到符号的导数
        - 摘要：6.5.6　一般化的反向传播
          關鍵詞：一般化的反向传播
        - 摘要：6.5.7　实例：用于MLP训练的反向传播
          關鍵詞：用于, 训练的反向传播, 实例
        - 摘要：6.5.8　复杂化
          關鍵詞：复杂化
        - 摘要：6.5.9　深度学习界以外的微分
          關鍵詞：深度学习界以外的微分
        - 摘要：6.5.10　高阶微分
          關鍵詞：高阶微分
        - 摘要：当我们使用前馈神经网络接收输入 x 并产生输出  时，信息通过网络向
          關鍵詞：并产生输出, 信息通过网络向, 当我们使用前馈神经网络接收输入
        - 摘要：前流动。输入 x  提供初始信息，然后传播到每一层的隐藏单元，最终产；生输出   。这称之为前向传播  （forward  propagation）。在训练过程；中，前向传播可以持续向前直到它产生一个标量代价函数J( θ )。反向传
          關鍵詞：最终产, 这称之为前向传播, 然后传播到每一层的隐藏单元, 反向传, 前向传播可以持续向前直到它产生一个标量代价函数
        - 摘要：计算梯度的解析表达式是很直观的，但是数值化地求解这样的表达式在；计算上的代价可能很大。反向传播算法使用简单和廉价的程序来实现这；个目标。
          關鍵詞：计算梯度的解析表达式是很直观的, 计算上的代价可能很大, 但是数值化地求解这样的表达式在, 个目标, 反向传播算法使用简单和廉价的程序来实现这
        - 摘要：反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实；际上，反向传播仅指用于计算梯度的方法，而另一种算法，例如随机梯；度下降，使用该梯度来进行学习。此外，反向传播经常被误解为仅适用
          關鍵詞：反向传播仅指用于计算梯度的方法, 例如随机梯, 此外, 使用该梯度来进行学习, 而另一种算法
        - 摘要：6.5.1　计算图
          關鍵詞：计算图
        - 摘要：到目前为止，我们已经用相对非正式的图形语言讨论了神经网络。为了；更精确地描述反向传播算法，使用更精确的计算图；（computational
          關鍵詞：为了, 我们已经用相对非正式的图形语言讨论了神经网络, 到目前为止, 使用更精确的计算图, 更精确地描述反向传播算法
        - 摘要：将计算形式化为图形的方法有很多。
          關鍵詞：将计算形式化为图形的方法有很多
        - 摘要：这里，我们使用图中的每一个节点来表示一个变量。变量可以是标量、；向量、矩阵、张量或者甚至是另一类型的变量。
          關鍵詞：张量或者甚至是另一类型的变量, 矩阵, 变量可以是标量, 我们使用图中的每一个节点来表示一个变量, 向量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；为了形式化图形，我们还需引入操作 （operation）这一概念。操作是指；一个或多个变量的简单函数。图形语言伴随着一组被允许的操作。我们
          關鍵詞：一个或多个变量的简单函数, 我们还需引入操作, 操作是指, 图形语言伴随着一组被允许的操作, 我们
        - 摘要：为了不失一般性，我们定义一个操作仅返回单个输出变量。这并没有失；去一般性，是因为输出变量可以有多个条目，例如向量。反向传播的软；件实现通常支持具有多个输出的操作，但是我们在描述中避免这种情
          關鍵詞：件实现通常支持具有多个输出的操作, 但是我们在描述中避免这种情, 反向传播的软, 我们定义一个操作仅返回单个输出变量, 是因为输出变量可以有多个条目
        - 摘要：如果变量y是变量x通过一个操作计算得到的，那么我们画一条从x到y的；有向边。有时我们用操作的名称来注释输出的节点，当上下文很明确；时，有时也会省略这个标注。
          關鍵詞：有向边, 通过一个操作计算得到的, 是变量, 那么我们画一条从, 当上下文很明确
        - 摘要：计算图的示例可以参考图6.8。
          關鍵詞：计算图的示例可以参考图
        - 摘要：图6.8　一些计算图的示例。（a）使用×操作计算z＝xy的图。（b）用于逻辑回归预测
          關鍵詞：的图, 一些计算图的示例, 用于逻辑回归预测, 使用, 操作计算
        - 摘要：的图。一些中间表达式在代数表达式中没有名称，但在图形中却需要。
          關鍵詞：的图, 一些中间表达式在代数表达式中没有名称, 但在图形中却需要
        - 摘要：我们简单地将第i个这样的变量命名为u (i) 。（c）表达式；的计算图，在给定包含小批量输入数据的设计矩阵 x 时，它计算整流线性单元激活的设计矩阵 H；。（d）示例（a）到（c）对每个变量最多只实施一个操作，但是对变量实施多个操作也是可能
          關鍵詞：表达式, 的计算图, 个这样的变量命名为, 对每个变量最多只实施一个操作, 在给定包含小批量输入数据的设计矩阵
        - 摘要：，也用于权重衰减罚项
          關鍵詞：也用于权重衰减罚项
        - 摘要：6.5.2　微积分中的链式法则
          關鍵詞：微积分中的链式法则
        - 摘要：微积分中的链式法则（为了不与概率中的链式法则相混淆）用于计算复；合函数的导数。反向传播是一种计算链式法则的算法，使用高效的特定；运算顺序。
          關鍵詞：微积分中的链式法则, 使用高效的特定, 合函数的导数, 反向传播是一种计算链式法则的算法, 用于计算复
        - 摘要：设x是实数，f和g是从实数映射到实数的函数。假设y＝g(x)并且z＝；f(g(x))＝f(y)。那么链式法则是说
          關鍵詞：是实数, 是从实数映射到实数的函数, 并且, 假设, 那么链式法则是说
        - 摘要：我们可以将这种标量情况进行扩展。假设；是从
          關鍵詞：假设, 是从, 我们可以将这种标量情况进行扩展
        - 摘要：的映射，f是从
          關鍵詞：是从, 的映射
        - 摘要：，g；的映射。如果
          關鍵詞：如果, 的映射
        - 摘要：，那么
          關鍵詞：那么
        - 摘要：使用向量记法，可以等价地写成
          關鍵詞：使用向量记法, 可以等价地写成
        - 摘要：这里
          關鍵詞：这里
        - 摘要：是g的n×m的Jacobian矩阵。
          關鍵詞：矩阵
        - 摘要：从这里我们看到，变量  x  的梯度可以通过Jacobian矩阵
          關鍵詞：矩阵, 的梯度可以通过, 从这里我们看到, 变量
        - 摘要：和梯度
          關鍵詞：和梯度
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；相乘来得到。反向传播算法由图中每一个这样的Jacobian梯度的
          關鍵詞：反向传播算法由图中每一个这样的, 梯度的, 相乘来得到
        - 摘要：乘积操作所组成。
          關鍵詞：乘积操作所组成
        - 摘要：通常我们将反向传播算法应用于任意维度的张量，而不仅仅用于向量。；从概念上讲，这与使用向量的反向传播完全相同。唯一的区别是如何将；数字排列成网格以形成张量。我们可以想象，在运行反向传播之前，将
          關鍵詞：在运行反向传播之前, 数字排列成网格以形成张量, 而不仅仅用于向量, 我们可以想象, 这与使用向量的反向传播完全相同
        - 摘要：为了表示值z关于张量Χ  的梯度，我们记为；，就像Χ  是向量一；样。Χ  的索引现在有多个坐标——例如，一个3维的张量由3个坐标索
          關鍵詞：我们记为, 是向量一, 个坐标索, 维的张量由, 例如
        - 摘要：。这与向量中索引的方
          關鍵詞：这与向量中索引的方
        - 摘要：式完全一致，
          關鍵詞：式完全一致
        - 摘要：。使用这种记法，我们
          關鍵詞：我们, 使用这种记法
        - 摘要：可以写出适用于张量的链式法则。如果；，那么
          關鍵詞：那么, 如果, 可以写出适用于张量的链式法则
        - 摘要：6.5.3　递归地使用链式法则来实现反向传播
          關鍵詞：递归地使用链式法则来实现反向传播
        - 摘要：使用链式规则，我们可以直接写出某个标量关于计算图中任何产生该标；量的节点的梯度的代数表达式。然而，实际在计算机中计算该表达式时；会引入一些额外的考虑。
          關鍵詞：使用链式规则, 量的节点的梯度的代数表达式, 然而, 我们可以直接写出某个标量关于计算图中任何产生该标, 会引入一些额外的考虑
        - 摘要：具体来说，许多子表达式可能在梯度的整个表达式中重复若干次。任何；计算梯度的程序都需要选择是存储这些子表达式还是重新计算它们几；次。图6.9给出了一个例子来说明这些重复的子表达式是如何出现的。
          關鍵詞：任何, 具体来说, 许多子表达式可能在梯度的整个表达式中重复若干次, 给出了一个例子来说明这些重复的子表达式是如何出现的, 计算梯度的程序都需要选择是存储这些子表达式还是重新计算它们几
        - 摘要：图6.9　计算梯度时导致重复子表达式的计算图。令；一步使用相同的操作函数
          關鍵詞：一步使用相同的操作函数, 计算梯度时导致重复子表达式的计算图
        - 摘要：为图的输入。我们对链中的每
          關鍵詞：为图的输入, 我们对链中的每
        - 摘要：，这样 x ＝ f ( w )， y ＝ f ( x )， z ＝ f ( y )。为了计算
          關鍵詞：为了计算, 这样
        - 摘要：，我们应用式（6.44）得到
          關鍵詞：得到, 我们应用式
        - 摘要：式（6.50）建议我们采用的实现方式是，仅计算f  (w  )的值一次并将它存；储在变量x中。这是反向传播算法所采用的方法。式（6.51）提出了一；种替代方法，其中子表达式f (w )出现了不止一次。在替代方法中，每次
          關鍵詞：种替代方法, 其中子表达式, 建议我们采用的实现方式是, 在替代方法中, 的值一次并将它存
        - 摘要：我们首先给出一个版本的反向传播算法，它指明了梯度的直接计算方式；（算法6.2以及相关的正向计算的算法6.1  ），按照它实际完成的顺序并；且递归地使用链式法则。我们可以直接执行这些计算或者将算法的描述
          關鍵詞：按照它实际完成的顺序并, 算法, 它指明了梯度的直接计算方式, 我们首先给出一个版本的反向传播算法, 以及相关的正向计算的算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；首先考虑描述如何计算单个标量u  (n)  （例如训练样本上的损失函数）的；计算图。我们想要计算这个标量对n i 个输入节点u (1) 到
          關鍵詞：例如训练样本上的损失函数, 首先考虑描述如何计算单个标量, 计算图, 个输入节点, 我们想要计算这个标量对
        - 摘要：换句话说，我们希望对所有的
          關鍵詞：我们希望对所有的, 换句话说
        - 摘要：计算
          關鍵詞：计算
        - 摘要：。
          關鍵詞：
        - 摘要：在使用反向传播计算梯度来实现参数的梯度下降时，u  (n)  将对应单个或；者小批量实例的代价函数，而u (1) 到
          關鍵詞：者小批量实例的代价函数, 在使用反向传播计算梯度来实现参数的梯度下降时, 将对应单个或
        - 摘要：则对应于模型的参数。
          關鍵詞：则对应于模型的参数
        - 摘要：假设图的节点已经以一种特殊的方式被排序，使得我们可以一个接一个；开始，一直上升到u  (n)  。如算法6.1中；地计算他们的输出，从
          關鍵詞：使得我们可以一个接一个, 开始, 如算法, 地计算他们的输出, 假设图的节点已经以一种特殊的方式被排序
        - 摘要：其中
          關鍵詞：其中
        - 摘要：是u (i) 所有父节点的集合。
          關鍵詞：所有父节点的集合
        - 摘要：该算法详细说明了前向传播的计算，我们可以将其放入图   中。为了；执行反向传播，我们可以构造一个依赖于   并添加额外一组节点的计；算图。这形成了一个子图   ，它的每个节点都是   的节点。   中的
          關鍵詞：该算法详细说明了前向传播的计算, 我们可以构造一个依赖于, 我们可以将其放入图, 这形成了一个子图, 的节点
        - 摘要：与前向图中的节点u  (i)  相关联。这通过对标量输出u  (n)  使用链
          關鍵詞：与前向图中的节点, 相关联, 使用链, 这通过对标量输出
        - 摘要：式法则来完成：
          關鍵詞：式法则来完成
        - 摘要：算法6.1 　计算将n  i  个输入u  (1)  到；这定义了一个计算图，其中每个节点通过将函数f；(i)  的值，
          關鍵詞：这定义了一个计算图, 计算将, 算法, 个输入, 的值
        - 摘要：映射到一个输出u  (n)  的程序。；(i)  应用到变量集合；(j)  的值满足j＜i且
          關鍵詞：的程序, 的值满足, 映射到一个输出, 应用到变量集合
        - 摘要：上来计算u
          關鍵詞：上来计算
        - 摘要：(1) 到
          關鍵詞：
        - 摘要：。计算图的输出可以从最后一个（输出）节点u (n) 读出。
          關鍵詞：输出, 计算图的输出可以从最后一个, 节点, 读出
        - 摘要：算法6.2  　反向传播算法的简化版本，用于计算u  (n)  关于图中变量的导；数。这个示例旨在通过演示所有变量都是标量的简化情况来进一步理解；反向传播算法，这里我们希望计算关于
          關鍵詞：用于计算, 这个示例旨在通过演示所有变量都是标量的简化情况来进一步理解, 算法, 反向传播算法, 反向传播算法的简化版本
        - 摘要：例。这与前向传播的计算次数具有相同的阶。每个
          關鍵詞：每个, 这与前向传播的计算次数具有相同的阶
        - 摘要：是u  (i)  的父
          關鍵詞：的父
        - 摘要：(j)  的函数，从而将前向图的节点链接到反向传播图中添加的节
          關鍵詞：的函数, 从而将前向图的节点链接到反向传播图中添加的节
        - 摘要：节点u；点。
          關鍵詞：节点
        - 摘要：运行前向传播（对于此例是算法6.1 ）获得网络的激活。
          關鍵詞：运行前向传播, 对于此例是算法, 获得网络的激活
        - 摘要：初始化grad  table  ，用于存储计算好的导数的数据结构。grad  table
          關鍵詞：初始化, 用于存储计算好的导数的数据结构
        - 摘要：［u (i) ］将存储
          關鍵詞：将存储
        - 摘要：计算好的值。
          關鍵詞：计算好的值
        - 摘要：grad table ［u (n) ］←1
          關鍵詞：
        - 摘要：for j＝n−1 down to 1 do
          關鍵詞：
        - 摘要：下一行使用存储的值计算
          關鍵詞：下一行使用存储的值计算
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；end for
          關鍵詞：
        - 摘要：这在算法6.2中详细说明。子图   恰好包含每一条对应着   中从节点u
          關鍵詞：子图, 中详细说明, 中从节点, 恰好包含每一条对应着, 这在算法
        - 摘要：(j) 到节点u  (i) 的边。从u  (j) 到u  (i)  的边对应着计算
          關鍵詞：到节点, 的边对应着计算, 的边
        - 摘要：。另外，对于
          關鍵詞：对于, 另外
        - 摘要：每个节点都要执行一个内积，内积的一个因子是对于u  j  子节点u  (i)  的已
          關鍵詞：子节点, 内积的一个因子是对于, 的已, 每个节点都要执行一个内积
        - 摘要：经计算的梯度，另一个因子是对于相同子节点u  (i)  的偏导数
          關鍵詞：另一个因子是对于相同子节点, 的偏导数, 经计算的梯度
        - 摘要：组
          關鍵詞：
        - 摘要：成的向量。总而言之，执行反向传播所需的计算量与   中的边的数量；成比例，其中每条边的计算包括计算偏导数（节点关于它的一个父节点；的偏导数）以及执行一次乘法和一次加法。下面，我们将此分析推广到
          關鍵詞：总而言之, 节点关于它的一个父节点, 其中每条边的计算包括计算偏导数, 成的向量, 执行反向传播所需的计算量与
        - 摘要：反向传播算法被设计为减少公共子表达式的数量而不考虑存储的开销。；具体来说，它大约对图中的每个节点执行一个Jacobian乘积。这可以从；算法6.2中看出，反向传播算法访问了图中的节点u  (j)  到节点u  (i)  的每条
          關鍵詞：反向传播算法被设计为减少公共子表达式的数量而不考虑存储的开销, 它大约对图中的每个节点执行一个, 这可以从, 算法, 到节点
        - 摘要：边一次，以获得相关的偏导数
          關鍵詞：边一次, 以获得相关的偏导数
        - 摘要：。反向传播因此避免了重复子表
          關鍵詞：反向传播因此避免了重复子表
        - 摘要：达式的指数爆炸。然而，其他算法可能通过对计算图进行简化来避免更；多的子表达式，或者也可能通过重新计算而不是存储这些子表达式来节；省内存。我们将在描述完反向传播算法本身后再重新审视这些想法。
          關鍵詞：其他算法可能通过对计算图进行简化来避免更, 达式的指数爆炸, 我们将在描述完反向传播算法本身后再重新审视这些想法, 然而, 省内存
        - 摘要：6.5.4　全连接MLP中的反向传播计算
          關鍵詞：全连接, 中的反向传播计算
        - 摘要：为了阐明反向传播的上述定义，让我们考虑一个与全连接的多层MLP相；关联的特定图。
          關鍵詞：关联的特定图, 为了阐明反向传播的上述定义, 让我们考虑一个与全连接的多层
        - 摘要：算法6.3首先给出了前向传播，它将参数映射到与单个训练样本（输；入，目标）（  x,y  ）相关联的监督损失函数；提供输入时神经网络的输出。
          關鍵詞：算法, 目标, 相关联的监督损失函数, 提供输入时神经网络的输出, 它将参数映射到与单个训练样本
        - 摘要：，其中   是当  x
          關鍵詞：是当, 其中
        - 摘要：算法6.3  　典型深度神经网络中的前向传播和代价函数的计算。损失函；数；取决于输出   和目标  y  （参考第6.2.1.1节中损失函数的示
          關鍵詞：取决于输出, 算法, 和目标, 参考第, 典型深度神经网络中的前向传播和代价函数的计算
        - 摘要：，模型的权重矩阵
          關鍵詞：模型的权重矩阵
        - 摘要：，模型的偏置参数
          關鍵詞：模型的偏置参数
        - 摘要：Require： 网络深度，l
          關鍵詞：网络深度
        - 摘要：Require：W
          關鍵詞：
        - 摘要：Require：
          關鍵詞：
        - 摘要：Require：x ，程序的输入
          關鍵詞：程序的输入
        - 摘要：Require：y ，目标输出
          關鍵詞：目标输出
        - 摘要：h (0) ＝ x
          關鍵詞：
        - 摘要：for k＝1，…，l do
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；算法6.4随后说明了将反向传播应用于该图所需的相关计算。
          關鍵詞：随后说明了将反向传播应用于该图所需的相关计算, 算法
        - 摘要：算法6.3和算法6.4是简单而直观的演示。然而，它们专门针对特定的问；题。
          關鍵詞：它们专门针对特定的问, 是简单而直观的演示, 算法, 和算法, 然而
        - 摘要：现在的软件实现基于之后第6.5.6节中描述的一般形式的反向传播，它可；以通过显式地操作表示符号计算的数据结构，来适应任何计算图。
          關鍵詞：现在的软件实现基于之后第, 来适应任何计算图, 节中描述的一般形式的反向传播, 它可, 以通过显式地操作表示符号计算的数据结构
        - 摘要：6.5.5　符号到符号的导数
          關鍵詞：符号到符号的导数
        - 摘要：代数表达式和计算图都对符号  （symbol）或不具有特定值的变量进行；操作。这些代数或者基于图的表达式被称为符号表示；（symbolic
          關鍵詞：代数表达式和计算图都对符号, 操作, 或不具有特定值的变量进行, 这些代数或者基于图的表达式被称为符号表示
        - 摘要：。
          關鍵詞：
        - 摘要：算法6.4 　深度神经网络中算法6.3的反向计算，它不止使用了输入  x  和；的梯度，从输出层；目标  y  。该计算对于每一层k都产生了对激活
          關鍵詞：算法, 目标, 从输出层, 它不止使用了输入, 的梯度
        - 摘要：在前向计算完成后，计算顶层的梯度：
          關鍵詞：计算顶层的梯度, 在前向计算完成后
        - 摘要：for k＝l，l−1，…，1 do
          關鍵詞：
        - 摘要：将关于层输出的梯度转换为非线性激活输入前的梯度（如果f是逐；元素的，则逐元素地相乘）：
          關鍵詞：元素的, 如果, 则逐元素地相乘, 将关于层输出的梯度转换为非线性激活输入前的梯度, 是逐
        - 摘要：计算关于权重和偏置的梯度（如果需要的话，还要包括正则项）：
          關鍵詞：计算关于权重和偏置的梯度, 还要包括正则项, 如果需要的话
        - 摘要：关于下一更低层的隐藏层传播梯度：
          關鍵詞：关于下一更低层的隐藏层传播梯度
        - 摘要：end for
          關鍵詞：
        - 摘要：一些反向传播的方法采用计算图和一组用于图的输入的数值，然后返回；在这些输入值处梯度的一组数值。我们将这种方法称为符号到数值  的；微分。这种方法用在诸如Torch（Collobert
          關鍵詞：我们将这种方法称为符号到数值, 然后返回, 在这些输入值处梯度的一组数值, 微分, 这种方法用在诸如
        - 摘要：et
          關鍵詞：
        - 摘要：另一种方法是采用计算图以及添加一些额外的节点到计算图中，这些额；外的节点提供了我们所需导数的符号描述。这是Theano（Bergstra  et  al.；，2010b；Bastien  et  al.  ，2012b）和TensorFlow（Abadi  et  al.  ，2015）
          關鍵詞：另一种方法是采用计算图以及添加一些额外的节点到计算图中, 这是, 这些额, 外的节点提供了我们所需导数的符号描述
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图6.10　使用符号到符号的方法计算导数的示例。在这种方法中，反向传播算法不需要访问任；何实际的特定数值。相反，它将节点添加到计算图中来描述如何计算这些导数。通用图形求值
          關鍵詞：在这种方法中, 它将节点添加到计算图中来描述如何计算这些导数, 通用图形求值, 何实际的特定数值, 使用符号到符号的方法计算导数的示例
        - 摘要：开始。（右）我们运行反向传播算法，指导它构造表达式
          關鍵詞：我们运行反向传播算法, 开始, 指导它构造表达式
        - 摘要：对应的图。在这个例子中，我
          關鍵詞：在这个例子中, 对应的图
        - 摘要：们不解释反向传播算法如何工作。我们的目的只是说明想要的结果是什么：符号描述的导数的；计算图
          關鍵詞：们不解释反向传播算法如何工作, 我们的目的只是说明想要的结果是什么, 计算图, 符号描述的导数的
        - 摘要：这种方法的主要优点是导数可以使用与原始表达式相同的语言来描述。；因为导数只是另外一张计算图，我们可以再次运行反向传播，对导数再；进行求导就能得到更高阶的导数。高阶导数的计算在第6.5.10节中描
          關鍵詞：我们可以再次运行反向传播, 因为导数只是另外一张计算图, 进行求导就能得到更高阶的导数, 对导数再, 这种方法的主要优点是导数可以使用与原始表达式相同的语言来描述
        - 摘要：我们将使用后一种方法，并且使用构造导数的计算图的方法来描述反向；传播算法。图的任意子集之后都可以使用特定的数值来求值。这允许我；们避免精确地指明每个操作应该在何时计算。相反，通用的图计算引擎
          關鍵詞：我们将使用后一种方法, 并且使用构造导数的计算图的方法来描述反向, 传播算法, 图的任意子集之后都可以使用特定的数值来求值, 这允许我
        - 摘要：基于符号到符号的方法的描述包含了符号到数值的方法。符号到数值的；方法可以理解为执行了与符号到符号的方法中构建图的过程中完全相同；的计算。关键的区别是符号到数值的方法不会显示出计算图。
          關鍵詞：方法可以理解为执行了与符号到符号的方法中构建图的过程中完全相同, 的计算, 关键的区别是符号到数值的方法不会显示出计算图, 基于符号到符号的方法的描述包含了符号到数值的方法, 符号到数值的
        - 摘要：6.5.6　一般化的反向传播
          關鍵詞：一般化的反向传播
        - 摘要：反向传播算法非常简单。为了计算某个标量z关于图中它的一个祖先  x
          關鍵詞：关于图中它的一个祖先, 反向传播算法非常简单, 为了计算某个标量
        - 摘要：的梯度，首先观察到它关于z的梯度由
          關鍵詞：的梯度由, 的梯度, 首先观察到它关于
        - 摘要：给出。然后，我们
          關鍵詞：然后, 我们, 给出
        - 摘要：可以计算对图中z的每个父节点的梯度，通过现有的梯度乘以产生z的操；作的Jacobian。我们继续乘以Jacobian，以这种方式向后穿过图，直到到；达  x  。对于从z出发可以经过两个或更多路径向后行进而到达的任意节
          關鍵詞：可以计算对图中, 作的, 对于从, 出发可以经过两个或更多路径向后行进而到达的任意节, 直到到
        - 摘要：更正式地，图   中的每个节点对应着一个变量。为了实现最大的一般；化，我们将这个变量描述为一个张量V  。张量通常可以具有任意维度，；并且包含标量、向量和矩阵。
          關鍵詞：我们将这个变量描述为一个张量, 中的每个节点对应着一个变量, 向量和矩阵, 并且包含标量, 为了实现最大的一般
        - 摘要：我们假设每个变量V 与下列子程序相关联：
          關鍵詞：与下列子程序相关联, 我们假设每个变量
        - 摘要：get_operation(V )：它返回用于计算V 的操作，代表了在计算图中流；入V  的边。例如，可能有一个Python或者C＋＋的类表示矩阵乘法；操作，以及get_operation 函数。假设我们的一个变量是由矩阵乘法
          關鍵詞：的类表示矩阵乘法, 可能有一个, 它返回用于计算, 操作, 假设我们的一个变量是由矩阵乘法
        - 摘要：：它返回一组变量，是计算图   中
          關鍵詞：是计算图, 它返回一组变量
        - 摘要：V 的子节点。
          關鍵詞：的子节点
        - 摘要：父节点。
          關鍵詞：父节点
        - 摘要：：它返回一组变量，是计算图   中V  的
          關鍵詞：是计算图, 它返回一组变量
        - 摘要：每个操作op  也与bprop  操作相关联。该bprop  操作可以计算如式；（6.47）所描述的Jacobian向量积。这是反向传播算法能够实现很大通；用性的原因。每个操作负责了解如何通过它参与的图中的边来反向传
          關鍵詞：这是反向传播算法能够实现很大通, 向量积, 操作可以计算如式, 操作相关联, 每个操作负责了解如何通过它参与的图中的边来反向传
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；梯度是；需要使用正确的参数调用每个操作的bprop
          關鍵詞：梯度是, 需要使用正确的参数调用每个操作的
        - 摘要：。反向传播算法本身并不需要知道任何微分法则。它只；方法即可。正式地，
          關鍵詞：方法即可, 正式地, 反向传播算法本身并不需要知道任何微分法则, 它只
        - 摘要：这只是如式（6.47）所表达的链式法则的实现。这里，inputs  是提供给；操作的一组输入，op.f  是操作实现的数学函数，X  是输入，我们想要计；算关于它的梯度，G 是操作对于输出的梯度。
          關鍵詞：是操作实现的数学函数, 这只是如式, 是操作对于输出的梯度, 是输入, 我们想要计
        - 摘要：op.bprop 方法应该总是假装它的所有输入彼此不同，即使它们不是。例；如，如果mul  操作传递两个x来计算x  2  ，op.bprop  方法应该仍然返回x；作为对于两个输入的导数。反向传播算法后面会将这些变量加起来获得
          關鍵詞：如果, 即使它们不是, 反向传播算法后面会将这些变量加起来获得, 方法应该总是假装它的所有输入彼此不同, 来计算
        - 摘要：反向传播算法的软件实现通常提供操作和其bprop  方法，所以深度学习；软件库的用户能够对使用诸如矩阵乘法、指数运算、对数运算等常用操；作构建的图进行反向传播。构建反向传播新实现的软件工程师或者需要
          關鍵詞：对数运算等常用操, 指数运算, 方法, 反向传播算法的软件实现通常提供操作和其, 构建反向传播新实现的软件工程师或者需要
        - 摘要：反向传播算法的正式描述参考算法6.5。
          關鍵詞：反向传播算法的正式描述参考算法
        - 摘要：算法6.5  　反向传播算法最外围的骨架。这部分做简单的设置和清理工；作。大多数重要的工作发生在算法6.6的子程序build_grad 中。
          關鍵詞：这部分做简单的设置和清理工, 算法, 的子程序, 反向传播算法最外围的骨架, 大多数重要的工作发生在算法
        - 摘要：Require：
          關鍵詞：
        - 摘要：，需要计算梯度的目标变量集
          關鍵詞：需要计算梯度的目标变量集
        - 摘要：Require：
          關鍵詞：
        - 摘要：，计算图
          關鍵詞：计算图
        - 摘要：Require： z，要微分的变量
          關鍵詞：要微分的变量
        - 摘要：令
          關鍵詞：
        - 摘要：剪枝后的计算图，其中仅包括z的祖先以及   中节点的
          關鍵詞：其中仅包括, 的祖先以及, 中节点的, 剪枝后的计算图
        - 摘要：后代。
          關鍵詞：后代
        - 摘要：初始化grad table ，它是关联张量和对应导数的数据结构。
          關鍵詞：初始化, 它是关联张量和对应导数的数据结构
        - 摘要：grad table ［z］←1
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：Return grad table restricted to
          關鍵詞：
        - 摘要：在第6.5.2节中，我们使用反向传播作为一种策略来避免多次计算链式法；则中的相同子表达式。由于这些重复子表达式的存在，简单的算法可能；具有指数运行时间。现在我们已经详细说明了反向传播算法，可以去理
          關鍵詞：现在我们已经详细说明了反向传播算法, 节中, 具有指数运行时间, 在第, 可以去理
        - 摘要：2
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；算法6.6
          關鍵詞：算法
        - 摘要：向传播算法调用。
          關鍵詞：向传播算法调用
        - 摘要：反向传播算法的内循环子程序；，由算法6.5中定义的反
          關鍵詞：反向传播算法的内循环子程序, 中定义的反, 由算法
        - 摘要：由于节点j到节点n的路径数目可以关于这些路径的长度上指数地增长，；所以上述求和符号中的项数（这些路径的数目），可能以前向传播图的
          關鍵詞：由于节点, 到节点, 这些路径的数目, 可能以前向传播图的, 的路径数目可以关于这些路径的长度上指数地增长
        - 摘要：深度的指数级增长。会产生如此大的成本是因为对于
          關鍵詞：会产生如此大的成本是因为对于, 深度的指数级增长
        - 摘要：，相同的
          關鍵詞：相同的
        - 摘要：计算会重复进行很多次。为了避免这种重新计算，我们可以将反向传播
          關鍵詞：我们可以将反向传播, 计算会重复进行很多次, 为了避免这种重新计算
        - 摘要：看作一种表填充算法，利用存储的中间结果
          關鍵詞：利用存储的中间结果, 看作一种表填充算法
        - 摘要：来对表进行填充。
          關鍵詞：来对表进行填充
        - 摘要：图中的每个节点对应着表中的一个位置，这个位置存储对该节点的梯；度。通过顺序填充这些表的条目，反向传播算法避免了重复计算许多公；共子表达式。这种表填充策略有时被称为动态规划
          關鍵詞：图中的每个节点对应着表中的一个位置, 共子表达式, 这个位置存储对该节点的梯, 这种表填充策略有时被称为动态规划, 通过顺序填充这些表的条目
        - 摘要：6.5.7　实例：用于MLP训练的反向传播
          關鍵詞：用于, 训练的反向传播, 实例
        - 摘要：作为一个例子，我们利用反向传播算法来训练多层感知机。
          關鍵詞：我们利用反向传播算法来训练多层感知机, 作为一个例子
        - 摘要：这里，我们考虑一个具有单个隐藏层的非常简单的多层感知机。为了训；练这个模型，我们将使用小批量随机梯度下降算法。反向传播算法用于；计算单个小批量上的代价的梯度。具体来说，我们使用训练集上的一小
          關鍵詞：我们使用训练集上的一小, 为了训, 我们将使用小批量随机梯度下降算法, 反向传播算法用于, 练这个模型
        - 摘要：包含了交叉熵和系数为λ的权重衰减项。它的计算图在图6.11中给出。
          關鍵詞：中给出, 它的计算图在图, 的权重衰减项, 包含了交叉熵和系数为
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图6.11　用于计算代价函数的计算图，这个代价函数是使用交叉熵损失以及权重衰减训练我们；的单层MLP示例所产生的
          關鍵詞：用于计算代价函数的计算图, 示例所产生的, 的单层, 这个代价函数是使用交叉熵损失以及权重衰减训练我们
        - 摘要：这个示例的梯度计算图实在太大，以至于绘制或者阅读都将是乏味的。；这显示出了反向传播算法的优点之一，即它可以自动生成梯度，而这种；计算对于软件工程师来说需要进行直观但冗长的手动推导。
          關鍵詞：计算对于软件工程师来说需要进行直观但冗长的手动推导, 即它可以自动生成梯度, 以至于绘制或者阅读都将是乏味的, 这显示出了反向传播算法的优点之一, 这个示例的梯度计算图实在太大
        - 摘要：我们可以通过观察图6.11中的正向传播图来粗略地描述反向传播算法的；行为。为了训练，我们希望计算；。有两种不同
          關鍵詞：有两种不同, 行为, 中的正向传播图来粗略地描述反向传播算法的, 我们希望计算, 我们可以通过观察图
        - 摘要：另一条通过交叉熵代价的路径稍微复杂一些。令  G  是由cross_entropy；操作提供的对未归一化对数概率  U  (2)  的梯度。反向传播算法现在需要；探索两个不同的分支。在较短的分支上，它使用对矩阵乘法的第二个变
          關鍵詞：它使用对矩阵乘法的第二个变, 是由, 另一条通过交叉熵代价的路径稍微复杂一些, 反向传播算法现在需要, 的梯度
        - 摘要：量的反向传播规则，将
          關鍵詞：量的反向传播规则
        - 摘要：加到 W (2) 的梯度上。
          關鍵詞：加到, 的梯度上
        - 摘要：在计算了这些梯度以后，梯度下降算法或者其他优化算法所要做的就是；使用这些梯度来更新参数。
          關鍵詞：在计算了这些梯度以后, 梯度下降算法或者其他优化算法所要做的就是, 使用这些梯度来更新参数
        - 摘要：对于MLP，计算成本主要来源于矩阵乘法。在前向传播阶段，我们乘以；每个权重矩阵，得到了O(w)数量的乘  -  加，其中w是权重的数量。在反；向传播阶段，我们乘以每个权重矩阵的转置，这具有相同的计算成本。
          關鍵詞：在反, 我们乘以每个权重矩阵的转置, 数量的乘, 这具有相同的计算成本, 向传播阶段
        - 摘要：6.5.8　复杂化
          關鍵詞：复杂化
        - 摘要：我们这里描述的反向传播算法要比实践中实际使用的实现要简单。
          關鍵詞：我们这里描述的反向传播算法要比实践中实际使用的实现要简单
        - 摘要：正如前面提到的，我们将操作的定义限制为返回单个张量的函数。大多；数软件实现需要支持可以返回多个张量的操作。例如，如果我们希望计；算张量中的最大值和该值的索引，则最好在单次运算中计算两者，因此
          關鍵詞：算张量中的最大值和该值的索引, 正如前面提到的, 因此, 数软件实现需要支持可以返回多个张量的操作, 例如
        - 摘要：我们还没有描述如何控制反向传播的内存消耗。反向传播经常涉及将许；多张量加在一起。在朴素方法中，将分别计算这些张量中的每一个，然；后在第二步中对所有这些张量求和。朴素方法具有过高的存储瓶颈，可
          關鍵詞：将分别计算这些张量中的每一个, 后在第二步中对所有这些张量求和, 朴素方法具有过高的存储瓶颈, 在朴素方法中, 我们还没有描述如何控制反向传播的内存消耗
        - 摘要：反向传播的现实实现还需要处理各种数据类型，例如32位浮点数、64位；浮点数和整型。处理这些类型的策略需要特别的设计考虑。
          關鍵詞：浮点数和整型, 反向传播的现实实现还需要处理各种数据类型, 例如, 位浮点数, 处理这些类型的策略需要特别的设计考虑
        - 摘要：一些操作具有未定义的梯度，并且重要的是跟踪这些情况并且确定用户；请求的梯度是否是未定义的。
          關鍵詞：并且重要的是跟踪这些情况并且确定用户, 一些操作具有未定义的梯度, 请求的梯度是否是未定义的
        - 摘要：各种其他技术的特性使现实世界的微分更加复杂。这些技术性并不是不；可逾越的，本章已经描述了计算微分所需的关键知识工具，但重要的是；要知道还有许多的精妙之处存在。
          關鍵詞：这些技术性并不是不, 但重要的是, 各种其他技术的特性使现实世界的微分更加复杂, 本章已经描述了计算微分所需的关键知识工具, 要知道还有许多的精妙之处存在
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；6.5.9　深度学习界以外的微分
          關鍵詞：深度学习界以外的微分
        - 摘要：深度学习界在某种程度上已经与更广泛的计算机科学界隔离开来，并且；在很大程度上发展了自己关于如何进行微分的文化态度。一般来说，自；动微分  （automatic
          關鍵詞：一般来说, 动微分, 深度学习界在某种程度上已经与更广泛的计算机科学界隔离开来, 在很大程度上发展了自己关于如何进行微分的文化态度, 并且
        - 摘要：例如，假设有变量p 1 ，p  2  …，p  n  表示概率，以及变量z  1  ，z  2  ，…z  n；表示未归一化的对数概率。假设定义
          關鍵詞：假设定义, 假设有变量, 表示概率, 例如, 表示未归一化的对数概率
        - 摘要：其中我们通过指数化、求和与除法运算构建softmax函数，并构造交叉；。人类数学家可以观察到J对z i 的；熵损失函数
          關鍵詞：熵损失函数, 并构造交叉, 函数, 人类数学家可以观察到, 求和与除法运算构建
        - 摘要：当前向图  具有单个输出节点，并且每个偏导数
          關鍵詞：具有单个输出节点, 并且每个偏导数, 当前向图
        - 摘要：都可以用恒定
          關鍵詞：都可以用恒定
        - 摘要：的计算量来计算时，反向传播保证梯度计算的计算数目和前向计算的计；算数目是同一个量级：这可以在算法6.2中看出，因为每个局部偏导数
          關鍵詞：算数目是同一个量级, 反向传播保证梯度计算的计算数目和前向计算的计, 这可以在算法, 因为每个局部偏导数, 中看出
        - 摘要：以及递归链式公式（式(6.53)）中相关的乘和加都只需计算一
          關鍵詞：中相关的乘和加都只需计算一, 以及递归链式公式
        - 摘要：次。因此，总的计算量是O(#edges)。然而，可能通过对反向传播算法构；建的计算图进行简化来减少这些计算量，并且这是NP完全问题。诸如；Theano和TensorFlow的实现使用基于匹配已知简化模式的试探法，以便
          關鍵詞：以便, 因此, 并且这是, 可能通过对反向传播算法构, 然而
        - 摘要：重复地尝试去简化图。我们定义反向传播仅用于计算标量输出的梯度，；但是反向传播可以扩展到计算Jacobian矩阵（该Jacobian矩阵或者来源于；图中的k个不同标量节点，或者来源于包含k个值的张量值节点）。朴素
          關鍵詞：我们定义反向传播仅用于计算标量输出的梯度, 朴素, 或者来源于包含, 矩阵, 图中的
        - 摘要：其中的矩阵可以认为是Jacobian矩阵。例如，如果 D 是列向量，而 A 有；很多行，那么这对应于一幅具有单个输出和多个输入的图，并且从最后；开始乘，反向进行，只需要矩阵-向量的乘积。这对应着反向模式。相
          關鍵詞：如果, 反向进行, 其中的矩阵可以认为是, 矩阵, 是列向量
        - 摘要：在机器学习以外的许多社区中，更常见的是使用传统的编程语言来直接；实现微分软件，例如用Python或者C来编程，并且自动生成使用这些语；言编写的不同函数的程序。在深度学习界中，计算图通常使用由专用库
          關鍵詞：更常见的是使用传统的编程语言来直接, 来编程, 在机器学习以外的许多社区中, 并且自动生成使用这些语, 实现微分软件
        - 摘要：因此，反向传播不是计算梯度的唯一方式或最佳方式，但它是一个非常；实用的方法，继续为深度学习社区服务。在未来，深度网络的微分技术；可能会提高，因为深度学习的从业者更加懂得了更广泛的自动微分领域
          關鍵詞：实用的方法, 因此, 可能会提高, 反向传播不是计算梯度的唯一方式或最佳方式, 深度网络的微分技术
        - 摘要：6.5.10　高阶微分
          關鍵詞：高阶微分
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；一些软件框架支持使用高阶导数。在深度学习软件框架中，这至少包括；Theano和TensorFlow。这些库使用一种数据结构来描述要被微分的原始
          關鍵詞：这些库使用一种数据结构来描述要被微分的原始, 这至少包括, 在深度学习软件框架中, 一些软件框架支持使用高阶导数
        - 摘要：在深度学习的相关领域，很少会计算标量函数的单个二阶导数。相反，；我们通常对Hessian矩阵的性质比较感兴趣。如果我们有函数
          關鍵詞：矩阵的性质比较感兴趣, 在深度学习的相关领域, 很少会计算标量函数的单个二阶导数, 我们通常对, 相反
        - 摘要：，那么Hessian矩阵的大小是n×n。在典型的深度学
          關鍵詞：那么, 矩阵的大小是, 在典型的深度学
        - 摘要：习应用中，n将是模型的参数数量，可能很容易达到数十亿。因此，完；整的Hessian矩阵甚至不能表示。
          關鍵詞：矩阵甚至不能表示, 因此, 将是模型的参数数量, 整的, 可能很容易达到数十亿
        - 摘要：典型的深度学习方法是使用Krylov方法  （Krylov  method），而不是显；式地计算Hessian矩阵。Krylov方法是用于执行各种操作的一组迭代技；术，这些操作包括像近似求解矩阵的逆或者近似矩阵的特征值/特征向
          關鍵詞：方法是用于执行各种操作的一组迭代技, 特征向, 矩阵, 而不是显, 式地计算
        - 摘要：为了在Hesssian矩阵上使用Krylov方法，我们只需要能够计算Hessian矩；阵  H  和一个任意向量ν间的乘积即可。实现这一目标的一种直观方法；（Christianson，1992）是
          關鍵詞：间的乘积即可, 我们只需要能够计算, 矩阵上使用, 为了在, 和一个任意向量
        - 摘要：该表达式中两个梯度的计算都可以由适当的软件库自动完成。注意，外；部梯度表达式是内部梯度表达式的函数的梯度。
          關鍵詞：部梯度表达式是内部梯度表达式的函数的梯度, 注意, 该表达式中两个梯度的计算都可以由适当的软件库自动完成
        - 摘要：如果ν本身是由计算图产生的一个向量，那么重要的是指定自动微分软；件不要对产生ν的图进行微分。
          關鍵詞：如果, 的图进行微分, 本身是由计算图产生的一个向量, 件不要对产生, 那么重要的是指定自动微分软
        - 摘要：虽然计算Hessian通常是不可取的，但是可以使用Hessian向量积。可以；对所有的i＝1,...,n简单地计算 He (i )，其中e (i )是；都为0的one-hot向量。
          關鍵詞：向量积, 简单地计算, 但是可以使用, 都为, 其中
        - 摘要：并且其他元素
          關鍵詞：并且其他元素
        - 摘要：6.5.1　计算图
          關鍵詞：计算图
        - 摘要：6.5.2　微积分中的链式法；则
          關鍵詞：微积分中的链式法
        - 摘要：6.5.3　递归地使用链式法；则来实现反向传播
          關鍵詞：递归地使用链式法, 则来实现反向传播
        - 摘要：6.5.4　全连接MLP中的反；向传播计算
          關鍵詞：向传播计算, 全连接, 中的反
        - 摘要：6.5.5　符号到符号的导数
          關鍵詞：符号到符号的导数
        - 摘要：6.5.6　一般化的反向传播
          關鍵詞：一般化的反向传播
        - 摘要：6.5.7　实例：用于MLP训；练的反向传播
          關鍵詞：用于, 练的反向传播, 实例
        - 摘要：6.5.8　复杂化
          關鍵詞：复杂化
        - 摘要：6.5.9　深度学习界以外的；微分
          關鍵詞：深度学习界以外的, 微分
        - 摘要：6.5.10　高阶微分
          關鍵詞：高阶微分
    6.6：历史小记
        - 摘要：前馈网络可以被视为一种高效的非线性函数近似器，它以使用梯度下降；来最小化函数近似误差为基础。从这个角度来看，现代前馈网络是一般
          關鍵詞：来最小化函数近似误差为基础, 前馈网络可以被视为一种高效的非线性函数近似器, 现代前馈网络是一般, 它以使用梯度下降, 从这个角度来看
        - 摘要：函数近似任务的几个世纪进步的结晶。
          關鍵詞：函数近似任务的几个世纪进步的结晶
        - 摘要：处于反向传播算法底层的链式法则是17世纪发明的（Leibniz，1676；；L'Hôpital，1696）。微积分和代数长期以来被用于求解优化问题的封闭；形式，但梯度下降直到19世纪才作为优化问题的一种迭代近似的求解方
          關鍵詞：形式, 世纪才作为优化问题的一种迭代近似的求解方, 微积分和代数长期以来被用于求解优化问题的封闭, 但梯度下降直到, 世纪发明的
        - 摘要：从20世纪40年代开始，这些函数近似技术被用于导出诸如感知机的机器；学习模型。然而，最早的模型都是基于线性模型。来自包括Marvin；Minsky的批评指出了线性模型族的几个缺陷，例如它无法学习XOR函
          關鍵詞：的批评指出了线性模型族的几个缺陷, 世纪, 这些函数近似技术被用于导出诸如感知机的机器, 例如它无法学习, 然而
        - 摘要：学习非线性函数需要多层感知机的发展和计算该模型梯度的方法。基于；动态规划的链式法则的高效应用开始出现在20世纪60年代和70年代，主；要用于控制领域（Kelley，1960；Bryson and Denham，1961；Dreyfus，
          關鍵詞：要用于控制领域, 学习非线性函数需要多层感知机的发展和计算该模型梯度的方法, 世纪, 年代, 基于
        - 摘要：在反向传播的成功之后，神经网络研究获得了普及，并在20世纪90年代；初达到高峰。随后，其他机器学习技术变得更受欢迎，直到2006年开始；的现代深度学习复兴。
          關鍵詞：随后, 世纪, 的现代深度学习复兴, 年代, 初达到高峰
        - 摘要：现代前馈网络的核心思想自20世纪80年代以来没有发生重大变化，仍然；使用相同的反向传播算法和相同的梯度下降方法。1986∼2015年，神经；网络性能的大部分改进可归因于两个因素：第一，较大的数据集减少了
          關鍵詞：第一, 世纪, 使用相同的反向传播算法和相同的梯度下降方法, 神经, 年代以来没有发生重大变化
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；统计泛化对神经网络的挑战的程度。第二，神经网络由于更强大的计算；机和更好的软件基础设施已经变得更大。然而，少量算法上的变化也显
          關鍵詞：机和更好的软件基础设施已经变得更大, 神经网络由于更强大的计算, 然而, 统计泛化对神经网络的挑战的程度, 少量算法上的变化也显
        - 摘要：其中一个算法上的变化是用损失函数的交叉熵族替代均方误差。均方误；差在20世纪80年代和90年代流行，但逐渐被交叉熵损失替代，并且最大；似然原理的想法在统计学界和机器学习界之间广泛传播。使用交叉熵损
          關鍵詞：世纪, 但逐渐被交叉熵损失替代, 均方误, 并且最大, 年代流行
        - 摘要：另一个显著改善前馈网络性能的算法上的主要变化是使用分段线性隐藏；单元来替代sig-moid隐藏单元，例如用整流线性单元。使用max{0,z}函；数的整流在早期神经网络中已经被引入，并且至少可以追溯到认知机
          關鍵詞：隐藏单元, 单元来替代, 数的整流在早期神经网络中已经被引入, 例如用整流线性单元, 使用
        - 摘要：对于小的数据集，Jarrett  et  al.  （2009b）观察到，使用整流非线性甚至；比学习隐藏层的权重值更加重要。随机的权重足以通过整流网络传播有；用的信息，允许在顶部的分类器层学习如何将不同的特征向量映射到类
          關鍵詞：允许在顶部的分类器层学习如何将不同的特征向量映射到类, 随机的权重足以通过整流网络传播有, 比学习隐藏层的权重值更加重要, 对于小的数据集, 用的信息
        - 摘要：当有更多数据可用时，学习开始提取足够的有用知识来超越随机选择参；数的性能。Glorot  et  al.  （2011a）说明，在深度整流网络中的学习比在；激活函数具有曲率或两侧饱和的深度网络中的学习更容易。
          關鍵詞：激活函数具有曲率或两侧饱和的深度网络中的学习更容易, 说明, 在深度整流网络中的学习比在, 数的性能, 学习开始提取足够的有用知识来超越随机选择参
        - 摘要：整流线性单元还具有历史意义，因为它们表明神经科学继续对深度学习；算法的发展产生影响。Glorot  et  al.  （2011a）从生物学考虑整流线性单；元的导出。半整流非线性旨在描述生物神经元的这些性质：（1）对于
          關鍵詞：元的导出, 从生物学考虑整流线性单, 对于, 半整流非线性旨在描述生物神经元的这些性质, 算法的发展产生影响
        - 摘要：们不活跃的状态下进行操作（即它们应该具有稀疏激活；activation））。
          關鍵詞：即它们应该具有稀疏激活, 们不活跃的状态下进行操作
        - 摘要：（sparse
          關鍵詞：
        - 摘要：当2006年深度学习开始现代复兴时，前馈网络仍然有不良的声誉。从；2006∼2012年，人们普遍认为，前馈网络不会表现良好，除非它们得到；其他模型的辅助，例如概率模型。现在已经知道，只要具备适当的资源
          關鍵詞：年深度学习开始现代复兴时, 例如概率模型, 前馈网络不会表现良好, 前馈网络仍然有不良的声誉, 只要具备适当的资源
        - 摘要：前馈网络还有许多未实现的潜力。未来，我们期望它们用于更多的任；务，优化算法和模型设计的进步将进一步提高它们的性能。本章主要描；述了神经网络模型族。在接下来的章节中，我们将讨论如何使用这些模
          關鍵詞：本章主要描, 优化算法和模型设计的进步将进一步提高它们的性能, 我们期望它们用于更多的任, 前馈网络还有许多未实现的潜力, 述了神经网络模型族
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)   译者注：这里原文是“If  we  use  a  diagonal  matrix，or  a  scalar  times  the  diagonal  matrix…”，；即“如果我们使用对角矩阵，或者是一个标量乘以对角矩阵……  ”，但一个标量乘以对角矩阵和；对角矩阵没区别，结合上下文可以看出，这里原作者误把“identity”写成了“diagonal  matrix”，因
          關鍵詞：或者是一个标量乘以对角矩阵, 写成了, 这里原作者误把, 译者注, 如果我们使用对角矩阵
        - 摘要：(2)  之所以认为c是潜在的，是因为我们不能直接在数据中观测到它：给定输入 x 和目标 y ，不；可能确切地知道是哪个高斯组件产生 y ，但我们可以想象 y 是通过选择其中一个来产生的，并且；将那个未被观测到的选择作为随机变量。
          關鍵詞：是潜在的, 是通过选择其中一个来产生的, 将那个未被观测到的选择作为随机变量, 和目标, 是因为我们不能直接在数据中观测到它
第7章：深度学习中的正则化
    6.6：历史小记
        - 摘要：机器学习中的一个核心问题是设计不仅在训练数据上表现好，而且能在；新输入上泛化好的算法。在机器学习中，许多策略被显式地设计来减少；测试误差（可能会以增大训练误差为代价）。这些策略被统称为正则
          關鍵詞：新输入上泛化好的算法, 许多策略被显式地设计来减少, 测试误差, 机器学习中的一个核心问题是设计不仅在训练数据上表现好, 而且能在
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；第5章介绍了泛化、欠拟合、过拟合、偏差、方差和正则化的基本概；念。如果你不熟悉这些概念，请先参考第5章，然后再继续阅读本章。
          關鍵詞：过拟合, 然后再继续阅读本章, 章介绍了泛化, 请先参考第, 欠拟合
        - 摘要：在本章中，我们会更详细地介绍正则化，重点介绍深度模型（或组成深；度模型的模块）的正则化策略。
          關鍵詞：或组成深, 的正则化策略, 度模型的模块, 重点介绍深度模型, 我们会更详细地介绍正则化
        - 摘要：本章中的某些章节涉及机器学习中的标准概念。如果你已经熟悉了这些；概念，可以随意跳过相关章节。然而，本章的大多数内容是关于这些基；本概念在特定神经网络中的扩展概念。
          關鍵詞：可以随意跳过相关章节, 概念, 然而, 本章的大多数内容是关于这些基, 本章中的某些章节涉及机器学习中的标准概念
        - 摘要：在第5.2.2节中，我们将正则化定义为“对学习算法的修改——旨在减少；泛化误差而不是训练误差”。目前有许多正则化策略。有些策略向机器；学习模型添加限制参数值的额外约束。有些策略向目标函数增加额外项
          關鍵詞：对学习算法的修改, 有些策略向机器, 有些策略向目标函数增加额外项, 节中, 目前有许多正则化策略
        - 摘要：在深度学习的背景下，大多数正则化策略都会对估计进行正则化。估计；的正则化以偏差的增加换取方差的减少。一个有效的正则化是有利；的“交易”，也就是能显著减少方差而不过度增加偏差。我们在第5章中
          關鍵詞：估计, 我们在第, 章中, 也就是能显著减少方差而不过度增加偏差, 一个有效的正则化是有利
        - 摘要：在实践中，过于复杂的模型族不一定包括目标函数或真实数据生成过；程，甚至也不包括近似过程。我们几乎从未知晓真实数据的生成过程，；所以我们永远不知道被估计的模型族是否包括生成过程。然而，深度学
          關鍵詞：深度学, 我们几乎从未知晓真实数据的生成过程, 然而, 甚至也不包括近似过程, 过于复杂的模型族不一定包括目标函数或真实数据生成过
        - 摘要：（模型族）。
          關鍵詞：模型族
        - 摘要：这意味着控制模型的复杂度不是找到合适规模的模型（带有正确的参数；个数）这样一个简单的事情。相反，我们可能会发现，或者说在实际的；深度学习场景中我们几乎总是会发现，最好的拟合模型（从最小化泛化
          關鍵詞：深度学习场景中我们几乎总是会发现, 这样一个简单的事情, 个数, 从最小化泛化, 最好的拟合模型
        - 摘要：现在我们回顾几种策略，以创建这些正则化的大型深度模型。
          關鍵詞：以创建这些正则化的大型深度模型, 现在我们回顾几种策略
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；7.1　参数范数惩罚
          關鍵詞：参数范数惩罚
        - 摘要：7.1.1　L^2参数正则化
          關鍵詞：参数正则化
        - 摘要：7.1.2　L^1正则化
          關鍵詞：正则化
    7.1：参数范数惩罚
        - 摘要：7.1.1　L 2 参数正则化
          關鍵詞：参数正则化
        - 摘要：7.1.2　L 1 正则化
          關鍵詞：正则化
        - 摘要：正则化在深度学习的出现前就已经被使用了数十年。线性模型，如线性；回归和逻辑回归，可以使用简单、直接、有效的正则化策略。
          關鍵詞：线性模型, 回归和逻辑回归, 有效的正则化策略, 可以使用简单, 如线性
        - 摘要：，限制；许多正则化方法通过对目标函数J  添加一个参数范数惩罚；模型（如神经网络、线性回归或逻辑回归）的学习能力。我们将正则化
          關鍵詞：许多正则化方法通过对目标函数, 添加一个参数范数惩罚, 线性回归或逻辑回归, 如神经网络, 的学习能力
        - 摘要：其中α∈［0，∞）是权衡范数惩罚项Ω和标准目标函数；贡献的超参数。将α设为0表示没有正则化。α越大，对应正则化惩罚越；大。
          關鍵詞：和标准目标函数, 是权衡范数惩罚项, 贡献的超参数, 对应正则化惩罚越, 其中
        - 摘要：相对
          關鍵詞：相对
        - 摘要：当我们的训练算法最小化正则化后的目标函数  时，它会降低原始目标；J关于训练数据的误差并同时减小在某些衡量标准下参数  θ  （或参数子；集）的规模。选择不同的参数范数Ω会偏好不同的解。在本节中，我们
          關鍵詞：或参数子, 在本节中, 关于训练数据的误差并同时减小在某些衡量标准下参数, 选择不同的参数范数, 会偏好不同的解
        - 摘要：在探究不同范数的正则化表现之前，需要说明一下，在神经网络中，参；数包括每一层仿射变换的权重和偏置，我们通常只对权重做惩罚而不对；偏置做正则惩罚。精确拟合偏置所需的数据通常比拟合权重少得多。每
          關鍵詞：我们通常只对权重做惩罚而不对, 在探究不同范数的正则化表现之前, 需要说明一下, 偏置做正则惩罚, 数包括每一层仿射变换的权重和偏置
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；数惩罚影响的权重，而向量 θ 表示所有参数（包括 w 和无须正则化的参；数）。
          關鍵詞：数惩罚影响的权重, 表示所有参数, 而向量, 包括, 和无须正则化的参
        - 摘要：在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分；配不同的α系数。寻找合适的多个超参数的代价很大，因此为了减少搜；索空间，我们会在所有层使用相同的权重衰减。
          關鍵詞：系数, 寻找合适的多个超参数的代价很大, 在神经网络的情况下, 有时希望对网络的每一层使用单独的惩罚, 并分
        - 摘要：7.1.1　L 2 参数正则化
          關鍵詞：参数正则化
        - 摘要：在第5.2节中我们已经看到过最简单而又最常见的参数范数惩罚，即通；常被称为权重衰减 （weight decay）的L 2 参数范数惩罚。这个正则化策
          關鍵詞：节中我们已经看到过最简单而又最常见的参数范数惩罚, 这个正则化策, 参数范数惩罚, 在第, 常被称为权重衰减
        - 摘要：略通过向目标函数添加一个正则项
          關鍵詞：略通过向目标函数添加一个正则项
        - 摘要：，使
          關鍵詞：
        - 摘要：权重更加接近原点  (1)  。在其他学术圈，L  2  也被称为岭回归或Tikhonov；正则。
          關鍵詞：正则, 也被称为岭回归或, 在其他学术圈, 权重更加接近原点
        - 摘要：我们可以通过研究正则化后目标函数的梯度，洞察一些权重衰减的正则；化表现。为了简单起见，我们假定其中没有偏置参数，因此 θ 就是 w 。；这样一个模型具有以下总的目标函数：
          關鍵詞：洞察一些权重衰减的正则, 就是, 因此, 化表现, 这样一个模型具有以下总的目标函数
        - 摘要：与之对应的梯度为
          關鍵詞：与之对应的梯度为
        - 摘要：使用单步梯度下降更新权重，即执行以下更新：
          關鍵詞：使用单步梯度下降更新权重, 即执行以下更新
        - 摘要：换种写法就是
          關鍵詞：换种写法就是
        - 摘要：我们可以看到，加入权重衰减后会引起学习规则的修改，即在每步执行；通常的梯度更新之前先收缩权重向量（将权重向量乘以一个常数因；子）。这是单个步骤发生的变化。但是，在训练的整个过程会发生什么
          關鍵詞：通常的梯度更新之前先收缩权重向量, 将权重向量乘以一个常数因, 但是, 加入权重衰减后会引起学习规则的修改, 我们可以看到
        - 摘要：呢？
          關鍵詞：
        - 摘要：为未正则化的目标函数取得最小训练误；我们进一步简化分析，令；差时的权重向量，即
          關鍵詞：我们进一步简化分析, 为未正则化的目标函数取得最小训练误, 差时的权重向量
        - 摘要：，并在
          關鍵詞：并在
        - 摘要：如下
          關鍵詞：如下
        - 摘要：其中  H  是J在；义为最优，即梯度消失为0，所以该二次近似中没有一阶项。同样地，；因为
          關鍵詞：因为, 同样地, 即梯度消失为, 其中, 义为最优
        - 摘要：是J的一个最优点，我们可以得出 H 是半正定的结论。
          關鍵詞：是半正定的结论, 的一个最优点, 我们可以得出
        - 摘要：处计算的Hessian矩阵（关于  w  ）。因为
          關鍵詞：因为, 矩阵, 关于, 处计算的
        - 摘要：被定
          關鍵詞：被定
        - 摘要：当  取得最小时，其梯度
          關鍵詞：其梯度, 取得最小时
        - 摘要：为0。
          關鍵詞：
        - 摘要：为了研究权重衰减带来的影响，我们在式（7.7）中添加权重衰减的梯；度。现在我们探讨最小化正则化后的   。我们使用变量   表示此时的；最优点：
          關鍵詞：为了研究权重衰减带来的影响, 我们在式, 现在我们探讨最小化正则化后的, 我们使用变量, 表示此时的
        - 摘要：。那么当α增加时会发生什；当α趋向于0时，正则化的解  会趋向；么呢？因为  H  是实对称的，所以我们可以将其分解为一个对角矩阵Λ
          關鍵詞：因为, 所以我们可以将其分解为一个对角矩阵, 那么当, 会趋向, 是实对称的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；我们可以看到权重衰减的效果是沿着由  H  的特征向量所定义的轴缩放
          關鍵詞：的特征向量所定义的轴缩放, 我们可以看到权重衰减的效果是沿着由
        - 摘要：。具体来说，我们会根据
          關鍵詞：我们会根据, 具体来说
        - 摘要：因子缩放与  H  第i个特征向量
          關鍵詞：个特征向量, 因子缩放与
        - 摘要：对齐的
          關鍵詞：对齐的
        - 摘要：的分量。（不妨查看图2.3，回顾这种缩放的原理）。
          關鍵詞：回顾这种缩放的原理, 不妨查看图, 的分量
        - 摘要：沿着  H  特征值较大的方向（如
          關鍵詞：特征值较大的方向, 沿着
        - 摘要：）正则化的影响较小。而
          關鍵詞：正则化的影响较小
        - 摘要：的分量将会收缩到几乎为零。这种效应如图7.1所示。
          關鍵詞：所示, 这种效应如图, 的分量将会收缩到几乎为零
        - 摘要：图7.1　L 2 （或权重衰减）正则化对最佳 w 值的影响。实线椭圆表示没有正则化目标的等值；线。虚线圆圈表示L 2 正则化项的等值线。在；Hessian的第一维特征值很小。当从
          關鍵詞：值的影响, 虚线圆圈表示, 当从, 正则化对最佳, 的第一维特征值很小
        - 摘要：点，这两个竞争目标达到平衡。目标函数J的；水平移动时，目标函数不会增加得太多。因为目标函
          關鍵詞：目标函数不会增加得太多, 因为目标函, 目标函数, 这两个竞争目标达到平衡, 水平移动时
        - 摘要：的移动非常敏感。对应的特征值较大，表示高曲率。因
          關鍵詞：表示高曲率, 对应的特征值较大, 的移动非常敏感
        - 摘要：只有在显著减小目标函数方向上的参数会保留得相对完好。在无助于目；标函数减小的方向（对应Hessian矩阵较小的特征值）上改变参数不会显
          關鍵詞：上改变参数不会显, 在无助于目, 矩阵较小的特征值, 标函数减小的方向, 对应
        - 摘要：著增加梯度。这种不重要方向对应的分量会在训练过程中因正则化而衰；减掉。
          關鍵詞：减掉, 这种不重要方向对应的分量会在训练过程中因正则化而衰, 著增加梯度
        - 摘要：目前为止，我们讨论了权重衰减对优化一个抽象通用的二次代价函数的；影响。这些影响具体是怎么和机器学习关联的呢？我们可以研究线性回；归，它的真实代价函数是二次的，因此我们可以使用相同的方法分析。
          關鍵詞：影响, 它的真实代价函数是二次的, 目前为止, 因此我们可以使用相同的方法分析, 这些影响具体是怎么和机器学习关联的呢
        - 摘要：我们添加L 2 正则项后，目标函数变为
          關鍵詞：我们添加, 正则项后, 目标函数变为
        - 摘要：这将正规方程的解从
          關鍵詞：这将正规方程的解从
        - 摘要：变为
          關鍵詞：变为
        - 摘要：与协方差矩阵
          關鍵詞：与协方差矩阵
        - 摘要：式（7.16）中的矩阵；将这个矩阵替换为式（7.17）中的；来的是一样的，不同的仅仅是在对角加了α。这个矩阵的对角项对应每
          關鍵詞：将这个矩阵替换为式, 中的矩阵, 这个矩阵的对角项对应每, 来的是一样的, 不同的仅仅是在对角加了
        - 摘要：成正比。L 2 正则项；，这个新矩阵与原
          關鍵詞：成正比, 这个新矩阵与原, 正则项
        - 摘要：7.1.2　L 1 正则化
          關鍵詞：正则化
        - 摘要：L  2  权重衰减是权重衰减最常见的形式，我们还可以使用其他的方法限；制模型参数的规模。一个选择是使用L 1 正则化。
          關鍵詞：权重衰减是权重衰减最常见的形式, 我们还可以使用其他的方法限, 正则化, 一个选择是使用, 制模型参数的规模
        - 摘要：形式地，对模型参数 w 的L 1 正则化被定义为
          關鍵詞：正则化被定义为, 形式地, 对模型参数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；即各个参数的绝对值之和  (2)  。接着我们将讨论L  1  正则化对简单线性回；归模型的影响，与分析L  2  正则化时一样不考虑偏置参数。我们尤其感
          關鍵詞：即各个参数的绝对值之和, 正则化对简单线性回, 我们尤其感, 正则化时一样不考虑偏置参数, 接着我们将讨论
        - 摘要：对应的梯度（实际上是次梯度）：
          關鍵詞：对应的梯度, 实际上是次梯度
        - 摘要：其中sign( w )只是简单地取 w 各个元素的正负号。
          關鍵詞：各个元素的正负号, 只是简单地取, 其中
        - 摘要：观察式（7.20），我们立刻发现L  1  的正则化效果与L  2  大不一样。具体；来说，我们可以看到正则化对梯度的影响不再是线性地缩放每个w  i  ；；而是添加了一项与sign(w  i  )同号的常数。使用这种形式的梯度之后，我
          關鍵詞：我们可以看到正则化对梯度的影响不再是线性地缩放每个, 具体, 的正则化效果与, 同号的常数, 使用这种形式的梯度之后
        - 摘要：简单线性模型具有二次代价函数，我们可以通过泰勒级数表示。或者我；们可以设想，这是逼近更复杂模型的代价函数的截断泰勒级数。在这个；设定下，梯度由下式给出
          關鍵詞：设定下, 们可以设想, 这是逼近更复杂模型的代价函数的截断泰勒级数, 在这个, 简单线性模型具有二次代价函数
        - 摘要：同样， H 是J在
          關鍵詞：同样
        - 摘要：处的Hessian矩阵（关于 w ）。
          關鍵詞：矩阵, 关于, 处的
        - 摘要：由于L  1  惩罚项在完全一般化的Hessian的情况下，无法得到直接清晰的；代数表达式，因此我们将进一步简化假设Hessian是对角的，即
          關鍵詞：惩罚项在完全一般化的, 因此我们将进一步简化假设, 代数表达式, 由于, 无法得到直接清晰的
        - 摘要：。如果线；性回归问题中的数据已被预处理（如可以使用PCA），去除了输入特征；之间的相关性，那么这一假设成立。
          關鍵詞：那么这一假设成立, 如果线, 去除了输入特征, 之间的相关性, 如可以使用
        - 摘要：，其中每个
          關鍵詞：其中每个
        - 摘要：我们可以将L 1 正则化目标函数的二次近似分解成关于参数的求和：
          關鍵詞：正则化目标函数的二次近似分解成关于参数的求和, 我们可以将
        - 摘要：如下列形式的解析解（对每一维i）可以最小化这个近似代价函数：
          關鍵詞：对每一维, 如下列形式的解析解, 可以最小化这个近似代价函数
        - 摘要：对每个i，考虑
          關鍵詞：对每个, 考虑
        - 摘要：的情形，会有两种可能结果：
          關鍵詞：的情形, 会有两种可能结果
        - 摘要：（1）
          關鍵詞：
        - 摘要：的情况。正则化后目标中的w i 最优值是w
          關鍵詞：最优值是, 的情况, 正则化后目标中的
        - 摘要：i  ＝0。这是因为在方向i上；L 1 正则化项将w i 推至0。
          關鍵詞：推至, 这是因为在方向, 正则化项将
        - 摘要：的贡献被抵消，
          關鍵詞：的贡献被抵消
        - 摘要：（2）
          關鍵詞：
        - 摘要：的情况。在这种情况下，正则化不会将w  i
          關鍵詞：正则化不会将, 在这种情况下, 的情况
        - 摘要：的最优值推至0，而仅仅在那个方向上移动
          關鍵詞：而仅仅在那个方向上移动, 的最优值推至
        - 摘要：的距离。
          關鍵詞：的距离
        - 摘要：的情况与此类似，但是L 1 惩罚项使w i 更接近0（增加
          關鍵詞：但是, 更接近, 的情况与此类似, 惩罚项使, 增加
        - 摘要：）或者为0。
          關鍵詞：或者为
        - 摘要：相比L  2  正则化，L  1  正则化会产生更稀疏  （sparse）的解。此处稀疏性；指的是最优值中的一些参数为0。和L  2  正则化相比，L  1  正则化的稀疏；性具有本质的不同。式（7.13）给出了L  2  正则化的解  。如果我们使
          關鍵詞：正则化相比, 如果我们使, 正则化会产生更稀疏, 正则化, 此处稀疏性
        - 摘要：重新考虑这个等式，会发现
          關鍵詞：重新考虑这个等式, 会发现
        - 摘要：。如果   不是
          關鍵詞：如果, 不是
        - 摘要：零，那么   也会保持非零。这表明L  2  正则化不会使参数变得稀疏，；而L 1 正则化有可能通过足够大的α实现稀疏。
          關鍵詞：也会保持非零, 那么, 正则化不会使参数变得稀疏, 实现稀疏, 这表明
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；1  正则化导出的稀疏性质已经被广泛地用于特征选择  （feature
          關鍵詞：正则化导出的稀疏性质已经被广泛地用于特征选择
        - 摘要：由L；selection）机制。特征选择从可用的特征子集选择出有意义的特征，化；简机器学习问题。著名的LASSO（Tibshirani，1995）（Least  Absolute
          關鍵詞：简机器学习问题, 特征选择从可用的特征子集选择出有意义的特征, 著名的, 机制
        - 摘要：在第5.6.1节，我们看到许多正则化策略可以被解释为MAP贝叶斯推断，；特别是L 2 正则化相当于权重是高斯先验的MAP贝叶斯推断。对于L  1  正；则化，用于正则化代价函数的惩罚项
          關鍵詞：则化, 正则化相当于权重是高斯先验的, 特别是, 贝叶斯推断, 对于
        - 摘要：断最大化的对数先验项是等价的（；同性的拉普拉斯分布（式（3.26）））：
          關鍵詞：断最大化的对数先验项是等价的, 同性的拉普拉斯分布
        - 摘要：与通过MAP贝叶斯推
          關鍵詞：贝叶斯推, 与通过
        - 摘要：并且权重先验是各向
          關鍵詞：并且权重先验是各向
        - 摘要：因为是关于  w  最大化进行学习，我们可以忽略logα−log  2项，因为它们；与 w 无关。
          關鍵詞：我们可以忽略, 无关, 因为它们, 因为是关于, 最大化进行学习
    7.2：作为约束的范数惩罚
        - 摘要：考虑经过参数范数正则化的代价函数：
          關鍵詞：考虑经过参数范数正则化的代价函数
        - 摘要：回顾第4.4节，我们可以构造一个广义Lagrange函数来最小化带约束的函；数，即在原始目标函数上添加一系列惩罚项。每个惩罚是一个被称；为Karush-Kuhn-Tucker  （Karush-Kuhn-Tucker）乘子的系数以及一个
          關鍵詞：即在原始目标函数上添加一系列惩罚项, 每个惩罚是一个被称, 回顾第, 函数来最小化带约束的函, 我们可以构造一个广义
        - 摘要：这个约束问题的解由下式给出
          關鍵詞：这个约束问题的解由下式给出
        - 摘要：如第4.4节中描述的，要解决这个问题，我们需要对  θ  和α都做出调整。；第4.5节给出了一个带L  2  约束的线性回归实例。还有许多不同的优化方；法，有些可能会使用梯度下降而其他可能会使用梯度为0的解析解，但
          關鍵詞：节中描述的, 还有许多不同的优化方, 我们需要对, 的解析解, 如第
        - 摘要：为了洞察约束的影响，我们可以固定α  *  ，把这个问题看成只跟  θ  有关；的函数：
          關鍵詞：为了洞察约束的影响, 有关, 的函数, 把这个问题看成只跟, 我们可以固定
        - 摘要：这和最小化  的正则化训练问题是完全一样的。因此，我们可以把参数；范数惩罚看作对权重强加的约束。如果Ω是L 2 范数，那么权重就是被约；束在一个L 2 球中。如果Ω是L 1 范数，那么权重就是被约束在一个L 1 范
          關鍵詞：如果, 范数惩罚看作对权重强加的约束, 我们可以把参数, 因此, 球中
        - 摘要：有时候，我们希望使用显式的限制，而不是惩罚。如第4.4节所述，我；们可以修改下降算法（如随机梯度下降算法），使其先计算J( θ )的下降；步，然后将  θ  投影到满足Ω(  θ  )＜k的最近点。如果我们知道什么样的k
          關鍵詞：的最近点, 的下降, 如随机梯度下降算法, 然后将, 如第
        - 摘要：另一个使用显式约束和重投影而不是使用惩罚强加约束的原因是，惩罚；可能会导致目标函数非凸而使算法陷入局部极小（对应于小的 θ ）。当；训练神经网络时，这通常表现为训练带有几个“死亡单元”的神经网络。
          關鍵詞：可能会导致目标函数非凸而使算法陷入局部极小, 训练神经网络时, 的神经网络, 惩罚, 对应于小的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；最后，因为重投影的显式约束还对优化过程增加了一定的稳定性，所以；这是另一个好处。当使用较高的学习率时，很可能进入正反馈，即大的
          關鍵詞：当使用较高的学习率时, 很可能进入正反馈, 这是另一个好处, 最后, 即大的
        - 摘要：Hinton et al. （2012c）尤其推荐由Srebro and Shraibman（2005）引入的；策略：约束神经网络层的权重矩阵每列的范数，而不是限制整个权重矩；阵的Frobenius范数。分别限制每一列的范数可以防止某一隐藏单元有非
          關鍵詞：策略, 约束神经网络层的权重矩阵每列的范数, 阵的, 引入的, 分别限制每一列的范数可以防止某一隐藏单元有非
    7.3：正则化和欠约束问题
        - 摘要：在某些情况下，为了正确定义机器学习问题，正则化是必要的。机器学；求；习中许多线性模型，包括线性回归和PCA，都依赖于对矩阵
          關鍵詞：习中许多线性模型, 都依赖于对矩阵, 正则化是必要的, 为了正确定义机器学习问题, 包括线性回归和
        - 摘要：相关矩阵可逆时，这些线性问题有闭式解。没有闭式解的问题也可能是；欠定的。一个例子是应用于线性可分问题的逻辑回归。如果权重向量  w；能够实现完美分类，那么2  w  也会以更高似然实现完美分类。类似随机
          關鍵詞：也会以更高似然实现完美分类, 类似随机, 欠定的, 这些线性问题有闭式解, 那么
        - 摘要：大多数形式的正则化能够保证应用于欠定问题的迭代方法收敛。例如，；当似然的斜率等于权重衰减的系数时，权重衰减将阻止梯度下降继续增
          關鍵詞：当似然的斜率等于权重衰减的系数时, 大多数形式的正则化能够保证应用于欠定问题的迭代方法收敛, 例如, 权重衰减将阻止梯度下降继续增
        - 摘要：加权重的大小。
          關鍵詞：加权重的大小
        - 摘要：使用正则化解决欠定问题的想法不局限于机器学习。同样的想法在几个；基本线性代数问题中也非常有用。
          關鍵詞：同样的想法在几个, 基本线性代数问题中也非常有用, 使用正则化解决欠定问题的想法不局限于机器学习
        - 摘要：正如我们在第2.9节看到的，我们可以使用Moore-Penrose求解欠定线性；方程。回想  伪逆
          關鍵詞：我们可以使用, 方程, 伪逆, 回想, 节看到的
        - 摘要：的一个定义：
          關鍵詞：的一个定义
        - 摘要：现在我们可以将式（7.29）看作进行具有权重衰减的线性回归。具体来；说，当正则化系数趋向0时，式（7.29）是式（7.17）的极限。因此，我；们可以将伪逆解释为使用正则化来稳定欠定问题。
          關鍵詞：具体来, 们可以将伪逆解释为使用正则化来稳定欠定问题, 是式, 因此, 现在我们可以将式
    7.4：数据集增强
        - 摘要：让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练。当；然，在实践中，我们拥有的数据量是很有限的。解决这个问题的一种方；法是创建假数据并添加到训练集中。对于一些机器学习任务，创建新的
          關鍵詞：创建新的, 对于一些机器学习任务, 让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练, 法是创建假数据并添加到训练集中, 我们拥有的数据量是很有限的
        - 摘要：对分类来说这种方法是最简单的。分类器需要一个复杂的高维输入 x ，；并用单个类别标识y概括  x  。这意味着分类面临的一个主要任务是要对；各种各样的变换保持不变。我们可以轻易通过转换训练集中的 x 来生成
          關鍵詞：分类器需要一个复杂的高维输入, 对分类来说这种方法是最简单的, 各种各样的变换保持不变, 这意味着分类面临的一个主要任务是要对, 概括
        - 摘要：这种方法对于其他许多任务来说并不那么容易。例如，除非我们已经解；决了密度估计问题，否则在密度估计任务中生成新的假数据是很困难；的。
          關鍵詞：决了密度估计问题, 除非我们已经解, 否则在密度估计任务中生成新的假数据是很困难, 例如, 这种方法对于其他许多任务来说并不那么容易
        - 摘要：数据集增强对一个具体的分类问题来说是特别有效的方法：对象识别。；图像是高维的并包括各种巨大的变化因素，其中有许多可以轻易地模；拟。即使模型已使用卷积和池化技术（第9章）对部分平移保持不变，
          關鍵詞：其中有许多可以轻易地模, 对象识别, 数据集增强对一个具体的分类问题来说是特别有效的方法, 对部分平移保持不变, 即使模型已使用卷积和池化技术
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；我们必须要小心，不能使用会改变类别的转换。例如，光学字符识别任；务需要认识到“b”和“d”以及“6”和“9”的区别，所以对这些任务来说，水
          關鍵詞：的区别, 所以对这些任务来说, 例如, 不能使用会改变类别的转换, 以及
        - 摘要：能保持我们希望的分类不变，但不容易执行的转换也是存在的。例如，；平面外绕轴转动难以通过简单的几何运算在输入像素上实现。
          關鍵詞：但不容易执行的转换也是存在的, 平面外绕轴转动难以通过简单的几何运算在输入像素上实现, 能保持我们希望的分类不变, 例如
        - 摘要：数据集增强对语音识别任务也是有效的（Jaitly and Hinton，2013）。
          關鍵詞：数据集增强对语音识别任务也是有效的
        - 摘要：在神经网络的输入层注入噪声（Sietsma and Dow，1991）也可以看作数；据增强的一种方式。对于许多分类甚至一些回归任务而言，即使小的随；机噪声被加到输入，任务仍应该是能够被解决的。然而，神经网络被证
          關鍵詞：据增强的一种方式, 在神经网络的输入层注入噪声, 即使小的随, 也可以看作数, 然而
        - 摘要：在比较机器学习基准测试的结果时，考虑其采取的数据集增强是很重要；的。通常情况下，人工设计的数据集增强方案可以大大减少机器学习技；术的泛化误差。将一个机器学习算法的性能与另一个进行对比时，对照
          關鍵詞：在比较机器学习基准测试的结果时, 对照, 通常情况下, 术的泛化误差, 人工设计的数据集增强方案可以大大减少机器学习技
    7.5：噪声鲁棒性
        - 摘要：7.5.1　向输出目标注入噪声
          關鍵詞：向输出目标注入噪声
        - 摘要：第7.4节已经提出将噪声作用于输入，作为数据集增强策略。对于某些；模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚；（Bishop，1995a，b）。在一般情况下，注入噪声远比简单地收缩参数
          關鍵詞：对于某些, 注入噪声远比简单地收缩参数, 模型而言, 在一般情况下, 作为数据集增强策略
        - 摘要：另一种正则化模型的噪声使用方式是将其加到的权重。这项技术主要用；于循环神经网络（Jim  et  al.  ，1996；Graves，2011）。这可以被解释为；关于权重的贝叶斯推断的随机实现。贝叶斯学习过程将权重视为不确定
          關鍵詞：关于权重的贝叶斯推断的随机实现, 另一种正则化模型的噪声使用方式是将其加到的权重, 这可以被解释为, 这项技术主要用, 于循环神经网络
        - 摘要：在某些假设下，施加于权重的噪声可以被解释为与更传统的正则化形式；等同，鼓励要学习的函数保持稳定。我们研究回归的情形，也就是训练；将一组特征 x 映射成一个标量的函数
          關鍵詞：也就是训练, 在某些假设下, 等同, 我们研究回归的情形, 施加于权重的噪声可以被解释为与更传统的正则化形式
        - 摘要：与真实值y的误差：
          關鍵詞：的误差, 与真实值
        - 摘要：训练集包含m对标注样例
          關鍵詞：训练集包含, 对标注样例
        - 摘要：。
          關鍵詞：
        - 摘要：现在我们假设对每个输入表示，网络权重添加随机扰动
          關鍵詞：现在我们假设对每个输入表示, 网络权重添加随机扰动
        - 摘要：。想象我们有一个标准的l层MLP。我们将扰动；。尽管有噪声注入，我们仍然希望减少网络输出误
          關鍵詞：尽管有噪声注入, 我们仍然希望减少网络输出误, 我们将扰动, 想象我们有一个标准的
        - 摘要：模型记为；差的平方。因此目标函数变为：
          關鍵詞：因此目标函数变为, 模型记为, 差的平方
        - 摘要：对于小的η，最小化带权重噪声（方差为   ）的J等同于最小化附加正；则化项：；的J。这种形式的正则化鼓励参数进
          關鍵詞：等同于最小化附加正, 对于小的, 这种形式的正则化鼓励参数进, 则化项, 方差为
        - 摘要：and
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；1995）。在简化的线性回归中（例如，；项退化为
          關鍵詞：在简化的线性回归中, 项退化为, 例如
        - 摘要：），正则；，这与函数的参数无关，因此不会对   关于模
          關鍵詞：正则, 因此不会对, 关于模, 这与函数的参数无关
        - 摘要：7.5.1　向输出目标注入噪声
          關鍵詞：向输出目标注入噪声
        - 摘要：大多数数据集的y标签都有一定错误。错误的y不利于最大化
          關鍵詞：标签都有一定错误, 错误的, 大多数数据集的, 不利于最大化
        - 摘要：。避免这种情况的一种方法是显式地对标签上的噪声；进行建模。例如，我们可以假设，对于一些小常数   ，训练集标记y是；正确的概率是
          關鍵詞：训练集标记, 例如, 正确的概率是, 我们可以假设, 避免这种情况的一种方法是显式地对标签上的噪声
        - 摘要：标从0和1替换成
          關鍵詞：标从, 替换成
        - 摘要：，正则化具有k个输出
          關鍵詞：正则化具有, 个输出
        - 摘要：的softmax函数的模型。标准交叉熵损失可以用在这些非确切目标的输；出上。使用softmax函数和明确目标的最大似然学习可能永远不会收敛；——softmax函数永远无法真正预测0概率或1概率，因此它会继续学习
          關鍵詞：函数和明确目标的最大似然学习可能永远不会收敛, 函数永远无法真正预测, 标准交叉熵损失可以用在这些非确切目标的输, 函数的模型, 概率
        - 摘要：7.5.1　向输出目标注入噪；声
          關鍵詞：向输出目标注入噪
    7.6：半监督学习
        - 摘要：在半监督学习的框架下，P(x)产生的未标记样本和P(x,y)中的标记样本都；用于估计P(y｜x)或者根据x预测y。
          關鍵詞：用于估计, 或者根据, 在半监督学习的框架下, 中的标记样本都, 预测
        - 摘要：在深度学习的背景下，半监督学习通常指的是学习一个表示  h  ＝f(x)。；学习表示的目的是使相同类中的样本有类似的表示。无监督学习可以为；如何在表示空间聚集样本提供有用线索。在输入空间紧密聚集的样本应
          關鍵詞：无监督学习可以为, 学习表示的目的是使相同类中的样本有类似的表示, 半监督学习通常指的是学习一个表示, 如何在表示空间聚集样本提供有用线索, 在深度学习的背景下
        - 摘要：我们可以构建这样一个模型，其中生成模型P(x)或P(x,y)与判别模型P(y；｜x)共享参数，而不用分离无监督和监督部分。我们权衡监督模型准则；−log  P(y｜x)和无监督或生成模型准则（如−log  P(x)或−log  P(x,y)）。生
          關鍵詞：我们权衡监督模型准则, 共享参数, 和无监督或生成模型准则, 其中生成模型, 我们可以构建这样一个模型
        - 摘要：Salakhutdinov  and  Hinton（2008）描述了一种学习回归核机器中核函数；的方法，其中建模P(x)时使用的未标记样本大大提高了P(y｜x)的效果。
          關鍵詞：的效果, 其中建模, 描述了一种学习回归核机器中核函数, 的方法, 时使用的未标记样本大大提高了
        - 摘要：更多半监督学习的信息，请参阅Chapelle et al. （2006）。
          關鍵詞：请参阅, 更多半监督学习的信息
    7.7：多任务学习
        - 摘要：多任务学习（Caruana，1993）是通过合并几个任务中的样例（可以视；为对参数施加的软约束）来提高泛化的一种方式。正如额外的训练样本；能够将模型参数推向具有更好泛化能力的值一样，当模型的一部分被多
          關鍵詞：正如额外的训练样本, 可以视, 能够将模型参数推向具有更好泛化能力的值一样, 当模型的一部分被多, 多任务学习
        - 摘要：图7.2展示了多任务学习中非常普遍的一种形式，其中不同的监督任务；（给定x预测y (i) ）共享相同的输入x以及一些中间层表示 h （share） ，能；学习共同的因素池。该模型通常可以分为两类相关的参数：
          關鍵詞：其中不同的监督任务, 该模型通常可以分为两类相关的参数, 学习共同的因素池, 以及一些中间层表示, 预测
        - 摘要：（1）具体任务的参数（只能从各自任务的样本中实现良好的泛化），；如图7.2中的上层。
          關鍵詞：只能从各自任务的样本中实现良好的泛化, 具体任务的参数, 如图, 中的上层
        - 摘要：（2）所有任务共享的通用参数（从所有任务的汇集数据中获益），如；图7.2中的下层。
          關鍵詞：从所有任务的汇集数据中获益, 所有任务共享的通用参数, 中的下层
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图7.2　多任务学习在深度学习框架中可以以多种方式进行，该图说明了任务共享相同输入但涉；及不同目标随机变量的常见情况。深度网络的较低层（无论是监督前馈的，还是包括向下箭头
          關鍵詞：无论是监督前馈的, 还是包括向下箭头, 及不同目标随机变量的常见情况, 该图说明了任务共享相同输入但涉, 多任务学习在深度学习框架中可以以多种方式进行
        - 摘要：因为共享参数，其统计强度可大大提高（共享参数的样本数量相对于单；任务模式增加的比例），并能改善泛化和泛化误差的范围（Baxter，；1995）。当然，仅当不同的任务之间存在某些统计关系的假设是合理
          關鍵詞：并能改善泛化和泛化误差的范围, 共享参数的样本数量相对于单, 其统计强度可大大提高, 任务模式增加的比例, 当然
        - 摘要：从深度学习的观点看，底层的先验知识如下：能解释数据变化（在与之；相关联的不同任务中观察到）的因素中，某些因素是跨两个或更多任务；共享的。
          關鍵詞：的因素中, 某些因素是跨两个或更多任务, 在与之, 相关联的不同任务中观察到, 能解释数据变化
    7.8：提前终止
        - 摘要：当训练有足够的表示能力甚至会过拟合的大模型时，我们经常观察到，；训练误差会随着时间的推移逐渐降低但验证集的误差会再次上升。图；7.3是这些现象的一个例子，这种现象几乎一定会出现。
          關鍵詞：我们经常观察到, 是这些现象的一个例子, 这种现象几乎一定会出现, 训练误差会随着时间的推移逐渐降低但验证集的误差会再次上升, 当训练有足够的表示能力甚至会过拟合的大模型时
        - 摘要：图7.3　学习曲线显示负对数似然损失如何随时间变化（表示为遍历数据集的训练迭代数，或 轮；数 （epochs））。在这个例子中，我们在MNIST上训练了一个maxout网络。我们可以观察到训；练目标随时间持续减小，但验证集上的平均损失最终会再次增加，形成不对称的U形曲线
          關鍵詞：网络, 学习曲线显示负对数似然损失如何随时间变化, 上训练了一个, 形成不对称的, 我们可以观察到训
        - 摘要：这意味着我们只要返回使验证集误差最低的参数设置，就可以获得验证；集误差更低的模型（并且因此有希望获得更好的测试误差）。在每次验；证集误差有所改善后，我们存储模型参数的副本。当训练算法终止时，
          關鍵詞：我们存储模型参数的副本, 在每次验, 就可以获得验证, 并且因此有希望获得更好的测试误差, 集误差更低的模型
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；这种策略被称为提前终止 （early stopping）。这可能是深度学习中最常；用的正则化形式。它的流行主要是因为有效性和简单性。
          關鍵詞：用的正则化形式, 这种策略被称为提前终止, 这可能是深度学习中最常, 它的流行主要是因为有效性和简单性
        - 摘要：我们可以认为提前终止是非常高效的超参数选择算法。按照这种观点，；训练步数仅是另一个超参数。我们从图7.3可以看到，这个超参数在验；证集上具有U型性能曲线。很多控制模型容量的超参数在验证集上都是
          關鍵詞：我们从图, 训练步数仅是另一个超参数, 按照这种观点, 证集上具有, 可以看到
        - 摘要：另一个提前终止的额外代价是需要保持最佳的参数副本。这种代价一般；是可忽略的，因为可以将它储存在较慢较大的存储器上（例如，在GPU；内存中训练，但将最佳参数存储在主存储器或磁盘驱动器上）。由于最
          關鍵詞：内存中训练, 是可忽略的, 但将最佳参数存储在主存储器或磁盘驱动器上, 例如, 另一个提前终止的额外代价是需要保持最佳的参数副本
        - 摘要：提前终止是一种非常不显眼的正则化形式，它几乎不需要改变基本训练；过程、目标函数或一组允许的参数值。这意味着，无须破坏学习动态就；能很容易地使用提前终止。相对于权重衰减，必须小心不能使用太多的
          關鍵詞：它几乎不需要改变基本训练, 无须破坏学习动态就, 提前终止是一种非常不显眼的正则化形式, 相对于权重衰减, 必须小心不能使用太多的
        - 摘要：提前终止可单独使用或与其他的正则化策略结合使用。即使为鼓励更好；泛化，使用正则化策略改进目标函数，在训练目标的局部极小点达到最；好泛化也是非常罕见的。
          關鍵詞：提前终止可单独使用或与其他的正则化策略结合使用, 使用正则化策略改进目标函数, 在训练目标的局部极小点达到最, 好泛化也是非常罕见的, 泛化
        - 摘要：提前终止需要验证集，这意味着某些训练数据不能被馈送到模型。为了；更好地利用这一额外的数据，我们可以在完成提前终止的首次训练之；后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的训练数
          關鍵詞：为了, 在第二轮, 更好地利用这一额外的数据, 所有的训练数, 我们可以在完成提前终止的首次训练之
        - 摘要：算法7.1  　用于确定最佳训练时间量的提前终止元算法。这种元算法是；一种通用策略，可以很好地在各种训练算法和各种量化验证集误差的方；法上工作。
          關鍵詞：可以很好地在各种训练算法和各种量化验证集误差的方, 用于确定最佳训练时间量的提前终止元算法, 这种元算法是, 算法, 法上工作
        - 摘要：令n为评估间隔的步数。
          關鍵詞：为评估间隔的步数
        - 摘要：令p为“耐心（patience）”，即观察到较坏的验证集表现p次后终止。
          關鍵詞：即观察到较坏的验证集表现, 耐心, 次后终止
        - 摘要：令 θ o 为初始参数。
          關鍵詞：为初始参数
        - 摘要：θ ← θ o
          關鍵詞：
        - 摘要：i←0
          關鍵詞：
        - 摘要：j←0
          關鍵詞：
        - 摘要：ν←∞
          關鍵詞：
        - 摘要：θ * ← θ
          關鍵詞：
        - 摘要：i * ←i
          關鍵詞：
        - 摘要：while j＜p do
          關鍵詞：
        - 摘要：运行训练算法n步，更新 θ 。
          關鍵詞：运行训练算法, 更新
        - 摘要：i←i＋n
          關鍵詞：
        - 摘要：ν＇←ValidationSetError( θ )
          關鍵詞：
        - 摘要：if ν＇＜ν then
          關鍵詞：
        - 摘要：j←0
          關鍵詞：
        - 摘要：θ * ← θ
          關鍵詞：
        - 摘要：i * ←i
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；ν←ν＇
          關鍵詞：
        - 摘要：else
          關鍵詞：
        - 摘要：j←j＋1
          關鍵詞：
        - 摘要：end if
          關鍵詞：
        - 摘要：end while
          關鍵詞：
        - 摘要：最佳参数为 θ * ，最佳训练步数为i *
          關鍵詞：最佳训练步数为, 最佳参数为
        - 摘要：一个策略（算法7.2  ）是再次初始化模型，然后使用所有数据再次训；练。在这个第二轮训练过程中，我们使用第一轮提前终止训练确定的最；佳步数。此过程有一些细微之处。例如，我们没有办法知道重新训练
          關鍵詞：一个策略, 然后使用所有数据再次训, 算法, 是再次初始化模型, 在这个第二轮训练过程中
        - 摘要：另一个策略是保持从第一轮训练获得的参数，然后使用全部的数据继续；训练。在这个阶段，已经没有验证集指导我们需要在训练多少步后终；止。取而代之，我们可以监控验证集的平均损失函数，并继续训练，直
          關鍵詞：训练, 取而代之, 然后使用全部的数据继续, 另一个策略是保持从第一轮训练获得的参数, 在这个阶段
        - 摘要：提前终止对减少训练过程的计算成本也是有用的。除了由于限制训练的；迭代次数而明显减少的计算成本，还带来了正则化的益处（不需要添加；惩罚项的代价函数或计算这种附加项的梯度）。
          關鍵詞：惩罚项的代价函数或计算这种附加项的梯度, 除了由于限制训练的, 不需要添加, 还带来了正则化的益处, 提前终止对减少训练过程的计算成本也是有用的
        - 摘要：算法7.2  　使用提前终止确定训练步数，然后在所有数据上训练的元算；法。
          關鍵詞：使用提前终止确定训练步数, 然后在所有数据上训练的元算, 算法
        - 摘要：令
          關鍵詞：
        - 摘要：为训练集。
          關鍵詞：为训练集
        - 摘要：从随机  θ  开始，使用
          關鍵詞：使用, 开始, 从随机
        - 摘要：作为训练集，
          關鍵詞：作为训练集
        - 摘要：作为验证集，
          關鍵詞：作为验证集
        - 摘要：运行（算法7.1 ）。这将返回最佳训练步数i * 。
          關鍵詞：这将返回最佳训练步数, 运行, 算法
        - 摘要：将 θ 再次设为随机值。
          關鍵詞：再次设为随机值
        - 摘要：上训练i * 步。
          關鍵詞：上训练
        - 摘要：算法7.3  　使用提前终止确定将会过拟合的目标值，然后在所有数据上；训练直到再次达到该值的元算法。
          關鍵詞：训练直到再次达到该值的元算法, 然后在所有数据上, 使用提前终止确定将会过拟合的目标值, 算法
        - 摘要：提前终止为何具有正则化效果：  目前为止，我们已经声明提前终止是；一种正则化策略，但只通过展示验证集误差的学习曲线是一个U型曲线；来支持这种说法。提前终止正则化模型的真正机制是什么呢？
          關鍵詞：一种正则化策略, 型曲线, 目前为止, 来支持这种说法, 但只通过展示验证集误差的学习曲线是一个
        - 摘要：的效果就好像是权重衰减系数的倒数。
          關鍵詞：的效果就好像是权重衰减系数的倒数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图7.4　提前终止效果的示意图。（左）实线轮廓线表示负对数似然的轮廓。虚线表示从原点开；始的SGD所经过的轨迹。提前终止的轨迹在较早的点
          關鍵詞：始的, 提前终止效果的示意图, 提前终止的轨迹在较早的点, 虚线表示从原点开, 所经过的轨迹
        - 摘要：处停止，而不是停止在最小化代价的点
          關鍵詞：而不是停止在最小化代价的点, 处停止
        - 摘要：事实上，在二次误差的简单线性模型和简单的梯度下降情况下，我们可；以展示提前终止相当于L 2 正则化。
          關鍵詞：我们可, 事实上, 正则化, 以展示提前终止相当于, 在二次误差的简单线性模型和简单的梯度下降情况下
        - 摘要：为了与经典L
          關鍵詞：为了与经典
        - 摘要：2  正则化比较，我们只考察唯一的参数是线性权重；的简单情形。我们在权重 w 的经验最佳值 w * 附近以二次近
          關鍵詞：的经验最佳值, 我们只考察唯一的参数是线性权重, 正则化比较, 附近以二次近, 的简单情形
        - 摘要：似建模代价函数J：
          關鍵詞：似建模代价函数
        - 摘要：其中 H 是J关于 w 在 w * 点的Hessian。鉴于假设 w * 是J( w )的最小点，；我们知道 H 为半正定。在局部泰勒级数逼近下，梯度由下式给出：
          關鍵詞：鉴于假设, 为半正定, 其中, 梯度由下式给出, 在局部泰勒级数逼近下
        - 摘要：接下来我们研究训练时参数向量的轨迹。为简化起见，我们将参数向量；初始化为原点 (3) ，也就是 w (0) ＝0。我们通过分析  上的梯度下降来研；究J上近似的梯度下降的效果：
          關鍵詞：我们将参数向量, 上的梯度下降来研, 也就是, 我们通过分析, 初始化为原点
        - 摘要：现在让我们在  H  特征向量的空间中改写表达式，利用  H  的特征分解：
          關鍵詞：的特征分解, 现在让我们在, 特征向量的空间中改写表达式, 利用
        - 摘要：，其中Λ  是对角矩阵，  Q  是特征向量的一组标准正交
          關鍵詞：是特征向量的一组标准正交, 是对角矩阵, 其中
        - 摘要：基。
          關鍵詞：
        - 摘要：假定  w  (0)  ＝0并且   选择得足够小以保证；参数更新后轨迹如下：
          關鍵詞：并且, 选择得足够小以保证, 假定, 参数更新后轨迹如下
        - 摘要：，经过τ次
          關鍵詞：经过
        - 摘要：现在，式（7.13）中
          關鍵詞：现在
        - 摘要：的表达式能被重写为
          關鍵詞：的表达式能被重写为
        - 摘要：比较式（7.40）和式（7.42），我们能够发现，如果超参数；满足
          關鍵詞：比较式, 满足, 和式, 我们能够发现, 如果超参数
        - 摘要：和τ
          關鍵詞：
        - 摘要：那么L  2  正则化和提前终止可以看作等价的（至少在目标函数的二次近；似下）。进一步取对数，使用log(1＋x)的级数展开，我们可以得出结；论：如果所有λ i 是小的（即
          關鍵詞：如果所有, 的级数展开, 进一步取对数, 至少在目标函数的二次近, 那么
        - 摘要：），那么
          關鍵詞：那么
        - 摘要：且
          關鍵詞：
        - 摘要：也就是说，在这些假设下，训练迭代次数τ起着与L  2  参数成反比的作；用，  的倒数与权重衰减系数的作用类似。
          關鍵詞：在这些假设下, 起着与, 参数成反比的作, 的倒数与权重衰减系数的作用类似, 训练迭代次数
        - 摘要：在大曲率（目标函数）方向上的参数值受正则化影响小于小曲率方向。；当然，在提前终止的情况下，这实际上意味着在大曲率方向的参数比较；小曲率方向的参数更早地学习到。
          關鍵詞：在提前终止的情况下, 在大曲率, 小曲率方向的参数更早地学习到, 这实际上意味着在大曲率方向的参数比较, 目标函数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；本节中的推导表明，长度为τ的轨迹结束于L  2  正则化目标的极小点。当；然，提前终止比简单的轨迹长度限制更丰富；取而代之，提前终止通常
          關鍵詞：长度为, 取而代之, 的轨迹结束于, 本节中的推导表明, 提前终止比简单的轨迹长度限制更丰富
    7.9：参数绑定和参数共享
        - 摘要：7.9.1　卷积神经网络
          關鍵詞：卷积神经网络
        - 摘要：目前为止，本章讨论对参数添加约束或惩罚时，一直是相对于固定的区；域或点。例如，L  2  正则化（或权重衰减）对参数偏离零的固定值进行；惩罚。然而，有时我们可能需要其他的方式来表达我们对模型参数适当
          關鍵詞：对参数偏离零的固定值进行, 正则化, 一直是相对于固定的区, 目前为止, 例如
        - 摘要：我们经常想要表达的一种常见依赖是某些参数应当彼此接近。考虑以下；情形：有两个模型执行相同的分类任务（具有相同类别），但输入分布；稍有不同。形式地，我们有参数为 w  (A)  的模型A和参数为 w  (B)  的模型
          關鍵詞：情形, 考虑以下, 我们有参数为, 形式地, 的模型
        - 摘要：我们可以想象，这些任务会足够相似（或许具有相似的输入和输出分
          關鍵詞：我们可以想象, 或许具有相似的输入和输出分, 这些任务会足够相似
        - 摘要：布），因此我们认为模型参数应彼此靠近：
          關鍵詞：因此我们认为模型参数应彼此靠近
        - 摘要：应该与
          關鍵詞：应该与
        - 摘要：接近。我们可以通过正则化利用此信息。具体来说，可以使
          關鍵詞：具体来说, 我们可以通过正则化利用此信息, 可以使, 接近
        - 摘要：用以下形式的参数范数惩罚：
          關鍵詞：用以下形式的参数范数惩罚
        - 摘要：这里我
          關鍵詞：这里我
        - 摘要：们使用L 2 惩罚，但也可以使用其他选择。
          關鍵詞：但也可以使用其他选择, 惩罚, 们使用
        - 摘要：这种方法由Lasserre et al. （2006）提出，正则化一个模型（监督模式下；训练的分类器）的参数，使其接近另一个无监督模式下训练的模型（捕；捉观察到的输入数据的分布）的参数。构造的这种架构使得分类模型中
          關鍵詞：监督模式下, 的参数, 这种方法由, 提出, 训练的分类器
        - 摘要：的许多参数能与无监督模型中对应的的参数匹配。
          關鍵詞：的许多参数能与无监督模型中对应的的参数匹配
        - 摘要：参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法；是使用约束：强迫某些参数相等。由于我们将各种模型或模型组件解释；为共享唯一的一组参数，这种正则化方法通常被称为参数共享
          關鍵詞：这种正则化方法通常被称为参数共享, 为共享唯一的一组参数, 由于我们将各种模型或模型组件解释, 而更流行的方法, 参数范数惩罚是正则化参数使其彼此接近的一种方式
        - 摘要：7.9.1　卷积神经网络
          關鍵詞：卷积神经网络
        - 摘要：目前为止，最流行和广泛使用的参数共享出现在应用于计算机视觉的卷；积神经网络 （CNN）中。
          關鍵詞：最流行和广泛使用的参数共享出现在应用于计算机视觉的卷, 积神经网络, 目前为止
        - 摘要：自然图像有许多统计属性是对转换不变的。例如，猫的照片即使向右边；移了一个像素，仍保持猫的照片。CNN通过在图像多个位置共享参数来；考虑这个特性。相同的特征（具有相同权重的隐藏单元）在输入的不同
          關鍵詞：在输入的不同, 自然图像有许多统计属性是对转换不变的, 相同的特征, 具有相同权重的隐藏单元, 例如
        - 摘要：参数共享显著降低了CNN模型的参数数量，并显著提高了网络的大小而；不需要相应地增加训练数据。它仍然是将领域知识有效地整合到网络架；构的最佳范例之一。
          關鍵詞：模型的参数数量, 它仍然是将领域知识有效地整合到网络架, 并显著提高了网络的大小而, 构的最佳范例之一, 不需要相应地增加训练数据
        - 摘要：我们将会在第9章中更详细地讨论卷积神经网络。
          關鍵詞：我们将会在第, 章中更详细地讨论卷积神经网络
        - 摘要：7.9.1　卷积神经网络
          關鍵詞：卷积神经网络
    7.10：稀疏表示
        - 摘要：前文所述的权重衰减直接惩罚模型参数。另一种策略是惩罚神经网络中；的激活单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂；惩罚。
          關鍵詞：前文所述的权重衰减直接惩罚模型参数, 稀疏化激活单元, 这种策略间接地对模型参数施加了复杂, 惩罚, 的激活单元
        - 摘要：我们已经讨论过（在第7.1.2节中）L  1  惩罚如何诱导稀疏的参数，即许；多参数为零（或接近于零）。另一方面，表示的稀疏描述了许多元素是；零（或接近零）的表示。我们可以线性回归的情况简单说明这种区别：
          關鍵詞：我们已经讨论过, 的表示, 惩罚如何诱导稀疏的参数, 节中, 另一方面
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；第一个表达式是参数稀疏的线性回归模型的例子。第二个表达式是数据；x 具有稀疏表示 h 的线性回归。也就是说， h 是 x 的一个函数，在某种
          關鍵詞：在某种, 的一个函数, 的线性回归, 第一个表达式是参数稀疏的线性回归模型的例子, 第二个表达式是数据
        - 摘要：表示的正则化可以使用参数正则化中同种类型的机制实现。
          關鍵詞：表示的正则化可以使用参数正则化中同种类型的机制实现
        - 摘要：表示的范数惩罚正则化是通过向损失函数J添加对表示的范数惩罚来实；现的。我们将这个惩罚记作Ω(  h  )。和以前一样，我们将正则化后的损；失函数记作  ：
          關鍵詞：和以前一样, 现的, 失函数记作, 我们将这个惩罚记作, 添加对表示的范数惩罚来实
        - 摘要：其中α∈[0,∞]权衡范数惩罚项的相对贡献，越大的α对应越多的正则化。
          關鍵詞：对应越多的正则化, 其中, 权衡范数惩罚项的相对贡献, 越大的
        - 摘要：正如对参数的L  1  惩罚诱导参数稀疏性，对表示元素的L  1  惩罚诱导稀疏；当然L  1  惩罚是使表示稀疏的方法之；的表示：Ω(h)＝
          關鍵詞：的表示, 惩罚诱导稀疏, 正如对参数的, 惩罚是使表示稀疏的方法之, 当然
        - 摘要：and
          關鍵詞：
        - 摘要：化几个样本平均激活的例子，即令
          關鍵詞：化几个样本平均激活的例子, 即令
        - 摘要：接近某些目标值
          關鍵詞：接近某些目标值
        - 摘要：（如每项都是.01的向量）。
          關鍵詞：的向量, 如每项都是
        - 摘要：还有一些其他方法通过激活值的硬性约束来获得表示稀疏。例如，正交；匹配追踪 （orthogonal matching pursuit）（Pati et al. ，1993）通过解决；以下约束优化问题将输入值 x 编码成表示 h
          關鍵詞：匹配追踪, 编码成表示, 通过解决, 例如, 还有一些其他方法通过激活值的硬性约束来获得表示稀疏
        - 摘要：是  h  中非零项的个数。当  W  被约束为正交时，我们可以高
          關鍵詞：中非零项的个数, 被约束为正交时, 我们可以高
        - 摘要：其中；效地解决这个问题。这种方法通常被称为OMP-k，通过k指定允许的非；零特征数量。Coates  and  Ng（2011）证明OMP-1可以成为深度架构中非
          關鍵詞：效地解决这个问题, 这种方法通常被称为, 其中, 指定允许的非, 证明
        - 摘要：含有隐藏单元的模型在本质上都能变得稀疏。在本书中，我们将看到在；各种情况下使用稀疏正则化的例子。
          關鍵詞：在本书中, 含有隐藏单元的模型在本质上都能变得稀疏, 我们将看到在, 各种情况下使用稀疏正则化的例子
    7.11：Bagging和其他集成方法
        - 摘要：Bagging  （bootstrap  aggregating）是通过结合几个模型降低泛化误差的；技术（Breiman，1994）。主要想法是分别训练几个不同的模型，然后；让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例
          關鍵詞：主要想法是分别训练几个不同的模型, 技术, 是通过结合几个模型降低泛化误差的, 这是机器学习中常规策略的一个例, 然后
        - 摘要：模型平均 （model averaging）奏效的原因是不同的模型通常不会在测试；集上产生完全相同的误差。
          關鍵詞：奏效的原因是不同的模型通常不会在测试, 集上产生完全相同的误差, 模型平均
        - 摘要：假设我们有k个回归模型。假设每个模型在每个例子上的误差是   ，；的多；这个误差服从零均值方差为
          關鍵詞：的多, 假设每个模型在每个例子上的误差是, 这个误差服从零均值方差为, 个回归模型, 假设我们有
        - 摘要：且协方差为
          關鍵詞：且协方差为
        - 摘要：维正态分布。通过所有集成模型的平均预测所得误差是
          關鍵詞：通过所有集成模型的平均预测所得误差是, 维正态分布
        - 摘要：。集成预测器平方误差的期望是
          關鍵詞：集成预测器平方误差的期望是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在误差完全相关即c＝ν的情况下，均方误差减少到ν，所以模型平均没；有任何帮助。在错误完全不相关即c＝0的情况下，该集成平方误差的期
          關鍵詞：均方误差减少到, 在误差完全相关即, 所以模型平均没, 在错误完全不相关即, 的情况下
        - 摘要：望仅为
          關鍵詞：望仅为
        - 摘要：。这意味着集成平方误差的期望会随着集成规模增大而线
          關鍵詞：这意味着集成平方误差的期望会随着集成规模增大而线
        - 摘要：性减小。换言之，平均上，集成至少与它的任何成员表现得一样好，并；且如果成员的误差是独立的，集成将显著地比其成员表现得更好。
          關鍵詞：换言之, 且如果成员的误差是独立的, 平均上, 集成至少与它的任何成员表现得一样好, 集成将显著地比其成员表现得更好
        - 摘要：不同的集成方法以不同的方式构建集成模型。例如，集成的每个成员可；以使用不同的算法和目标函数训练成完全不同的模型。Bagging是一种；允许重复多次使用同一种模型、训练算法和目标函数的方法。
          關鍵詞：以使用不同的算法和目标函数训练成完全不同的模型, 是一种, 例如, 允许重复多次使用同一种模型, 集成的每个成员可
        - 摘要：具体来说，Bagging涉及构造k个不同的数据集。每个数据集从原始数据；集中重复采样构成，和原始数据集具有相同数量的样例。这意味着，每；个数据集以高概率缺少一些来自原始数据集的例子，还包含若干重复的
          關鍵詞：还包含若干重复的, 个不同的数据集, 个数据集以高概率缺少一些来自原始数据集的例子, 和原始数据集具有相同数量的样例, 具体来说
        - 摘要：图7.5　描述Bagging如何工作的草图。假设我们在上述数据集（包含一个8、一个6和一个9）上；训练数字8的检测器，假设我们制作了两个不同的重采样数据集，Bagging训练程序通过有放回；采样构建这些数据集。第一个数据集忽略9并重复8。在这个数据集上，检测器得知数字顶部有
          關鍵詞：并重复, 如何工作的草图, 在这个数据集上, 检测器得知数字顶部有, 假设我们在上述数据集
        - 摘要：一个环就对应于一个8。第二个数据集中，我们忽略6并重复9。在这种情况下，检测器得知数字；底部有一个环就对应于一个8。这些单独的分类规则中的每一个都是不可靠的，但如果我们平均；它们的输出，就能得到鲁棒的检测器，只有当8的两个环都存在时才能实现最大置信度
          關鍵詞：并重复, 的两个环都存在时才能实现最大置信度, 只有当, 这些单独的分类规则中的每一个都是不可靠的, 检测器得知数字
        - 摘要：神经网络能找到足够多的不同的解，意味着它们可以从模型平均中受益；（即使所有模型都在同一数据集上训练）。神经网络中随机初始化的差；异、小批量的随机选择、超参数的差异或不同输出的非确定性实现往往
          關鍵詞：意味着它们可以从模型平均中受益, 小批量的随机选择, 神经网络中随机初始化的差, 超参数的差异或不同输出的非确定性实现往往, 即使所有模型都在同一数据集上训练
        - 摘要：模型平均是一个减少泛化误差的非常强大可靠的方法。在作为科学论文；算法的基准时，它通常是不鼓励使用的，因为任何机器学习算法都可以；从模型平均中大幅获益（以增加计算和存储为代价）。
          關鍵詞：它通常是不鼓励使用的, 模型平均是一个减少泛化误差的非常强大可靠的方法, 以增加计算和存储为代价, 在作为科学论文, 算法的基准时
        - 摘要：机器学习比赛中的取胜算法通常是使用超过几十种模型平均的方法。最；近一个突出的例子是Netflix Grand Prize（Koren，2009）。
          關鍵詞：机器学习比赛中的取胜算法通常是使用超过几十种模型平均的方法, 近一个突出的例子是
        - 摘要：不是所有构建集成的技术都是为了让集成模型比单一模型更加正则化。；例如，一种被称为Boosting 的技术（Freund and Schapire，1996b，a）构；建比单个模型容量更高的集成模型。通过向集成逐步添加神经网络，
          關鍵詞：建比单个模型容量更高的集成模型, 例如, 的技术, 不是所有构建集成的技术都是为了让集成模型比单一模型更加正则化, 一种被称为
        - 摘要：and  Bengio，
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；7.12　Dropout
          關鍵詞：
        - 摘要：Dropout  （Srivastava  et  al.  ，2014）提供了正则化一大类模型的方法，；计算方便但功能强大。在第一种近似下，Dropout可以被认为是集成大；量深层神经网络的实用Bagging方法。Bagging涉及训练多个模型，并在
          關鍵詞：在第一种近似下, 计算方便但功能强大, 并在, 可以被认为是集成大, 提供了正则化一大类模型的方法
        - 摘要：具体而言，Dropout训练的集成包括所有从基础网络除去非输出单元后；形成的子网络，如图7.6所示。最先进的神经网络基于一系列仿射变换；和非线性变换，我们只需将一些单元的输出乘零就能有效地删除一个单
          關鍵詞：具体而言, 和非线性变换, 训练的集成包括所有从基础网络除去非输出单元后, 我们只需将一些单元的输出乘零就能有效地删除一个单, 形成的子网络
        - 摘要：图7.6　Dropout训练由所有子网络组成的集成，其中子网络通过从基本网络中删除非输出单元构；建。我们从具有两个可见单元和两个隐藏单元的基本网络开始。这4个单元有16个可能的子集。；右图展示了从原始网络中丢弃不同的单元子集而形成的所有16个子网络。在这个小例子中，所
          關鍵詞：训练由所有子网络组成的集成, 个可能的子集, 其中子网络通过从基本网络中删除非输出单元构, 在这个小例子中, 我们从具有两个可见单元和两个隐藏单元的基本网络开始
        - 摘要：回想一下Bagging学习，我们定义k个不同的模型，从训练集有放回采样；构造k个不同的数据集，然后在训练集i上训练模型i。Dropout的目标是；在指数级数量的神经网络上近似这个过程。具体来说，在训练中使用
          關鍵詞：从训练集有放回采样, 个不同的数据集, 构造, 在指数级数量的神经网络上近似这个过程, 个不同的模型
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图7.7　在使用Dropout的前馈网络中前向传播的示例。（顶部）在此示例中，我们使用具有两个；输入单元，具有两个隐藏单元的隐藏层以及一个输出单元的前馈网络。（底部）为了执行具有
          關鍵詞：我们使用具有两个, 的前馈网络中前向传播的示例, 底部, 顶部, 输入单元
        - 摘要：是由；更正式地说，假设一个掩码向量  μ  指定被包括的单元，；参数  θ  和掩码  μ  定义的模型代价。那么Dropout训练的目标是最小化
          關鍵詞：是由, 参数, 定义的模型代价, 更正式地说, 假设一个掩码向量
        - 摘要：。这个期望包含多达指数级的项，但我们可以通过抽样  μ
          關鍵詞：但我们可以通过抽样, 这个期望包含多达指数级的项
        - 摘要：获得梯度的无偏估计。
          關鍵詞：获得梯度的无偏估计
        - 摘要：Dropout训练与Bagging训练不太一样。在Bagging的情况下，所有模型都；是独立的。在Dropout的情况下，所有模型共享参数，其中每个模型继；承父神经网络参数的不同子集。参数共享使得在有限可用的内存下表示
          關鍵詞：训练不太一样, 承父神经网络参数的不同子集, 参数共享使得在有限可用的内存下表示, 其中每个模型继, 训练与
        - 摘要：Bagging集成必须根据所有成员的累积投票做一个预测。在这种背景；下，我们将这个过程称为推断  （inference）。目前为止，我们在介绍；Bagging和Dropout时没有要求模型具有明确的概率。现在，我们假定该
          關鍵詞：我们将这个过程称为推断, 目前为止, 我们在介绍, 集成必须根据所有成员的累积投票做一个预测, 我们假定该
        - 摘要：在Dropout的情况下，通过掩码
          關鍵詞：的情况下, 通过掩码
        - 摘要：μ
          關鍵詞：
        - 摘要：定义每个子模型的概率分布
          關鍵詞：定义每个子模型的概率分布
        - 摘要：。所有掩码的算术平均值由下式给出：
          關鍵詞：所有掩码的算术平均值由下式给出
        - 摘要：其中p( μ )是训练时采样 μ 的概率分布。
          關鍵詞：是训练时采样, 的概率分布, 其中
        - 摘要：因为这个求和包含多达指数级的项，除非该模型的结构允许某种形式的；简化，否则是不可能计算的。目前为止，无法得知深度神经网络是否允；许某种可行的简化。相反，我们可以通过采样近似推断，即平均许多掩
          關鍵詞：无法得知深度神经网络是否允, 目前为止, 简化, 我们可以通过采样近似推断, 除非该模型的结构允许某种形式的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；然而，一个更好的方法能不错地近似整个集成的预测，且只需一个前向；传播的代价。要做到这一点，我们改用集成成员预测分布的几何平均而
          關鍵詞：一个更好的方法能不错地近似整个集成的预测, 传播的代价, 然而, 要做到这一点, 我们改用集成成员预测分布的几何平均而
        - 摘要：多个概率分布的几何平均不能保证是一个概率分布。为了保证结果是一；个概率分布，我们要求没有子模型给某一事件分配概率0，并重新标准；化所得分布。通过几何平均直接定义的非标准化概率分布由下式给出：
          關鍵詞：并重新标准, 我们要求没有子模型给某一事件分配概率, 个概率分布, 通过几何平均直接定义的非标准化概率分布由下式给出, 化所得分布
        - 摘要：其中d是可被丢弃的单元数。这里为简化介绍，我们使用均匀分布的  μ；，但非均匀分布也是可以的。为了作出预测，我们必须重新标准化集；成：
          關鍵詞：我们必须重新标准化集, 其中, 我们使用均匀分布的, 但非均匀分布也是可以的, 这里为简化介绍
        - 摘要：涉及Dropout的一个重要观点（Hinton et al. ，2012c）是，我们可以通过；来近似p  ensemble  ：该模型具有所有单元，但我们；评估模型中
          關鍵詞：涉及, 但我们, 我们可以通过, 该模型具有所有单元, 的一个重要观点
        - 摘要：因为我们通常使用  的包含概率，权重比例规则一般相当于在训练结束
          關鍵詞：因为我们通常使用, 权重比例规则一般相当于在训练结束, 的包含概率
        - 摘要：后将权重除2，然后像平常一样使用模型。实现相同结果的另一种方法；是在训练期间将单元的状态乘2。无论哪种方式，我们的目标是确保在；测试时一个单元的期望总输入与在训练时该单元的期望总输入是大致相
          關鍵詞：实现相同结果的另一种方法, 测试时一个单元的期望总输入与在训练时该单元的期望总输入是大致相, 然后像平常一样使用模型, 我们的目标是确保在, 后将权重除
        - 摘要：对许多不具有非线性隐藏单元的模型族而言，权重比例推断规则是精确；的。举个简单的例子，考虑softmax函数回归分类，其中由向量v  表示n；个输入变量：
          關鍵詞：表示, 个输入变量, 对许多不具有非线性隐藏单元的模型族而言, 函数回归分类, 权重比例推断规则是精确
        - 摘要：我们可以根据二值向量 d 逐元素的乘法将一类子模型进行索引：
          關鍵詞：逐元素的乘法将一类子模型进行索引, 我们可以根据二值向量
        - 摘要：集成预测器被定义为重新标准化所有集成成员预测的几何平均：
          關鍵詞：集成预测器被定义为重新标准化所有集成成员预测的几何平均
        - 摘要：其中
          關鍵詞：其中
        - 摘要：为了证明权重比例推断规则是精确的，我们简化
          關鍵詞：我们简化, 为了证明权重比例推断规则是精确的
        - 摘要：：
          關鍵詞：
        - 摘要：由于  将被标准化，我们可以放心地忽略那些相对y不变的乘法：
          關鍵詞：不变的乘法, 我们可以放心地忽略那些相对, 将被标准化, 由于
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；将其代入式（7.58），我们得到了一个权重为
          關鍵詞：我们得到了一个权重为, 将其代入式
        - 摘要：的softmax函数
          關鍵詞：函数
        - 摘要：分类器。
          關鍵詞：分类器
        - 摘要：权重比例推断规则在其他设定下也是精确的，包括条件正态输出的回归；网络以及那些隐藏层不包含非线性的深度网络。然而，权重比例推断规；则对具有非线性的深度模型仅仅是一个近似。虽然这个近似尚未有理论
          關鍵詞：虽然这个近似尚未有理论, 权重比例推断规则在其他设定下也是精确的, 然而, 网络以及那些隐藏层不包含非线性的深度网络, 权重比例推断规
        - 摘要：Srivastava et al. （2014）显示，Dropout比其他标准的计算开销小的正则；化方法（如权重衰减、过滤器范数约束和稀疏激活的正则化）更有效。；Dropout也可以与其他形式的正则化合并，得到进一步的提升。
          關鍵詞：化方法, 显示, 比其他标准的计算开销小的正则, 更有效, 过滤器范数约束和稀疏激活的正则化
        - 摘要：计算方便是Dropout的一个优点。训练过程中使用Dropout产生n个随机二；的计算复杂度。根据；进制数与状态相乘，每个样本每次更新只需
          關鍵詞：进制数与状态相乘, 根据, 训练过程中使用, 产生, 计算方便是
        - 摘要：Dropout的另一个显著优点是不怎么限制适用的模型或训练过程。几乎；在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很；好。包括前馈神经网络、概率模型，如受限玻尔兹曼机（Srivastava
          關鍵詞：几乎, 包括前馈神经网络, 如受限玻尔兹曼机, 的另一个显著优点是不怎么限制适用的模型或训练过程, 在所有使用分布式表示且可以用随机梯度下降训练的模型上都表现很
        - 摘要：虽然Dropout在特定模型上每一步的代价是微不足道的，但在一个完整；的系统上使用Dropout的代价可能非常显著。因为Dropout是一个正则化；技术，它减少了模型的有效容量。为了抵消这种影响，我们必须增大模
          關鍵詞：的系统上使用, 因为, 它减少了模型的有效容量, 为了抵消这种影响, 虽然
        - 摘要：常大的数据集，正则化带来的泛化误差减少得很小。在这些情况下，使；用Dropout和更大模型的计算代价可能超过正则化带来的好处。
          關鍵詞：在这些情况下, 正则化带来的泛化误差减少得很小, 和更大模型的计算代价可能超过正则化带来的好处, 常大的数据集
        - 摘要：只有极少的训练样本可用时，Dropout不会很有效。在只有不到5000的；样本的Alternative Splicing数据集上（Xiong et al. ，2011），贝叶斯神经；网络（Neal，1996）比Dropout表现得更好（Srivastava et al. ，2014）。
          關鍵詞：网络, 贝叶斯神经, 表现得更好, 样本的, 只有极少的训练样本可用时
        - 摘要：Wager  et  al.  （2013）表明，当Dropout作用于线性回归时，相当于每个；输入特征具有不同权重衰减系数的L  2  权重衰减。每个特征的权重衰减；系数的大小是由其方差来确定的。其他线性模型也有类似的结果。而对
          關鍵詞：输入特征具有不同权重衰减系数的, 权重衰减, 而对, 相当于每个, 系数的大小是由其方差来确定的
        - 摘要：使用Dropout训练时的随机性不是这个方法成功的必要条件。它仅仅是；近似所有子模型总和的一个方法。Wang  and  Manning（2013）导出了近；似这种边缘分布的解析解。他们的近似被称为快速Dropout
          關鍵詞：近似所有子模型总和的一个方法, 它仅仅是, 训练时的随机性不是这个方法成功的必要条件, 似这种边缘分布的解析解, 使用
        - 摘要：随机性对实现Dropout的正则化效果不是必要的，同时也不是充分的。；为了证明这一点，Warde-Farley  et  al.  （2014）使用一种被称为Dropout；Boosting  的方法设计了一个对照实验，具有与传统Dropout方法完全相
          關鍵詞：方法完全相, 的正则化效果不是必要的, 的方法设计了一个对照实验, 具有与传统, 使用一种被称为
        - 摘要：Dropout启发其他以随机方法训练指数量级的共享权重的集成。；DropConnect是Dropout的一个特殊情况，其中一个标量权重和单个隐藏；单元状态之间的每个乘积被认为是可以丢弃的一个单元（Wan  et  al.  ，
          關鍵詞：启发其他以随机方法训练指数量级的共享权重的集成, 单元状态之间的每个乘积被认为是可以丢弃的一个单元, 的一个特殊情况, 其中一个标量权重和单个隐藏
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；2013）。随机池化是构造卷积神经网络集成的一种随机化池化的形式；（见第9.3节），其中每个卷积网络参与每个特征图的不同空间位置。
          關鍵詞：其中每个卷积网络参与每个特征图的不同空间位置, 随机池化是构造卷积神经网络集成的一种随机化池化的形式, 见第
        - 摘要：一个关于Dropout的重要见解是，通过随机行为训练网络并平均多个随；机决定进行预测，实现了一种参数共享的Bagging形式。早些时候，我；们将Dropout描述为通过包括或排除单元形成模型集成的Bagging。然
          關鍵詞：一个关于, 形式, 的重要见解是, 机决定进行预测, 实现了一种参数共享的
        - 摘要：推断规则。
          關鍵詞：推断规则
        - 摘要：目前为止，我们将Dropout介绍为一种纯粹高效近似Bagging的方法。然；而，还有比这更进一步的Dropout观点。Dropout不仅仅是训练一个；Bagging的集成模型，而且是共享隐藏单元的集成模型。这意味着无论
          關鍵詞：这意味着无论, 我们将, 的集成模型, 目前为止, 观点
        - 摘要：Dropout强大的大部分原因来自施加到隐藏单元的掩码噪声，了解这一；事实是重要的。这可以看作对输入内容的信息高度智能化、自适应破坏；的一种形式，而不是对输入原始值的破坏。例如，如果模型学得通过鼻
          關鍵詞：自适应破坏, 的一种形式, 如果模型学得通过鼻, 了解这一, 这可以看作对输入内容的信息高度智能化
        - 摘要：型必须学习另一种h i ，要么是鼻子存在的冗余编码，要么是像嘴这样的；脸部的另一特征。传统的噪声注入技术，在输入端加非结构化的噪声不；能够随机地从脸部图像中抹去关于鼻子的信息，除非噪声的幅度大到几
          關鍵詞：传统的噪声注入技术, 型必须学习另一种, 要么是像嘴这样的, 在输入端加非结构化的噪声不, 能够随机地从脸部图像中抹去关于鼻子的信息
        - 摘要：Dropout的另一个重要方面是噪声是乘性的。如果是固定规模的加性噪；声，那么加了噪声   的整流线性隐藏单元可以简单地学会使h  i  变得很；大（使增加的噪声   变得不显著）。乘性噪声不允许这样病态地解决
          關鍵詞：的另一个重要方面是噪声是乘性的, 使增加的噪声, 变得不显著, 如果是固定规模的加性噪, 的整流线性隐藏单元可以简单地学会使
        - 摘要：另一种深度学习算法——批标准化，在训练时向隐藏单元引入加性和乘；性噪声重新参数化模型。批标准化的主要目的是改善优化，但噪声具有；正则化的效果，有时没必要再使用Dropout。批标准化将会在第8.7.1节
          關鍵詞：但噪声具有, 性噪声重新参数化模型, 在训练时向隐藏单元引入加性和乘, 批标准化将会在第, 正则化的效果
    7.13：对抗训练
        - 摘要：在许多情况下，神经网络在独立同分布的测试集上进行评估已经达到了；人类表现。因此，我们自然要怀疑这些模型在这些任务上是否获得了真；正的人类层次的理解。为了探索网络对底层任务的理解层次，我们可以
          關鍵詞：我们可以, 人类表现, 在许多情况下, 因此, 神经网络在独立同分布的测试集上进行评估已经达到了
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图7.8　在ImageNet上应用GoogLeNet（Szegedy et al. ，2014a）的对抗样本生成的演示。通过添；加一个不可察觉的小向量（其中元素等于代价函数相对于输入的梯度元素的符号），我们可以
          關鍵詞：上应用, 我们可以, 通过添, 的对抗样本生成的演示, 加一个不可察觉的小向量
        - 摘要：对抗样本在很多领域有很多影响，例如计算机安全，这超出了本章的范；围。然而，它们在正则化的背景下很有意思，因为我们可以通过对抗训；练 （adversarial  training）减少原有独立同分布的测试集的错误率——在
          關鍵詞：它们在正则化的背景下很有意思, 例如计算机安全, 然而, 减少原有独立同分布的测试集的错误率, 因为我们可以通过对抗训
        - 摘要：Goodfellow et  al.  （2014b）表明，这些对抗样本的主要原因之一是过度；线性。神经网络主要是基于线性块构建的。因此在一些实验中，它们实；现的整体函数被证明是高度线性的。这些线性函数很容易优化。不幸的
          關鍵詞：神经网络主要是基于线性块构建的, 因此在一些实验中, 这些线性函数很容易优化, 它们实, 不幸的
        - 摘要：对抗训练有助于体现积极正则化与大型函数族结合的力量。纯粹的线性；模型，如逻辑回归，由于它们被限制为线性而无法抵抗对抗样本。神经；网络能够将函数从接近线性转化为局部近似恒定，从而可以灵活地捕获
          關鍵詞：纯粹的线性, 神经, 如逻辑回归, 模型, 对抗训练有助于体现积极正则化与大型函数族结合的力量
        - 摘要：对抗样本也提供了一种实现半监督学习的方法。在与数据集中的标签不；相关联的点 x 处，模型本身为其分配一些标签  。模型的标记  未必是；真正的标签，但如果模型是高品质的，那么   提供正确标签的可能性很
          關鍵詞：未必是, 但如果模型是高品质的, 模型的标记, 对抗样本也提供了一种实现半监督学习的方法, 那么
    7.14：切面距离、正切传播和流形正切分
        - 摘要：如第5.11.3节所述，许多机器学习通过假设数据位于低维流形附近来克；服维数灾难。
          關鍵詞：服维数灾难, 如第, 节所述, 许多机器学习通过假设数据位于低维流形附近来克
        - 摘要：distance）算法；一个利用流形假设的早期尝试是切面距离  （tangent；（Simard  et  al.  ，1993，1998）。它是一种非参数的最近邻算法，其中
          關鍵詞：它是一种非参数的最近邻算法, 一个利用流形假设的早期尝试是切面距离, 算法, 其中
        - 摘要：受相关启发，正切传播  （tangent  prop）算法（Simard  et  al.  ，1992）；（见图7.9）训练带有额外惩罚的神经网络分类器，使神经网络的每个；输出f( x )对已知的变化因素是局部不变的。这些变化因素对应于沿着的
          關鍵詞：这些变化因素对应于沿着的, 算法, 受相关启发, 对已知的变化因素是局部不变的, 见图
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；与已知流形的切向  ν  (i)  正交，或者等价地通过正则化惩罚Ω
          關鍵詞：正交, 或者等价地通过正则化惩罚, 与已知流形的切向
        - 摘要：使f在 x 的 ν (i) 方向的导数较小：
          關鍵詞：方向的导数较小
        - 摘要：这个正则化项当然可以通过适当的超参数缩放，并且对于大多数神经网；络，我们需要对许多输出求和（此处为了描述简单，f(；)为唯一输
          關鍵詞：我们需要对许多输出求和, 并且对于大多数神经网, 此处为了描述简单, 这个正则化项当然可以通过适当的超参数缩放, 为唯一输
        - 摘要：x
          關鍵詞：
        - 摘要：图7.9　正切传播算法（Simard et al. ，1992）和流形正切分类器主要思想的示意图（Rifai et al.；，2011c），它们都正则化分类器的输出函数f( x )。每条曲线表示不同类别的流形，这里表示嵌；入二维空间中的一维流形。在一条曲线上，我们选择单个点并绘制一个与类别流形（平行并接
          關鍵詞：和流形正切分类器主要思想的示意图, 正切传播算法, 每条曲线表示不同类别的流形, 在一条曲线上, 我们选择单个点并绘制一个与类别流形
        - 摘要：正切传播与数据集增强密切相关。在这两种情况下，该算法的用户通过
          關鍵詞：正切传播与数据集增强密切相关, 该算法的用户通过, 在这两种情况下
        - 摘要：指定一组应当不会改变网络输出的转换，将其先验知识编码至算法中。；不同的是在数据集增强的情况下，网络显式地训练正确分类这些施加大；量变换后产生的不同输入。正切传播不需要显式访问一个新的输入点。
          關鍵詞：网络显式地训练正确分类这些施加大, 不同的是在数据集增强的情况下, 将其先验知识编码至算法中, 量变换后产生的不同输入, 正切传播不需要显式访问一个新的输入点
        - 摘要：正切传播也和双反向传播（Drucker  and  LeCun，1992）以及对抗训练；（Szegedy et al. ，2014b；Goodfellow et al. ，2014b）有关联。双反向传；播正则化使Jacobian矩阵偏小，而对抗训练找到原输入附近的点，训练
          關鍵詞：双反向传, 训练, 有关联, 以及对抗训练, 而对抗训练找到原输入附近的点
        - 摘要：流形正切分类器（Rifai et al. ，2011d）无须知道切线向量的先验。我们；将在第14章看到，自编码器可以估算流形的切向量。流形正切分类器使；用这种技术来避免用户指定切向量。如图14.10所示，这些估计的切向
          關鍵詞：所示, 流形正切分类器使, 流形正切分类器, 我们, 章看到
        - 摘要：在本章中，我们已经描述了大多数用于正则化神经网络的通用策略。正；则化是机器学习的中心主题，因此我们将不时在其余各章中重新回顾。；机器学习的另一个中心主题是优化，我们将在下一章描述。
          關鍵詞：机器学习的另一个中心主题是优化, 则化是机器学习的中心主题, 我们将在下一章描述, 因此我们将不时在其余各章中重新回顾, 我们已经描述了大多数用于正则化神经网络的通用策略
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；(1)    更一般地，我们可以将参数正则化为接近空间中的任意特定点，令人惊讶的是这样也仍有；正则化效果，但是特定点越接近真实值结果越好。当我们不知道正确的值应该是正还是负时，
          關鍵詞：更一般地, 正则化效果, 令人惊讶的是这样也仍有, 当我们不知道正确的值应该是正还是负时, 但是特定点越接近真实值结果越好
        - 摘要：(2)  如同L 2 正则化，我们能将参数正则化到其他非零值 w (o) 。在这种情况下，L 1 正则化将会；引入不同的项
          關鍵詞：引入不同的项, 正则化将会, 正则化, 如同, 我们能将参数正则化到其他非零值
        - 摘要：(3)    对于神经网络，我们需要打破隐藏单元间的对称平衡，因此不能将所有参数都初始化为  0；（如第6.2节所讨论的）。然而，对于其他任何初始值 w (0) 该论证都成立。
          關鍵詞：然而, 因此不能将所有参数都初始化为, 如第, 该论证都成立, 对于其他任何初始值
第8章：深度模型中的优化
    7.14：切面距离、正切传播和流形正切分
        - 摘要：深度学习算法在许多情况下都涉及优化。例如，模型中的进行推断（如；PCA）涉及求解优化问题。我们经常使用解析优化去证明或设计算法。；在深度学习涉及的诸多优化问题中，最难的是神经网络训练。甚至是用
          關鍵詞：我们经常使用解析优化去证明或设计算法, 最难的是神经网络训练, 涉及求解优化问题, 例如, 在深度学习涉及的诸多优化问题中
        - 摘要：如果你不熟悉基于梯度优化的基本原则，我们建议回顾第4章。该章简；要概述了一般的数值优化。
          關鍵詞：如果你不熟悉基于梯度优化的基本原则, 该章简, 要概述了一般的数值优化, 我们建议回顾第
        - 摘要：本章主要关注这一类特定的优化问题：寻找神经网络上的一组参数 θ ，；它能显著地降低代价函数J（  θ  ），该代价函数通常包括整个训练集上；的性能评估和额外的正则化项。
          關鍵詞：它能显著地降低代价函数, 本章主要关注这一类特定的优化问题, 该代价函数通常包括整个训练集上, 寻找神经网络上的一组参数, 的性能评估和额外的正则化项
        - 摘要：首先，我们会介绍在机器学习任务中作为训练算法使用的优化与纯优化；有哪些不同。其次，我们会介绍导致神经网络优化困难的几个具体挑；战。再次，我们会介绍几个实用算法，包括优化算法本身和初始化参数
          關鍵詞：其次, 我们会介绍在机器学习任务中作为训练算法使用的优化与纯优化, 我们会介绍导致神经网络优化困难的几个具体挑, 再次, 有哪些不同
    8.1：学习和纯优化有什么不同
        - 摘要：8.1.1　经验风险最小化
          關鍵詞：经验风险最小化
        - 摘要：8.1.2　代理损失函数和提前终止
          關鍵詞：代理损失函数和提前终止
        - 摘要：8.1.3　批量算法和小批量算法
          關鍵詞：批量算法和小批量算法
        - 摘要：用于深度模型训练的优化算法与传统的优化算法在几个方面有所不同。；机器学习通常是间接作用的。在大多数机器学习问题中，我们关注某些；性能度量P，其定义于测试集上并且可能是不可解的。因此，我们只是
          關鍵詞：机器学习通常是间接作用的, 在大多数机器学习问题中, 性能度量, 其定义于测试集上并且可能是不可解的, 因此
        - 摘要：通常，代价函数可写为训练集上的平均，如
          關鍵詞：通常, 代价函数可写为训练集上的平均
        - 摘要：其中L是每个样本的损失函数，
          關鍵詞：是每个样本的损失函数, 其中
        - 摘要：是输入  x  时所预测的输出，；是经验分布。监督学习中，y是目标输出。在本章中，我们会介；绍不带正则化的无监督学习，L的变量是
          關鍵詞：的变量是, 我们会介, 监督学习中, 时所预测的输出, 绍不带正则化的无监督学习
        - 摘要：式（8.1）定义了训练集上的目标函数。通常，我们更希望最小化取自；数据生成分布p data 的期望，而不仅仅是有限训练集上的对应目标函数：
          關鍵詞：定义了训练集上的目标函数, 通常, 我们更希望最小化取自, 数据生成分布, 而不仅仅是有限训练集上的对应目标函数
        - 摘要：8.1.1　经验风险最小化
          關鍵詞：经验风险最小化
        - 摘要：机器学习算法的目标是降低式（8.2）所示的期望泛化误差。这个数据；量被称为风险  （risk）。在这里，我们强调该期望取自真实的潜在分布；p data 。如果我们知道了真实分布p  data ( x ,y)，那么最小化风险变成了一
          關鍵詞：在这里, 那么最小化风险变成了一, 这个数据, 机器学习算法的目标是降低式, 所示的期望泛化误差
        - 摘要：将机器学习问题转化回一个优化问题的最简单方法是最小化训练集上的；替代真实分布p(  x；期望损失。这意味着用训练集上的经验分布
          關鍵詞：期望损失, 替代真实分布, 这意味着用训练集上的经验分布, 将机器学习问题转化回一个优化问题的最简单方法是最小化训练集上的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；其中m表示训练样本的数目。
          關鍵詞：表示训练样本的数目, 其中
        - 摘要：基于最小化这种平均训练误差的训练过程被称为经验风险最小化；（empirical risk minimization）在这种情况下，机器学习仍然和传统的直；接优化很相似。我们并不直接最优化风险，而是最优化经验风险，希望
          關鍵詞：机器学习仍然和传统的直, 而是最优化经验风险, 在这种情况下, 接优化很相似, 基于最小化这种平均训练误差的训练过程被称为经验风险最小化
        - 摘要：然而，经验风险最小化很容易导致过拟合。高容量的模型会简单地记住；训练集。在很多情况下，经验风险最小化并非真的可行。最有效的现代；优化算法是基于梯度下降的，但是很多有用的损失函数，如0-1损失，
          關鍵詞：训练集, 经验风险最小化很容易导致过拟合, 最有效的现代, 然而, 经验风险最小化并非真的可行
        - 摘要：8.1.2　代理损失函数和提前终止
          關鍵詞：代理损失函数和提前终止
        - 摘要：有时，我们真正关心的损失函数（比如分类误差）并不能被高效地优；化。例如，即使对于线性分类器而言，精确地最小化0-1损失通常是不；可解的（复杂度是输入维数的指数级别）（Marcotte
          關鍵詞：比如分类误差, 损失通常是不, 有时, 例如, 我们真正关心的损失函数
        - 摘要：and
          關鍵詞：
        - 摘要：在某些情况下，代理损失函数比原函数学到的更多。例如，使用对数似；然替代函数时，在训练集上的0-1损失达到0之后，测试集上的0-1损失还；能持续下降很长一段时间。这是因为即使0-1损失期望是零时，我们还
          關鍵詞：使用对数似, 之后, 我们还, 然替代函数时, 在训练集上的
        - 摘要：一般的优化和我们用于训练算法的优化有一个重要不同：训练算法通常；不会停止在局部极小点。反之，机器学习通常优化代理损失函数，但是
          關鍵詞：训练算法通常, 机器学习通常优化代理损失函数, 但是, 不会停止在局部极小点, 一般的优化和我们用于训练算法的优化有一个重要不同
        - 摘要：在基于提前终止（第7.8节）的收敛条件满足时停止。通常，提前终止；使用真实潜在损失函数，如验证集上的0-1损失，并设计为在过拟合发；生之前终止。与纯优化不同的是，提前终止时代理损失函数仍然有较大
          關鍵詞：生之前终止, 通常, 提前终止时代理损失函数仍然有较大, 在基于提前终止, 的收敛条件满足时停止
        - 摘要：8.1.3　批量算法和小批量算法
          關鍵詞：批量算法和小批量算法
        - 摘要：机器学习算法和一般优化算法不同的一点是，机器学习算法的目标函数；通常可以分解为训练样本上的求和。机器学习中的优化算法在计算参数；的每一次更新时通常仅使用整个代价函数中一部分项来估计代价函数的
          關鍵詞：通常可以分解为训练样本上的求和, 机器学习中的优化算法在计算参数, 机器学习算法和一般优化算法不同的一点是, 机器学习算法的目标函数, 的每一次更新时通常仅使用整个代价函数中一部分项来估计代价函数的
        - 摘要：例如，最大似然估计问题可以在对数空间中分解成各个样本的总和：
          關鍵詞：例如, 最大似然估计问题可以在对数空间中分解成各个样本的总和
        - 摘要：最大化这个总和等价于最大化训练集在经验分布上的期望：
          關鍵詞：最大化这个总和等价于最大化训练集在经验分布上的期望
        - 摘要：优化算法用到的目标函数J中的大多数属性也是训练集上的期望。例；如，最常用的属性是梯度：
          關鍵詞：最常用的属性是梯度, 中的大多数属性也是训练集上的期望, 优化算法用到的目标函数
        - 摘要：准确计算这个期望的计算代价非常大，因为我们需要在整个数据集上的；每个样本上评估模型。在实践中，我们可以从数据集中随机采样少量的；样本，然后计算这些样本上的平均值。
          關鍵詞：我们可以从数据集中随机采样少量的, 样本, 准确计算这个期望的计算代价非常大, 在实践中, 然后计算这些样本上的平均值
        - 摘要：，其中σ是；回想一下，n个样本均值的标准差（式（5.46））是；样本值真实的标准差。分母  表明使用更多样本来估计梯度的方法的
          關鍵詞：分母, 表明使用更多样本来估计梯度的方法的, 其中, 样本值真实的标准差, 个样本均值的标准差
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；另一个促使我们从小数目样本中获得梯度的统计估计的动机是训练集的；冗余。在最坏的情况下，训练集中所有的m个样本都是彼此相同的拷
          關鍵詞：训练集中所有的, 在最坏的情况下, 另一个促使我们从小数目样本中获得梯度的统计估计的动机是训练集的, 个样本都是彼此相同的拷, 冗余
        - 摘要：使用整个训练集的优化算法被称为批量；（deterministic）梯度算法，因为它们会在一个大批量中同时处理所有；样本。这个术语可能有点令人困惑，因为这个词“批量”也经常被用来描
          關鍵詞：因为这个词, 样本, 梯度算法, 这个术语可能有点令人困惑, 使用整个训练集的优化算法被称为批量
        - 摘要：（batch）或确定性
          關鍵詞：或确定性
        - 摘要：每次只使用单个样本的优化算法有时被称为随机  （stochastic）或者在；线  （online）算法。术语“在线”通常是指从连续产生样本的数据流中抽；取样本的情况，而不是从一个固定大小的训练集中遍历多次采样的情
          關鍵詞：或者在, 算法, 在线, 术语, 每次只使用单个样本的优化算法有时被称为随机
        - 摘要：大多数用于深度学习的算法介于以上两者之间，使用一个以上而又不是；全部的训练样本。传统上，这些会被称为小批量  （minibatch）或小批；量随机  （minibatch  stochastic）方法，现在通常将它们简单地称为随机
          關鍵詞：量随机, 大多数用于深度学习的算法介于以上两者之间, 或小批, 现在通常将它们简单地称为随机, 使用一个以上而又不是
        - 摘要：随机方法的典型示例是随机梯度下降，这将在第8.3.1节中详细描述。
          關鍵詞：节中详细描述, 这将在第, 随机方法的典型示例是随机梯度下降
        - 摘要：小批量的大小通常由以下几个因素决定：
          關鍵詞：小批量的大小通常由以下几个因素决定
        - 摘要：更大的批量会计算更精确的梯度估计，但是回报却是小于线性的。；极小批量通常难以充分利用多核架构。这促使我们使用一些绝对最；小批量，低于这个值的小批量处理不会减少计算时间。
          關鍵詞：更大的批量会计算更精确的梯度估计, 这促使我们使用一些绝对最, 极小批量通常难以充分利用多核架构, 低于这个值的小批量处理不会减少计算时间, 但是回报却是小于线性的
        - 摘要：可能是由于小批量在学习过程中加入了噪声，它们会有一些正则化；效果（Wilson  and  Martinez，2003）。泛化误差通常在批量大小为1；时最好。因为梯度估计的高方差，小批量训练需要较小的学习率以
          關鍵詞：效果, 因为梯度估计的高方差, 泛化误差通常在批量大小为, 小批量训练需要较小的学习率以, 可能是由于小批量在学习过程中加入了噪声
        - 摘要：不同的算法使用不同的方法从小批量中获取不同的信息。有些算法对采；样误差比其他算法更敏感，这通常有两个可能原因。一个是它们使用了；很难在少量样本上精确估计的信息，另一个是它们以放大采样误差的方
          關鍵詞：不同的算法使用不同的方法从小批量中获取不同的信息, 很难在少量样本上精确估计的信息, 一个是它们使用了, 另一个是它们以放大采样误差的方, 有些算法对采
        - 摘要：小批量是随机抽取的这点也很重要。从一组样本中计算出梯度期望的无；偏估计要求这些样本是独立的。我们也希望两个连续的梯度估计是互相；独立的，因此两个连续的小批量样本也应该是彼此独立的。很多现实的
          關鍵詞：很多现实的, 从一组样本中计算出梯度期望的无, 我们也希望两个连续的梯度估计是互相, 因此两个连续的小批量样本也应该是彼此独立的, 偏估计要求这些样本是独立的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；很多机器学习上的优化问题都可以分解成并行地计算不同样本上单独的；更新。换言之，我们在计算小批量样本 X 上最小化J ( X )的更新时，同
          關鍵詞：我们在计算小批量样本, 更新, 换言之, 上最小化, 很多机器学习上的优化问题都可以分解成并行地计算不同样本上单独的
        - 摘要：小批量随机梯度下降的一个有趣动机是，只要没有重复使用样本，它将；遵循着真实泛化误差（式（8.2））的梯度。很多小批量随机梯度下降；方法的实现都会打乱数据顺序一次，然后多次遍历数据来更新参数。第
          關鍵詞：它将, 然后多次遍历数据来更新参数, 很多小批量随机梯度下降, 小批量随机梯度下降的一个有趣动机是, 的梯度
        - 摘要：我们不难从在线学习的情况中看出随机梯度下降最小化泛化误差的原；因。这时样本或者小批量都是从数据流  （stream）中抽取出来的。换言；之，学习器好像是一个每次看到新样本的人，每个样本（  x  ,y）都来自
          關鍵詞：每个样本, 这时样本或者小批量都是从数据流, 学习器好像是一个每次看到新样本的人, 我们不难从在线学习的情况中看出随机梯度下降最小化泛化误差的原, 中抽取出来的
        - 摘要：在  x  和y是离散时，以上的等价性很容易得到。在这种情况下，泛化误；差（式（8.2））可以表示为
          關鍵詞：是离散时, 泛化误, 在这种情况下, 以上的等价性很容易得到, 可以表示为
        - 摘要：上式的准确梯度为
          關鍵詞：上式的准确梯度为
        - 摘要：在式（8.5）和式（8.6）中，我们已经在对数似然中看到了相同的结；果，现在我们发现这一点在包括似然的其他函数L上也是成立的。在一；些关于p  data  和L的温和假设下，在  x  和y是连续时也能得到类似的结
          關鍵詞：上也是成立的, 的温和假设下, 在式, 我们已经在对数似然中看到了相同的结, 在一
        - 摘要：因此，我们可以从数据生成分布p
          關鍵詞：我们可以从数据生成分布, 因此
        - 摘要：抽取小批量样本；以及对应的目标y  (i)  ，然后计算该小批量上损失函
          關鍵詞：抽取小批量样本, 然后计算该小批量上损失函, 以及对应的目标
        - 摘要：data
          關鍵詞：
        - 摘要：数关于对应参数的梯度
          關鍵詞：数关于对应参数的梯度
        - 摘要：以此获得泛化误差准确梯度的无偏估计。最后，在泛化误差上使用SGD；方法在方向  上更新 θ 。
          關鍵詞：方法在方向, 在泛化误差上使用, 上更新, 以此获得泛化误差准确梯度的无偏估计, 最后
        - 摘要：当然，这个解释只能用于样本没有重复使用的情况。然而，除非训练集；特别大，通常最好是多次遍历训练集。当多次遍历数据集更新时，只有；第一遍满足泛化误差梯度的无偏估计。但是，额外的遍历更新当然会由
          關鍵詞：当多次遍历数据集更新时, 除非训练集, 但是, 然而, 这个解释只能用于样本没有重复使用的情况
        - 摘要：随着数据集的规模迅速增长，超越了计算能力的增速，机器学习应用每；个样本只使用一次的情况变得越来越常见，甚至是不完整地使用训练；集。在使用一个非常大的训练集时，过拟合不再是问题，而欠拟合和计
          關鍵詞：过拟合不再是问题, 个样本只使用一次的情况变得越来越常见, 在使用一个非常大的训练集时, 甚至是不完整地使用训练, 随着数据集的规模迅速增长
        - 摘要：8.1.1　经验风险最小化
          關鍵詞：经验风险最小化
        - 摘要：8.1.2　代理损失函数和提；前终止
          關鍵詞：代理损失函数和提, 前终止
        - 摘要：8.1.3　批量算法和小批量；算法
          關鍵詞：批量算法和小批量, 算法
    8.2：神经网络优化中的挑战
        - 摘要：8.2.1　病态
          關鍵詞：病态
        - 摘要：8.2.2　局部极小值
          關鍵詞：局部极小值
        - 摘要：8.2.3　高原、鞍点和其他平坦区域
          關鍵詞：鞍点和其他平坦区域, 高原
        - 摘要：8.2.4　悬崖和梯度爆炸
          關鍵詞：悬崖和梯度爆炸
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；8.2.5　长期依赖
          關鍵詞：长期依赖
        - 摘要：8.2.6　非精确梯度
          關鍵詞：非精确梯度
        - 摘要：8.2.7　局部和全局结构间的弱对应
          關鍵詞：局部和全局结构间的弱对应
        - 摘要：8.2.8　优化的理论限制
          關鍵詞：优化的理论限制
        - 摘要：优化通常是一个极其困难的任务。传统的机器学习会小心设计目标函数；和约束，以确保优化问题是凸的，从而避免一般优化问题的复杂度。在；训练神经网络时，我们肯定会遇到一般的非凸情况。即使是凸优化，也
          關鍵詞：优化通常是一个极其困难的任务, 训练神经网络时, 我们肯定会遇到一般的非凸情况, 即使是凸优化, 从而避免一般优化问题的复杂度
        - 摘要：8.2.1　病态
          關鍵詞：病态
        - 摘要：在优化凸函数时，会遇到一些挑战。这其中最突出的是Hessian矩阵  H；的病态。这是数值优化、凸优化或其他形式的优化中普遍存在的问题，；更多细节请回顾第4.3.1节。
          關鍵詞：矩阵, 会遇到一些挑战, 凸优化或其他形式的优化中普遍存在的问题, 更多细节请回顾第, 这是数值优化
        - 摘要：病态问题一般被认为存在于神经网络训练过程中。病态体现在随机梯度
          關鍵詞：病态体现在随机梯度, 病态问题一般被认为存在于神经网络训练过程中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；下降会“卡”在某些情况，此时即使很小的更新步长也会增加代价函数。
          關鍵詞：下降会, 在某些情况, 此时即使很小的更新步长也会增加代价函数
        - 摘要：回顾式（4.9），代价函数的二阶泰勒级数展开预测梯度下降中的
          關鍵詞：回顾式, 代价函数的二阶泰勒级数展开预测梯度下降中的
        - 摘要：会增加
          關鍵詞：会增加
        - 摘要：到代价中。当
          關鍵詞：到代价中
        - 摘要：超过
          關鍵詞：超过
        - 摘要：时，梯度的病态会成
          關鍵詞：梯度的病态会成
        - 摘要：为问题。判断病态是否不利于神经网络训练任务，我们可以监测平方梯；。在很多情况中，梯度范数不会在训练过程；度范数
          關鍵詞：为问题, 梯度范数不会在训练过程, 我们可以监测平方梯, 度范数, 在很多情况中
        - 摘要：图8.1　梯度下降通常不会到达任何类型的临界点。此示例中，在用于对象检测的卷积网络的整；个训练期间，梯度范数持续增加。（左）各个梯度计算的范数如何随时间分布的散点图。为了；方便作图，每轮仅绘制一个梯度范数。我们将所有梯度范数的移动平均绘制为实曲线。梯度范
          關鍵詞：我们将所有梯度范数的移动平均绘制为实曲线, 每轮仅绘制一个梯度范数, 为了, 个训练期间, 梯度范数持续增加
        - 摘要：尽管病态还存在于除了神经网络训练的其他情况中，有些适用于其他情；况的解决病态的技术并不适用于神经网络。例如，牛顿法在解决带有病；态条件的Hessian矩阵的凸优化问题时，是一个非常优秀的工具，但是我
          關鍵詞：是一个非常优秀的工具, 况的解决病态的技术并不适用于神经网络, 牛顿法在解决带有病, 但是我, 态条件的
        - 摘要：8.2.2　局部极小值
          關鍵詞：局部极小值
        - 摘要：凸优化问题的一个突出特点是其可以简化为寻找一个局部极小点的问；题。任何一个局部极小点都是全局最小点。有些凸函数的底部是一个平；坦的区域，而不是单一的全局最小点，但该平坦区域中的任意点都是一
          關鍵詞：但该平坦区域中的任意点都是一, 任何一个局部极小点都是全局最小点, 而不是单一的全局最小点, 有些凸函数的底部是一个平, 凸优化问题的一个突出特点是其可以简化为寻找一个局部极小点的问
        - 摘要：对于非凸函数时，如神经网络，有可能会存在多个局部极小值。事实；上，几乎所有的深度模型基本上都会有非常多的局部极小值。然而，我；们会发现这并不是主要问题。
          關鍵詞：们会发现这并不是主要问题, 有可能会存在多个局部极小值, 如神经网络, 然而, 对于非凸函数时
        - 摘要：由于模型可辨识性  （model  identifiability）问题，神经网络和任意具有；多个等效参数化潜变量的模型都会具有多个局部极小值。如果一个足够；大的训练集可以唯一确定一组模型参数，那么该模型被称为可辨认的。
          關鍵詞：大的训练集可以唯一确定一组模型参数, 那么该模型被称为可辨认的, 神经网络和任意具有, 多个等效参数化潜变量的模型都会具有多个局部极小值, 问题
        - 摘要：除了权重空间对称性，很多神经网络还有其他导致不可辨认的原因。例；如，在任意整流线性网络或者maxout网络中，我们可以将传入权重和偏
          關鍵詞：网络中, 很多神经网络还有其他导致不可辨认的原因, 我们可以将传入权重和偏, 除了权重空间对称性, 在任意整流线性网络或者
        - 摘要：置扩大α倍，然后将传出权重扩大   倍，而保持模型等价。这意味着，
          關鍵詞：这意味着, 而保持模型等价, 置扩大, 然后将传出权重扩大
        - 摘要：如果代价函数不包括如权重衰减这种直接依赖于权重而非模型输出的；项，那么整流线性网络或者maxout网络的每一个局部极小点都在等价的；局部极小值的（m×n）维双曲线上。
          關鍵詞：局部极小值的, 网络的每一个局部极小点都在等价的, 维双曲线上, 那么整流线性网络或者, 如果代价函数不包括如权重衰减这种直接依赖于权重而非模型输出的
        - 摘要：这些模型可辨识性问题意味着，神经网络代价函数具有非常多甚至不可；数无限多的局部极小值。然而，所有这些由于不可辨识性问题而产生的；局部极小值都有相同的代价函数值。因此，这些局部极小值并非是非凸
          關鍵詞：这些模型可辨识性问题意味着, 因此, 数无限多的局部极小值, 然而, 神经网络代价函数具有非常多甚至不可
        - 摘要：如果局部极小值相比全局最小点拥有很大的代价，局部极小值会带来很；大的隐患。我们可以构建没有隐藏单元的小规模神经网络，其局部极小
          關鍵詞：局部极小值会带来很, 其局部极小, 我们可以构建没有隐藏单元的小规模神经网络, 大的隐患, 如果局部极小值相比全局最小点拥有很大的代价
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；and  Sussman，1989；；值的代价比全局最小点的代价大很多（Sontag
          關鍵詞：值的代价比全局最小点的代价大很多
        - 摘要：对于实际中感兴趣的网络，是否存在大量代价很高的局部极小值，优化；算法是否会碰到这些局部极小值，都是尚未解决的公开问题。多年来，；大多数从业者认为局部极小值是困扰神经网络优化的常见问题。如今，
          關鍵詞：大多数从业者认为局部极小值是困扰神经网络优化的常见问题, 算法是否会碰到这些局部极小值, 多年来, 如今, 是否存在大量代价很高的局部极小值
        - 摘要：很多从业者将神经网络优化中的所有困难都归结于局部极小值。我们鼓；励从业者要仔细分析特定的问题。一种能够排除局部极小值是主要问题；的检测方法是画出梯度范数随时间的变化。如果梯度范数没有缩小到一
          關鍵詞：如果梯度范数没有缩小到一, 我们鼓, 一种能够排除局部极小值是主要问题, 励从业者要仔细分析特定的问题, 很多从业者将神经网络优化中的所有困难都归结于局部极小值
        - 摘要：8.2.3　高原、鞍点和其他平坦区域
          關鍵詞：鞍点和其他平坦区域, 高原
        - 摘要：对于很多高维非凸函数而言，局部极小值（以及极大值）事实上都远少；于另一类梯度为零的点：鞍点。鞍点附近的某些点比鞍点有更大的代；价，而其他点则有更小的代价。在鞍点处，Hessian矩阵同时具有正负特
          關鍵詞：鞍点附近的某些点比鞍点有更大的代, 而其他点则有更小的代价, 矩阵同时具有正负特, 局部极小值, 以及极大值
        - 摘要：多类随机函数表现出以下性质：低维空间中，局部极小值很普遍。在更；高维空间中，局部极小值很罕见，而鞍点则很常见。对于这类函数
          關鍵詞：在更, 对于这类函数, 局部极小值很罕见, 多类随机函数表现出以下性质, 高维空间中
        - 摘要：而言，鞍点和局部极小值的数目比率的期望随n指数；级增长。我们可以从直觉上理解这种现象——Hessian矩阵在局部极小点；处只有正特征值。而在鞍点处，Hessian矩阵则同时具有正负特征值。试
          關鍵詞：级增长, 而言, 鞍点和局部极小值的数目比率的期望随, 我们可以从直觉上理解这种现象, 矩阵则同时具有正负特征值
        - 摘要：想一下，每个特征值的正负号由抛硬币决定。在一维情况下，很容易抛；硬币得到正面朝上一次而获取局部极小点。在n-维空间中，要抛掷n次；硬币都正面朝上的难度是指数级的。具体可以参考Dauphin
          關鍵詞：每个特征值的正负号由抛硬币决定, 维空间中, 要抛掷, 具体可以参考, 在一维情况下
        - 摘要：et
          關鍵詞：
        - 摘要：al.
          關鍵詞：
        - 摘要：很多随机函数一个惊人性质是，当我们到达代价较低的区间时，Hessian；矩阵的特征值为正的可能性更大。和抛硬币类比，这意味着如果我们处；于低代价的临界点时，抛掷硬币正面朝上n次的概率更大。这也意味
          關鍵詞：和抛硬币类比, 这也意味, 当我们到达代价较低的区间时, 于低代价的临界点时, 矩阵的特征值为正的可能性更大
        - 摘要：以上现象出现在许多种类的随机函数中。那么是否在神经网络中也有发；生呢？Baldi  and  Hornik（1989）从理论上证明，不具非线性的浅层自编；码器（第14章中将介绍的一种将输出训练为输入拷贝的前馈网络）只有
          關鍵詞：不具非线性的浅层自编, 章中将介绍的一种将输出训练为输入拷贝的前馈网络, 从理论上证明, 以上现象出现在许多种类的随机函数中, 码器
        - 摘要：鞍点激增对于训练算法来说有哪些影响呢？对于只使用梯度信息的一阶；优化算法而言，目前情况还不清楚。鞍点附近的梯度通常会非常小。另；一方面，实验中梯度下降似乎可以在许多情况下逃离鞍点。Goodfellow
          關鍵詞：鞍点附近的梯度通常会非常小, 目前情况还不清楚, 一方面, 实验中梯度下降似乎可以在许多情况下逃离鞍点, 对于只使用梯度信息的一阶
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图8.2　神经网络代价函数的可视化。这些可视化对应用于真实对象识别和自然语言处理任务的；前馈神经网络、卷积网络和循环网络而言是类似的。令人惊讶的是，这些可视化通常不会显示
          關鍵詞：前馈神经网络, 这些可视化通常不会显示, 这些可视化对应用于真实对象识别和自然语言处理任务的, 令人惊讶的是, 神经网络代价函数的可视化
        - 摘要：对于牛顿法而言，鞍点显然是一个问题。梯度下降旨在朝“下坡”移动，；而非明确寻求临界点。而牛顿法的目标是寻求梯度为零的点。如果没有；适当的修改，牛顿法就会跳进一个鞍点。高维空间中鞍点的激增或许解
          關鍵詞：下坡, 移动, 对于牛顿法而言, 如果没有, 高维空间中鞍点的激增或许解
        - 摘要：除了极小值和鞍点，还存在其他梯度为零的点。例如从优化的角度看与；鞍点很相似的极大值，很多算法不会被吸引到极大值，除了未经修改的；牛顿法。和极小值一样，许多种类的随机函数的极大值在高维空间中也
          關鍵詞：除了极小值和鞍点, 牛顿法, 许多种类的随机函数的极大值在高维空间中也, 很多算法不会被吸引到极大值, 例如从优化的角度看与
        - 摘要：也可能存在恒值的、宽且平坦的区域。在这些区域，梯度和Hessian矩阵；都是零。这种退化的情形是所有数值优化算法的主要问题。在凸问题；中，一个宽而平坦的区间肯定包含全局极小值，但是对于一般的优化问
          關鍵詞：也可能存在恒值的, 宽且平坦的区域, 一个宽而平坦的区间肯定包含全局极小值, 都是零, 这种退化的情形是所有数值优化算法的主要问题
        - 摘要：题而言，这样的区域可能会对应着目标函数中一个较高的值。
          關鍵詞：这样的区域可能会对应着目标函数中一个较高的值, 题而言
        - 摘要：8.2.4　悬崖和梯度爆炸
          關鍵詞：悬崖和梯度爆炸
        - 摘要：多层神经网络通常存在像悬崖一样的斜率较大区域，如图8.3所示。这；是由于几个较大的权重相乘导致的。遇到斜率极大的悬崖结构时，梯度；更新会很大程度地改变参数值，通常会完全跳过这类悬崖结构。
          關鍵詞：梯度, 是由于几个较大的权重相乘导致的, 遇到斜率极大的悬崖结构时, 多层神经网络通常存在像悬崖一样的斜率较大区域, 所示
        - 摘要：图8.3　高度非线性的深度神经网络或循环神经网络的目标函数通常包含由几个参数连乘而导致；的参数空间中尖锐非线性。这些非线性在某些区域会产生非常大的导数。当参数接近这样的悬；崖区域时，梯度下降更新可以使参数弹射得非常远，可能会使大量已完成的优化工作成为无用
          關鍵詞：可能会使大量已完成的优化工作成为无用, 的参数空间中尖锐非线性, 梯度下降更新可以使参数弹射得非常远, 当参数接近这样的悬, 崖区域时
        - 摘要：不管我们是从上还是从下接近悬崖，情况都很糟糕，但幸运的是，我们；可以使用第10.11.1节介绍的启发式梯度截断 （gradient clipping）来避免；其严重的后果。其基本想法源自梯度并没有指明最佳步长，只说明了在
          關鍵詞：情况都很糟糕, 节介绍的启发式梯度截断, 其基本想法源自梯度并没有指明最佳步长, 我们, 但幸运的是
        - 摘要：8.2.5　长期依赖
          關鍵詞：长期依赖
        - 摘要：当计算图变得极深时，神经网络优化算法会面临的另一个难题就是长期；依赖问题——由于变深的结构使模型丧失了学习到先前信息的能力，让
          關鍵詞：神经网络优化算法会面临的另一个难题就是长期, 依赖问题, 当计算图变得极深时, 由于变深的结构使模型丧失了学习到先前信息的能力
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；优化变得极其困难。深层的计算图不仅存在于前馈网络，还存在于之后；介绍的循环网络中（在第10章中描述）。因为循环网络要在很长时间序
          關鍵詞：介绍的循环网络中, 还存在于之后, 深层的计算图不仅存在于前馈网络, 章中描述, 在第
        - 摘要：例如，假设某个计算图中包含一条反复与矩阵  W  相乘的路径。那么t步；后，相当于乘以 W t 。假设 W 有特征值分解；。
          關鍵詞：假设某个计算图中包含一条反复与矩阵, 相当于乘以, 相乘的路径, 例如, 那么
        - 摘要：当特征值λ  i  不在1附近时，若在量级上大于1则会爆炸；若小于1时则会；消失。梯度消失与爆炸问题  （vanishing；gradient
          關鍵詞：若在量级上大于, 则会爆炸, 消失, 时则会, 附近时
        - 摘要：exploding
          關鍵詞：
        - 摘要：and
          關鍵詞：
        - 摘要：此处描述的在各时间步重复与 W 相乘非常类似于寻求矩阵 W 的最大特；征值及对应特征向量的幂方法  （power  method）。从这个观点来看，
          關鍵詞：的最大特, 征值及对应特征向量的幂方法, 相乘非常类似于寻求矩阵, 此处描述的在各时间步重复与, 从这个观点来看
        - 摘要：最终会丢弃 x 中所有与 W 的主特征向量正交的成分。
          關鍵詞：最终会丢弃, 中所有与, 的主特征向量正交的成分
        - 摘要：循环网络在各时间步上使用相同的矩阵  W  ，而前馈网络并没有。所以；即使使用非常深层的前馈网络，也能很大程度上有效地避免梯度消失与；爆炸问题（Sussillo，2014）。
          關鍵詞：即使使用非常深层的前馈网络, 循环网络在各时间步上使用相同的矩阵, 也能很大程度上有效地避免梯度消失与, 爆炸问题, 而前馈网络并没有
        - 摘要：在更详细地描述循环网络之后，我们将会在第10.7节进一步讨论循环网；络训练中的挑战。
          關鍵詞：我们将会在第, 节进一步讨论循环网, 在更详细地描述循环网络之后, 络训练中的挑战
        - 摘要：8.2.6　非精确梯度
          關鍵詞：非精确梯度
        - 摘要：大多数优化算法的先决条件都是我们知道精确的梯度或是Hessian矩阵。；在实践中，通常这些量会有噪声，甚至是有偏的估计。几乎每一个深度；学习算法都需要基于采样的估计，至少使用训练样本的小批量来计算梯
          關鍵詞：大多数优化算法的先决条件都是我们知道精确的梯度或是, 至少使用训练样本的小批量来计算梯, 几乎每一个深度, 矩阵, 通常这些量会有噪声
        - 摘要：在其他情况下，我们希望最小化的目标函数实际上是难以处理的。当目；标函数不可解时，通常其梯度也是难以处理的。在这种情况下，我们只；能近似梯度。这些问题主要出现在本书第3部分更高级的模型中。例
          關鍵詞：这些问题主要出现在本书第, 通常其梯度也是难以处理的, 我们只, 我们希望最小化的目标函数实际上是难以处理的, 当目
        - 摘要：各种神经网络优化算法的设计都考虑到了梯度估计的缺陷。我们可以选；择比真实损失函数更容易估计的代理损失函数来避免这个问题。
          關鍵詞：择比真实损失函数更容易估计的代理损失函数来避免这个问题, 各种神经网络优化算法的设计都考虑到了梯度估计的缺陷, 我们可以选
        - 摘要：8.2.7　局部和全局结构间的弱对应
          關鍵詞：局部和全局结构间的弱对应
        - 摘要：迄今为止，我们讨论的许多问题都是关于损失函数在单个点的性质——；若J（ θ ）是当前点 θ 的病态条件，或者 θ 在悬崖中，或者 θ 是一个下；降方向不明显的鞍点，那么会很难更新当前步。
          關鍵詞：是当前点, 是一个下, 的病态条件, 降方向不明显的鞍点, 或者
        - 摘要：如果该方向在局部改进很大，但并没有指向代价低得多的遥远区域，那；么我们有可能在单点处克服以上所有困难，但仍然表现不佳。
          關鍵詞：如果该方向在局部改进很大, 但并没有指向代价低得多的遥远区域, 么我们有可能在单点处克服以上所有困难, 但仍然表现不佳
        - 摘要：Goodfellow  et  al.  （2015）认为大部分训练的运行时间取决于到达解决；方案的轨迹长度。如图8.2所示，学习轨迹将花费大量的时间探寻一个；围绕山形结构的宽弧。
          關鍵詞：认为大部分训练的运行时间取决于到达解决, 所示, 围绕山形结构的宽弧, 方案的轨迹长度, 如图
        - 摘要：大多数优化研究的难点集中于训练是否找到了全局最小点、局部极小点；或是鞍点，但在实践中神经网络不会到达任何一种临界点。图8.1表明；神经网络通常不会到达梯度很小的区域。甚至，这些临界点不一定存
          關鍵詞：大多数优化研究的难点集中于训练是否找到了全局最小点, 但在实践中神经网络不会到达任何一种临界点, 或是鞍点, 这些临界点不一定存, 表明
        - 摘要：可以没有全局最小点，而是
          關鍵詞：可以没有全局最小点, 而是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图8.4　如果局部表面没有指向全局解，基于局部下坡移动的优化可能就会失败。这里我们提供；一个例子，说明即使在没有鞍点或局部极小值的情况下，优化过程会如何失败。此例中的代价
          關鍵詞：如果局部表面没有指向全局解, 基于局部下坡移动的优化可能就会失败, 此例中的代价, 一个例子, 说明即使在没有鞍点或局部极小值的情况下
        - 摘要：未来的研究需要进一步探索影响学习轨迹长度和更好地表征训练过程的；结果。
          關鍵詞：未来的研究需要进一步探索影响学习轨迹长度和更好地表征训练过程的, 结果
        - 摘要：许多现有研究方法在求解具有困难全局结构的问题时，旨在寻求良好的；初始点，而不是开发非局部范围更新的算法。
          關鍵詞：许多现有研究方法在求解具有困难全局结构的问题时, 而不是开发非局部范围更新的算法, 旨在寻求良好的, 初始点
        - 摘要：梯度下降和基本上所有的可以有效训练神经网络的学习算法，都是基于；局部较小更新。之前的小节主要集中于为何这些局部范围更新的正确方；向难以计算。我们也许能计算目标函数的一些性质，如近似的有偏梯度
          關鍵詞：局部较小更新, 都是基于, 梯度下降和基本上所有的可以有效训练神经网络的学习算法, 之前的小节主要集中于为何这些局部范围更新的正确方, 向难以计算
        - 摘要：心，朝着下坡方向移动，却和所有可行解南辕北辙，如图8.4所示，或；者是用舍近求远的方法来求解问题，如图8.2所示。目前，我们还不了；解这些问题中的哪一个与神经网络优化中的难点最相关，这是研究领域
          關鍵詞：却和所有可行解南辕北辙, 我们还不了, 这是研究领域, 所示, 目前
        - 摘要：不管哪个问题最重要，如果存在一个区域，我们遵循局部下降便能合理；地直接到达某个解，并且我们能够在该良好区域上初始化学习，那么这；些问题都可以避免。最终的观点还是建议在传统优化算法上研究怎样选
          關鍵詞：并且我们能够在该良好区域上初始化学习, 最终的观点还是建议在传统优化算法上研究怎样选, 不管哪个问题最重要, 我们遵循局部下降便能合理, 些问题都可以避免
        - 摘要：8.2.8　优化的理论限制
          關鍵詞：优化的理论限制
        - 摘要：一些理论结果表明，我们为神经网络设计的任何优化算法都有性能限制；（Blum  and  Rivest，1992；Judd，1989；Wolpert  and  MacReady，；1997）。通常这些结果不影响神经网络在实践中的应用。
          關鍵詞：我们为神经网络设计的任何优化算法都有性能限制, 通常这些结果不影响神经网络在实践中的应用, 一些理论结果表明
        - 摘要：一些理论结果仅适用于神经网络的单元输出离散值的情况。然而，大多；数神经网络单元输出光滑的连续值，使得局部搜索求解优化可行。一些；理论结果表明，存在某类问题是不可解的，但很难判断一个特定问题是
          關鍵詞：一些, 数神经网络单元输出光滑的连续值, 然而, 但很难判断一个特定问题是, 一些理论结果仅适用于神经网络的单元输出离散值的情况
        - 摘要：8.2.1　病态
          關鍵詞：病态
        - 摘要：8.2.2　局部极小值
          關鍵詞：局部极小值
        - 摘要：8.2.3　高原、鞍点和其他；平坦区域
          關鍵詞：平坦区域, 高原, 鞍点和其他
        - 摘要：8.2.4　悬崖和梯度爆炸
          關鍵詞：悬崖和梯度爆炸
        - 摘要：8.2.5　长期依赖
          關鍵詞：长期依赖
        - 摘要：8.2.6　非精确梯度
          關鍵詞：非精确梯度
        - 摘要：8.2.7　局部和全局结构间；的弱对应
          關鍵詞：局部和全局结构间, 的弱对应
        - 摘要：8.2.8　优化的理论限制
          關鍵詞：优化的理论限制
    8.3：基本算法
        - 摘要：8.3.1　随机梯度下降
          關鍵詞：随机梯度下降
        - 摘要：8.3.2　动量
          關鍵詞：动量
        - 摘要：8.3.3　Nesterov动量
          關鍵詞：动量
        - 摘要：之前我们已经介绍了梯度下降（第4.3节），即沿着整个训练集的梯度；方向下降。这可以使用随机梯度下降很大程度地加速，沿着随机挑选的；小批量数据的梯度下降方向，就像第5.9节和第8.1.3节中讨论的一样。
          關鍵詞：小批量数据的梯度下降方向, 就像第, 方向下降, 节和第, 沿着随机挑选的
        - 摘要：8.3.1　随机梯度下降
          關鍵詞：随机梯度下降
        - 摘要：随机梯度下降（SGD）及其变种很可能是一般机器学习中应用最多的的；优化算法，特别是在深度学习中。如第8.1.3节中所讨论的，按照数据生
          關鍵詞：及其变种很可能是一般机器学习中应用最多的的, 特别是在深度学习中, 节中所讨论的, 优化算法, 如第
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；成分布抽取m个小批量（独立同分布的）样本，通过计算它们梯度均；值，我们可以得到梯度的无偏估计。
          關鍵詞：样本, 通过计算它们梯度均, 成分布抽取, 独立同分布的, 个小批量
        - 摘要：算法8.1展示了如何沿着这个梯度的估计下降。
          關鍵詞：展示了如何沿着这个梯度的估计下降, 算法
        - 摘要：算法8.1 　随机梯度下降（SGD）在第k个训练迭代的更新。
          關鍵詞：随机梯度下降, 在第, 算法, 个训练迭代的更新
        - 摘要：Require： 学习率
          關鍵詞：学习率
        - 摘要：Require： 初始参数 θ
          關鍵詞：初始参数
        - 摘要：while 停止准则未满足do
          關鍵詞：停止准则未满足
        - 摘要：从训练集中采包含m个样本；(i) 对应目标为 y (i) 。
          關鍵詞：对应目标为, 个样本, 从训练集中采包含
        - 摘要：计算梯度估计：
          關鍵詞：计算梯度估计
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：end while
          關鍵詞：
        - 摘要：的小批量，其中  x
          關鍵詞：的小批量, 其中
        - 摘要：SGD算法中的一个关键参数是学习率。之前，我们介绍的SGD使用固定；的学习率。在实践中，有必要随着时间的推移逐渐降低学习率，因此我；们将第k步迭代的学习率记作  。
          關鍵詞：算法中的一个关键参数是学习率, 因此我, 使用固定, 步迭代的学习率记作, 的学习率
        - 摘要：这是因为SGD中梯度估计引入的噪声源（m个训练样本的随机采样）并；不会在极小点处消失。相比之下，当我们使用批量梯度下降到达极小点；时，整个代价函数的真实梯度会变得很小，之后为0  ，因此批量梯度下
          關鍵詞：之后为, 不会在极小点处消失, 这是因为, 当我们使用批量梯度下降到达极小点, 中梯度估计引入的噪声源
        - 摘要：且
          關鍵詞：
        - 摘要：实践中，一般会线性衰减学习率直到第τ次迭代：
          關鍵詞：一般会线性衰减学习率直到第, 次迭代, 实践中
        - 摘要：其中
          關鍵詞：其中
        - 摘要：。在τ步迭代之后，一般使  保持常数。
          關鍵詞：一般使, 步迭代之后, 保持常数
        - 摘要：学习率可通过试验和误差来选取，通常最好的选择方法是监测目标函数；值随时间变化的学习曲线。与其说是科学，这更像是一门艺术，我们应；该谨慎地参考关于这个问题的大部分指导。使用线性策略时，需要选择
          關鍵詞：需要选择, 值随时间变化的学习曲线, 这更像是一门艺术, 通常最好的选择方法是监测目标函数, 学习率可通过试验和误差来选取
        - 摘要：SGD及相关的小批量亦或更广义的基于梯度优化的在线学习算法，一个；重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡。即使；训练样本数目非常大时，它们也能收敛。对于足够大的数据集，SGD可
          關鍵詞：对于足够大的数据集, 及相关的小批量亦或更广义的基于梯度优化的在线学习算法, 即使, 重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡, 训练样本数目非常大时
        - 摘要：研究优化算法的收敛率，一般会衡量额外误差
          關鍵詞：研究优化算法的收敛率, 一般会衡量额外误差
        - 摘要：error）；，即当前代价函数超出最低可能代价的量。
          關鍵詞：即当前代价函数超出最低可能代价的量
        - 摘要：（excess
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；SGD应用于凸问题时，k步迭代后的额外误差量级是
          關鍵詞：步迭代后的额外误差量级是, 应用于凸问题时
        - 摘要：，在强
          關鍵詞：在强
        - 摘要：凸情况下是
          關鍵詞：凸情况下是
        - 摘要：。除非假定额外的条件，否则这些界限不能进一
          關鍵詞：除非假定额外的条件, 否则这些界限不能进一
        - 摘要：步改进。批量梯度下降在理论上比随机梯度下降有更好的收敛率。然；而，Cramér-Rao界限（Cramér，1946；Rao，1945）指出，泛化误差的
          關鍵詞：指出, 步改进, 界限, 批量梯度下降在理论上比随机梯度下降有更好的收敛率, 泛化误差的
        - 摘要：下降速度不会快于
          關鍵詞：下降速度不会快于
        - 摘要：）。Bottou and Bousquet（2008b）因此认
          關鍵詞：因此认
        - 摘要：为对于机器学习任务，不值得探寻收敛快于
          關鍵詞：不值得探寻收敛快于, 为对于机器学习任务
        - 摘要：）的优化算法
          關鍵詞：的优化算法
        - 摘要：——更快的收敛可能对应着过拟合。此外，渐近分析掩盖了随机梯度下；降在少量更新步之后的很多优点。对于大数据集，SGD只需非常少量样；本计算梯度从而实现初始快速更新，远远超过了其缓慢的渐近收敛。本
          關鍵詞：只需非常少量样, 本计算梯度从而实现初始快速更新, 远远超过了其缓慢的渐近收敛, 此外, 渐近分析掩盖了随机梯度下
        - 摘要：常数倍
          關鍵詞：常数倍
        - 摘要：）的渐近分析。我们也可以在学习过程中逐渐增大小
          關鍵詞：我们也可以在学习过程中逐渐增大小, 的渐近分析
        - 摘要：批量的大小，以此权衡批量梯度下降和随机梯度下降两者的优点。
          關鍵詞：批量的大小, 以此权衡批量梯度下降和随机梯度下降两者的优点
        - 摘要：了解SGD更多的信息，请查看Bottou（1998）。
          關鍵詞：了解, 请查看, 更多的信息
        - 摘要：8.3.2　动量
          關鍵詞：动量
        - 摘要：虽然随机梯度下降仍然是非常受欢迎的优化方法，但其学习过程有时会；很慢。动量方法（Polyak，1964）旨在加速学习，特别是处理高曲率、；小但一致的梯度，或是带噪声的梯度。动量算法积累了之前梯度指数级
          關鍵詞：很慢, 动量方法, 特别是处理高曲率, 动量算法积累了之前梯度指数级, 虽然随机梯度下降仍然是非常受欢迎的优化方法
        - 摘要：从形式上看，动量算法引入了变量 ν  充当速度角色——它代表参数在参；数空间移动的方向和速率。速度被设为负梯度的指数衰减平均。名称动；量  （momentum）来自物理类比，根据牛顿运动定律，负梯度是移动参
          關鍵詞：从形式上看, 根据牛顿运动定律, 它代表参数在参, 负梯度是移动参, 数空间移动的方向和速率
        - 摘要：速度  ν  累积了梯度元素
          關鍵詞：速度, 累积了梯度元素
        - 摘要：。相对于，；越大，之前梯度对现在方向的影响也越大。带动量的SGD算法如算
          關鍵詞：算法如算, 相对于, 之前梯度对现在方向的影响也越大, 越大, 带动量的
        - 摘要：法8.2所示。
          關鍵詞：所示
        - 摘要：图8.5　动量的主要目的是解决两个问题：Hessian矩阵的病态条件和随机梯度的方差。我们通过；此图说明动量如何克服这两个问题的第一个。等高线描绘了一个二次损失函数（具有病态条件；的Hessian矩阵）。横跨轮廓的红色路径表示动量学习规则所遵循的路径，它使该函数最小化。
          關鍵詞：我们通过, 它使该函数最小化, 此图说明动量如何克服这两个问题的第一个, 等高线描绘了一个二次损失函数, 矩阵
        - 摘要：算法8.2 　使用动量的随机梯度下降（SGD）。
          關鍵詞：算法, 使用动量的随机梯度下降
        - 摘要：Require： 学习率  ，动量参数α
          關鍵詞：动量参数, 学习率
        - 摘要：Require： 初始参数 θ ，初始速度 ν
          關鍵詞：初始速度, 初始参数
        - 摘要：while 没有达到停止准则do
          關鍵詞：没有达到停止准则
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；的小批量，对应目标
          關鍵詞：对应目标, 的小批量
        - 摘要：从训练集中采包含m个样本；为  。
          關鍵詞：个样本, 从训练集中采包含
        - 摘要：计算梯度估计：
          關鍵詞：计算梯度估计
        - 摘要：计算速度更新：
          關鍵詞：计算速度更新
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：end while
          關鍵詞：
        - 摘要：之前，步长只是梯度范数乘以学习率。现在，步长取决于梯度序列的大；小和排列。当许多连续的梯度指向相同的方向时，步长最大。如果动量；算法总是观测到梯度 g ，那么它会在方向−g上不停加速，直到达到最终
          關鍵詞：步长只是梯度范数乘以学习率, 步长最大, 上不停加速, 那么它会在方向, 如果动量
        - 摘要：因此将动量的超参数视为
          關鍵詞：因此将动量的超参数视为
        - 摘要：有助于理解。例如，α＝0.9对应着最
          關鍵詞：例如, 有助于理解, 对应着最
        - 摘要：大速度10倍于梯度下降算法。
          關鍵詞：倍于梯度下降算法, 大速度
        - 摘要：在实践中，α的一般取值为0.5、0.9和0.99。和学习率一样，α也会随着；时间不断调整。一般初始值是一个较小的值，随后会慢慢变大。随着时；间推移调整α没有收缩  重要。
          關鍵詞：时间不断调整, 和学习率一样, 随后会慢慢变大, 一般初始值是一个较小的值, 没有收缩
        - 摘要：我们可以将动量算法视为模拟连续时间下牛顿动力学下的粒子。这种物；理类比有助于直觉上理解动量和梯度下降算法是如何表现的。
          關鍵詞：理类比有助于直觉上理解动量和梯度下降算法是如何表现的, 这种物, 我们可以将动量算法视为模拟连续时间下牛顿动力学下的粒子
        - 摘要：粒子在任意时间点的位置由  θ (t)给定。粒子会受到净力 f  (t)。该力会导；致粒子加速：
          關鍵詞：该力会导, 粒子会受到净力, 粒子在任意时间点的位置由, 致粒子加速, 给定
        - 摘要：与其将其视为位置的二阶微分方程，我们不如引入表示粒子在时间t处；速度的变量 ν (t)，将牛顿动力学重写为一阶微分方程：
          關鍵詞：与其将其视为位置的二阶微分方程, 我们不如引入表示粒子在时间, 速度的变量, 将牛顿动力学重写为一阶微分方程
        - 摘要：由此，动量算法包括通过数值模拟求解微分方程。求解微分方程的一个；简单数值方法是欧拉方法，通过在每个梯度方向上小且有限的步来简单；模拟该等式定义的动力学。
          關鍵詞：动量算法包括通过数值模拟求解微分方程, 模拟该等式定义的动力学, 由此, 求解微分方程的一个, 简单数值方法是欧拉方法
        - 摘要：这解释了动量更新的基本形式，但具体什么是力呢？力正比于代价函数；的负梯度；。该力推动粒子沿着代价函数表面下坡的方向移
          關鍵詞：但具体什么是力呢, 该力推动粒子沿着代价函数表面下坡的方向移, 力正比于代价函数, 的负梯度, 这解释了动量更新的基本形式
        - 摘要：另一个力也是必要的。如果代价函数的梯度是唯一的力，那么粒子可能；永远不会停下来。想象一下，假设理想情况下冰面没有摩擦，一个冰球；从山谷的一端下滑，上升到另一端，永远来回振荡。要解决这个问题，
          關鍵詞：那么粒子可能, 永远来回振荡, 另一个力也是必要的, 上升到另一端, 如果代价函数的梯度是唯一的力
        - 摘要：为什么要特别使用− ν (t)和黏性阻力呢？部分原因是因为− ν (t)在数学上；的便利——速度的整数幂很容易处理。然而，其他物理系统具有基于速；度的其他整数幂的其他类型的阻力。例如，颗粒通过空气时会受到正比
          關鍵詞：速度的整数幂很容易处理, 在数学上, 度的其他整数幂的其他类型的阻力, 颗粒通过空气时会受到正比, 其他物理系统具有基于速
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；干摩擦，那么力太强了。当代价函数的梯度表示的力很小但非零时，由；于摩擦导致的恒力会使得粒子在达到局部极小点之前就停下来。黏性阻
          關鍵詞：当代价函数的梯度表示的力很小但非零时, 干摩擦, 黏性阻, 那么力太强了, 于摩擦导致的恒力会使得粒子在达到局部极小点之前就停下来
        - 摘要：8.3.3　Nesterov动量
          關鍵詞：动量
        - 摘要：受Nesterov加速梯度算法（Nesterov，1983，2004）启发，Sutskever；al. （2013）提出了动量算法的一个变种。这种情况的更新规则如下：
          關鍵詞：启发, 提出了动量算法的一个变种, 这种情况的更新规则如下, 加速梯度算法
        - 摘要：et
          關鍵詞：
        - 摘要：其中参数α和发挥了和标准动量方法中类似的作用。Nesterov动量和标准；动量之间的区别体现在梯度计算上。Nesterov动量中，梯度计算在施加；当前速度之后。因此，Nesterov动量可以解释为往标准动量方法中添加
          關鍵詞：动量和标准, 动量中, 因此, 和发挥了和标准动量方法中类似的作用, 当前速度之后
        - 摘要：算法8.3 　使用Nesterov动量的随机梯度下降（SGD）。
          關鍵詞：使用, 算法, 动量的随机梯度下降
        - 摘要：Require： 学习率  ，动量参数α
          關鍵詞：动量参数, 学习率
        - 摘要：Require： 初始参数 θ ，初始速度 ν
          關鍵詞：初始速度, 初始参数
        - 摘要：while 没有达到停止准则do
          關鍵詞：没有达到停止准则
        - 摘要：从训练集中采包含m个样本；标为y (i) 。
          關鍵詞：标为, 个样本, 从训练集中采包含
        - 摘要：应用临时更新：
          關鍵詞：应用临时更新
        - 摘要：计算梯度（在临时点）：
          關鍵詞：计算梯度, 在临时点
        - 摘要：的小批量，对应目
          關鍵詞：的小批量, 对应目
        - 摘要：计算速度更新：
          關鍵詞：计算速度更新
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：end while
          關鍵詞：
        - 摘要：在凸批量梯度的情况下，Nesterov动量将额外误差收敛率从O(1/k)（k步；后）改进到O(1/k 2 )，如Nesterov（1983）所示。可惜，在随机梯度的情；况下，Nesterov动量没有改进收敛率。
          關鍵詞：在随机梯度的情, 况下, 动量没有改进收敛率, 在凸批量梯度的情况下, 改进到
        - 摘要：8.3.1　随机梯度下降
          關鍵詞：随机梯度下降
        - 摘要：8.3.2　动量
          關鍵詞：动量
        - 摘要：8.3.3　Nesterov动量
          關鍵詞：动量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；8.4　参数初始化策略
          關鍵詞：参数初始化策略
    8.4：参数初始化策略
        - 摘要：有些优化算法本质上是非迭代的，只是求解一个解点。有些其他优化算；法本质上是迭代的，但是应用于这一类的优化问题时，能在可接受的时；间内收敛到可接受的解，并且与初始值无关。深度学习训练算法通常没
          關鍵詞：但是应用于这一类的优化问题时, 能在可接受的时, 有些其他优化算, 只是求解一个解点, 并且与初始值无关
        - 摘要：现代的初始化策略是简单的、启发式的。设定改进的初始化策略是一项；困难的任务，因为神经网络优化至今还未被很好地理解。大多数初始化；策略基于在神经网络初始化时实现一些很好的性质。然而，我们并没有
          關鍵詞：设定改进的初始化策略是一项, 大多数初始化, 策略基于在神经网络初始化时实现一些很好的性质, 我们并没有, 启发式的
        - 摘要：也许完全确知的唯一特性是初始参数需要在不同单元间“破坏对称性”。；如果具有相同激活函数的两个隐藏单元连接到相同的输入，那么这些单；元必须具有不同的初始参数。如果它们具有相同的初始参数，然后应用
          關鍵詞：如果它们具有相同的初始参数, 也许完全确知的唯一特性是初始参数需要在不同单元间, 元必须具有不同的初始参数, 然后应用, 如果具有相同激活函数的两个隐藏单元连接到相同的输入
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；单元。即使模型或训练算法能够使用随机性为不同的单元计算不同的更；新（例如使用Dropout的训练），通常来说，最好还是初始化每个单元
          關鍵詞：例如使用, 即使模型或训练算法能够使用随机性为不同的单元计算不同的更, 通常来说, 最好还是初始化每个单元, 的训练
        - 摘要：通常情况下，我们可以为每个单元的偏置设置启发式挑选的常数，仅随；机初始化权重。额外的参数（例如用于编码预测条件方差的参数）通常；和偏差一样设置为启发式选择的常数。
          關鍵詞：通常情况下, 机初始化权重, 仅随, 额外的参数, 例如用于编码预测条件方差的参数
        - 摘要：我们几乎总是初始化模型的权重为高斯或均匀分布中随机抽取的值。高；斯或均匀分布的选择似乎不会有很大的差别，但也没有被详尽地研究。；然而，初始分布的大小确实对优化过程的结果和网络泛化能力都有很大
          關鍵詞：斯或均匀分布的选择似乎不会有很大的差别, 但也没有被详尽地研究, 然而, 我们几乎总是初始化模型的权重为高斯或均匀分布中随机抽取的值, 初始分布的大小确实对优化过程的结果和网络泛化能力都有很大
        - 摘要：更大的初始权重具有更强的破坏对称性的作用，有助于避免冗余的单；元。它们也有助于避免在每层线性成分的前向或反向传播中丢失信号；——矩阵中更大的值在矩阵乘法中有更大的输出。如果初始权重太大，
          關鍵詞：更大的初始权重具有更强的破坏对称性的作用, 矩阵中更大的值在矩阵乘法中有更大的输出, 有助于避免冗余的单, 如果初始权重太大, 它们也有助于避免在每层线性成分的前向或反向传播中丢失信号
        - 摘要：关于如何初始化网络，正则化和优化有着非常不同的观点。优化观点建；议权重应该足够大以成功传播信息，但是正则化希望其小一点。诸如随；机梯度下降这类对权重较小的增量更新，趋于停止在更靠近初始参数的
          關鍵詞：机梯度下降这类对权重较小的增量更新, 关于如何初始化网络, 优化观点建, 诸如随, 议权重应该足够大以成功传播信息
        - 摘要：情况下，提前终止的梯度下降和权重衰减不同，但是提供了一个宽松的；类比去考虑初始化的影响。我们可以将初始化参数 θ  为  θ  0  类比于强置；均值为 θ 0 的高斯先验p( θ )。从这个角度来看，选择 θ 0 接近0是有道理
          關鍵詞：提前终止的梯度下降和权重衰减不同, 我们可以将初始化参数, 情况下, 类比于强置, 均值为
        - 摘要：有些启发式方法可用于选择权重的初始大小。一种初始化m个输入和n；输出的全连接层的权重的启发式方法是从分布
          關鍵詞：一种初始化, 输出的全连接层的权重的启发式方法是从分布, 有些启发式方法可用于选择权重的初始大小, 个输入和
        - 摘要：中采样权重，而Glorot and Bengio（2010）建
          關鍵詞：中采样权重
        - 摘要：议使用标准初始化 （normalized initialization）
          關鍵詞：议使用标准初始化
        - 摘要：后一种启发式方法初始化所有的层，折衷于使其具有相同激活方差和使；其具有相同梯度方差之间。这假设网络是不含非线性的链式矩阵乘法，；据此推导得出。现实的神经网络显然会违反这个假设，但很多设计于线
          關鍵詞：但很多设计于线, 折衷于使其具有相同激活方差和使, 这假设网络是不含非线性的链式矩阵乘法, 后一种启发式方法初始化所有的层, 其具有相同梯度方差之间
        - 摘要：Saxe  et  al.  （2013）推荐初始化为随机正交矩阵，仔细挑选负责每一层；非线性缩放或增益  （gain）因子g。他们得到了用于不同类型的非线性；激活函数的特定缩放因子。这种初始化方案也是启发于不含非线性的矩
          關鍵詞：推荐初始化为随机正交矩阵, 激活函数的特定缩放因子, 这种初始化方案也是启发于不含非线性的矩, 他们得到了用于不同类型的非线性, 仔细挑选负责每一层
        - 摘要：增加缩放因子g将网络推向网络前向传播时激活范数增加，反向传播时；梯度范数增加的区域。Sussillo（2014）表明，正确设置缩放因子足以训；练深达1000层的网络，而不需要使用正交初始化。这种方法的一个重要
          關鍵詞：将网络推向网络前向传播时激活范数增加, 梯度范数增加的区域, 反向传播时, 层的网络, 这种方法的一个重要
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；可惜，这些初始权重的最佳准则往往不会带来最佳效果。这可能有三种；不同的原因。首先，我们可能使用了错误的标准——它实际上并不利于
          關鍵詞：我们可能使用了错误的标准, 这些初始权重的最佳准则往往不会带来最佳效果, 它实际上并不利于, 这可能有三种, 不同的原因
        - 摘要：数值范围准则的一个缺点是，设置所有的初始权重具有相同的标准差，
          關鍵詞：数值范围准则的一个缺点是, 设置所有的初始权重具有相同的标准差
        - 摘要：例如
          關鍵詞：例如
        - 摘要：，会使得层很大时每个单一权重会变得极其小。
          關鍵詞：会使得层很大时每个单一权重会变得极其小
        - 摘要：Martens（2010）提出了一种被称为稀疏初始化  （sparse  initialization）；的替代方案，每个单元初始化为恰好有k个非零权重。这个想法保持该；单元输入的总数量独立于输入数目m，而不使单一权重元素的大小随m
          關鍵詞：个非零权重, 这个想法保持该, 而不使单一权重元素的大小随, 单元输入的总数量独立于输入数目, 提出了一种被称为稀疏初始化
        - 摘要：如果计算资源允许，将每层权重的初始数值范围设为超参数通常是个好；主意，使用第11.4.2节介绍的超参数搜索算法，如随机搜索，挑选这些；数值范围。是否选择使用密集或稀疏初始化也可以设为一个超参数。作
          關鍵詞：主意, 数值范围, 如随机搜索, 节介绍的超参数搜索算法, 使用第
        - 摘要：目前为止，我们关注在权重的初始化上。幸运的是，其他参数的初始化；通常更容易。
          關鍵詞：其他参数的初始化, 幸运的是, 目前为止, 我们关注在权重的初始化上, 通常更容易
        - 摘要：设置偏置的方法必须和设置权重的方法协调。设置偏置为零通常在大多
          關鍵詞：设置偏置为零通常在大多, 设置偏置的方法必须和设置权重的方法协调
        - 摘要：数权重初始化方案中是可行的。存在一些我们可能设置偏置为非零值的；情况：
          關鍵詞：存在一些我们可能设置偏置为非零值的, 数权重初始化方案中是可行的, 情况
        - 摘要：如果偏置是作为输出单元，那么初始化偏置以获取正确的输出边缘；统计通常是有利的。要做到这一点，我们假设初始权重足够小，该；单元的输出仅由偏置决定。这说明设置偏置为应用于训练集上输出
          關鍵詞：单元的输出仅由偏置决定, 统计通常是有利的, 我们假设初始权重足够小, 如果偏置是作为输出单元, 这说明设置偏置为应用于训练集上输出
        - 摘要：，那么我们可
          關鍵詞：那么我们可
        - 摘要：另一种常见类型的参数是方差或精确度参数。例如，我们用以下模型进；行带条件方差估计的线性回归
          關鍵詞：另一种常见类型的参数是方差或精确度参数, 行带条件方差估计的线性回归, 例如, 我们用以下模型进
        - 摘要：其中β是精确度参数。通常我们能安全地初始化方差或精确度参数为1。；另一种方法假设初始权重足够接近零，设置偏置可以忽略权重的影响，；然后设定偏置以产生输出的正确边缘均值，并将方差参数设置为训练集
          關鍵詞：设置偏置可以忽略权重的影响, 通常我们能安全地初始化方差或精确度参数为, 其中, 然后设定偏置以产生输出的正确边缘均值, 另一种方法假设初始权重足够接近零
        - 摘要：除了这些初始化模型参数的简单常数或随机方法，还有可能使用机器学；习初始化模型参数。在本书第3部分讨论的一个常用策略是使用相同的
          關鍵詞：除了这些初始化模型参数的简单常数或随机方法, 在本书第, 部分讨论的一个常用策略是使用相同的, 还有可能使用机器学, 习初始化模型参数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；输入数据集，用无监督模型训练出来的参数来初始化监督模型。我们也；可以在相关问题上使用监督训练。即使是在一个不相关的任务上运行监
          關鍵詞：可以在相关问题上使用监督训练, 输入数据集, 用无监督模型训练出来的参数来初始化监督模型, 我们也, 即使是在一个不相关的任务上运行监
    8.5：自适应学习率算法
        - 摘要：8.5.1　AdaGrad
          關鍵詞：
        - 摘要：8.5.2　RMSProp
          關鍵詞：
        - 摘要：8.5.3　Adam
          關鍵詞：
        - 摘要：8.5.4　选择正确的优化算法
          關鍵詞：选择正确的优化算法
        - 摘要：神经网络研究员早就意识到学习率肯定是难以设置的超参数之一，因为；它对模型的性能有显著的影响。正如我们在第4.3节和第8.2节中所探讨；的，损失通常高度敏感于参数空间中的某些方向，而不敏感于其他。动
          關鍵詞：因为, 它对模型的性能有显著的影响, 节和第, 神经网络研究员早就意识到学习率肯定是难以设置的超参数之一, 节中所探讨
        - 摘要：Delta-bar-delta  算法（Jacobs，1988）是一个早期的在训练时适应模型；参数各自学习率的启发式方法。该方法基于一个很简单的想法，如果损；失对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增
          關鍵詞：那么学习率应该增, 算法, 参数各自学习率的启发式方法, 该方法基于一个很简单的想法, 如果损
        - 摘要：最近，提出了一些增量（或者基于小批量）的算法来自适应模型参数的；学习率。这节将简要回顾其中一些算法。
          關鍵詞：这节将简要回顾其中一些算法, 的算法来自适应模型参数的, 提出了一些增量, 最近, 或者基于小批量
        - 摘要：8.5.1　AdaGrad
          關鍵詞：
        - 摘要：AdaGrad算法，如算法8.4所示，独立地适应所有模型参数的学习率，缩；放每个参数反比于其所有梯度历史平方值总和的平方根（Duchi et al. ，；2011）。具有损失最大偏导的参数相应地有一个快速下降的学习率，而
          關鍵詞：算法, 独立地适应所有模型参数的学习率, 具有损失最大偏导的参数相应地有一个快速下降的学习率, 放每个参数反比于其所有梯度历史平方值总和的平方根, 如算法
        - 摘要：在凸优化背景中，AdaGrad算法具有一些令人满意的理论性质。然而，
          關鍵詞：算法具有一些令人满意的理论性质, 然而, 在凸优化背景中
        - 摘要：经验上已经发现，对于训练深度神经网络模型而言，从训练开始时积累；梯度平方会导致有效学习率过早和过量的减小。AdaGrad在某些深度学；习模型上效果不错，但不是全部。
          關鍵詞：在某些深度学, 经验上已经发现, 但不是全部, 习模型上效果不错, 梯度平方会导致有效学习率过早和过量的减小
        - 摘要：算法8.4 　AdaGrad算法。
          關鍵詞：算法
        - 摘要：Require： 全局学习率
          關鍵詞：全局学习率
        - 摘要：Require： 初始参数 θ
          關鍵詞：初始参数
        - 摘要：Require： 小常数δ，为了数值稳定大约设为10 −7
          關鍵詞：为了数值稳定大约设为, 小常数
        - 摘要：初始化梯度累积变量 r ＝0
          關鍵詞：初始化梯度累积变量
        - 摘要：while 没有达到停止准则do
          關鍵詞：没有达到停止准则
        - 摘要：从训练集中采包含m个样本；标为  。
          關鍵詞：标为, 个样本, 从训练集中采包含
        - 摘要：计算梯度：
          關鍵詞：计算梯度
        - 摘要：累积平方梯度：
          關鍵詞：累积平方梯度
        - 摘要：的小批量，对应目
          關鍵詞：的小批量, 对应目
        - 摘要：计算更新：
          關鍵詞：计算更新
        - 摘要：（逐元素地应用除和求平方根）
          關鍵詞：逐元素地应用除和求平方根
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：end while
          關鍵詞：
        - 摘要：8.5.2　RMSProp
          關鍵詞：
        - 摘要：RMSProp  算法（Hinton，2012）修改AdaGrad以在非凸设定下效果更；好，改变梯度积累为指数加权的移动平均。AdaGrad旨在应用于凸问题；时快速收敛。当应用于非凸函数训练神经网络时，学习轨迹可能穿过了
          關鍵詞：算法, 修改, 学习轨迹可能穿过了, 改变梯度积累为指数加权的移动平均, 旨在应用于凸问题
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；很多不同的结构，最终到达一个局部是凸碗的区域。AdaGrad根据平方；梯度的整个历史收缩学习率，可能使得学习率在达到这样的凸结构前就
          關鍵詞：可能使得学习率在达到这样的凸结构前就, 最终到达一个局部是凸碗的区域, 梯度的整个历史收缩学习率, 很多不同的结构, 根据平方
        - 摘要：RMSProp的标准形式如算法8.5所示，结合Nesterov动量的形式如算法8.6；所示。相比于AdaGrad，使用移动平均引入了一个新的超参数ρ，用来控；制移动平均的长度范围。
          關鍵詞：使用移动平均引入了一个新的超参数, 动量的形式如算法, 相比于, 用来控, 制移动平均的长度范围
        - 摘要：算法8.5 　RMSProp算法。
          關鍵詞：算法
        - 摘要：Require： 全局学习率  ，衰减速率ρ
          關鍵詞：衰减速率, 全局学习率
        - 摘要：Require： 初始参数 θ
          關鍵詞：初始参数
        - 摘要：Require： 小常数δ，通常设为10 −6 （用于被小数除时的数值稳定）
          關鍵詞：通常设为, 用于被小数除时的数值稳定, 小常数
        - 摘要：初始化累积变量 r ＝0
          關鍵詞：初始化累积变量
        - 摘要：while 没有达到停止准则do
          關鍵詞：没有达到停止准则
        - 摘要：从训练集中采包含m个样本；标为  。
          關鍵詞：标为, 个样本, 从训练集中采包含
        - 摘要：计算梯度：
          關鍵詞：计算梯度
        - 摘要：累积平方梯度：
          關鍵詞：累积平方梯度
        - 摘要：的小批量，对应目
          關鍵詞：的小批量, 对应目
        - 摘要：计算参数更新：
          關鍵詞：计算参数更新
        - 摘要：（
          關鍵詞：
        - 摘要：逐元素应用）
          關鍵詞：逐元素应用
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：end while
          關鍵詞：
        - 摘要：算法8.6 　使用Nesterov动量的RMSProp算法。
          關鍵詞：使用, 算法, 动量的
        - 摘要：Require： 全局学习率  ，衰减速率ρ，动量系数α
          關鍵詞：衰减速率, 动量系数, 全局学习率
        - 摘要：Require： 初始参数 θ ，初始参数 ν
          關鍵詞：初始参数
        - 摘要：初始化累积变量 r ＝0
          關鍵詞：初始化累积变量
        - 摘要：while 没有达到停止准则do
          關鍵詞：没有达到停止准则
        - 摘要：从训练集中采包含m个样本；标为  。
          關鍵詞：标为, 个样本, 从训练集中采包含
        - 摘要：计算临时更新：
          關鍵詞：计算临时更新
        - 摘要：计算梯度：
          關鍵詞：计算梯度
        - 摘要：累积梯度：
          關鍵詞：累积梯度
        - 摘要：的小批量，对应目
          關鍵詞：的小批量, 对应目
        - 摘要：计算速度更新：
          關鍵詞：计算速度更新
        - 摘要：（  逐元素应用）
          關鍵詞：逐元素应用
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：end while
          關鍵詞：
        - 摘要：经验上，RMSProp已被证明是一种有效且实用的深度神经网络优化算；法。目前它是深度学习从业者经常采用的优化方法之一。
          關鍵詞：已被证明是一种有效且实用的深度神经网络优化算, 经验上, 目前它是深度学习从业者经常采用的优化方法之一
        - 摘要：8.5.3　Adam
          關鍵詞：
        - 摘要：Adam （Kingma and Ba，2014）是另一种学习率自适应的优化算法，如
          關鍵詞：是另一种学习率自适应的优化算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；算法8.7所示。“Adam”这个名字派生自短语“adaptive  moments”。早期算；法背景下，它也许最好被看作结合RMSProp和具有一些重要区别的动量
          關鍵詞：算法, 和具有一些重要区别的动量, 这个名字派生自短语, 早期算, 所示
        - 摘要：算法8.7 　Adam算法。
          關鍵詞：算法
        - 摘要：Require： 步长  （建议默认为：0.001）
          關鍵詞：建议默认为, 步长
        - 摘要：Require： 矩估计的指数衰减速率，ρ  1  和ρ  2  在区间［0,1）内。（建议；默认为：分别为0.9和0.999）
          關鍵詞：默认为, 分别为, 矩估计的指数衰减速率, 在区间, 建议
        - 摘要：Require： 用于数值稳定的小常数δ（建议默认为：10 −8 ）
          關鍵詞：用于数值稳定的小常数, 建议默认为
        - 摘要：Require： 初始参数 θ
          關鍵詞：初始参数
        - 摘要：初始化一阶和二阶矩变量 s ＝0， r ＝0
          關鍵詞：初始化一阶和二阶矩变量
        - 摘要：初始化时间步t＝0
          關鍵詞：初始化时间步
        - 摘要：while 没有达到停止准则do
          關鍵詞：没有达到停止准则
        - 摘要：从训练集中采包含m个样本；标为  。
          關鍵詞：标为, 个样本, 从训练集中采包含
        - 摘要：计算梯度：
          關鍵詞：计算梯度
        - 摘要：t←t＋1
          關鍵詞：
        - 摘要：的小批量，对应目
          關鍵詞：的小批量, 对应目
        - 摘要：更新有偏一阶矩估计：
          關鍵詞：更新有偏一阶矩估计
        - 摘要：更新有偏二阶矩估计：
          關鍵詞：更新有偏二阶矩估计
        - 摘要：修正一阶矩的偏差：
          關鍵詞：修正一阶矩的偏差
        - 摘要：修正二阶矩的偏差：
          關鍵詞：修正二阶矩的偏差
        - 摘要：计算更新：
          關鍵詞：计算更新
        - 摘要：（逐元素应用操作）
          關鍵詞：逐元素应用操作
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：end while
          關鍵詞：
        - 摘要：8.5.4　选择正确的优化算法
          關鍵詞：选择正确的优化算法
        - 摘要：在本节中，我们讨论了一系列算法，通过自适应每个模型参数的学习率；以解决优化深度模型中的难题。此时，一个自然的问题是：该选择哪种；算法呢？
          關鍵詞：算法呢, 在本节中, 通过自适应每个模型参数的学习率, 一个自然的问题是, 该选择哪种
        - 摘要：遗憾的是，目前在这一点上没有达成共识。Schaul  et  al.  （2014）展示；了许多优化算法在大量学习任务上极具价值的比较。虽然结果表明，具；有自适应学习率（以RMSProp和AdaDelta为代表）的算法族表现得相当
          關鍵詞：虽然结果表明, 了许多优化算法在大量学习任务上极具价值的比较, 的算法族表现得相当, 有自适应学习率, 目前在这一点上没有达成共识
        - 摘要：目前，最流行并且使用很高的优化算法包括SGD、具动量的SGD、；RMSProp、具动量的RMSProp、AdaDelta和Adam。此时，选择哪一个；算法似乎主要取决于使用者对算法的熟悉程度（以便调节超参数）。
          關鍵詞：具动量的, 选择哪一个, 以便调节超参数, 目前, 算法似乎主要取决于使用者对算法的熟悉程度
        - 摘要：8.5.1　AdaGrad
          關鍵詞：
        - 摘要：8.5.2　RMSProp
          關鍵詞：
        - 摘要：8.5.3　Adam
          關鍵詞：
        - 摘要：8.5.4　选择正确的优化算；法
          關鍵詞：选择正确的优化算
    8.6：二阶近似方法
        - 摘要：8.6.1　牛顿法
          關鍵詞：牛顿法
        - 摘要：8.6.2　共轭梯度
          關鍵詞：共轭梯度
        - 摘要：8.6.3　BFGS
          關鍵詞：
        - 摘要：在本节中，我们会讨论训练深度神经网络的二阶方法。参考LeCun et al.；（1998a）了解该问题的早期处理方法。为表述简单起见，我们只考察；目标函数为经验风险：
          關鍵詞：参考, 我们会讨论训练深度神经网络的二阶方法, 在本节中, 目标函数为经验风险, 为表述简单起见
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；然而，我们在这里讨论的方法很容易扩展到更一般的目标函数，例如，；第7章讨论的包括参数正则项的函数。
          關鍵詞：章讨论的包括参数正则项的函数, 我们在这里讨论的方法很容易扩展到更一般的目标函数, 然而, 例如
        - 摘要：8.6.1　牛顿法
          關鍵詞：牛顿法
        - 摘要：在第4.3节，我们介绍了二阶梯度方法。与一阶方法相比，二阶方法使；用二阶导数改进了优化。最广泛使用的二阶方法是牛顿法。我们现在更；详细地描述牛顿法，重点在其应用于神经网络的训练。
          關鍵詞：我们介绍了二阶梯度方法, 与一阶方法相比, 重点在其应用于神经网络的训练, 在第, 用二阶导数改进了优化
        - 摘要：牛顿法是基于二阶泰勒级数展开在某点  θ  0  附近来近似J  (  θ  )的优化方；法，其忽略了高阶导数：
          關鍵詞：牛顿法是基于二阶泰勒级数展开在某点, 的优化方, 其忽略了高阶导数, 附近来近似
        - 摘要：其中 H 是J相对于 θ 的Hessian矩阵在 θ  0  处的估计。如果我们再求解这；个函数的临界点，将得到牛顿参数更新规则：
          關鍵詞：处的估计, 如果我们再求解这, 相对于, 其中, 矩阵在
        - 摘要：因此，对于局部的二次函数（具有正定的  H  ），用  H  -1  重新调整梯；度，牛顿法会直接跳到极小值。如果目标函数是凸的但非二次的（有高；阶项），该更新将是迭代的，得到和牛顿法相关的算法，如算法8.8所
          關鍵詞：对于局部的二次函数, 牛顿法会直接跳到极小值, 具有正定的, 因此, 有高
        - 摘要：对于非二次的表面，只要Hessian矩阵保持正定，牛顿法能够迭代地应；用。这意味着一个两步迭代过程。首先，更新或计算Hessian逆（通过更；新二阶近似）。其次，根据式（8.27）更新参数。
          關鍵詞：其次, 对于非二次的表面, 矩阵保持正定, 牛顿法能够迭代地应, 这意味着一个两步迭代过程
        - 摘要：算法8.8 　目标为
          關鍵詞：算法, 目标为
        - 摘要：的牛顿法。
          關鍵詞：的牛顿法
        - 摘要：Require： 初始参数 θ 0
          關鍵詞：初始参数
        - 摘要：Require： 包含m个样本的训练集
          關鍵詞：包含, 个样本的训练集
        - 摘要：while 没有达到停止准则do
          關鍵詞：没有达到停止准则
        - 摘要：计算梯度：
          關鍵詞：计算梯度
        - 摘要：计算Hessian矩阵：
          關鍵詞：计算, 矩阵
        - 摘要：计算Hessian逆：
          關鍵詞：计算
        - 摘要：计算更新：
          關鍵詞：计算更新
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：end while
          關鍵詞：
        - 摘要：在第8.2.3节，我们讨论了牛顿法只适用于Hessian矩阵是正定的情况。在；深度学习中，目标函数的表面通常非凸（有很多特征），如鞍点。因此；使用牛顿法是有问题的。如果Hessian矩阵的特征值并不都是正的，例
          關鍵詞：如果, 我们讨论了牛顿法只适用于, 因此, 深度学习中, 使用牛顿法是有问题的
        - 摘要：这个正则化策略用于牛顿法的近似，例如Levenberg-Marquardt算法；（Levenberg，1944；Marquardt，1963），只要Hessian矩阵的负特征值；仍然相对接近零，效果就会很好。在曲率方向更极端的情况下，α的值
          關鍵詞：算法, 效果就会很好, 例如, 这个正则化策略用于牛顿法的近似, 在曲率方向更极端的情况下
        - 摘要：除了目标函数的某些特征带来的挑战，如鞍点，牛顿法用于训练大型神；经网络还受限于其显著的计算负担。Hessian矩阵中元素数目是参数数量；的平方，因此，如果参数数目为k（甚至是在非常小的神经网络中k也可
          關鍵詞：经网络还受限于其显著的计算负担, 如果参数数目为, 因此, 牛顿法用于训练大型神, 也可
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；能是百万级别），牛顿法需要计算k×k矩阵的逆，计算复杂度为O(k；)。另外，由于参数将每次更新都会改变，每次训练迭代都需要计算
          關鍵詞：由于参数将每次更新都会改变, 牛顿法需要计算, 每次训练迭代都需要计算, 矩阵的逆, 能是百万级别
        - 摘要：3
          關鍵詞：
        - 摘要：8.6.2　共轭梯度
          關鍵詞：共轭梯度
        - 摘要：共轭梯度是一种通过迭代下降的共轭方向  （conjugate  directions）以有；效避免Hessian矩阵求逆计算的方法。这种方法的灵感来自对最速下降方；法弱点的仔细研究（详细信息请查看第4.3节），其中线搜索迭代地用
          關鍵詞：效避免, 详细信息请查看第, 共轭梯度是一种通过迭代下降的共轭方向, 法弱点的仔细研究, 以有
        - 摘要：图8.6　将最速下降法应用于二次代价表面。在每个步骤中，最速下降法沿着由初始点处的梯度；定义的线跳到最低代价的点。这解决了图4.6中使用固定学习率所遇到的一些问题，但即使使用；最佳步长，算法仍然朝最优方向曲折前进。根据定义，在沿着给定方向的目标最小值处，最终
          關鍵詞：最速下降法沿着由初始点处的梯度, 最终, 定义的线跳到最低代价的点, 但即使使用, 这解决了图
        - 摘要：假设上一个搜索方向是
          關鍵詞：假设上一个搜索方向是
        - 摘要：处的方向导数为零：
          關鍵詞：处的方向导数为零
        - 摘要：度定义了当前的搜索方向，
          關鍵詞：度定义了当前的搜索方向
        - 摘要：。因此方向   正交于
          關鍵詞：正交于, 因此方向
        - 摘要：。在极小值处，线搜索终止，方向；。因为该点的梯；将不会贡献于方向
          關鍵詞：因为该点的梯, 在极小值处, 将不会贡献于方向, 方向, 线搜索终止
        - 摘要：和   之间的关系如图8.6所示。如图展示的那样，下降正交方；向的选择不会保持前一搜索方向上的最小值。这产生了锯齿形的过程。；在当前梯度方向下降到极小值，我们必须重新最小化之前梯度方向上的
          關鍵詞：之间的关系如图, 向的选择不会保持前一搜索方向上的最小值, 我们必须重新最小化之前梯度方向上的, 这产生了锯齿形的过程, 如图展示的那样
        - 摘要：在共轭梯度法中，我们寻求一个和先前线搜索方向共轭 （conjugate）的；搜索方向，即它不会撤销该方向上的进展。在训练迭代t时，下一步的；搜索方向 d t 的形式如下：
          關鍵詞：的形式如下, 我们寻求一个和先前线搜索方向共轭, 下一步的, 搜索方向, 即它不会撤销该方向上的进展
        - 摘要：其中，系数β  t  的大小控制我们应沿方向  d  t−1  加回多少到当前搜索方向；上。
          關鍵詞：加回多少到当前搜索方向, 其中, 系数, 的大小控制我们应沿方向
        - 摘要：如果
          關鍵詞：如果
        - 摘要：，其中  H  是Hessian矩阵，则两
          關鍵詞：则两, 矩阵, 其中
        - 摘要：个方向 d t 和 d t−1 被称为共轭的。
          關鍵詞：个方向, 被称为共轭的
        - 摘要：适应共轭的直接方法会涉及  H  特征向量的计算以选择β  t  。这将无法满；足我们的开发目标：寻找在大问题比牛顿法计算更加可行的方法。我们；能否不进行这些计算而得到共轭方向？幸运的是，这个问题的答案是肯
          關鍵詞：这个问题的答案是肯, 特征向量的计算以选择, 幸运的是, 我们, 能否不进行这些计算而得到共轭方向
        - 摘要：两种用于计算β t 的流行方法是
          關鍵詞：两种用于计算, 的流行方法是
        - 摘要：（1）Fletcher-Reeves：
          關鍵詞：
        - 摘要：（2）Polak-Ribi`ere：
          關鍵詞：
        - 摘要：对于二次曲面而言，共轭方向确保梯度沿着前一方向大小不变。因此，
          關鍵詞：共轭方向确保梯度沿着前一方向大小不变, 因此, 对于二次曲面而言
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；我们在前一方向上仍然是极小值。其结果是，在k-维参数空间中，共轭；梯度只需要至多k次线搜索就能达到极小值。共轭梯度算法如算法8.9所
          關鍵詞：其结果是, 我们在前一方向上仍然是极小值, 次线搜索就能达到极小值, 共轭梯度算法如算法, 维参数空间中
        - 摘要：算法8.9 　共轭梯度方法。
          關鍵詞：共轭梯度方法, 算法
        - 摘要：Require： 初始参数 θ 0
          關鍵詞：初始参数
        - 摘要：Require： 包含m个样本的训练集
          關鍵詞：包含, 个样本的训练集
        - 摘要：初始化 ρ 0 ＝0
          關鍵詞：初始化
        - 摘要：初始化g 0 ＝0
          關鍵詞：初始化
        - 摘要：初始化t＝1
          關鍵詞：初始化
        - 摘要：while 没有达到停止准则do
          關鍵詞：没有达到停止准则
        - 摘要：初始化梯度g t ＝0
          關鍵詞：初始化梯度
        - 摘要：计算梯度：
          關鍵詞：计算梯度
        - 摘要：计算
          關鍵詞：计算
        - 摘要：（非线性共轭梯度：视情况可重置β；时，如k＝5）
          關鍵詞：视情况可重置, 非线性共轭梯度
        - 摘要：t  为零，例如t是常数k的倍数
          關鍵詞：为零, 的倍数, 例如, 是常数
        - 摘要：计算搜索方向：
          關鍵詞：计算搜索方向
        - 摘要：执行线搜索寻找：
          關鍵詞：执行线搜索寻找
        - 摘要：（对于真正二次的代价函数，存在   的解析解，而无须显式地搜
          關鍵詞：对于真正二次的代价函数, 的解析解, 而无须显式地搜, 存在
        - 摘要：索）
          關鍵詞：
        - 摘要：应用更新：
          關鍵詞：应用更新
        - 摘要：t←t＋1
          關鍵詞：
        - 摘要：end while
          關鍵詞：
        - 摘要：非线性共轭梯度：  目前，我们已经讨论了用于二次目标函数的共轭梯；度法。当然，本章我们主要关注于探索训练神经网络和其他相关深度学；习模型的优化方法，其对应的目标函数比二次函数复杂得多。或许令人
          關鍵詞：本章我们主要关注于探索训练神经网络和其他相关深度学, 其对应的目标函数比二次函数复杂得多, 习模型的优化方法, 或许令人, 目前
        - 摘要：实践者报告，在实践中使用非线性共轭梯度算法训练神经网络是合理；的，尽管在开始非线性共轭梯度前使用随机梯度下降迭代若干步来初始；化效果更好。另外，尽管（非线性）共轭梯度算法传统上作为批方法，
          關鍵詞：共轭梯度算法传统上作为批方法, 实践者报告, 化效果更好, 在实践中使用非线性共轭梯度算法训练神经网络是合理, 非线性
        - 摘要：8.6.3　BFGS
          關鍵詞：
        - 摘要：Broyden-Fletcher-Goldfarb-Shanno（BFGS）  算法具有牛顿法的一些；优点，但没有牛顿法的计算负担。在这方面，BFGS和CG很像。然而，；BFGS使用了一个更直接的方法近似牛顿更新。回顾牛顿更新，由下式
          關鍵詞：回顾牛顿更新, 但没有牛顿法的计算负担, 然而, 优点, 使用了一个更直接的方法近似牛顿更新
        - 摘要：其中， H 是J相对于  θ 的Hessian矩阵在 θ  0  处的估计。运用牛顿法的主；要计算难点在于计算Hessian逆 H  -1  。拟牛顿法所采用的方法（BFGS是；其中最突出的）是使用矩阵  M  t  近似逆，迭代地低秩更新精度以更好地
          關鍵詞：处的估计, 相对于, 拟牛顿法所采用的方法, 其中, 矩阵在
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；近似 H -1 。
          關鍵詞：近似
        - 摘要：BFGS近似的说明和推导出现在很多关于优化的教科书中，包括；Luenberger（1984）。
          關鍵詞：包括, 近似的说明和推导出现在很多关于优化的教科书中
        - 摘要：当Hessian逆近似 M t 更新时，下降方向 ρ t 为 ρ t =M t g t 。该方向上的线；搜索用于决定该方向上的步长  。参数的最后更新为
          關鍵詞：参数的最后更新为, 更新时, 搜索用于决定该方向上的步长, 该方向上的线, 逆近似
        - 摘要：和共轭梯度法相似，BFGS算法迭代一系列线搜索，其方向含二阶信；息。然而和共轭梯度不同的是，该方法的成功并不严重依赖于线搜索寻；找该方向上和真正极小值很近的一点。因此，相比于共轭梯度，BFGS
          關鍵詞：然而和共轭梯度不同的是, 算法迭代一系列线搜索, 相比于共轭梯度, 因此, 该方法的成功并不严重依赖于线搜索寻
        - 摘要：存储受限的BFGS（或L-BFGS）  　通过避免存储完整的Hessian逆近似；M  ，BFGS算法的存储代价可以显著降低。L-BFGS算法使用和BFGS算；法相同的方法计算  M  的近似，但起始假设是  M  (t−1)  是单位矩阵，而不
          關鍵詞：但起始假设是, 算法的存储代价可以显著降低, 的近似, 存储受限的, 是单位矩阵
        - 摘要：8.6.1　牛顿法
          關鍵詞：牛顿法
        - 摘要：8.6.2　共轭梯度
          關鍵詞：共轭梯度
        - 摘要：8.6.3　BFGS
          關鍵詞：
    8.7：优化策略和元算法
        - 摘要：8.7.1　批标准化
          關鍵詞：批标准化
        - 摘要：8.7.2　坐标下降
          關鍵詞：坐标下降
        - 摘要：8.7.3　Polyak平均
          關鍵詞：平均
        - 摘要：8.7.4　监督预训练
          關鍵詞：监督预训练
        - 摘要：8.7.5　设计有助于优化的模型
          關鍵詞：设计有助于优化的模型
        - 摘要：8.7.6　延拓法和课程学习
          關鍵詞：延拓法和课程学习
        - 摘要：许多优化技术并非真正的算法，而是一般化的模板，可以特定地产生算；法，或是并入到很多不同的算法中。
          關鍵詞：而是一般化的模板, 可以特定地产生算, 或是并入到很多不同的算法中, 许多优化技术并非真正的算法
        - 摘要：8.7.1　批标准化
          關鍵詞：批标准化
        - 摘要：批标准化（Ioffe  and  Szegedy，2015）是优化深度神经网络中最激动人；心的最新创新之一。实际上它并不是一个优化算法，而是一个自适应的；重参数化的方法，试图解决训练非常深的模型的困难。
          關鍵詞：是优化深度神经网络中最激动人, 试图解决训练非常深的模型的困难, 实际上它并不是一个优化算法, 批标准化, 心的最新创新之一
        - 摘要：非常深的模型会涉及多个函数或层组合。在其他层不改变的假设下，梯；度用于如何更新每一个参数。在实践中，我们同时更新所有层。当我们；进行更新时，可能会发生一些意想不到的结果，这是因为许多组合在一
          關鍵詞：这是因为许多组合在一, 进行更新时, 我们同时更新所有层, 可能会发生一些意想不到的结果, 度用于如何更新每一个参数
        - 摘要：。想想我们在更新；什么。近似   的一阶泰勒级数会预测   的值下降；望   下降0.1，那么梯度中的一阶信息表明我们应设置学习率   为
          關鍵詞：什么, 的值下降, 下降, 的一阶泰勒级数会预测, 近似
        - 摘要：这个更新中所产生的一个二阶项示例是
          關鍵詞：这个更新中所产生的一个二阶项示例是
        - 摘要：。如果
          關鍵詞：如果
        - 摘要：很小，那么该项可以忽略不计。而如果层3到层
          關鍵詞：而如果层, 那么该项可以忽略不计, 到层, 很小
        - 摘要：l的权重都比1大时，该项可能会指数级大。这使得我们很难选择一个合；适的学习率，因为某一层中参数更新的效果很大程度上取决于其他所有；层。二阶优化算法通过考虑二阶相互影响来解决这个问题，但我们可以
          關鍵詞：大时, 二阶优化算法通过考虑二阶相互影响来解决这个问题, 适的学习率, 的权重都比, 这使得我们很难选择一个合
        - 摘要：批标准化提出了一种几乎可以重参数化所有深度网络的优雅方法。重参；数化显著减少了多层之间协调更新的问题。批标准化可应用于网络的任；何输入层或隐藏层。设  H  是需要标准化的某层的小批量激活函数，排
          關鍵詞：是需要标准化的某层的小批量激活函数, 重参, 何输入层或隐藏层, 批标准化提出了一种几乎可以重参数化所有深度网络的优雅方法, 数化显著减少了多层之间协调更新的问题
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；其中 µ 是包含每个单元均值的向量， σ 是包含每个单元标准差的向量。；此处的算术是基于广播向量  µ 和向量  σ  应用于矩阵  H  的每一行。在每
          關鍵詞：是包含每个单元均值的向量, 和向量, 其中, 应用于矩阵, 在每
        - 摘要：在训练阶段，
          關鍵詞：在训练阶段
        - 摘要：和
          關鍵詞：
        - 摘要：其中δ是个很小的正值，比如10  −8  ，以强制避免遇到   的梯度在z＝0；处未定义的问题。至关重要的是，我们反向传播这些操作，来计算均值；和标准差，并应用它们于标准化  H  。这意味着，梯度不会再简单地增
          關鍵詞：和标准差, 是个很小的正值, 处未定义的问题, 以强制避免遇到, 其中
        - 摘要：在测试阶段， µ 和 σ 可以被替换为训练阶段收集的运行均值。这使得模；型可以对单一样本评估，而无须使用定义于整个小批量的 µ 和 σ 。
          關鍵詞：型可以对单一样本评估, 可以被替换为训练阶段收集的运行均值, 在测试阶段, 这使得模, 而无须使用定义于整个小批量的
        - 摘要：回顾例子；很大程度地解决学习这个模型的问题。假设x采样自一个单位高斯，那；么h  l−1 也是来自高斯，因为从x到h  l  的变换是线性的。然而，h  l−1  不再
          關鍵詞：然而, 采样自一个单位高斯, 不再, 也是来自高斯, 的变换是线性的
        - 摘要：，我们看到，可以通过标准化h  l−1
          關鍵詞：可以通过标准化, 我们看到
        - 摘要：了零均值和单位方差的特性。对于底层的几乎任意更新而言，；然保持着单位高斯。然后输出
          關鍵詞：了零均值和单位方差的特性, 然后输出, 对于底层的几乎任意更新而言, 然保持着单位高斯
        - 摘要：仍；可以学习为一个简单的线性函数；。现在学习这个模型非常简单，因为低层的参数在大多数
          關鍵詞：因为低层的参数在大多数, 现在学习这个模型非常简单, 可以学习为一个简单的线性函数
        - 摘要：由于网络的最后一层能够学习线性变换，实际上我们可能希望移除一层；内单元之间的所有线性关系。事实上，这是Guillaume；Desjardins（2015）中采用的方法，为批标准化提供了灵感。令人遗憾
          關鍵詞：由于网络的最后一层能够学习线性变换, 事实上, 这是, 内单元之间的所有线性关系, 令人遗憾
        - 摘要：，而不是简单地使用标准化的
          關鍵詞：而不是简单地使用标准化的
        - 摘要：标准化一个单元的均值和标准差会降低包含该单元的神经网络的表达能；力。为了保持网络的表现力，通常会将批量隐藏单元激活  H  替换为；。变量γ和β是允许新变
          關鍵詞：标准化一个单元的均值和标准差会降低包含该单元的神经网络的表达能, 通常会将批量隐藏单元激活, 为了保持网络的表现力, 替换为, 是允许新变
        - 摘要：大多数神经网络层会采取φ ( X W +b ) 的形式，其中φ是某个固定的非线；性激活函数，如整流线性变换。自然想到我们应该将批标准化应用于输；入 X 还是变换后的值 X W +b 。Ioffe and Szegedy（2015）推荐后者。更
          關鍵詞：如整流线性变换, 自然想到我们应该将批标准化应用于输, 其中, 大多数神经网络层会采取, 是某个固定的非线
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；具体地讲， X W +b 应替换为 X W 的标准化形式。偏置项应被忽略，因；为参数β  会加入批标准化重参数化，它是冗余的。一层的输入通常是前
          關鍵詞：具体地讲, 会加入批标准化重参数化, 的标准化形式, 偏置项应被忽略, 为参数
        - 摘要：第9章所述的卷积网络，在特征映射中每个空间位置同样地标准化μ和σ；是很重要的，能使特征映射的统计量不因空间位置而保持相同。
          關鍵詞：在特征映射中每个空间位置同样地标准化, 能使特征映射的统计量不因空间位置而保持相同, 章所述的卷积网络, 是很重要的
        - 摘要：8.7.2　坐标下降
          關鍵詞：坐标下降
        - 摘要：在某些情况下，将一个优化问题分解成几个部分，可以更快地解决原问；题。如果我们相对于某个单一变量x i 最小化f( x )，然后相对于另一个变；量x j 等等，反复循环所有的变量，我们会保证到达（局部）极小值。这
          關鍵詞：如果我们相对于某个单一变量, 反复循环所有的变量, 局部, 然后相对于另一个变, 可以更快地解决原问
        - 摘要：当优化问题中的不同变量能够清楚地分成相对独立的组，或是当优化一；组变量明显比优化所有变量效率更高时，坐标下降最有意义。例如，考；虑代价函数
          關鍵詞：坐标下降最有意义, 例如, 当优化问题中的不同变量能够清楚地分成相对独立的组, 虑代价函数, 组变量明显比优化所有变量效率更高时
        - 摘要：该函数描述了一种被称为稀疏编码的学习问题，其目标是寻求一个权重；矩阵 W ，可以线性解码激活值矩阵 H 以重构训练集 X 。稀疏编码的大；多数应用还涉及权重衰减或 W 列范数的约束，以避免极小 H 和极大 W
          關鍵詞：可以线性解码激活值矩阵, 稀疏编码的大, 列范数的约束, 和极大, 矩阵
        - 摘要：函数J不是凸的。然而，我们可以将训练算法的输入分成两个集合：字；典参数 W 和编码表示  H  。最小化关于这两者之一的任意一组变量的目；标函数都是凸问题。因此，块坐标下降允许我们使用高效的凸优化算
          關鍵詞：典参数, 标函数都是凸问题, 函数, 因此, 块坐标下降允许我们使用高效的凸优化算
        - 摘要：当一个变量的值很大程度地影响另一个变量的最优值时，坐标下降不是
          關鍵詞：坐标下降不是, 当一个变量的值很大程度地影响另一个变量的最优值时
        - 摘要：一个很好的方法，如函数；中α是正值常数。第一项鼓励两个变量具有相似的值，而第二项鼓励它；们接近零。解是两者都为零。牛顿法可以一步解决这个问题，因为它是
          關鍵詞：们接近零, 因为它是, 牛顿法可以一步解决这个问题, 一个很好的方法, 是正值常数
        - 摘要：，其
          關鍵詞：
        - 摘要：8.7.3　Polyak平均
          關鍵詞：平均
        - 摘要：Polyak平均（Polyak  and  Juditsky，1992）会平均优化算法在参数空间访；，那；问轨迹中的几个点。如果t次迭代梯度下降访问了点
          關鍵詞：如果, 平均, 问轨迹中的几个点, 会平均优化算法在参数空间访, 次迭代梯度下降访问了点
        - 摘要：么Polyak平均算法的输出是
          關鍵詞：平均算法的输出是
        - 摘要：。在某些
          關鍵詞：在某些
        - 摘要：问题中，如梯度下降应用于凸问题时，这种方法具有较强的收敛保证。；当应用于神经网络时，其验证更多是启发式的，但在实践中表现良好。；基本想法是，优化算法可能会来回穿过山谷好几次而没经过山谷底部附
          關鍵詞：问题中, 但在实践中表现良好, 优化算法可能会来回穿过山谷好几次而没经过山谷底部附, 其验证更多是启发式的, 当应用于神经网络时
        - 摘要：在非凸问题中，优化轨迹的路径可以非常复杂，并且经过了许多不同的；区域。包括参数空间中遥远过去的点，可能与当前点在代价函数上相隔；很大的障碍，看上去不像一个有用的行为。其结果是，当应用Polyak平
          關鍵詞：当应用, 其结果是, 区域, 并且经过了许多不同的, 优化轨迹的路径可以非常复杂
        - 摘要：这个计算平均值的方法被用于大量数值应用中。最近的例子请查阅；Szegedy et al. （2015）。
          關鍵詞：这个计算平均值的方法被用于大量数值应用中, 最近的例子请查阅
        - 摘要：8.7.4　监督预训练
          關鍵詞：监督预训练
        - 摘要：有时，如果模型太复杂难以优化或是任务非常困难，直接训练模型来解；决特定任务的挑战可能太大。有时训练一个较简单的模型来求解问题，；然后使模型更复杂会更有效。训练模型来求解一个简化的问题，然后转
          關鍵詞：有时训练一个较简单的模型来求解问题, 如果模型太复杂难以优化或是任务非常困难, 训练模型来求解一个简化的问题, 有时, 直接训练模型来解
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；贪心算法 （greedy algorithm）将问题分解成许多部分，然后独立地在每；个部分求解最优值。令人遗憾的是，结合各个最佳的部分不能保证得到
          關鍵詞：个部分求解最优值, 将问题分解成许多部分, 贪心算法, 然后独立地在每, 结合各个最佳的部分不能保证得到
        - 摘要：预训练算法，特别是贪心预训练，在深度学习中是普遍存在的。在本节；中，我们会具体描述这些将监督学习问题分解成其他简化的监督学习问；题的预训练算法。这种方法被称为贪心监督预训练  （greedy  supervised
          關鍵詞：这种方法被称为贪心监督预训练, 我们会具体描述这些将监督学习问题分解成其他简化的监督学习问, 在深度学习中是普遍存在的, 题的预训练算法, 在本节
        - 摘要：在贪心监督预训练的原始版本（Bengio et al.  ，2007c）中，每个阶段包；括一个仅涉及最终神经网络的子集层的监督学习训练任务。贪心监督预；训练的一个例子如图8.7所示，其中每个附加的隐藏层作为浅层监督多
          關鍵詞：括一个仅涉及最终神经网络的子集层的监督学习训练任务, 每个阶段包, 贪心监督预, 训练的一个例子如图, 在贪心监督预训练的原始版本
        - 摘要：图8.7　一种形式的贪心监督预训练的示意图（Bengio et al. ，2007a）。（a）我们从训练一个足；够浅的架构开始。（b）同一个架构的另一描绘。（c）我们只保留原始网络的输入到隐藏层，；并丢弃隐藏到输出层。我们将第一层隐藏层的输出作为输入发送到另一监督单隐层MLP（使用
          關鍵詞：同一个架构的另一描绘, 我们从训练一个足, 我们只保留原始网络的输入到隐藏层, 一种形式的贪心监督预训练的示意图, 我们将第一层隐藏层的输出作为输入发送到另一监督单隐层
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；（d）所得架构的另一种描绘，可视为前馈网络。为了进一步改进优化，我们可以联合地精调所；有层（仅在该过程的结束或者该过程的每个阶段）
          關鍵詞：为了进一步改进优化, 我们可以联合地精调所, 可视为前馈网络, 所得架构的另一种描绘, 仅在该过程的结束或者该过程的每个阶段
        - 摘要：为什么贪心监督预训练会有帮助呢？最初由Bengio et al. （2007d）提出；的假说是，其有助于更好地指导深层结构的中间层的学习。一般情况；下，预训练对于优化和泛化都是有帮助的。
          關鍵詞：提出, 的假说是, 最初由, 为什么贪心监督预训练会有帮助呢, 一般情况
        - 摘要：另一个与监督预训练有关的方法扩展了迁移学习的想法：Yosinski  et  al.；（2014）在一组任务上预训练了8层权重的深度卷积网络（1000个；ImageNet对象类的子集），然而用该网络的前k层初始化同样规模的网
          關鍵詞：然而用该网络的前, 层初始化同样规模的网, 另一个与监督预训练有关的方法扩展了迁移学习的想法, 在一组任务上预训练了, 层权重的深度卷积网络
        - 摘要：另一条相关的工作线是FitNets （Romero et al. ，2015）方法。这种方法；始于训练深度足够低和宽度足够大（每层单元数），容易训练的网络。；然后，这个网络成为第二个网络（被指定为学生  ）的老师  。学生网络
          關鍵詞：每层单元数, 的老师, 这种方法, 容易训练的网络, 被指定为学生
        - 摘要：8.7.5　设计有助于优化的模型
          關鍵詞：设计有助于优化的模型
        - 摘要：改进优化的最好方法并不总是改进优化算法。相反，深度模型中优化的；许多改进来自设计易于优化的模型。
          關鍵詞：改进优化的最好方法并不总是改进优化算法, 许多改进来自设计易于优化的模型, 相反, 深度模型中优化的
        - 摘要：原则上，我们可以使用呈锯齿非单调模式上上下下的激活函数，但是，；这将使优化极为困难。在实践中，选择一族容易优化的模型比使用一个
          關鍵詞：但是, 在实践中, 原则上, 我们可以使用呈锯齿非单调模式上上下下的激活函数, 这将使优化极为困难
        - 摘要：强大的优化算法更重要。神经网络学习在过去30年的大多数进步主要来；自改变模型族，而非改变优化过程。20世纪80年代用于训练神经网络的；带动量的随机梯度下降，仍然是现代神经网络应用中的前沿算法。
          關鍵詞：世纪, 仍然是现代神经网络应用中的前沿算法, 年的大多数进步主要来, 年代用于训练神经网络的, 强大的优化算法更重要
        - 摘要：具体来说，现代神经网络的设计选择体现在层之间的线性变换，几乎处；处可导的激活函数，和大部分定义域都有明显的梯度。特别是，创新的；模型，如LSTM、整流线性单元和maxout单元都比先前的模型（如基于
          關鍵詞：创新的, 几乎处, 特别是, 和大部分定义域都有明显的梯度, 现代神经网络的设计选择体现在层之间的线性变换
        - 摘要：其他的模型设计策略有助于使优化更简单。例如，层之间的线性路径或；是跳跃连接减少了从较低层参数到输出最短路径的长度，因而缓解了梯；度消失的问题（Srivastava  et  al.  ，2015）。一个和跳跃连接相关的想法
          關鍵詞：是跳跃连接减少了从较低层参数到输出最短路径的长度, 其他的模型设计策略有助于使优化更简单, 层之间的线性路径或, 度消失的问题, 因而缓解了梯
        - 摘要：8.7.6　延拓法和课程学习
          關鍵詞：延拓法和课程学习
        - 摘要：正如第8.2.7节探讨的，许多优化挑战都来自代价函数的全局结构，不能；仅通过局部更新方向上更好的估计来解决。解决这个问题的主要方法是；尝试初始化参数到某种区域内，该区域可以通过局部下降很快连接到参
          關鍵詞：许多优化挑战都来自代价函数的全局结构, 正如第, 节探讨的, 该区域可以通过局部下降很快连接到参, 仅通过局部更新方向上更好的估计来解决
        - 摘要：延拓法  （continuation  method）是一族通过挑选初始点使优化更容易的；方法，以确保局部优化花费大部分时间在表现良好的空间。延拓法的背；后想法是构造一系列具有相同参数的目标函数。为了最小化代价函数
          關鍵詞：延拓法, 为了最小化代价函数, 后想法是构造一系列具有相同参数的目标函数, 是一族通过挑选初始点使优化更容易的, 以确保局部优化花费大部分时间在表现良好的空间
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；，我们构建新的代价函数
          關鍵詞：我们构建新的代价函数
        - 摘要：。这些代价函数的；难度逐步提高，其中J  (0)  是最容易最小化的，J  (n)  是最难的，真正的代；价函数驱动整个过程。当我们说J (i) 比J (i＋1) 更容易时，是指其在更多的
          關鍵詞：是最容易最小化的, 是指其在更多的, 价函数驱动整个过程, 真正的代, 其中
        - 摘要：传统的延拓法（用于神经网络训练之前的延拓法）通常基于平滑目标函；数。读者可以查看Wu（1997）了解这类方法的示例，以及一些相关方；法的综述。延拓法也和参数中加入噪声的模拟退火紧密相关
          關鍵詞：法的综述, 延拓法也和参数中加入噪声的模拟退火紧密相关, 了解这类方法的示例, 通常基于平滑目标函, 读者可以查看
        - 摘要：et
          關鍵詞：
        - 摘要：传统上，延拓法主要用来克服局部极小值的问题。具体地，它被设计用；来在有很多局部极小值的情况下，求解一个全局最小点。这些连续方法；会通过“模糊”原来的代价函数来构建更容易的代价函数。这些模糊操作
          關鍵詞：这些连续方法, 传统上, 这些模糊操作, 它被设计用, 求解一个全局最小点
        - 摘要：这个方法的直觉是有些非凸函数在模糊后会近似凸的。在许多情况下，；这种模糊保留了关于全局极小值的足够信息，我们可以通过逐步求解模；糊更少的问题来求解全局极小值。这种方法有三种可能失败的方式。首
          關鍵詞：这个方法的直觉是有些非凸函数在模糊后会近似凸的, 这种方法有三种可能失败的方式, 在许多情况下, 这种模糊保留了关于全局极小值的足够信息, 我们可以通过逐步求解模
        - 摘要：。其二，函数可能在模糊后是凸的，但模糊函数的最；小值可能会追踪到一个局部最小值，而非原始代价函数的全局最小值。
          關鍵詞：但模糊函数的最, 其二, 而非原始代价函数的全局最小值, 小值可能会追踪到一个局部最小值, 函数可能在模糊后是凸的
        - 摘要：尽管延拓法最初用来解决局部最小值的问题，而局部最小值已不再认为；是神经网络优化中的主要问题了。幸运的是，延拓法仍然有所帮助。延；拓法引入的简化目标函数能够消除平坦区域，减少梯度估计的方差，提
          關鍵詞：拓法引入的简化目标函数能够消除平坦区域, 幸运的是, 是神经网络优化中的主要问题了, 尽管延拓法最初用来解决局部最小值的问题, 减少梯度估计的方差
        - 摘要：高Hessian矩阵的条件数，使局部更新更容易计算，或是改进局部更新方；向与朝向全局解方向之间的对应关系。
          關鍵詞：矩阵的条件数, 向与朝向全局解方向之间的对应关系, 或是改进局部更新方, 使局部更新更容易计算
        - 摘要：Bengio et al. （2009）指出被称为课程学习 （curriculum  learning）或者；塑造  （shaping）的方法可以被解释为延拓法。课程学习基于规划学习；过程的想法，首先学习简单的概念，然后逐步学习依赖于这些简化概念
          關鍵詞：过程的想法, 然后逐步学习依赖于这些简化概念, 首先学习简单的概念, 塑造, 或者
        - 摘要：课程学习研究的另一个重要贡献体现在训练循环神经网络捕获长期依；赖：Zaremba  and  Sutskever（2014）发现使用随机课程获得了更好的结；果，其中容易和困难的示例混合在一起，随机提供给学习者，更难示例
          關鍵詞：发现使用随机课程获得了更好的结, 其中容易和困难的示例混合在一起, 课程学习研究的另一个重要贡献体现在训练循环神经网络捕获长期依, 更难示例, 随机提供给学习者
        - 摘要：现在我们已经介绍了一些基本的神经网络模型，以及如何进行正则化和；优化。在接下来的章节中，我们转向特化的神经网络家族，允许其扩展；到能够处理很大规模的数据和具有特殊结构的数据。在本章中讨论的优
          關鍵詞：以及如何进行正则化和, 现在我们已经介绍了一些基本的神经网络模型, 允许其扩展, 在接下来的章节中, 到能够处理很大规模的数据和具有特殊结构的数据
        - 摘要：8.7.1　批标准化
          關鍵詞：批标准化
        - 摘要：8.7.2　坐标下降
          關鍵詞：坐标下降
        - 摘要：8.7.3　Polyak平均
          關鍵詞：平均
        - 摘要：8.7.4　监督预训练
          關鍵詞：监督预训练
        - 摘要：8.7.5　设计有助于优化的；模型
          關鍵詞：模型, 设计有助于优化的
        - 摘要：8.7.6　延拓法和课程学习
          關鍵詞：延拓法和课程学习
第9章：卷积网络
    8.7：优化策略和元算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；卷积网络 （convolutional network）（LeCun，1989），也叫作卷积神经；网络 （convolutional neural network，CNN），是一种专门用来处理具有
          關鍵詞：也叫作卷积神经, 卷积网络, 是一种专门用来处理具有, 网络
        - 摘要：本章我们首先说明什么是卷积运算，接着会解释在神经网络中使用卷积；运算的动机，然后会介绍池化  （pooling）。池化是一种几乎所有的卷；积网络都会用到的操作。通常来说，卷积神经网络中用到的卷积运算和
          關鍵詞：积网络都会用到的操作, 通常来说, 然后会介绍池化, 运算的动机, 卷积神经网络中用到的卷积运算和
    9.1：卷积运算
        - 摘要：在通常形式中，卷积是对两个实变函数的一种数学运算 (1) 。为了给出卷；积的定义，我们从两个可能会用到的函数的例子出发。
          關鍵詞：卷积是对两个实变函数的一种数学运算, 积的定义, 我们从两个可能会用到的函数的例子出发, 为了给出卷, 在通常形式中
        - 摘要：假设我们正在用激光传感器追踪一艘宇宙飞船的位置。我们的激光传感；器给出一个单独的输出x（t），表示宇宙飞船在时刻t的位置。x和t都是；实值的，这意味着我们可以在任意时刻从传感器中读出飞船的位置。
          關鍵詞：实值的, 的位置, 这意味着我们可以在任意时刻从传感器中读出飞船的位置, 我们的激光传感, 表示宇宙飞船在时刻
        - 摘要：现在假设我们的传感器受到一定程度的噪声干扰。为了得到飞船位置的；低噪声估计，我们对得到的测量结果进行平均。显然，时间上越近的测
          關鍵詞：显然, 为了得到飞船位置的, 现在假设我们的传感器受到一定程度的噪声干扰, 低噪声估计, 我们对得到的测量结果进行平均
        - 摘要：量结果越相关，所以我们采用一种加权平均的方法，对于最近的测量结；果赋予更高的权重。我们可以采用一个加权函数w(a)来实现，其中a表；示测量结果距当前时刻的时间间隔。如果我们对任意时刻都采用这种加
          關鍵詞：量结果越相关, 对于最近的测量结, 我们可以采用一个加权函数, 如果我们对任意时刻都采用这种加, 所以我们采用一种加权平均的方法
        - 摘要：这种运算就叫作卷积 （convolution）。卷积运算通常用星号表示：
          關鍵詞：这种运算就叫作卷积, 卷积运算通常用星号表示
        - 摘要：在我们的例子中，w必须是一个有效的概率密度函数，否则输出就不再；是一个加权平均。另外，在参数为负值时，w的取值必须为0，否则它；会预测到未来，这不是我们能够推测得了的。但这些限制仅仅是对我们
          關鍵詞：这不是我们能够推测得了的, 在我们的例子中, 的取值必须为, 会预测到未来, 但这些限制仅仅是对我们
        - 摘要：在卷积网络的术语中，卷积的第一个参数（在这个例子中，函数x）通；常叫作输入  （input），第二个参数（函数w）叫作核函数  （kernel；function）。输出有时被称作特征映射 （feature map）。
          關鍵詞：常叫作输入, 叫作核函数, 函数, 在这个例子中, 第二个参数
        - 摘要：在本例中，激光传感器在每个瞬间反馈测量结果的想法是不切实际的。；一般来讲，当我们用计算机处理数据时，时间会被离散化，传感器会定；期地反馈数据。所以在我们的例子中，假设传感器每秒反馈一次测量结
          關鍵詞：所以在我们的例子中, 当我们用计算机处理数据时, 在本例中, 期地反馈数据, 假设传感器每秒反馈一次测量结
        - 摘要：在机器学习的应用中，输入通常是多维数组的数据，而核通常是由学习；算法优化得到的多维数组的参数。我们把这些多维数组叫作张量。因为；在输入与核中的每一个元素都必须明确地分开存储，我们通常假设在存
          關鍵詞：在机器学习的应用中, 因为, 我们把这些多维数组叫作张量, 而核通常是由学习, 我们通常假设在存
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；最后，我们经常一次在多个维度上进行卷积运算。例如，如果把一张二；维的图像I作为输入，我们也许也想要使用一个二维的核K：
          關鍵詞：维的图像, 作为输入, 如果把一张二, 我们经常一次在多个维度上进行卷积运算, 例如
        - 摘要：卷积是可交换的（commutative），我们可以等价地写作：
          關鍵詞：卷积是可交换的, 我们可以等价地写作
        - 摘要：通常，下面的公式在机器学习库中实现更为简单，因为m和n的有效取；值范围相对较小。
          關鍵詞：因为, 通常, 下面的公式在机器学习库中实现更为简单, 值范围相对较小, 的有效取
        - 摘要：卷积运算可交换性的出现是因为我们将核相对输入进行了翻转；（flip），从m增大的角度来看，输入的索引在增大，但是核的索引在减；小。我们将核翻转的唯一目的是实现可交换性。尽管可交换性在证明时
          關鍵詞：尽管可交换性在证明时, 但是核的索引在减, 增大的角度来看, 我们将核翻转的唯一目的是实现可交换性, 卷积运算可交换性的出现是因为我们将核相对输入进行了翻转
        - 摘要：许多机器学习的库实现的是互相关函数但是称之为卷积。在这本书中我；们遵循把两种运算都叫作卷积的这个传统，在与核翻转有关的上下文；中，我们会特别指明是否对核进行了翻转。在机器学习中，学习算法会
          關鍵詞：我们会特别指明是否对核进行了翻转, 学习算法会, 许多机器学习的库实现的是互相关函数但是称之为卷积, 在这本书中我, 们遵循把两种运算都叫作卷积的这个传统
        - 摘要：图9.1演示了一个在二维张量上的卷积运算（没有对核进行翻转）的例；子。
          關鍵詞：的例, 演示了一个在二维张量上的卷积运算, 没有对核进行翻转
        - 摘要：图9.1　一个二维卷积的例子（没有对核进行翻转）。我们限制只对核完全处在图像中的位置进；行输出，在一些上下文中称为“有效”卷积。我们用画有箭头的盒子来说明输出张量的左上角元；素是如何通过对输入张量相应的左上角区域应用核进行卷积得到的
          關鍵詞：我们用画有箭头的盒子来说明输出张量的左上角元, 行输出, 在一些上下文中称为, 有效, 卷积
        - 摘要：离散卷积可以看作矩阵的乘法，然而，这个矩阵的一些元素被限制为必；须和另外一些元素相等。例如对于单变量的离散卷积，矩阵每一行中的；元素都与上一行对应位置平移一个单位的元素相同。这种矩阵叫作
          關鍵詞：元素都与上一行对应位置平移一个单位的元素相同, 这种矩阵叫作, 然而, 例如对于单变量的离散卷积, 矩阵每一行中的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；9.2　动机
          關鍵詞：动机
        - 摘要：卷积运算通过三个重要的思想来帮助改进机器学习系统：稀疏交互；（sparse；interactions）、参数共享  （parameter  sharing）、等变表示
          關鍵詞：参数共享, 等变表示, 稀疏交互, 卷积运算通过三个重要的思想来帮助改进机器学习系统
        - 摘要：传统的神经网络使用矩阵乘法来建立输入与输出的连接关系。其中，参；数矩阵中每一个单独的参数都描述了一个输入单元与一个输出单元间的；交互。这意味着每一个输出单元与每一个输入单元都产生交互。然而，
          關鍵詞：数矩阵中每一个单独的参数都描述了一个输入单元与一个输出单元间的, 传统的神经网络使用矩阵乘法来建立输入与输出的连接关系, 这意味着每一个输出单元与每一个输入单元都产生交互, 交互, 其中
        - 摘要：图9.2　稀疏连接，对每幅图从下往上看。我们强调了一个输入单元x 3 以及在s中受该单元影响；的输出单元。（上）当s是由核宽度为3的卷积产生时，只有3个输出受到x的影响。（下）当s是；由矩阵乘法产生时，连接不再是稀疏的，所以所有的输出都会受到x 3 的影响
          關鍵詞：的输出单元, 我们强调了一个输入单元, 以及在, 的影响, 连接不再是稀疏的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图9.3　稀疏连接，对每幅图从上往下看。我们强调了一个输出单元s 3 以及x中影响该单元的输；入单元。这些单元被称为s 3 的接受域（receptive field）。（上）当s是由核宽度为3的卷积产生
          關鍵詞：的接受域, 入单元, 中影响该单元的输, 稀疏连接, 这些单元被称为
        - 摘要：图9.4　处于卷积网络更深的层中的单元，它们的接受域要比处在浅层的单元的接受域更大。如；果网络还包含类似步幅卷积（见图9.12）或者池化（第9.3节）之类的结构特征，这种效应会加；强。这意味着在卷积网络中尽管直接连接都是很稀疏的，但处在更深的层中的单元可以间接地
          關鍵詞：这意味着在卷积网络中尽管直接连接都是很稀疏的, 处于卷积网络更深的层中的单元, 或者池化, 之类的结构特征, 这种效应会加
        - 摘要：参数共享  （parameter  sharing）是指在一个模型的多个函数中使用相同；的参数。在传统的神经网络中，当计算一层的输出时，权重矩阵的每一；个元素只使用一次，当它乘以输入的一个元素后就再也不会用到了。作
          關鍵詞：参数共享, 的参数, 权重矩阵的每一, 个元素只使用一次, 当计算一层的输出时
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图9.5　参数共享。黑色箭头表示在两个不同的模型中使用了特殊参数的连接。（上）黑色箭头；表示在卷积模型中对3元素核的中间元素的使用。因为参数共享，这个单独的参数被用于所有的
          關鍵詞：元素核的中间元素的使用, 参数共享, 黑色箭头表示在两个不同的模型中使用了特殊参数的连接, 表示在卷积模型中对, 黑色箭头
        - 摘要：作为前两条原则的一个实际例子，图9.6说明了稀疏连接和参数共享是；如何显著提高线性函数在一张图像上进行边缘检测的效率的。
          關鍵詞：如何显著提高线性函数在一张图像上进行边缘检测的效率的, 作为前两条原则的一个实际例子, 说明了稀疏连接和参数共享是
        - 摘要：图9.6　边缘检测的效率。右边的图像是通过先获得原始图像中的每个像素，然后减去左边相邻；像素的值而形成的。这个操作给出了输入图像中所有垂直方向上的边缘的强度，对目标检测来；说是有用的。两个图像的高度均为280个像素。输入图像的宽度为320个像素，而输出图像的宽
          關鍵詞：边缘检测的效率, 输入图像的宽度为, 个像素, 右边的图像是通过先获得原始图像中的每个像素, 对目标检测来
        - 摘要：度为319个像素。这个变换可以通过包含两个元素的卷积核来描述，使用卷积需要319×280×3＝；267 960次浮点运算（每个输出像素需要两次乘法和一次加法）。为了用矩阵乘法描述相同的变；换，需要一个包含320×280×319×280个或者说超过80亿个元素的矩阵，这使得卷积对于表示这
          關鍵詞：这个变换可以通过包含两个元素的卷积核来描述, 亿个元素的矩阵, 这使得卷积对于表示这, 个像素, 为了用矩阵乘法描述相同的变
        - 摘要：对于卷积，参数共享的特殊形式使得神经网络层具有对平移等变；（equivariance）的性质。如果一个函数满足输入改变，输出也以同样的；方式改变这一性质，我们就说它是等变（equivariant）的。特别的是，
          關鍵詞：我们就说它是等变, 如果一个函数满足输入改变, 输出也以同样的, 参数共享的特殊形式使得神经网络层具有对平移等变, 的性质
        - 摘要：卷积对其他的一些变换并不是天然等变的，例如对于图像的放缩或者旋；转变换，需要其他的一些机制来处理这些变换。
          關鍵詞：卷积对其他的一些变换并不是天然等变的, 需要其他的一些机制来处理这些变换, 转变换, 例如对于图像的放缩或者旋
        - 摘要：最后，一些不能被传统的由（固定大小的）矩阵乘法定义的神经网络处；理的特殊数据，可能通过卷积神经网络来处理，我们将在第9.7节中进；行讨论。
          關鍵詞：理的特殊数据, 可能通过卷积神经网络来处理, 一些不能被传统的由, 矩阵乘法定义的神经网络处, 我们将在第
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；9.3　池化
          關鍵詞：池化
        - 摘要：卷积网络中一个典型层包含三级（见图9.7）。在第一级中，这一层并；行地计算多个卷积产生一组线性激活响应。在第二级中，每一个线性激；活响应将会通过一个非线性的激活函数，例如整流线性激活函数。这一
          關鍵詞：在第一级中, 这一, 这一层并, 在第二级中, 行地计算多个卷积产生一组线性激活响应
        - 摘要：图9.7　一个典型卷积神经网络层的组件。有两组常用的术语用于描述这些层。（左）在这组术；语中，卷积网络被视为少量相对复杂的层，每层具有许多“级”。在这组术语中，核张量与网络；层之间存在一一对应关系。在本书中，我们通常使用这组术语。（右）在这组术语中，卷积网
          關鍵詞：卷积网络被视为少量相对复杂的层, 核张量与网络, 我们通常使用这组术语, 每层具有许多, 语中
        - 摘要：池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置；的输出。例如，最大池化  （max  pooling）函数（Zhou  and  Chellappa，；1988）给出相邻矩形区域内的最大值。其他常用的池化函数包括相邻矩
          關鍵詞：的输出, 其他常用的池化函数包括相邻矩, 函数, 给出相邻矩形区域内的最大值, 例如
        - 摘要：数。
          關鍵詞：
        - 摘要：不管采用什么样的池化函数，当输入做出少量平移时，池化能够帮助输；入的表示近似不变  （invariant）。平移的不变性是指当我们对输入进行；少量平移时，经过池化函数后的大多数输出并不会发生改变。图9.8用
          關鍵詞：入的表示近似不变, 少量平移时, 当输入做出少量平移时, 不管采用什么样的池化函数, 池化能够帮助输
        - 摘要：图9.8　最大池化引入了不变性。（上）卷积层中间输出的视图。下面一行显示非线性的输出。；上面一行显示最大池化的输出，每个池的宽度为三个像素并且池化区域的步幅为一个像素。；（下）相同网络的视图，不过对输入右移了一个像素。下面一行的所有值都发生了改变，但上
          關鍵詞：不过对输入右移了一个像素, 卷积层中间输出的视图, 最大池化引入了不变性, 但上, 相同网络的视图
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；使用池化可以看作增加了一个无限强的先验：这一层学得的函数必须具；有对少量平移的不变性。当这个假设成立时，池化可以极大地提高网络
          關鍵詞：这一层学得的函数必须具, 当这个假设成立时, 池化可以极大地提高网络, 使用池化可以看作增加了一个无限强的先验, 有对少量平移的不变性
        - 摘要：对空间区域进行池化产生了平移不变性，但当我们对分离参数的卷积的；输出进行池化时，特征能够学得应该对于哪种变换具有不变性（见图；9.9）。
          關鍵詞：但当我们对分离参数的卷积的, 见图, 输出进行池化时, 特征能够学得应该对于哪种变换具有不变性, 对空间区域进行池化产生了平移不变性
        - 摘要：图9.9　学习不变性的示例。使用分离的参数学得多个特征，再使用池化单元进行池化，可以学；得对输入的某些变换的不变性。这里我们展示了用三个学得的过滤器和一个最大池化单元可以；学得对旋转变换的不变性。这三个过滤器都旨在检测手写的数字5。每个过滤器尝试匹配稍微不
          關鍵詞：学得对旋转变换的不变性, 使用分离的参数学得多个特征, 再使用池化单元进行池化, 可以学, 这里我们展示了用三个学得的过滤器和一个最大池化单元可以
        - 摘要：因为池化综合了全部邻居的反馈，这使得池化单元少于探测单元成为可；能，我们可以通过综合池化区域的k个像素的统计特征而不是单个像素；来实现。图9.10给出了一个例子。这种方法提高了网络的计算效率，因
          關鍵詞：因为池化综合了全部邻居的反馈, 我们可以通过综合池化区域的, 这种方法提高了网络的计算效率, 个像素的统计特征而不是单个像素, 这使得池化单元少于探测单元成为可
        - 摘要：图9.10　带有降采样的池化。这里我们使用最大池化，池的宽度为三并且池之间的步幅为二。；这使得表示的大小减少了一半，减轻了下一层的计算和统计负担。注意到最右边的池化区域尺；寸较小，但如果我们不想忽略一些探测单元，就必须包含这个区域
          關鍵詞：这里我们使用最大池化, 带有降采样的池化, 寸较小, 但如果我们不想忽略一些探测单元, 减轻了下一层的计算和统计负担
        - 摘要：在很多任务中，池化对于处理不同大小的输入具有重要作用。例如我们；想对不同大小的图像进行分类时，分类层的输入必须是固定的大小，而；这通常通过调整池化区域的偏置大小来实现，这样分类层总是能接收到
          關鍵詞：池化对于处理不同大小的输入具有重要作用, 在很多任务中, 例如我们, 这通常通过调整池化区域的偏置大小来实现, 这样分类层总是能接收到
        - 摘要：一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导；（Boureau et  al. ，2010）。将特征一起动态地池化也是可行的，例如，；对于感兴趣特征的位置运行聚类算法（Boureau et al. ，2011）。这种方
          關鍵詞：将特征一起动态地池化也是可行的, 例如, 对于感兴趣特征的位置运行聚类算法, 这种方, 一些理论工作对于在不同情况下应当使用哪种池化函数给出了一些指导
        - 摘要：池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂，例如；玻尔兹曼机和自编码器。这些问题将在本书第3部分当我们遇到这些类；型的网络时进一步讨论。卷积玻尔兹曼机中的池化出现在第20.6节。一
          關鍵詞：部分当我们遇到这些类, 玻尔兹曼机和自编码器, 这些问题将在本书第, 例如, 池化可能会使得一些利用自顶向下信息的神经网络结构变得复杂
        - 摘要：图9.11给出了一些使用卷积和池化操作的用于分类的完整卷积网络结构；的例子。
          關鍵詞：给出了一些使用卷积和池化操作的用于分类的完整卷积网络结构, 的例子
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图9.11　卷积网络用于分类的结构示例。本图中使用的具体步幅和深度并不建议实际使用，因；为它们被设计得非常浅以适合页面。实际的卷积网络还常常涉及大量的分支，不同于这里为简
          關鍵詞：卷积网络用于分类的结构示例, 不同于这里为简, 实际的卷积网络还常常涉及大量的分支, 本图中使用的具体步幅和深度并不建议实际使用, 为它们被设计得非常浅以适合页面
    9.4：卷积与池化作为一种无限强的先验
        - 摘要：回忆一下第5.2节中先验概率分布  （prior  probability  distribution）的概；念。这是一个模型参数的概率分布，它刻画了我们在看到数据之前认为
          關鍵詞：这是一个模型参数的概率分布, 它刻画了我们在看到数据之前认为, 节中先验概率分布, 回忆一下第, 的概
        - 摘要：什么样的模型是合理的信念。
          關鍵詞：什么样的模型是合理的信念
        - 摘要：先验被认为是强或者弱取决于先验中概率密度的集中程度。弱先验具有；较高的熵值，例如方差很大的高斯分布。这样的先验允许数据对于参数；的改变具有或多或少的自由性。强先验具有较低的熵值，例如方差很小
          關鍵詞：例如方差很大的高斯分布, 弱先验具有, 这样的先验允许数据对于参数, 例如方差很小, 先验被认为是强或者弱取决于先验中概率密度的集中程度
        - 摘要：一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数；赋值，无论数据对于这些参数的值给出了多大的支持。
          關鍵詞：赋值, 无论数据对于这些参数的值给出了多大的支持, 一个无限强的先验需要对一些参数的概率置零并且完全禁止对这些参数
        - 摘要：我们可以把卷积网络类比成全连接网络，但对于这个全连接网络的权重；有一个无限强的先验。这个无限强的先验是说一个隐藏单元的权重必须；和它邻居的权重相同，但可以在空间上移动。这个先验也要求除了那些
          關鍵詞：有一个无限强的先验, 这个先验也要求除了那些, 但对于这个全连接网络的权重, 我们可以把卷积网络类比成全连接网络, 但可以在空间上移动
        - 摘要：当然，把卷积神经网络当作一个具有无限强先验的全连接网络来实现会；导致极大的计算浪费。但把卷积神经网络想成具有无限强先验的全连接；网络可以帮助我们更好地洞察卷积神经网络是如何工作的。
          關鍵詞：网络可以帮助我们更好地洞察卷积神经网络是如何工作的, 但把卷积神经网络想成具有无限强先验的全连接, 把卷积神经网络当作一个具有无限强先验的全连接网络来实现会, 导致极大的计算浪费, 当然
        - 摘要：其中一个关键的洞察是卷积和池化可能导致欠拟合。与任何其他先验类；似，卷积和池化只有当先验的假设合理且正确时才有用。如果一项任务；依赖于保存精确的空间信息，那么在所有的特征上使用池化将会增大训
          關鍵詞：卷积和池化只有当先验的假设合理且正确时才有用, 那么在所有的特征上使用池化将会增大训, 其中一个关键的洞察是卷积和池化可能导致欠拟合, 与任何其他先验类, 依赖于保存精确的空间信息
        - 摘要：另一个关键洞察是当我们比较卷积模型的统计学习表现时，只能以基准；中的其他卷积模型作为比较的对象。其他不使用卷积的模型即使我们把；图像中的所有像素点都置换后依然有可能进行学习。对于许多图像数据
          關鍵詞：只能以基准, 中的其他卷积模型作为比较的对象, 图像中的所有像素点都置换后依然有可能进行学习, 另一个关键洞察是当我们比较卷积模型的统计学习表现时, 其他不使用卷积的模型即使我们把
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；一些是针对模型设计者将空间关系的知识植入了它们的模型。
          關鍵詞：一些是针对模型设计者将空间关系的知识植入了它们的模型
    9.5：基本卷积函数的变体
        - 摘要：当在神经网络的上下文中讨论卷积时，我们通常不是特指数学文献中使；用的那种标准的离散卷积运算。实际应用中的函数略有不同。这里我们；详细讨论一下这些差异，并且对神经网络中用到的函数的一些重要性质
          關鍵詞：我们通常不是特指数学文献中使, 并且对神经网络中用到的函数的一些重要性质, 用的那种标准的离散卷积运算, 详细讨论一下这些差异, 实际应用中的函数略有不同
        - 摘要：首先，当提到神经网络中的卷积时，我们通常是指由多个并行卷积组成；的运算。这是因为具有单个核的卷积只能提取一种类型的特征，尽管它；作用在多个空间位置上。我们通常希望网络的每一层能够在多个位置提
          關鍵詞：这是因为具有单个核的卷积只能提取一种类型的特征, 我们通常是指由多个并行卷积组成, 作用在多个空间位置上, 我们通常希望网络的每一层能够在多个位置提, 尽管它
        - 摘要：另外，输入通常也不仅仅是实值的网格，而是由一系列观测数据的向量；构成的网格。例如，一幅彩色图像在每一个像素点都会有红、绿、蓝三；种颜色的亮度。在多层的卷积网络中，第二层的输入是第一层的输出，
          關鍵詞：而是由一系列观测数据的向量, 构成的网格, 第二层的输入是第一层的输出, 例如, 输入通常也不仅仅是实值的网格
        - 摘要：因为卷积网络通常使用多通道的卷积，所以即使使用了核翻转，也不一；定保证网络的线性运算是可交换的。只有当其中每个运算的输出和输入；具有相同的通道数时，这些多通道的运算才是可交换的。
          關鍵詞：因为卷积网络通常使用多通道的卷积, 只有当其中每个运算的输出和输入, 具有相同的通道数时, 定保证网络的线性运算是可交换的, 这些多通道的运算才是可交换的
        - 摘要：假定我们有一个4维的核张量K ，它的每一个元素是 K i,j,k，l ，表示输出；中处于通道i的一个单元和输入中处于通道j中的一个单元的连接强度，；并且在输出单元和输入单元之间有k行l列的偏置。假定我们的输入由观
          關鍵詞：的一个单元和输入中处于通道, 假定我们的输入由观, 列的偏置, 表示输出, 假定我们有一个
        - 摘要：这里对所有的l、m和n进行求和是对所有（在求和式中）有效的张量索；引的值进行求和。在线性代数中，向量的索引通常从1开始，这就是上；述公式中-1的由来。但是像C或Python这类编程语言索引通常从0开始，
          關鍵詞：这类编程语言索引通常从, 述公式中, 的由来, 在求和式中, 开始
        - 摘要：我们有时会希望跳过核中的一些位置来降低计算的开销（相应的代价是；提取特征没有先前那么好了）。我们可以把这一过程看作对全卷积函数；输出的下采样（downsampling）。如果只想在输出的每个方向上每间隔
          關鍵詞：相应的代价是, 提取特征没有先前那么好了, 如果只想在输出的每个方向上每间隔, 我们有时会希望跳过核中的一些位置来降低计算的开销, 我们可以把这一过程看作对全卷积函数
        - 摘要：我们把s称为下采样卷积的步幅  （stride）。当然也可以对每个移动方向；定义不同的步幅。图9.12演示了一个实例。
          關鍵詞：称为下采样卷积的步幅, 定义不同的步幅, 当然也可以对每个移动方向, 我们把, 演示了一个实例
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图9.12　带有步幅的卷积。在这个例子中，我们的步幅为2。（上）在单个操作中实现的步幅为；2的卷积。（下）步幅大于一个像素的卷积在数学上等价于单位步幅的卷积随后降采样。显然，
          關鍵詞：显然, 我们的步幅为, 在单个操作中实现的步幅为, 带有步幅的卷积, 在这个例子中
        - 摘要：在任何卷积网络的实现中都有一个重要性质，那就是能够隐含地对输；入V  用零进行填充（pad）使得它加宽。如果没有这个性质，表示的宽；度在每一层就会缩减，缩减的幅度是比核少一个像素这么多。对输入进
          關鍵詞：用零进行填充, 对输入进, 使得它加宽, 如果没有这个性质, 在任何卷积网络的实现中都有一个重要性质
        - 摘要：图9.13　零填充对网络大小的影响。考虑一个卷积网络，每层有一个宽度为6的核。在这个例子；中，我们不使用任何池化，所以只有卷积操作本身缩小网络的大小。（上）在这个卷积网络；中，我们不使用任何隐含的零填充。这使得表示在每层缩小5个像素。从16个像素的输入开始，
          關鍵詞：考虑一个卷积网络, 我们不使用任何隐含的零填充, 所以只有卷积操作本身缩小网络的大小, 个像素, 在这个卷积网络
        - 摘要：有三种零填充设定的情况值得注意。第一种是无论怎样都不使用零填充；的极端情况，并且卷积核只允许访问那些图像中能够完全包含整个核的；位置。在MATLAB的术语中，这称为有效  （valid）卷积。在这种情况
          關鍵詞：这称为有效, 的极端情况, 第一种是无论怎样都不使用零填充, 的术语中, 在这种情况
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；硬件支持，网络就能包含任意多的卷积层，这是因为卷积运算不改变下；一层的结构。然而，输入像素中靠近边界的部分相比于中间部分对于输
          關鍵詞：输入像素中靠近边界的部分相比于中间部分对于输, 硬件支持, 一层的结构, 然而, 这是因为卷积运算不改变下
        - 摘要：在一些情况下，我们并不是真的想使用卷积，而是想用一些局部连接的；网络层（LeCun，1986，1989）。在这种情况下，我们的多层感知机对；应的邻接矩阵是相同的，但每一个连接都有它自己的权重，用一个6维
          關鍵詞：我们的多层感知机对, 应的邻接矩阵是相同的, 我们并不是真的想使用卷积, 在这种情况下, 网络层
        - 摘要：这有时也被称为非共享卷积 （unshared convolution），因为它和具有一；个小核的离散卷积运算很像，但并不横跨位置来共享参数。图9.14比较；了局部连接、卷积和全连接的区别。
          關鍵詞：个小核的离散卷积运算很像, 卷积和全连接的区别, 比较, 了局部连接, 因为它和具有一
        - 摘要：图9.14　局部连接，卷积和全连接的比较。（上）每一小片（接受域）有两个像素的局部连接；层。每条边用唯一的字母标记，来显示每条边都有自身的权重参数。（中）核宽度为两个像素；的卷积层。该模型与局部连接层具有完全相同的连接。区别不在于哪些单元相互交互，而在于
          關鍵詞：来显示每条边都有自身的权重参数, 区别不在于哪些单元相互交互, 的卷积层, 接受域, 卷积和全连接的比较
        - 摘要：当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现；在所有的空间上时，局部连接层是很有用的。例如，如果想要辨别一张；图片是否是人脸图像，我们只需要去寻找嘴是否在图像下半部分即可。
          關鍵詞：局部连接层是很有用的, 图片是否是人脸图像, 例如, 在所有的空间上时, 当我们知道每一个特征都是一小块空间的函数并且相同的特征不会出现
        - 摘要：使用那些连接被更进一步限制的卷积或者局部连接层也是有用的，例
          關鍵詞：使用那些连接被更进一步限制的卷积或者局部连接层也是有用的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；如，限制每一个输出的通道i仅仅是输入通道l的一部分的函数时。实现；这种情况的一种通用方法是使输出的前m个通道仅仅连接到输入的前n
          關鍵詞：个通道仅仅连接到输入的前, 仅仅是输入通道, 实现, 的一部分的函数时, 限制每一个输出的通道
        - 摘要：图9.15　卷积网络的前两个输出通道只和前两个输入通道相连，随后的两个输出通道只和随后；的两个输入通道相连
          關鍵詞：卷积网络的前两个输出通道只和前两个输入通道相连, 的两个输入通道相连, 随后的两个输出通道只和随后
        - 摘要：平铺卷积 （tiled convolution）（Gregor and LeCun，2010a；Le et al. ，；2010）对卷积层和局部连接层进行了折衷。这里并不是对每一个空间位
          關鍵詞：对卷积层和局部连接层进行了折衷, 这里并不是对每一个空间位, 平铺卷积
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；置的权重集合进行学习，我们学习一组核使得当我们在空间移动时它们；可以循环利用。这意味着在近邻的位置上拥有不同的过滤器，就像局部
          關鍵詞：我们学习一组核使得当我们在空间移动时它们, 置的权重集合进行学习, 可以循环利用, 这意味着在近邻的位置上拥有不同的过滤器, 就像局部
        - 摘要：图9.16　局部连接层、平铺卷积和标准卷积的比较。当使用相同大小的核时，这三种方法在单；元之间具有相同的连接。此图是对使用两个像素宽的核的说明。这三种方法之间的区别在于它；们如何共享参数。（上）局部连接层根本没有共享参数。我们对每个连接使用唯一的字母标
          關鍵詞：们如何共享参数, 局部连接层, 我们对每个连接使用唯一的字母标, 平铺卷积和标准卷积的比较, 当使用相同大小的核时
        - 摘要：为“a”和“b”的边的核
          關鍵詞：的边的核
        - 摘要：为了用代数的方法定义平铺卷积，令K 是一个6维的张量  (3)  ，其中的两；维对应着输出映射中的不同位置。K  在这里并没有对输出映射中的每一；个位置使用单独的索引，输出的位置在每个方向上在t个不同的核组成
          關鍵詞：其中的两, 为了用代数的方法定义平铺卷积, 维的张量, 个不同的核组成, 输出的位置在每个方向上在
        - 摘要：这里百分号是取模运算，它的性质包括t%t＝0，(t+1)%t＝1等。在每一；维上使用不同的t可以很容易对这个方程进行扩展。
          關鍵詞：在每一, 这里百分号是取模运算, 可以很容易对这个方程进行扩展, 维上使用不同的, 它的性质包括
        - 摘要：局部连接层与平铺卷积层都和最大池化有一些有趣的关联：这些层的探；测单元都是由不同的过滤器驱动的。如果这些过滤器能够学会探测相同；隐含特征的不同变换形式，那么最大池化的单元对于学得的变换就具有
          關鍵詞：那么最大池化的单元对于学得的变换就具有, 局部连接层与平铺卷积层都和最大池化有一些有趣的关联, 测单元都是由不同的过滤器驱动的, 隐含特征的不同变换形式, 这些层的探
        - 摘要：实现卷积网络时，通常也需要除卷积以外的其他运算。为了实现学习，；必须在给定输出的梯度时能够计算核的梯度。在一些简单情况下，这种；运算可以通过卷积来实现，但在很多我们感兴趣的情况下，包括步幅大
          關鍵詞：通常也需要除卷积以外的其他运算, 为了实现学习, 但在很多我们感兴趣的情况下, 运算可以通过卷积来实现, 实现卷积网络时
        - 摘要：回忆一下，卷积是一种线性运算，所以可以表示成矩阵乘法的形式（如；果我们首先把输入张量变形为一个扁平的向量）。其中包含的矩阵是关；于卷积核的函数。这个矩阵是稀疏的，并且核的每个元素都复制给矩阵
          關鍵詞：回忆一下, 其中包含的矩阵是关, 所以可以表示成矩阵乘法的形式, 并且核的每个元素都复制给矩阵, 于卷积核的函数
        - 摘要：通过卷积定义的矩阵转置的乘法就是这样一种运算。这种运算用于在卷；积层反向传播误差的导数，所以它在训练多于一个隐藏层的卷积网络时；是必要的。如果我们想要从隐藏层单元重构可视化单元时，同样的运算
          關鍵詞：是必要的, 积层反向传播误差的导数, 同样的运算, 通过卷积定义的矩阵转置的乘法就是这样一种运算, 所以它在训练多于一个隐藏层的卷积网络时
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；向传播过程相协调。转置运算返回的输出的大小取决于三个方面：零填；充的策略、前向传播运算的步幅以及前向传播的输出映射的大小。在一
          關鍵詞：转置运算返回的输出的大小取决于三个方面, 充的策略, 在一, 零填, 前向传播运算的步幅以及前向传播的输出映射的大小
        - 摘要：这三种运算——卷积、从输出到权重的反向传播和从输出到输入的反向；传播——对于训练任意深度的前馈卷积网络，以及训练带有（基于卷积；的转置的）重构函数的卷积网络，这三种运算都足以计算它们所需的所
          關鍵詞：基于卷积, 重构函数的卷积网络, 这三种运算, 对于训练任意深度的前馈卷积网络, 卷积
        - 摘要：假设我们想要训练这样一个卷积网络，它包含步幅为s的步幅卷积，该；卷积的核为K  ，作用于多通道的图像V  ，定义为c(K；,s)，就像式
          關鍵詞：的步幅卷积, 定义为, 就像式, 它包含步幅为, 卷积的核为
        - 摘要：,V
          關鍵詞：
        - 摘要：满足
          關鍵詞：满足
        - 摘要：。
          關鍵詞：
        - 摘要：为了训练网络，我们需要对核中的权重求导。为了实现这个目的，我们；可以使用一个函数
          關鍵詞：可以使用一个函数, 为了训练网络, 我们需要对核中的权重求导, 为了实现这个目的, 我们
        - 摘要：如果这一层不是网络的底层，我们需要对V  求梯度来使得误差进一步反；向传播。我们可以使用如下的函数
          關鍵詞：向传播, 我们需要对, 如果这一层不是网络的底层, 我们可以使用如下的函数, 求梯度来使得误差进一步反
        - 摘要：第14章描述的自编码器网络，是一些被训练成把输入拷贝到输出的前馈；网络。一个简单的例子是PCA算法，将输入  x  拷贝到一个近似的重构值；来实现。使用权重矩阵转置的乘法，就像PCA
          關鍵詞：网络, 章描述的自编码器网络, 算法, 使用权重矩阵转置的乘法, 将输入
        - 摘要：算法这种，在一般的自编码器中是很常见的。为了使这些模型卷积化，；我们可以用函数h来实现卷积运算的转置。假定我们有和Z  相同形式的；隐藏单元H ，并且我们定义一种重构运算
          關鍵詞：隐藏单元, 假定我们有和, 并且我们定义一种重构运算, 在一般的自编码器中是很常见的, 算法这种
        - 摘要：为了训练自编码器，我们会得到关于R  的梯度，表示为一个张量E  。为；了训练解码器，我们需要获得对于K 的梯度，这通过g(H ,E ,s)来得到。；为了训练编码器，我们需要获得对于H  的梯度，这通过c(K  ,E  ,s)来得
          關鍵詞：为了训练自编码器, 我们需要获得对于, 这通过, 来得到, 了训练解码器
        - 摘要：一般来说，在卷积层从输入到输出的变换中我们不仅仅只用线性运算。；我们一般也会在进行非线性运算前，对每个输出加入一些偏置项。这样；就产生了如何在偏置项中共享参数的问题。对于局部连接层，很自然地
          關鍵詞：在卷积层从输入到输出的变换中我们不仅仅只用线性运算, 很自然地, 一般来说, 对每个输出加入一些偏置项, 对于局部连接层
    9.6：结构化输出
        - 摘要：卷积神经网络可以用于输出高维的结构化对象，而不仅仅是预测分类任；务的类标签或回归任务的实数值。通常这个对象只是一个张量，由标准；卷积层产生。例如，模型可以产生张量S ，其中S  i,j,k 是网络的输入像素
          關鍵詞：模型可以产生张量, 由标准, 卷积层产生, 通常这个对象只是一个张量, 其中
        - 摘要：经常出现的一个问题是输出平面可能比输入平面要小，如图9.13所示。；用于对图像中单个对象分类的常用结构中，网络空间维数的最大减少来；源于使用大步幅的池化层。为了产生与输入大小相似的输出映射，我们
          關鍵詞：经常出现的一个问题是输出平面可能比输入平面要小, 网络空间维数的最大减少来, 所示, 用于对图像中单个对象分类的常用结构中, 我们
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；生一张低分辨率的标签网格（Pinheiro and Collobert，2014，2015）。最；后，原则上可以使用具有单位步幅的池化操作。
          關鍵詞：生一张低分辨率的标签网格, 原则上可以使用具有单位步幅的池化操作
        - 摘要：对图像逐个像素标记的一种策略是先产生图像标签的原始猜测，然后使；用相邻像素之间的交互来修正该原始猜测。重复这个修正步骤数次对应；于在每一步使用相同的卷积，该卷积在深层网络的最后几层之间共享权
          關鍵詞：用相邻像素之间的交互来修正该原始猜测, 然后使, 对图像逐个像素标记的一种策略是先产生图像标签的原始猜测, 该卷积在深层网络的最后几层之间共享权, 于在每一步使用相同的卷积
        - 摘要：图9.17　用于像素标记的循环卷积网络的示例。输入是图像张量X，它的轴对应图像的行、列和；，它遵循每个像素的标签的概率分布。该张量的轴对；通道（红、绿、蓝）。目标是输出标签
          關鍵詞：通道, 目标是输出标签, 它遵循每个像素的标签的概率分布, 输入是图像张量, 列和
        - 摘要：进行卷积来提供隐
          關鍵詞：进行卷积来提供隐
        - 摘要：藏层的输入。在第一步中，此项由零代替。因为每一步使用相同的参数，所以这是一个循环网；络的例子，如第10章所述
          關鍵詞：藏层的输入, 此项由零代替, 络的例子, 如第, 在第一步中
        - 摘要：一旦对每个像素都进行了预测，我们就可以使用各种方法来进一步处理；这些预测，以便获得图像在区域上的分割（Briggman  et  al.  ，2009；；Turaga et al. ，2010；Farabet et al. ，2013）。一般的想法是假设大片相
          關鍵詞：我们就可以使用各种方法来进一步处理, 一般的想法是假设大片相, 以便获得图像在区域上的分割, 一旦对每个像素都进行了预测, 这些预测
    9.7：数据类型
        - 摘要：卷积网络使用的数据通常包含多个通道，每个通道是时间上或空间中某；一点的不同观测量。参考表9.1来了解具有不同维数和通道数的数据类；型的例子。
          關鍵詞：卷积网络使用的数据通常包含多个通道, 每个通道是时间上或空间中某, 一点的不同观测量, 参考表, 来了解具有不同维数和通道数的数据类
        - 摘要：表9.1　用于卷积网络的不同数据格式的示例
          關鍵詞：用于卷积网络的不同数据格式的示例
        - 摘要：单通道
          關鍵詞：单通道
        - 摘要：一；维
          關鍵詞：
        - 摘要：音频波形：卷积的轴对应于时间。我；们将时间离散化并且在每个时间点测；量一次波形的振幅
          關鍵詞：音频波形, 量一次波形的振幅, 们将时间离散化并且在每个时间点测, 卷积的轴对应于时间
        - 摘要：多通道
          關鍵詞：多通道
        - 摘要：骨架动画（skeleton；animation）数据：计算机渲；染的三维角色动画是通过随
          關鍵詞：计算机渲, 染的三维角色动画是通过随, 数据, 骨架动画
        - 摘要：已经使用傅里叶变换预处理过的音频；数据：我们可以将音频波形变换成二；维张量，不同的行对应不同的频率，
          關鍵詞：已经使用傅里叶变换预处理过的音频, 数据, 不同的行对应不同的频率, 我们可以将音频波形变换成二, 维张量
        - 摘要：二；维
          關鍵詞：
        - 摘要：彩色图像数据：其中一个；通道包含红色像素，另一个；包含绿色像素，最后一个包
          關鍵詞：最后一个包, 其中一个, 彩色图像数据, 包含绿色像素, 通道包含红色像素
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；等效于在频率上移动，这使得在不同；八度音阶中播放的相同旋律产生相同
          關鍵詞：这使得在不同, 等效于在频率上移动, 八度音阶中播放的相同旋律产生相同
        - 摘要：赋予了两个方向上平移等变；性
          關鍵詞：赋予了两个方向上平移等变
        - 摘要：三；维
          關鍵詞：
        - 摘要：体积数据：这种数据一般来源于医学；成像技术，例如CT扫描等
          關鍵詞：扫描等, 例如, 这种数据一般来源于医学, 体积数据, 成像技术
        - 摘要：彩色视频数据：其中一个；轴对应着时间，另一个轴对；应着视频帧的高度，最后一
          關鍵詞：应着视频帧的高度, 其中一个, 轴对应着时间, 彩色视频数据, 另一个轴对
        - 摘要：卷积网络用于视频的例子，可以参考Chen et al. （2010）。
          關鍵詞：可以参考, 卷积网络用于视频的例子
        - 摘要：到目前为止，我们仅讨论了训练和测试数据中的每个样例都有相同的空；间维度的情况。卷积网络的一个优点是它们还可以处理具有可变的空间；尺度的输入。这些类型的输入不能用传统的基于矩阵乘法的神经网络来
          關鍵詞：间维度的情况, 尺度的输入, 我们仅讨论了训练和测试数据中的每个样例都有相同的空, 到目前为止, 这些类型的输入不能用传统的基于矩阵乘法的神经网络来
        - 摘要：例如，考虑一组图像的集合，其中每个图像具有不同的高度和宽度。目；前还不清楚如何用固定大小的权重矩阵对这样的输入进行建模。卷积就；可以很直接地应用；核依据输入的大小简单地被使用不同次，并且卷积
          關鍵詞：前还不清楚如何用固定大小的权重矩阵对这样的输入进行建模, 可以很直接地应用, 并且卷积, 例如, 卷积就
        - 摘要：注意，使用卷积处理可变尺寸的输入，仅对输入是因为包含对同种事物；的不同量的观察（时间上不同长度的记录，空间上不同宽度的观察等）；而导致的尺寸变化这种情况才有意义。如果输入是因为它可以选择性地
          關鍵詞：而导致的尺寸变化这种情况才有意义, 空间上不同宽度的观察等, 注意, 仅对输入是因为包含对同种事物, 时间上不同长度的记录
        - 摘要：成绩特征和测试分数特征进行卷积是没有意义的。
          關鍵詞：成绩特征和测试分数特征进行卷积是没有意义的
    9.8：高效的卷积算法
        - 摘要：现代卷积网络的应用通常需要包含超过百万个单元的网络。利用并行计；算资源的强大实现是很关键的，如第12.1节中所描述的。然而，在很多；情况下，也可以通过选择适当的卷积算法来加速卷积。
          關鍵詞：节中所描述的, 也可以通过选择适当的卷积算法来加速卷积, 情况下, 然而, 现代卷积网络的应用通常需要包含超过百万个单元的网络
        - 摘要：卷积等效于使用傅里叶变换将输入与核都转换到频域、执行两个信号的；逐点相乘，再使用傅里叶逆变换转换回时域。对于某些问题的规模，这；种算法可能比离散卷积的朴素实现更快。
          關鍵詞：对于某些问题的规模, 逐点相乘, 执行两个信号的, 卷积等效于使用傅里叶变换将输入与核都转换到频域, 再使用傅里叶逆变换转换回时域
        - 摘要：当一个d维的核可以表示成d个向量（每一维一个向量）的外积时，该核；被称为可分离的 （separable）。当核可分离时，朴素的卷积是低效的。；它等价于组合d个一维卷积，每个卷积使用这些向量中的一个。组合方
          關鍵詞：维的核可以表示成, 的外积时, 当一个, 当核可分离时, 每个卷积使用这些向量中的一个
        - 摘要：设计更快的执行卷积或近似卷积，而不损害模型准确性的方法，是一个；活跃的研究领域。甚至仅提高前向传播效率的技术也是有用的，因为在；商业环境中，通常部署网络比训练网络还要耗资源。
          關鍵詞：因为在, 活跃的研究领域, 商业环境中, 通常部署网络比训练网络还要耗资源, 而不损害模型准确性的方法
    9.9：随机或无监督的特征
        - 摘要：通常，卷积网络训练中最昂贵的部分是学习特征。输出层的计算代价通；常相对不高，因为在通过若干层池化之后作为该层输入的特征的数量较；少。当使用梯度下降执行监督训练时，每步梯度计算需要完整地运行整
          關鍵詞：通常, 因为在通过若干层池化之后作为该层输入的特征的数量较, 当使用梯度下降执行监督训练时, 每步梯度计算需要完整地运行整, 输出层的计算代价通
        - 摘要：有三种基本策略可以不通过监督训练而得到卷积核。其中一种是简单地；随机初始化它们。另一种是手动设计它们，例如设置每个核在一个特定；的方向或尺度来检测边缘。最后，可以使用无监督的标准来学习核。例
          關鍵詞：例如设置每个核在一个特定, 有三种基本策略可以不通过监督训练而得到卷积核, 另一种是手动设计它们, 的方向或尺度来检测边缘, 可以使用无监督的标准来学习核
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；如，Coates  et  al.  （2011）将k均值聚类算法应用于小图像块，然后使用；每个学得的中心作为卷积核。本书第3部分描述了更多的无监督学习方
          關鍵詞：然后使用, 均值聚类算法应用于小图像块, 本书第, 每个学得的中心作为卷积核, 部分描述了更多的无监督学习方
        - 摘要：随机过滤器经常在卷积网络中表现得出乎意料得好Jarrett；al.；（2009b）；Saxe  et  al.  （2011）；Pinto  et  al.  （2011）；Cox  and
          關鍵詞：随机过滤器经常在卷积网络中表现得出乎意料得好
        - 摘要：et
          關鍵詞：
        - 摘要：一个中间方法是学习特征，但是使用那种不需要在每个梯度计算步骤中；都进行完整的前向和反向传播的方法。与多层感知机一样，我们使用贪；心逐层预训练，单独训练第一层，然后一次性地从第一层提取所有特
          關鍵詞：一个中间方法是学习特征, 心逐层预训练, 与多层感知机一样, 然后一次性地从第一层提取所有特, 单独训练第一层
        - 摘要：与其他无监督预训练的方法一样，使用这种方法的一些好处仍然难以说；清。无监督预训练可以提供一些相对于监督训练的正则化，或者它可以；简单地允许我们训练更大的结构，因为它的学习规则降低了计算成本。
          關鍵詞：无监督预训练可以提供一些相对于监督训练的正则化, 简单地允许我们训练更大的结构, 使用这种方法的一些好处仍然难以说, 或者它可以, 因为它的学习规则降低了计算成本
    9.10：卷积网络的神经科学基础
        - 摘要：卷积网络也许是生物学启发人工智能的最为成功的案例。虽然卷积网络；也经过许多其他领域的指导，但是神经网络的一些关键设计原则来自神；经科学。
          關鍵詞：经科学, 虽然卷积网络, 也经过许多其他领域的指导, 卷积网络也许是生物学启发人工智能的最为成功的案例, 但是神经网络的一些关键设计原则来自神
        - 摘要：卷积网络的历史始于神经科学实验，远早于相关计算模型的发展。为了；确定关于哺乳动物视觉系统如何工作的许多最基本的事实，神经生理学；家David  Hubel和Torsten  Wiesel合作多年（Hubel  and  Wiesel，1959，
          關鍵詞：为了, 确定关于哺乳动物视觉系统如何工作的许多最基本的事实, 远早于相关计算模型的发展, 卷积网络的历史始于神经科学实验, 神经生理学
        - 摘要：他们的工作有助于表征大脑功能的许多方面，这些方面超出了本书的范；围。从深度学习的角度来看，我们可以专注于简化的、草图形式的大脑；功能视图。
          關鍵詞：我们可以专注于简化的, 草图形式的大脑, 从深度学习的角度来看, 功能视图, 他们的工作有助于表征大脑功能的许多方面
        - 摘要：在这个简化的视图中，我们关注被称为V1的大脑的一部分，也称为初；级视觉皮层 （primary visual cortex）。V1是大脑对视觉输入开始执行显；著高级处理的第一个区域。在该草图视图中，图像是由光到达眼睛并刺
          關鍵詞：图像是由光到达眼睛并刺, 著高级处理的第一个区域, 我们关注被称为, 在该草图视图中, 的大脑的一部分
        - 摘要：卷积网络层被设计为描述V1的三个性质：
          關鍵詞：卷积网络层被设计为描述, 的三个性质
        - 摘要：（1）V1可以进行空间映射。它实际上具有二维结构来反映视网膜中的；图像结构。例如，到达视网膜下半部的光仅影响V1相应的一半。卷积；网络通过用二维映射定义特征的方式来描述该特性。
          關鍵詞：相应的一半, 图像结构, 它实际上具有二维结构来反映视网膜中的, 可以进行空间映射, 例如
        - 摘要：（2）V1包含许多简单细胞  （simple  cell）。简单细胞的活动在某种程
          關鍵詞：包含许多简单细胞, 简单细胞的活动在某种程
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；度上可以概括为在一个小的空间位置感受野内的图像的线性函数。卷积；网络的检测器单元被设计为模拟简单细胞的这些性质。
          關鍵詞：度上可以概括为在一个小的空间位置感受野内的图像的线性函数, 卷积, 网络的检测器单元被设计为模拟简单细胞的这些性质
        - 摘要：（3）V1还包括许多复杂细胞 （complex cell）。这些细胞响应类似于由；简单细胞检测的那些特征，但是复杂细胞对于特征的位置微小偏移具有；不变性。这启发了卷积网络的池化单元。复杂细胞对于照明中的一些变
          關鍵詞：还包括许多复杂细胞, 复杂细胞对于照明中的一些变, 这启发了卷积网络的池化单元, 这些细胞响应类似于由, 不变性
        - 摘要：虽然我们最了解V1，但是一般认为相同的基本原理也适用于视觉系统；的其他区域。在视觉系统的草图视图中，当我们逐渐深入大脑时，遵循；池化的基本探测策略被反复执行。当穿过大脑的多个解剖层时，我们最
          關鍵詞：在视觉系统的草图视图中, 池化的基本探测策略被反复执行, 当我们逐渐深入大脑时, 当穿过大脑的多个解剖层时, 的其他区域
        - 摘要：这些祖母细胞已经被证明确实存在于人脑中，在一个被称为内侧颞叶的；区域（Quiroga et  al.  ，2005）。研究人员测试了单个神经元是否会响应；名人的照片。他们发现了后来被称为“Halle  Berry神经元”的神经元：由
          關鍵詞：在一个被称为内侧颞叶的, 区域, 名人的照片, 他们发现了后来被称为, 神经元
        - 摘要：这些内侧颞叶神经元比现代卷积网络更通用一些，这些网络在读取名称；时不会自动联想到识别人或对象。与卷积网络的最后一层在特征上最接；近的类比是称为颞下皮质（IT）的脑区。当查看一个对象时，信息从视
          關鍵詞：近的类比是称为颞下皮质, 与卷积网络的最后一层在特征上最接, 这些内侧颞叶神经元比现代卷积网络更通用一些, 这些网络在读取名称, 当查看一个对象时
        - 摘要：类似（DiCarlo，2013）。
          關鍵詞：类似
        - 摘要：话虽如此，卷积网络和哺乳动物的视觉系统之间还是有许多区别。这些；区别有一些是计算神经科学家所熟知的，但超出了本书的范围。还有一；些区别尚未知晓，因为关于哺乳动物视觉系统如何工作的许多基本问题
          關鍵詞：这些, 话虽如此, 因为关于哺乳动物视觉系统如何工作的许多基本问题, 区别有一些是计算神经科学家所熟知的, 些区别尚未知晓
        - 摘要：人眼大部分是非常低的分辨率，除了一个被称为中央凹  （fovea）；的小块。中央凹仅观察在手臂长度距离内一块拇指大小的区域。虽；然我们觉得自己可以看到高分辨率的整个场景，但这是由大脑的潜
          關鍵詞：除了一个被称为中央凹, 的小块, 人眼大部分是非常低的分辨率, 然我们觉得自己可以看到高分辨率的整个场景, 中央凹仅观察在手臂长度距离内一块拇指大小的区域
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；还值得一提的是，神经科学很少告诉我们该如何训练卷积网络。具有跨；多个空间位置的参数共享的模型结构，可以追溯到早期关于视觉的联结
          關鍵詞：可以追溯到早期关于视觉的联结, 神经科学很少告诉我们该如何训练卷积网络, 多个空间位置的参数共享的模型结构, 具有跨, 还值得一提的是
        - 摘要：Lang  and  Hinton（1988）引入反向传播来训练时延神经网络  （time；delay  neural network，TDNN）。使用当代术语来说，TDNN是用于时间；序列的一维卷积网络。用于这些模型的反向传播不受任何神经科学观察
          關鍵詞：序列的一维卷积网络, 使用当代术语来说, 是用于时间, 用于这些模型的反向传播不受任何神经科学观察, 引入反向传播来训练时延神经网络
        - 摘要：到目前为止，我们已经描述了简单细胞对于某些特征是如何呈现粗略的；线性和选择性，复杂细胞是如何更加非线性，并且对于这些简单细胞特；征的某些变换具有不变性，以及在选择性和不变性之间交替放置的层可
          關鍵詞：征的某些变换具有不变性, 复杂细胞是如何更加非线性, 我们已经描述了简单细胞对于某些特征是如何呈现粗略的, 线性和选择性, 到目前为止
        - 摘要：反向相关向我们表明，大多数的V1细胞具有由Gabor函数；（Gabor；function）所描述的权重。Gabor函数描述在图像中的二维点处的权重。
          關鍵詞：反向相关向我们表明, 函数, 所描述的权重, 大多数的, 细胞具有由
        - 摘要：特别地，w(x,y)采用Gabor函数的形式：
          關鍵詞：函数的形式, 采用, 特别地
        - 摘要：其中
          關鍵詞：其中
        - 摘要：以及
          關鍵詞：以及
        - 摘要：这里α、β  x  、β  y  、f、φ、x  0  、y  0  、τ都是控制Gabor函数性质的参数。；图9.18给出了Gabor函数在不同参数集上的一些例子。
          關鍵詞：这里, 函数在不同参数集上的一些例子, 给出了, 函数性质的参数, 都是控制
        - 摘要：图9.18　具有各种参数设置的Gabor函数。白色表示绝对值大的正权重，黑色表示绝对值大的负；权重，背景灰色对应于零权重。（左）控制坐标系的参数具有不同值的Gabor函数，这些参数包；括：x 0 、y 0 和γ。在该网格中的每个Gabor函数被赋予和它在网格中的位置成比例的x 0 和y 0
          關鍵詞：具有各种参数设置的, 黑色表示绝对值大的负, 背景灰色对应于零权重, 函数, 在该网格中的每个
        - 摘要：参数x  0  、y  0  和τ定义坐标系。我们平移和旋转x和y来得到x′和y′。具体；地，简单细胞会响应以点(x 0 ，y 0 )为中心的图像特征，并且当我们沿着；从水平方向旋转τ弧度的线移动时，简单细胞将响应亮度的变化。
          關鍵詞：我们平移和旋转, 参数, 具体, 并且当我们沿着, 从水平方向旋转
        - 摘要：作为x′和y′的函数，函数w会响应当我们沿着x′移动时的亮度变化。它有；两个重要的因子：一个是高斯函数，另一个是余弦函数。
          關鍵詞：一个是高斯函数, 作为, 两个重要的因子, 函数, 会响应当我们沿着
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；高斯因子αexp(-β x x '2 -β  y  y  '2 )可以被视为阈值项，用于保证简单细胞仅；对接近x′和y′都为零点处的值响应，换句话说，接近细胞接受域的中
          關鍵詞：都为零点处的值响应, 高斯因子, 接近细胞接受域的中, 换句话说, 可以被视为阈值项
        - 摘要：余弦因子cos(fx′+φ)控制简单细胞如何响应延x′轴的亮度改变。参数f控；制余弦的频率，φ控制它的相位偏移。
          關鍵詞：参数, 余弦因子, 控制简单细胞如何响应延, 控制它的相位偏移, 制余弦的频率
        - 摘要：合在一起，简单细胞的这个草图视图意味着，简单细胞对在特定位置；处、特定方向上、特定空间频率的亮度进行响应。当图像中的光波与细；胞的权重具有相同的相位时，简单细胞是最兴奋的。这种情况发生在当
          關鍵詞：简单细胞的这个草图视图意味着, 特定方向上, 合在一起, 简单细胞是最兴奋的, 简单细胞对在特定位置
        - 摘要：复杂细胞的草图视图是它计算包含两个简单细胞响应的二维向量的L；2；。一个重要的特殊情况是当s 1 和s  0  具有
          關鍵詞：一个重要的特殊情况是当, 复杂细胞的草图视图是它计算包含两个简单细胞响应的二维向量的, 具有
        - 摘要：神经科学和机器学习之间最显著的对应关系，是从视觉上比较机器学习；模型学得的特征与使用V1得到的特征。Olshausen  and  Field（1996）说；明，一个简单的无监督学习算法——稀疏编码，学习的特征具有与简单
          關鍵詞：学习的特征具有与简单, 模型学得的特征与使用, 稀疏编码, 是从视觉上比较机器学习, 神经科学和机器学习之间最显著的对应关系
        - 摘要：（Hyvärinen et al. ，2009）来获得自然图像统计领域的综述。
          關鍵詞：来获得自然图像统计领域的综述
        - 摘要：图9.19　许多机器学习算法在应用于自然图像时，会学习那些用来检测边缘或边缘的特定颜色；的特征。这些特征检测器使人联想到已知存在于初级视觉皮层中的Gabor函数。（左）通过应用；于小图像块的无监督学习算法（尖峰和平板稀疏编码）学得的权重。（右）由完全监督的卷积
          關鍵詞：会学习那些用来检测边缘或边缘的特定颜色, 的特征, 由完全监督的卷积, 这些特征检测器使人联想到已知存在于初级视觉皮层中的, 学得的权重
    9.11：卷积网络与深度学习的历史
        - 摘要：卷积网络在深度学习的历史中发挥了重要作用。它们是将研究大脑获得；的深刻理解成功用于机器学习应用的关键例子。它们也是第一个表现良；好的深度模型之一，远远早于任意深度模型被认为是可行的。卷积网络
          關鍵詞：好的深度模型之一, 远远早于任意深度模型被认为是可行的, 卷积网络, 它们也是第一个表现良, 的深刻理解成功用于机器学习应用的关键例子
        - 摘要：卷积网络也被用作在许多比赛中的取胜手段。当前对深度学习的商业兴；趣的热度始于Krizhevsky  et  al.  （2012a）赢得了ImageNet对象识别挑；战，但是在那之前，卷积网络也已经被用于赢得前些年影响较小的其他
          關鍵詞：赢得了, 对象识别挑, 卷积网络也被用作在许多比赛中的取胜手段, 卷积网络也已经被用于赢得前些年影响较小的其他, 但是在那之前
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；卷积网络是第一批能使用反向传播有效训练的的深度网络之一。现在仍；不完全清楚为什么卷积网络在一般的反向传播网络被认为已经失败时反
          關鍵詞：现在仍, 不完全清楚为什么卷积网络在一般的反向传播网络被认为已经失败时反, 卷积网络是第一批能使用反向传播有效训练的的深度网络之一
        - 摘要：卷积网络提供了一种方法来特化神经网络，使其能够处理具有清楚的网；格结构拓扑的数据，以及将这样的模型扩展到非常大的规模。这种方法；在二维图像拓扑上是最成功的。为了处理一维序列数据，我们接下来转
          關鍵詞：使其能够处理具有清楚的网, 这种方法, 我们接下来转, 格结构拓扑的数据, 卷积网络提供了一种方法来特化神经网络
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)  译者注：本书中operation视语境有时翻译成“运算”，有时翻译成“操作”。
          關鍵詞：操作, 有时翻译成, 运算, 译者注, 本书中
        - 摘要：(2)  译者注：原文将此处误写成了I′。
          關鍵詞：译者注, 原文将此处误写成了
        - 摘要：(3)  译者注：原文将 K 误写成了k。
          關鍵詞：误写成了, 原文将, 译者注
第10章：序列建模：循环和递归网络
    9.11：卷积网络与深度学习的历史
        - 摘要：循环神经网络  （recurrent  neural  network）或RNN（Rumelhart  et  al.  ，；1986c）是一类用于处理序列数据的神经网络。就像卷积网络是专门用；于处理网格化数据X  （如一个图像）的神经网络，循环神经网络是专门
          關鍵詞：就像卷积网络是专门用, 是一类用于处理序列数据的神经网络, 循环神经网络, 于处理网格化数据, 如一个图像
        - 摘要：从多层网络出发到循环网络，我们需要利用20世纪80年代机器学习和统
          關鍵詞：我们需要利用, 从多层网络出发到循环网络, 年代机器学习和统, 世纪
        - 摘要：计模型早期思想的优点：在模型的不同部分共享参数。参数共享使得模；型能够扩展到不同形式的样本（这里指不同长度的样本）并进行泛化。；如果我们在每个时间点都有一个单独的参数，不但不能泛化到训练时没
          關鍵詞：计模型早期思想的优点, 并进行泛化, 这里指不同长度的样本, 如果我们在每个时间点都有一个单独的参数, 不但不能泛化到训练时没
        - 摘要：一个相关的想法是在一维时间序列上使用卷积。这种卷积方法是时延神；经网络的基础（Lang  and  Hinton，1988；Waibel  et  al.  ，1989；Lang  et；al.  ，1990）。卷积操作允许网络跨时间共享参数，但是浅层的。卷积
          關鍵詞：经网络的基础, 但是浅层的, 卷积, 一个相关的想法是在一维时间序列上使用卷积, 卷积操作允许网络跨时间共享参数
        - 摘要：为简单起见，我们说的RNN是指在序列上的操作，并且该序列在时刻；t（从1到τ）包含向量 x (t) 。在实际情况中，循环网络通常在序列的小批；量上操作，并且小批量的每项具有不同序列长度τ。我们省略了小批量
          關鍵詞：量上操作, 包含向量, 我们说的, 并且小批量的每项具有不同序列长度, 循环网络通常在序列的小批
        - 摘要：本章将计算图的思想扩展到包括循环。这些周期代表变量自身的值在未；来某一时间步对自身值的影响。这样的计算图允许我们定义循环神经网；络。然后，我们描述许多构建、训练和使用循环神经网络的不同方式。
          關鍵詞：这样的计算图允许我们定义循环神经网, 我们描述许多构建, 来某一时间步对自身值的影响, 训练和使用循环神经网络的不同方式, 本章将计算图的思想扩展到包括循环
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；本章将简要介绍循环神经网络，为获取更多详细信息，我们建议读者参；考Graves（2012）的著作。
          關鍵詞：为获取更多详细信息, 的著作, 本章将简要介绍循环神经网络, 我们建议读者参
    10.1：展开计算图
        - 摘要：计算图是形式化一组计算结构的方式，如那些涉及将输入和参数映射到；输出和损失的计算。综合的介绍请参考第6.5.1节。本节，我们对展开；（unfolding）递归或循环计算得到的重复结构进行解释，这些重复结构
          關鍵詞：这些重复结构, 递归或循环计算得到的重复结构进行解释, 如那些涉及将输入和参数映射到, 综合的介绍请参考第, 计算图是形式化一组计算结构的方式
        - 摘要：例如，考虑动态系统的经典形式：
          關鍵詞：考虑动态系统的经典形式, 例如
        - 摘要：其中 s (t) 称为系统的状态。
          關鍵詞：称为系统的状态, 其中
        - 摘要：s  在时刻t的定义需要参考时刻t-1时同样的定义，因此式（10.1）是循环；的。
          關鍵詞：在时刻, 时同样的定义, 因此式, 的定义需要参考时刻, 是循环
        - 摘要：对有限时间步τ，τ-1次应用这个定义可以展开这个图。例如τ＝3，我们；对式（10.1）展开，可以得到
          關鍵詞：展开, 可以得到, 次应用这个定义可以展开这个图, 例如, 我们
        - 摘要：以这种方式重复应用定义，展开等式，就能得到不涉及循环的表达。现；在我们可以使用传统的有向无环计算图呈现这样的表达。
          關鍵詞：在我们可以使用传统的有向无环计算图呈现这样的表达, 以这种方式重复应用定义, 就能得到不涉及循环的表达, 展开等式
        - 摘要：式（10.1）和式（10.3）的展开计算图如图10.1所示。
          關鍵詞：所示, 的展开计算图如图, 和式
        - 摘要：图10.1　将式（10.1）描述的经典动态系统表示为展开的计算图。每个节点表示在某个时刻t的；状态，并且函数f将t处的状态映射到t+1处的状态。所有时间步都使用相同的参数（用于参数化f
          關鍵詞：描述的经典动态系统表示为展开的计算图, 处的状态, 用于参数化, 状态, 所有时间步都使用相同的参数
        - 摘要：的相同 θ 值）
          關鍵詞：的相同
        - 摘要：作为另一个例子，让我们考虑由外部信号 x (t) 驱动的动态系统，
          關鍵詞：让我们考虑由外部信号, 作为另一个例子, 驱动的动态系统
        - 摘要：我们可以看到，当前状态包含了整个过去序列的信息。
          關鍵詞：当前状态包含了整个过去序列的信息, 我们可以看到
        - 摘要：循环神经网络可以通过许多不同的方式建立。就像几乎所有函数都可以；被认为是前馈网络，本质上任何涉及循环的函数都可以视为一个循环神；经网络。
          關鍵詞：本质上任何涉及循环的函数都可以视为一个循环神, 被认为是前馈网络, 就像几乎所有函数都可以, 循环神经网络可以通过许多不同的方式建立, 经网络
        - 摘要：很多循环神经网络使用式（10.5）或类似的公式定义隐藏单元的值。为；了表明状态是网络的隐藏单元，我们使用变量；h  代表状态重写式
          關鍵詞：或类似的公式定义隐藏单元的值, 很多循环神经网络使用式, 代表状态重写式, 了表明状态是网络的隐藏单元, 我们使用变量
        - 摘要：如图10.2所示，典型RNN会增加额外的架构特性，如读取状态信息 h  进；行预测的输出层。
          關鍵詞：行预测的输出层, 会增加额外的架构特性, 典型, 所示, 如图
        - 摘要：图10.2　没有输出的循环网络。此循环网络只处理来自输入 x 的信息，将其合并到经过时间向前；传播的状态 h 。（左）回路原理图。黑色方块表示单个时间步的延迟。（右）同一网络被视为；展开的计算图，其中每个节点现在与一个特定的时间实例相关联
          關鍵詞：同一网络被视为, 将其合并到经过时间向前, 没有输出的循环网络, 展开的计算图, 回路原理图
        - 摘要：当训练循环网络根据过去预测未来时，网络通常要学会使用 h  (t)  作为过；去序列（直到t）与任务相关方面的有损摘要。此摘要一般而言一定是；有损的，因为其映射任意长度的序列( x (t) , x （t-1） , x （t-2） ,…, x (2) , x (1)
          關鍵詞：因为其映射任意长度的序列, 与任务相关方面的有损摘要, 去序列, 直到, 此摘要一般而言一定是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；入序列中的所有信息；而仅仅存储足够预测句子其余部分的信息。最苛；刻的情况是我们要求  h  (t)  足够丰富，并能大致恢复输入序列，如自编码
          關鍵詞：并能大致恢复输入序列, 如自编码, 而仅仅存储足够预测句子其余部分的信息, 入序列中的所有信息, 刻的情况是我们要求
        - 摘要：式（10.5）可以用两种不同的方式绘制。一种方法是为可能在模型的物；理实现中存在的部分赋予一个节点，如生物神经网络。在这个观点下，；网络定义了实时操作的回路，如图10.2的左侧，其当前状态可以影响其
          關鍵詞：理实现中存在的部分赋予一个节点, 的左侧, 网络定义了实时操作的回路, 其当前状态可以影响其, 可以用两种不同的方式绘制
        - 摘要：我们可以用一个函数g (t) 代表经t步展开后的循环：
          關鍵詞：代表经, 我们可以用一个函数, 步展开后的循环
        - 摘要：函数g (t) 将全部的过去序列( x (t) , x (t-1) , x (t-2) ,…, x (2) , x  (1)  )作为输入来；生成当前状态，但是展开的循环架构允许我们将g  (t)  分解为函数f的重复；应用。因此，展开过程引入两个主要优点：
          關鍵詞：分解为函数, 的重复, 函数, 因此, 展开过程引入两个主要优点
        - 摘要：（1）无论序列的长度，学成的模型始终具有相同的输入大小，因为它；指定的是从一种状态到另一种状态的转移，而不是在可变长度的历史状；态上操作。
          關鍵詞：无论序列的长度, 而不是在可变长度的历史状, 指定的是从一种状态到另一种状态的转移, 态上操作, 学成的模型始终具有相同的输入大小
        - 摘要：（2）我们可以在每个时间步使用相同参数的相同转移函数f。
          關鍵詞：我们可以在每个时间步使用相同参数的相同转移函数
        - 摘要：这两个因素使得学习在所有时间步和所有序列长度上操作单一的模型f；是可能的，而不需要在所有可能时间步学习独立的模型g  (t)  。学习单一；的共享模型允许泛化到没有见过的序列长度（没有出现在训练集中），
          關鍵詞：是可能的, 学习单一, 没有出现在训练集中, 这两个因素使得学习在所有时间步和所有序列长度上操作单一的模型, 而不需要在所有可能时间步学习独立的模型
        - 摘要：无论是循环图还是展开图，都有其用途。循环图简洁。展开图能够明确
          關鍵詞：都有其用途, 无论是循环图还是展开图, 展开图能够明确, 循环图简洁
        - 摘要：描述其中的计算流程。展开图还通过显式的信息流动路径帮助说明信息；在时间上向前（计算输出和损失）和向后（计算梯度）的思想。
          關鍵詞：描述其中的计算流程, 的思想, 在时间上向前, 展开图还通过显式的信息流动路径帮助说明信息, 和向后
    10.2：循环神经网络
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；10.2.1　导师驱动过程和输出循环网络
          關鍵詞：导师驱动过程和输出循环网络
        - 摘要：10.2.2　计算循环神经网络的梯度
          關鍵詞：计算循环神经网络的梯度
        - 摘要：10.2.3　作为有向图模型的循环网络
          關鍵詞：作为有向图模型的循环网络
        - 摘要：10.2.4　基于上下文的RNN序列建模
          關鍵詞：基于上下文的, 序列建模
        - 摘要：基于第10.1节中的图展开和参数共享的思想，我们可以设计各种循环神；经网络。
          關鍵詞：我们可以设计各种循环神, 节中的图展开和参数共享的思想, 经网络, 基于第
        - 摘要：循环神经网络中一些重要的设计模式包括以下几种：
          關鍵詞：循环神经网络中一些重要的设计模式包括以下几种
        - 摘要：（1）每个时间步都有输出，并且隐藏单元之间有循环连接的循环网；络，如图10.3所示。
          關鍵詞：如图, 所示, 并且隐藏单元之间有循环连接的循环网, 每个时间步都有输出
        - 摘要：图10.3　计算循环网络（将 x 值的输入序列映射到输出值 o 的对应序列）训练损失的计算图。损；失 L 衡量每个 o 与相应的训练目标 y 的距离。当使用softmax输出时，我们假设 o 是未归一化的对；，并将其与目标 y 比较。RNN输入到隐藏的连接
          關鍵詞：输出时, 比较, 衡量每个, 输入到隐藏的连接, 训练损失的计算图
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；（2）每个时间步都产生一个输出，只有当前时刻的输出到下个时刻的；隐藏单元之间有循环连接的循环网络，如图10.4所示。
          關鍵詞：隐藏单元之间有循环连接的循环网络, 所示, 只有当前时刻的输出到下个时刻的, 如图, 每个时间步都产生一个输出
        - 摘要：图10.4　此类RNN的唯一循环是从输出到隐藏层的反馈连接。在每个时间步t，输入为 x t ，隐藏；层激活为 h (t) ，输出为 o (t) ，目标为 y (t) ，损失为 L (t) 。（左）回路原理图。（右）展开的计；算图。这样的RNN没有图10.3表示的RNN那样强大（只能表示更小的函数集合）。图10.3中的
          關鍵詞：中的, 那样强大, 此类, 回路原理图, 只能表示更小的函数集合
        - 摘要：（3）隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的；循环网络，如图10.5所示。
          關鍵詞：所示, 隐藏单元之间存在循环连接, 循环网络, 但读取整个序列后产生单个输出的, 如图
        - 摘要：图10.5　关于时间展开的循环神经网络，在序列结束时具有单个输出。这样的网络可以用于概；括序列并产生用于进一步处理的固定大小的表示。在结束处可能存在目标（如此处所示），或；者通过更下游模块的反向传播来获得输出 o (t) 上的梯度
          關鍵詞：者通过更下游模块的反向传播来获得输出, 这样的网络可以用于概, 如此处所示, 在结束处可能存在目标, 在序列结束时具有单个输出
        - 摘要：图10.3是非常具有代表性的例子，我们将会在本章大部分涉及这个例；子。
          關鍵詞：是非常具有代表性的例子, 我们将会在本章大部分涉及这个例
        - 摘要：任何图灵可计算的函数都可以通过这样一个有限维的循环网络计算，在；这个意义上图10.3和式（10.8）的循环神经网络是万能的。RNN经过若；干时间步后读取输出，这与由图灵机所用的时间步是渐近线性的，与输
          關鍵詞：任何图灵可计算的函数都可以通过这样一个有限维的循环网络计算, 干时间步后读取输出, 这个意义上图, 与输, 的循环神经网络是万能的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；现在我们研究图10.3中RNN的前向传播公式。这个图没有指定隐藏单元；的激活函数。假设使用双曲正切激活函数。此外，图中没有明确指定何
          關鍵詞：现在我们研究图, 的激活函数, 这个图没有指定隐藏单元, 此外, 图中没有明确指定何
        - 摘要：其中的参数的偏置向量 b 和 c 连同权重矩阵 U 、 V 和 W ，分别对应于；输入到隐藏、隐藏到输出和隐藏到隐藏的连接。这个循环网络将一个输；入序列映射到相同长度的输出序列。与 x 序列配对的 y 的总损失就是所
          關鍵詞：隐藏到输出和隐藏到隐藏的连接, 入序列映射到相同长度的输出序列, 的总损失就是所, 连同权重矩阵, 这个循环网络将一个输
        - 摘要：其中p model (y (t) ｜{ x (1) ,…, x (t) })需要读取模型输出向量  中对应于y；(t)  的项。关于各个参数计算这个损失函数的梯度是计算成本很高的操；作。梯度计算涉及执行一次前向传播（如在图10.3展开图中从左到右的
          關鍵詞：关于各个参数计算这个损失函数的梯度是计算成本很高的操, 如在图, 梯度计算涉及执行一次前向传播, 展开图中从左到右的, 其中
        - 摘要：。应用于展开图且代价为
          關鍵詞：应用于展开图且代价为
        - 摘要：10.2.1　导师驱动过程和输出循环网络
          關鍵詞：导师驱动过程和输出循环网络
        - 摘要：仅在一个时间步的输出和下一个时间步的隐藏单元间存在循环连接的网；络（见图10.4）确实没有那么强大（因为缺乏隐藏到隐藏的循环连；接）。例如，它不能模拟通用图灵机。因为这个网络缺少隐藏到隐藏的
          關鍵詞：仅在一个时间步的输出和下一个时间步的隐藏单元间存在循环连接的网, 它不能模拟通用图灵机, 例如, 确实没有那么强大, 见图
        - 摘要：由输出反馈到模型而产生循环连接的模型可用导师驱动过程  （teacher；forcing）进行训练。训练模型时，导师驱动过程不再使用最大似然准；则，而在时刻t+1接收真实值y  (t)  作为输入。我们可以通过检查两个时间
          關鍵詞：作为输入, 导师驱动过程不再使用最大似然准, 我们可以通过检查两个时间, 而在时刻, 进行训练
        - 摘要：在这个例子中，同时给定迄今为止的 x 序列和来自训练集的前一 y 值，；我们可以看到在时刻t＝2时，模型被训练为最大化  y  (2)  的条件概率。因；此最大似然在训练时指定正确反馈，而不是将自己的输出反馈到模型，
          關鍵詞：模型被训练为最大化, 我们可以看到在时刻, 同时给定迄今为止的, 在这个例子中, 的条件概率
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图10.6　导师驱动过程的示意图。导师驱动过程是一种训练技术，适用于输出与下一时间步的；隐藏状态存在连接的RNN。（左）训练时，我们将训练集中正确的输出 y (t) 反馈到 h （t+1） 。
          關鍵詞：隐藏状态存在连接的, 反馈到, 导师驱动过程的示意图, 训练时, 导师驱动过程是一种训练技术
        - 摘要：我们使用导师驱动过程的最初动机是为了在缺乏隐藏到隐藏连接的模型；中避免通过时间反向传播。只要模型一个时间步的输出与下一时间步计；算的值存在连接，导师驱动过程仍然可以应用到这些存在隐藏到隐藏连
          關鍵詞：算的值存在连接, 只要模型一个时间步的输出与下一时间步计, 中避免通过时间反向传播, 我们使用导师驱动过程的最初动机是为了在缺乏隐藏到隐藏连接的模型, 导师驱动过程仍然可以应用到这些存在隐藏到隐藏连
        - 摘要：如果之后网络在开环  （open-loop）模式下使用，即网络输出（或输出；分布的样本）反馈作为输入，那么完全使用导师驱动过程进行训练的缺；点就会出现。在这种情况下，训练期间该网络看到的输入与测试时看到
          關鍵詞：或输出, 那么完全使用导师驱动过程进行训练的缺, 点就会出现, 训练期间该网络看到的输入与测试时看到, 在这种情况下
        - 摘要：自由运行的输入进行训练，例如在展开循环的输出到输入路径上预测几；个步骤的正确目标值。通过这种方式，网络可以学会考虑在训练时没有；接触到的输入条件（如自由运行模式下，自身生成自身），以及将状态
          關鍵詞：个步骤的正确目标值, 自身生成自身, 例如在展开循环的输出到输入路径上预测几, 如自由运行模式下, 通过这种方式
        - 摘要：10.2.2　计算循环神经网络的梯度
          關鍵詞：计算循环神经网络的梯度
        - 摘要：计算循环神经网络的梯度是容易的。我们可以简单地将第6.5.6节中的推；广反向传播算法应用于展开的计算图，而不需要特殊化的算法。由反向；传播计算得到的梯度，并结合任何通用的基于梯度的技术就可以训练
          關鍵詞：传播计算得到的梯度, 由反向, 计算循环神经网络的梯度是容易的, 我们可以简单地将第, 广反向传播算法应用于展开的计算图
        - 摘要：为了获得BPTT算法行为的一些直观理解，我们举例说明如何通过BPTT；计算上述RNN公式（式（10.8）和式（10.12））的梯度。计算图的节点；包括参数 U 、 V 、 W 、 b 和 c ，以及以t为索引的节点序列 x (t) 、 h (t)
          關鍵詞：公式, 算法行为的一些直观理解, 为了获得, 为索引的节点序列, 计算上述
        - 摘要：在这个导数中，假设输出  o；(t)  作为softmax函数的参数，我们可以从；softmax函数可以获得关于输出概率的向量   。我们也假设损失是迄今
          關鍵詞：在这个导数中, 假设输出, 作为, 函数可以获得关于输出概率的向量, 我们可以从
        - 摘要：如下：
          關鍵詞：如下
        - 摘要：我们从序列的末尾开始，反向进行计算。在最后的时间步τ， h (τ) 只有 o；(τ) 作为后续节点，因此这个梯度很简单：
          關鍵詞：在最后的时间步, 因此这个梯度很简单, 反向进行计算, 作为后续节点, 我们从序列的末尾开始
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；然后，我们可以从时刻t＝τ-1到t＝1反向迭代，通过时间反向传播梯；度，注意 h (t) (t＜τ)同时具有 o (t) 和 h (t+1) 两个后续节点。因此，它的梯
          關鍵詞：我们可以从时刻, 两个后续节点, 反向迭代, 通过时间反向传播梯, 同时具有
        - 摘要：其中；时刻t+1与隐藏单元i关联的双曲正切的Jacobian。
          關鍵詞：时刻, 其中, 关联的双曲正切的, 与隐藏单元
        - 摘要：表示包含元素
          關鍵詞：表示包含元素
        - 摘要：的对角矩阵。这是关于
          關鍵詞：的对角矩阵, 这是关于
        - 摘要：一旦获得了计算图内部节点的梯度，我们就可以得到关于参数节点的梯；度。因为参数在许多时间步共享，我们必须在表示这些变量的微积分操；作时谨慎对待。我们希望实现的等式使用第6.5.6节中的bprop方法计算
          關鍵詞：方法计算, 我们希望实现的等式使用第, 我们必须在表示这些变量的微积分操, 节中的, 我们就可以得到关于参数节点的梯
        - 摘要：表示权重在时间步t对梯度的贡献。
          關鍵詞：表示权重在时间步, 对梯度的贡献
        - 摘要：使用这个表示，关于剩下参数的梯度可以由式（10.22）～式（10.28）；给出：
          關鍵詞：使用这个表示, 给出, 关于剩下参数的梯度可以由式
        - 摘要：因为计算图中定义的损失的任何参数都不是训练数据 x  (t)  的父节点，所；以我们不需要计算关于它的梯度。
          關鍵詞：以我们不需要计算关于它的梯度, 因为计算图中定义的损失的任何参数都不是训练数据, 的父节点
        - 摘要：10.2.3　作为有向图模型的循环网络
          關鍵詞：作为有向图模型的循环网络
        - 摘要：目前为止，我们接触的循环网络例子中损失L  (t)  是训练目标  y  (t)  和输出；o  (t)  之间的交叉熵。与前馈网络类似，原则上循环网络几乎可以使用任；何损失。但必须根据任务来选择损失。如前馈网络，通常我们希望将
          關鍵詞：与前馈网络类似, 我们接触的循环网络例子中损失, 如前馈网络, 原则上循环网络几乎可以使用任, 何损失
        - 摘要：当使用一个预测性对数似然的训练目标，如式（10.12），我们将RNN；训练为能够根据之前的输入估计下一个序列元素 y  (t)  的条件分布。这可；能意味着，我们最大化对数似然
          關鍵詞：如式, 我们将, 当使用一个预测性对数似然的训练目标, 能意味着, 我们最大化对数似然
        - 摘要：或者，如果模型包括来自一个时间步的输出到下一个时间步的连接，
          關鍵詞：如果模型包括来自一个时间步的输出到下一个时间步的连接, 或者
        - 摘要：将整个序列 y  的联合分布分解为一系列单步的概率预测是捕获关于整个；序列完整联合分布的一种方法。如果我们不把过去的 y  值反馈给下一步；作为预测的条件，那么有向图模型不包含任何从过去 y  (i) 到当前 y  (t)  的
          關鍵詞：作为预测的条件, 将整个序列, 如果我们不把过去的, 到当前, 值反馈给下一步
        - 摘要：举一个简单的例子，让我们考虑对标量随机变量序列   ＝{y  (1)  ,…,y；(τ)  }建模的RNN，也没有额外的输入x。在时间步t的输入仅仅是时间步t-
          關鍵詞：在时间步, 的输入仅仅是时间步, 也没有额外的输入, 建模的, 举一个简单的例子
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；1的输出。该RNN定义了关于y变量的有向图模型。我们使用链式法则；（用于条件概率的（3.6））参数化这些观察值的联合分布：
          關鍵詞：的输出, 用于条件概率的, 定义了关于, 我们使用链式法则, 变量的有向图模型
        - 摘要：其中当t＝1时竖杠右侧显然为空。因此，根据这样一个模型，一组值{y；(1) ,…,y (τ) }的负对数似然为
          關鍵詞：因此, 一组值, 其中当, 时竖杠右侧显然为空, 根据这样一个模型
        - 摘要：其中
          關鍵詞：其中
        - 摘要：图模型中的边表示哪些变量直接依赖于其他变量。许多图模型的目标是；省略不存在强相互作用的边以实现统计和计算的效率。例如，我们通常；可以作Markov假设，即图模型应该只包含从{y  (t-k)  ,…,y  (t-1)  }到y  (t)  的
          關鍵詞：许多图模型的目标是, 图模型中的边表示哪些变量直接依赖于其他变量, 例如, 省略不存在强相互作用的边以实现统计和计算的效率, 我们通常
        - 摘要：解释RNN作为图模型的一种方法是将RNN视为定义一个结构为完全图；的图模型，且能够表示任何一对y值之间的直接联系。图10.7是关于y值；且具有完全图结构的图模型。该RNN完全图的解释基于排除并忽略模型
          關鍵詞：的图模型, 是关于, 完全图的解释基于排除并忽略模型, 解释, 值之间的直接联系
        - 摘要：图10.7　序列y (1) ，y (2) ,…,y (t) ，· · ·的全连接图模型。给定先前的值，每个过去的观察值y；(i) 可以影响一些y (t) (t＞i)的条件分布。当序列中每个元素的输入和参数的数目越来越多，根据；此图直接参数化图模型（如式（10.6）中）可能是非常低效的。RNN可以通过高效的参数化获
          關鍵詞：可以影响一些, 给定先前的值, 根据, 如式, 可能是非常低效的
        - 摘要：更有趣的是，将隐藏单元 h (t)  视为随机变量，从而产生RNN的图模型结；构 (1) 。在图模型中包括隐藏单元预示RNN能对观测的联合分布提供非常；有效的参数化。假设我们用表格表示法来表示离散值上任意的联合分
          關鍵詞：视为随机变量, 将隐藏单元, 有效的参数化, 能对观测的联合分布提供非常, 假设我们用表格表示法来表示离散值上任意的联合分
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图10.8　在RNN图模型中引入状态变量，尽管它是输入的确定性函数，但它有助于我们根据式；（10.5）获得非常高效的参数化。序列中的每个阶段（对于 h (t) 和 y (t) ）使用相同的结构（每
          關鍵詞：但它有助于我们根据式, 获得非常高效的参数化, 图模型中引入状态变量, 对于, 序列中的每个阶段
        - 摘要：即便使用高效参数化的图模型，某些操作在计算上仍然具有挑战性。例；如，难以预测序列中缺少的值。
          關鍵詞：某些操作在计算上仍然具有挑战性, 即便使用高效参数化的图模型, 难以预测序列中缺少的值
        - 摘要：循环网络为减少的参数数目付出的代价是优化参数可能变得困难。
          關鍵詞：循环网络为减少的参数数目付出的代价是优化参数可能变得困难
        - 摘要：在循环网络中使用的参数共享的前提是相同参数可用于不同时间步的假；设。也就是说，假设给定时刻t的变量后，时刻t+1变量的条件概率分布；是平稳的  （stationary），这意味着之前的时间步与下个时间步之间的
          關鍵詞：的变量后, 是平稳的, 变量的条件概率分布, 这意味着之前的时间步与下个时间步之间的, 时刻
        - 摘要：为了完整描述将RNN作为图模型的观点，我们必须描述如何从模型采；样。我们需要执行的主要操作是简单地从每一时间步的条件分布采样。；然而，这会导致额外的复杂性。RNN必须有某种机制来确定序列的长
          關鍵詞：我们需要执行的主要操作是简单地从每一时间步的条件分布采样, 这会导致额外的复杂性, 我们必须描述如何从模型采, 然而, 作为图模型的观点
        - 摘要：在当输出是从词汇表获取的符号的情况下，我们可以添加一个对应于序；列末端的特殊符号（Schmidhuber，2012）。当产生该符号时，采样过；程停止。在训练集中，我们将该符号作为序列的一个额外成员，即紧跟
          關鍵詞：当产生该符号时, 我们可以添加一个对应于序, 在当输出是从词汇表获取的符号的情况下, 我们将该符号作为序列的一个额外成员, 即紧跟
        - 摘要：另一种选择是在模型中引入一个额外的Bernoulli输出，表示在每个时间；步决定继续生成或停止生成。相比向词汇表增加一个额外符号，这种方；法更普遍，因为它适用于任何RNN，而不仅仅是输出符号序列的
          關鍵詞：而不仅仅是输出符号序列的, 另一种选择是在模型中引入一个额外的, 法更普遍, 步决定继续生成或停止生成, 相比向词汇表增加一个额外符号
        - 摘要：RNN。例如，它可以应用于一个产生实数序列的RNN。新的输出单元；通常使用sigmoid单元，并通过交叉熵训练。在这种方法中，sigmoid被；训练为最大化正确预测的对数似然，即在每个时间步序列决定结束或继
          關鍵詞：在这种方法中, 例如, 训练为最大化正确预测的对数似然, 新的输出单元, 它可以应用于一个产生实数序列的
        - 摘要：确定序列长度τ的另一种方法是将一个额外的输出添加到模型并预测整；数τ本身。模型可以采出τ的值，然后采τ步有价值的数据。这种方法需；要在每个时间步的循环更新中增加一个额外输入，使得循环更新知道它
          關鍵詞：的另一种方法是将一个额外的输出添加到模型并预测整, 本身, 使得循环更新知道它, 模型可以采出, 要在每个时间步的循环更新中增加一个额外输入
        - 摘要：直接预测τ的例子见Goodfellow et al. （2014d）。
          關鍵詞：直接预测, 的例子见
        - 摘要：10.2.4　基于上下文的RNN序列建模
          關鍵詞：基于上下文的, 序列建模
        - 摘要：上一节描述了没有输入 x  时，关于随机变量序列y  (t)  的RNN如何对应于；有向图模型。当然，如式（10.8）所示的RNN包含一个输入序列 x  (1)  ，；x (2) ,…, x  (τ) 。一般情况下，RNN允许将图模型的观点扩展到不仅代表y
          關鍵詞：上一节描述了没有输入, 包含一个输入序列, 如式, 有向图模型, 所示的
        - 摘要：之前，我们已经讨论了将t＝1,…,τ的向量  x  (t)  序列作为输入的RNN。另；一种选择是只使用单个向量  x  作为输入。当  x  是一个固定大小的向量；时，我们可以简单地将其看作产生 y 序列RNN的额外输入。将额外输入
          關鍵詞：作为输入, 序列作为输入的, 我们已经讨论了将, 将额外输入, 的额外输入
        - 摘要：（1）在每个时刻作为一个额外输入，或
          關鍵詞：在每个时刻作为一个额外输入
        - 摘要：（2）作为初始状态 h (0) ，或
          關鍵詞：作为初始状态
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；（3）结合两种方式。
          關鍵詞：结合两种方式
        - 摘要：第一个也是最常用的方法如图10.9所示。输入  x  和每个隐藏单元向量  h；(t)  之间的相互作用是通过新引入的权重矩阵  R  参数化的，这是只包含y；在每个时间步作为隐藏单元
          關鍵詞：在每个时间步作为隐藏单元, 和每个隐藏单元向量, 第一个也是最常用的方法如图, 参数化的, 所示
        - 摘要：图10.9　将固定长度的向量 x 映射到序列 Y 上分布的RNN。这类RNN适用于很多任务（如图；注），其中单个图像作为模型的输入，然后产生描述图像的词序列。观察到的输出序列的每个；元素 y (t) 同时用作输入（对于当前时间步）和训练期间的目标（对于前一时间步）
          關鍵詞：适用于很多任务, 观察到的输出序列的每个, 对于当前时间步, 这类, 同时用作输入
        - 摘要：RNN可以接收向量序列 x (t) 作为输入，而不是仅接收单个向量 x 作为输；入。式（10.8）描述的RNN对应条件分布P( y (1) ,…, y (τ) ｜ x  (1)  ,…,  x  (τ)；)，并在条件独立的假设下这个分布分解为
          關鍵詞：可以接收向量序列, 而不是仅接收单个向量, 作为输入, 作为输, 并在条件独立的假设下这个分布分解为
        - 摘要：为去掉条件独立的假设，我们可以在时刻t的输出到时刻t+1的隐藏单元；添加连接，如图10.10所示。该模型就可以代表关于 y 序列的任意概率分；布。这种给定一个序列表示另一个序列分布的模型的还是有一个限制，
          關鍵詞：这种给定一个序列表示另一个序列分布的模型的还是有一个限制, 的隐藏单元, 所示, 添加连接, 序列的任意概率分
        - 摘要：图10.10　将可变长度的 x 值序列映射到相同长度的 y 值序列上分布的条件循环神经网络。对比；图10.3，此RNN包含从前一个输出到当前状态的连接。这些连接允许此RNN对给定 x 的序列后；相同长度的 y 序列上的任意分布建模。图10.3的RNN仅能表示在给定 x 值的情况下， y 值彼此条
          關鍵詞：包含从前一个输出到当前状态的连接, 仅能表示在给定, 对比, 值序列上分布的条件循环神经网络, 的序列后
        - 摘要：10.2.1　导师驱动过程和；输出循环网络
          關鍵詞：输出循环网络, 导师驱动过程和
        - 摘要：10.2.2　计算循环神经网；络的梯度
          關鍵詞：计算循环神经网, 络的梯度
        - 摘要：10.2.3　作为有向图模型；的循环网络
          關鍵詞：的循环网络, 作为有向图模型
        - 摘要：10.2.4　基于上下文的；RNN序列建模
          關鍵詞：基于上下文的, 序列建模
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；10.3　双向RNN
          關鍵詞：双向
    10.3：双向RNN
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；目前为止，我们考虑的所有循环神经网络有一个“因果”结构，意味着在；时刻t的状态只能从过去的序列 x (1) ,…, x (t-1) 以及当前的输入 x (t) 捕获信
          關鍵詞：我们考虑的所有循环神经网络有一个, 意味着在, 目前为止, 因果, 捕获信
        - 摘要：然而，在许多应用中，我们要输出的 y  (t)  的预测可能依赖于整个输入序；列。例如，在语音识别中，由于协同发音，当前声音作为音素的正确解；释可能取决于未来几个音素，甚至潜在的可能取决于未来的几个词，因
          關鍵詞：在语音识别中, 然而, 由于协同发音, 例如, 的预测可能依赖于整个输入序
        - 摘要：双向循环神经网络（或双向RNN）为满足这种需要而发明（Schuster；Paliwal，1997）。它们在需要双向信息的应用中非常成功；and
          關鍵詞：双向循环神经网络, 它们在需要双向信息的应用中非常成功, 或双向, 为满足这种需要而发明
        - 摘要：顾名思义，双向RNN结合时间上从序列起点开始移动的RNN和另一个；时间上从序列末尾开始移动的RNN。图10.11展示了典型的双向RNN，；其中 h (t) 代表通过时间向前移动的子RNN的状态， g (t) 代表通过时间向
          關鍵詞：代表通过时间向前移动的子, 代表通过时间向, 展示了典型的双向, 时间上从序列末尾开始移动的, 其中
        - 摘要：图10.11　典型的双向循环神经网络中的计算，意图学习将输入序列 x 映射到目标序列 y （在每；个步骤t具有损失L (t) ）。循环性 h 在时间上向前传播信息（向右），而循环性 g 在时间上向后
          關鍵詞：在时间上向前传播信息, 循环性, 个步骤, 典型的双向循环神经网络中的计算, 具有损失
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；传播信息（向左）。因此在每个点t，输出单元 o (t) 可以受益于输入 h (t) 中关于过去的相关概要；以及输入 g (t) 中关于未来的相关概要
          關鍵詞：以及输入, 向左, 传播信息, 中关于未来的相关概要, 中关于过去的相关概要
        - 摘要：这个想法可以自然地扩展到二维输入，如图像，由4个RNN组成，每一；个沿着4个方向中的一个计算：上、下、左、右。如果RNN能够学习到；承载长期信息，那在二维网格每个点（i,j）的输出O  i,j  就能计算一个能
          關鍵詞：每一, 组成, 个方向中的一个计算, 如果, 承载长期信息
    10.4：基于编码-解码的序列到序列架构
        - 摘要：我们已经在图10.5看到RNN如何将输入序列映射成固定大小的向量，在；图10.9中看到RNN如何将固定大小的向量映射成一个序列，在图10.3、；图10.4、图10.10和图10.11中看到RNN如何将一个输入序列映射到等长
          關鍵詞：如何将输入序列映射成固定大小的向量, 在图, 如何将一个输入序列映射到等长, 和图, 中看到
        - 摘要：本节我们讨论如何训练RNN，使其将输入序列映射到不一定等长的输出；序列。这在许多场景中都有应用，如语音识别、机器翻译或问答，其中；训练集的输入和输出序列的长度通常不相同（虽然它们的长度可能相
          關鍵詞：如语音识别, 其中, 机器翻译或问答, 训练集的输入和输出序列的长度通常不相同, 使其将输入序列映射到不一定等长的输出
        - 摘要：我们经常将RNN的输入称为“上下文”。我们希望产生此上下文的表示；C。这个上下文C可能是一个概括输入序列 X ＝( x  (1)  ,…,；或者向量序列。
          關鍵詞：的输入称为, 或者向量序列, 可能是一个概括输入序列, 这个上下文, 我们经常将
        - 摘要：)的向量
          關鍵詞：的向量
        - 摘要：用于映射可变长度序列到另一可变长度序列最简单的RNN架构最初由；Cho  et  al.  （2014a）提出，之后不久由Sutskever  et  al.  （2014）独立开；发，并且第一个使用这种方法获得翻译的最好结果。前一系统是对另一
          關鍵詞：架构最初由, 提出, 独立开, 前一系统是对另一, 并且第一个使用这种方法获得翻译的最好结果
        - 摘要：（input）RNN处理输入序列。编码器输出上下文；（reader）或输入；C（通常是最终隐藏状态的简单函数）。（2）解码器  （decoder）或写
          關鍵詞：或输入, 编码器输出上下文, 处理输入序列, 通常是最终隐藏状态的简单函数, 或写
        - 摘要：图10.12　在给定输入序列
          關鍵詞：在给定输入序列
        - 摘要：的情况下学习生成输出序列
          關鍵詞：的情况下学习生成输出序列
        - 摘要：的编码器-解码器或序列到序列的RNN架构的示例。它由读取输入序
          關鍵詞：解码器或序列到序列的, 它由读取输入序, 的编码器, 架构的示例
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；列的编码器RNN以及生成输出序列（或计算给定输出序列的概率）的解码器RNN组成。编码器；RNN的最终隐藏状态用于计算一般为固定大小的上下文变量C，C表示输入序列的语义概要并且
          關鍵詞：组成, 或计算给定输出序列的概率, 表示输入序列的语义概要并且, 列的编码器, 编码器
        - 摘要：如果上下文C是一个向量，则编码器RNN只是在第10.2.4节描述的向量；到序列RNN。正如我们所见，向量到序列RNN至少有两种接受输入的；方法。输入可以被提供为RNN的初始状态，或连接到每个时间步中的隐
          關鍵詞：正如我们所见, 至少有两种接受输入的, 的初始状态, 如果上下文, 或连接到每个时间步中的隐
        - 摘要：这里并不强制要求编码器与解码器的隐藏层具有相同的大小。
          關鍵詞：这里并不强制要求编码器与解码器的隐藏层具有相同的大小
        - 摘要：此架构的一个明显不足是，编码器RNN输出的上下文C的维度太小而难；以适当地概括一个长序列。这种现象由Bahdanau  et  al.  （2015）在机器；翻译中观察到。他们提出让C成为可变长度的序列，而不是一个固定大
          關鍵詞：输出的上下文, 而不是一个固定大, 成为可变长度的序列, 的维度太小而难, 此架构的一个明显不足是
    10.5：深度循环网络
        - 摘要：大多数RNN中的计算可以分解成3块参数及其相关的变换：
          關鍵詞：块参数及其相关的变换, 中的计算可以分解成, 大多数
        - 摘要：（1）从输入到隐藏状态。
          關鍵詞：从输入到隐藏状态
        - 摘要：（2）从前一隐藏状态到下一隐藏状态。
          關鍵詞：从前一隐藏状态到下一隐藏状态
        - 摘要：（3）从隐藏状态到输出。
          關鍵詞：从隐藏状态到输出
        - 摘要：根据图10.3中的RNN架构，这3个块都与单个权重矩阵相关联。换句话；说，当网络被展开时，每个块对应一个浅的变换。能通过深度MLP内单；个层来表示的变换称为浅变换。通常，这是由学成的仿射变换和一个固
          關鍵詞：能通过深度, 这是由学成的仿射变换和一个固, 当网络被展开时, 通常, 内单
        - 摘要：图10.13　循环神经网络可以通过许多方式变得更深（Pascanu et al. ，2014a）。（a）隐藏循环；状态可以被分解为具有层次的组。（b）可以向输入到隐藏、隐藏到隐藏以及隐藏到输出的部分；引入更深的计算（如MLP）。这可以延长链接不同时间步的最短路径。（c）可以引入跳跃连接
          關鍵詞：状态可以被分解为具有层次的组, 可以向输入到隐藏, 这可以延长链接不同时间步的最短路径, 隐藏循环, 循环神经网络可以通过许多方式变得更深
        - 摘要：在这些操作中引入深度会有利吗？实验证据（Graves  et  al.  ，2013；；Pascanu  et  al.  ，2014a）强烈暗示理应如此。实验证据与我们需要足够；的深度以执行所需映射的想法一致。读者可以参考
          關鍵詞：强烈暗示理应如此, 的深度以执行所需映射的想法一致, 实验证据, 实验证据与我们需要足够, 在这些操作中引入深度会有利吗
        - 摘要：Graves  et  al.  （2013）第一个展示了将RNN的状态分为多层的显著好；处，如图10.13（a）所示。我们可以认为，在图10.13（a）所示层次结；构中较低的层起到了将原始输入转化为对更高层的隐藏状态更合适表示
          關鍵詞：在图, 所示, 所示层次结, 第一个展示了将, 如图
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；优化困难而损害学习效果。在一般情况下，更容易优化较浅的架构，加；入图10.13（b）的额外深度导致从时间步t的变量到时间步t+1的最短路
          關鍵詞：优化困难而损害学习效果, 更容易优化较浅的架构, 的最短路, 在一般情况下, 入图
    10.6：递归神经网络
        - 摘要：递归神经网络 (2) 代表循环网络的另一个扩展，它被构造为深的树状结构；而不是RNN的链状结构，因此是不同类型的计算图。递归网络的典型计；算图如图10.14所示。递归神经网络由Pollack（1990）引入，而
          關鍵詞：因此是不同类型的计算图, 递归神经网络由, 递归神经网络, 的链状结构, 它被构造为深的树状结构
        - 摘要：递归网络的一个明显优势是，对于具有相同长度τ的序列，深度（通过；非线性操作的组合数量来衡量）可以急剧地从τ减小为O（logτ），这可；能有助于解决长期依赖。一个悬而未决的问题是如何以最佳的方式构造
          關鍵詞：一个悬而未决的问题是如何以最佳的方式构造, 对于具有相同长度, 这可, 能有助于解决长期依赖, 递归网络的一个明显优势是
        - 摘要：图10.14　递归网络将循环网络的链状计算图推广到树状计算图。可变大小的序列 x (1) ， x (2) ,；…, x (t) 可以通过固定的参数集合（权重矩阵 U ， V ， W ）映射到固定大小的表示（输出 o ）。；该图展示了监督学习的情况，其中提供了一些与整个序列相关的目标 y
          關鍵詞：输出, 权重矩阵, 可变大小的序列, 该图展示了监督学习的情况, 递归网络将循环网络的链状计算图推广到树状计算图
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；al.  （1997）和；递归网络想法的变种存在很多。例如，Frasconi
          關鍵詞：递归网络想法的变种存在很多, 例如
        - 摘要：al.
          關鍵詞：
        - 摘要：et
          關鍵詞：
        - 摘要：et
          關鍵詞：
    10.7：长期依赖的挑战
        - 摘要：学习循环网络长期依赖的数学挑战在第8.2.5节中引入。根本问题是，经；过许多阶段传播后的梯度倾向于消失（大部分情况）或爆炸（很少，但；对优化过程影响很大）。即使我们假设循环网络是参数稳定的（可存储
          關鍵詞：对优化过程影响很大, 很少, 即使我们假设循环网络是参数稳定的, 过许多阶段传播后的梯度倾向于消失, 节中引入
        - 摘要：循环网络涉及相同函数的多次组合，每个时间步一次。这些组合可以导；致极端非线性行为，如图10.15所示。
          關鍵詞：循环网络涉及相同函数的多次组合, 所示, 致极端非线性行为, 每个时间步一次, 这些组合可以导
        - 摘要：图10.15　重复组合函数。当组合许多非线性函数（如这里所示的线性tanh层）时，结果是高度；非线性的，通常大多数值与微小的导数相关联，也有一些具有大导数的值，以及在增加和减小；之间的多次交替。此处，我们绘制从100维隐藏状态降到单个维度的线性投影，绘制于y轴上。x
          關鍵詞：结果是高度, 也有一些具有大导数的值, 轴上, 绘制于, 重复组合函数
        - 摘要：特别的是，循环神经网络所使用的函数组合有点像矩阵乘法。我们可以；认为，循环联系
          關鍵詞：循环神经网络所使用的函数组合有点像矩阵乘法, 认为, 我们可以, 循环联系, 特别的是
        - 摘要：是一个非常简单的、缺少非线性激活函数和输入 x 的循环神经网络。如；第8.2.5节描述，这种递推关系本质上描述了幂法。它可以被简化为
          關鍵詞：节描述, 缺少非线性激活函数和输入, 它可以被简化为, 这种递推关系本质上描述了幂法, 是一个非常简单的
        - 摘要：而当 W 符合下列形式的特征分解
          關鍵詞：符合下列形式的特征分解, 而当
        - 摘要：其中 Q 正交，循环性可进一步简化为
          關鍵詞：正交, 其中, 循环性可进一步简化为
        - 摘要：特征值提升到t次后，导致幅值不到一的特征值衰减到零，而幅值大于；一的就会激增。任何不与最大特征向量对齐的  h  (0)  的部分将最终被丢；弃。
          關鍵詞：特征值提升到, 次后, 而幅值大于, 任何不与最大特征向量对齐的, 一的就会激增
        - 摘要：这个问题是针对循环网络的。在标量情况下，想象多次乘一个权重w。；该乘积w t 消失还是爆炸取决于w的幅值。然而，如果每个时刻使用不同；权重w  (t)  的非循环网络，情况就不同了。如果初始状态给定为1，那么
          關鍵詞：消失还是爆炸取决于, 如果每个时刻使用不同, 情况就不同了, 这个问题是针对循环网络的, 的幅值
        - 摘要：RNN梯度消失和爆炸问题是由不同研究人员独立发现（Hochreiter，；1991a；Bengio  et  al.  ，1993，1994b）。有人可能会希望通过简单地停；留在梯度不消失或爆炸的参数空间来避免这个问题。不幸的是，为了储
          關鍵詞：留在梯度不消失或爆炸的参数空间来避免这个问题, 梯度消失和爆炸问题是由不同研究人员独立发现, 有人可能会希望通过简单地停, 不幸的是, 为了储
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；依赖时，长期相互作用的梯度幅值就会变得指数小（相比短期相互作用；的梯度幅值）。这并不意味着这是不可能学习的，由于长期依赖关系的
          關鍵詞：的梯度幅值, 依赖时, 这并不意味着这是不可能学习的, 由于长期依赖关系的, 相比短期相互作用
        - 摘要：将循环网络作为动力系统更深入探讨的资料见Doya（1993）；Bengio  et；al.  （1994b）；Siegel-mann  and  Sontag（1995）及Pascanu  et  al.；（2013b）的回顾。本章的其余部分将讨论目前已经提出的降低学习长
          關鍵詞：将循环网络作为动力系统更深入探讨的资料见, 本章的其余部分将讨论目前已经提出的降低学习长, 的回顾
    10.8：回声状态网络
        - 摘要：从 h (t-1) 到 h (t) 的循环权重映射以及从 x (t) 到 h (t) 的输入权重映射是循环；网络中最难学习的参数。研究者（Jaeger，2003；Maass  et  al.  ，2002；；Jaeger  and  Haas，2004；Jaeger，2007b）提出避免这种困难的方法是设
          關鍵詞：研究者, 的输入权重映射是循环, 网络中最难学习的参数, 的循环权重映射以及从, 提出避免这种困难的方法是设
        - 摘要：（reservoir
          關鍵詞：
        - 摘要：储层计算循环网络类似于核机器，这是思考它们的一种方式：它们将任；意长度的序列（到时刻t的输入历史）映射为一个长度固定的向量（循；环状态  h  (t)  ），之后可以施加一个线性预测算子（通常是一个线性回
          關鍵詞：的输入历史, 之后可以施加一个线性预测算子, 储层计算循环网络类似于核机器, 环状态, 通常是一个线性回
        - 摘要：因此，重要的问题是：如何设置输入和循环权重，才能让一组丰富的历
          關鍵詞：如何设置输入和循环权重, 因此, 才能让一组丰富的历, 重要的问题是
        - 摘要：史可以在循环神经网络的状态中表示？储层计算研究给出的答案是将循；环网络视为动态系统，并设定让动态系统接近稳定边缘的输入和循环权；重。
          關鍵詞：并设定让动态系统接近稳定边缘的输入和循环权, 环网络视为动态系统, 储层计算研究给出的答案是将循, 史可以在循环神经网络的状态中表示
        - 摘要：最初的想法是使状态到状态转换函数的Jacobian矩阵的特征值接近1。如；第8.2.5节解释，循环网络的一个重要特征就是Jacobian矩阵的特征值谱
          關鍵詞：循环网络的一个重要特征就是, 矩阵的特征值接近, 节解释, 矩阵的特征值谱, 最初的想法是使状态到状态转换函数的
        - 摘要：。特别重要的是，  J  (t)  的谱半径  （spectral
          關鍵詞：的谱半径, 特别重要的是
        - 摘要：radius）定义为特征值的最大绝对值。
          關鍵詞：定义为特征值的最大绝对值
        - 摘要：为了解谱半径的影响，可以考虑反向传播中Jacobian矩阵  J  不随t改变的；简单情况。例如当网络是纯线性时，会发生这种情况。假设  J  特征值λ；对应的特征向量为 ν  。考虑当我们通过时间向后传播梯度向量时会发生
          關鍵詞：对应的特征向量为, 可以考虑反向传播中, 特征值, 矩阵, 会发生这种情况
        - 摘要：当｜λ｜＞1，偏差δ｜λ｜  n  就会指数增长。当｜λ｜＜1，偏差就会变得；指数减小。
          關鍵詞：指数减小, 偏差就会变得, 偏差, 就会指数增长
        - 摘要：当然，这个例子假定Jacobian矩阵在每个时间步是相同的，即对应于没；有非线性循环网络。当非线性存在时，非线性的导数将在许多时间步后；接近零，并有助于防止因过大的谱半径而导致的爆炸。事实上，关于回
          關鍵詞：这个例子假定, 当非线性存在时, 事实上, 即对应于没, 有非线性循环网络
        - 摘要：我们已经说过多次，通过反复矩阵乘法的反向传播同样适用于没有非线；性的正向传播的网络，其状态为
          關鍵詞：我们已经说过多次, 通过反复矩阵乘法的反向传播同样适用于没有非线, 其状态为, 性的正向传播的网络
        - 摘要：。
          關鍵詞：
        - 摘要：如果线性映射
          關鍵詞：如果线性映射
        - 摘要：在L 2 范数的测度下总是缩小 h ，那么我们说这个映
          關鍵詞：那么我们说这个映, 范数的测度下总是缩小
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；射是收缩（contractive）的。当谱半径小于一，则从 h (t) 到 h (t+1) 的映射；是收缩的，因此小变化在每个时间步后变得更小。当我们使用有限精度
          關鍵詞：则从, 是收缩的, 的映射, 射是收缩, 因此小变化在每个时间步后变得更小
        - 摘要：Jacobian矩阵告诉我们  h  (t)  一个微小的变化如何向前一步传播，或等价；地， h (t+1) 的梯度如何向后一步传播。需要注意的是， W 和 J 都不需要；是对称的（尽管它们是实方阵），因此它们可能有复的特征值和特征向
          關鍵詞：需要注意的是, 是对称的, 或等价, 尽管它们是实方阵, 因此它们可能有复的特征值和特征向
        - 摘要：非线性映射情况时，Jacobian会在每一步任意变化。因此，动态量变得；更加复杂。然而，一个小的初始变化多步之后仍然会变成一个大的变；化。纯线性和非线性情况的一个不同之处在于使用压缩非线性（如
          關鍵詞：非线性映射情况时, 因此, 纯线性和非线性情况的一个不同之处在于使用压缩非线性, 然而, 更加复杂
        - 摘要：回声状态网络的策略是简单地固定权重，使其具有一定的谱半径如3，；其中信息通过时间前向传播，但会由于饱和非线性单元（如tanh）的稳；定作用而不会爆炸。
          關鍵詞：但会由于饱和非线性单元, 定作用而不会爆炸, 回声状态网络的策略是简单地固定权重, 其中信息通过时间前向传播, 使其具有一定的谱半径如
        - 摘要：最近，已经有研究表明，用于设置ESN权重的技术可以用来初始化完全；可训练的循环网络的权重（通过时间反向传播来训练隐藏到隐藏的循环；et  al.  ，
          關鍵詞：已经有研究表明, 用于设置, 权重的技术可以用来初始化完全, 最近, 可训练的循环网络的权重
    10.9：渗漏单元和其他多时间尺度的策略
        - 摘要：10.9.1　时间维度的跳跃连接
          關鍵詞：时间维度的跳跃连接
        - 摘要：10.9.2　渗漏单元和一系列不同时间尺度
          關鍵詞：渗漏单元和一系列不同时间尺度
        - 摘要：10.9.3　删除连接
          關鍵詞：删除连接
        - 摘要：处理长期依赖的一种方法是设计工作在多个时间尺度的模型，使模型的；某些部分在细粒度时间尺度上操作并能处理小细节，而其他部分在粗时
          關鍵詞：某些部分在细粒度时间尺度上操作并能处理小细节, 而其他部分在粗时, 处理长期依赖的一种方法是设计工作在多个时间尺度的模型, 使模型的
        - 摘要：间尺度上操作并能把遥远过去的信息更有效地传递过来。存在多种同时；构建粗细时间尺度的策略。这些策略包括在时间轴增加跳跃连接，“渗；漏单元”使用不同时间常数整合信号，并去除一些用于建模细粒度时间
          關鍵詞：构建粗细时间尺度的策略, 使用不同时间常数整合信号, 漏单元, 并去除一些用于建模细粒度时间, 存在多种同时
        - 摘要：10.9.1　时间维度的跳跃连接
          關鍵詞：时间维度的跳跃连接
        - 摘要：增加从遥远过去的变量到目前变量的直接连接是得到粗时间尺度的一种；方法。使用这样跳跃连接的想法可以追溯到Lin et al. （1996），紧接是；向前馈网络引入延迟的想法（Lang  and  Hinton，1988）。在普通的循环
          關鍵詞：使用这样跳跃连接的想法可以追溯到, 方法, 紧接是, 在普通的循环, 增加从遥远过去的变量到目前变量的直接连接是得到粗时间尺度的一种
        - 摘要：正如我们在第8.2.5节看到，梯度可能关于时间步数呈指数消失或爆炸。；（Lin  et  al.  ，1996）引入了d延时的循环连接以减轻这个问题。现在导
          關鍵詞：延时的循环连接以减轻这个问题, 现在导, 梯度可能关于时间步数呈指数消失或爆炸, 节看到, 引入了
        - 摘要：数指数减小的速度与   相关而不是τ。既然同时存在延迟和单步连
          關鍵詞：数指数减小的速度与, 相关而不是, 既然同时存在延迟和单步连
        - 摘要：接，梯度仍可能成t指数爆炸。这允许学习算法捕获更长的依赖性，但；不是所有的长期依赖都能在这种方式下良好地表示。
          關鍵詞：不是所有的长期依赖都能在这种方式下良好地表示, 梯度仍可能成, 这允许学习算法捕获更长的依赖性, 指数爆炸
        - 摘要：10.9.2　渗漏单元和一系列不同时间尺度
          關鍵詞：渗漏单元和一系列不同时间尺度
        - 摘要：获得导数乘积接近1的另一方式是设置线性自连接单元，并且这些连接；的权重接近1。
          關鍵詞：并且这些连接, 的另一方式是设置线性自连接单元, 获得导数乘积接近, 的权重接近
        - 摘要：我们对某些ν值应用更新µ (t) ←αµ  (t-1)  +(1-α)ν  (t) 累积一个滑动平均值µ  (t)；，其中α是一个从µ  (t-1)  到µ  (t)  线性自连接的例子。当α接近1时，滑动平；均值能记住过去很长一段时间的信息，而当α接近0，关于过去的信息被
          關鍵詞：而当, 其中, 线性自连接的例子, 均值能记住过去很长一段时间的信息, 关于过去的信息被
        - 摘要：d时间步的跳跃连接可以确保单元总能被d个时间步前的那个值影响。使；用权重接近1的线性自连接是确保该单元可以访问过去值的不同方式。；线性自连接通过调节实值α更平滑灵活地调整这种效果，而不是调整整
          關鍵詞：时间步的跳跃连接可以确保单元总能被, 用权重接近, 线性自连接通过调节实值, 个时间步前的那个值影响, 更平滑灵活地调整这种效果
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；这个想法由Mozer（1992）和El  Hihi  and  Bengio（1996）提出。在回声；状态网络中，渗漏单元也被发现很有用（Jaeger et al. ，2007）。
          關鍵詞：在回声, 提出, 状态网络中, 渗漏单元也被发现很有用, 这个想法由
        - 摘要：我们可以通过两种基本策略设置渗漏单元使用的时间常数。一种策略是；手动将其固定为常数，例如在初始化时从某些分布采样它们的值。另一；种策略是使时间常数成为自由变量，并学习出来。在不同时间尺度使用
          關鍵詞：并学习出来, 在不同时间尺度使用, 种策略是使时间常数成为自由变量, 例如在初始化时从某些分布采样它们的值, 一种策略是
        - 摘要：10.9.3　删除连接
          關鍵詞：删除连接
        - 摘要：处理长期依赖的另一种方法是在多个时间尺度组织RNN状态的想法（El；Hihi  and  Bengio，1996），信息在较慢的时间尺度上更容易长距离流；动。
          關鍵詞：处理长期依赖的另一种方法是在多个时间尺度组织, 信息在较慢的时间尺度上更容易长距离流, 状态的想法
        - 摘要：这个想法与之前讨论的时间维度上的跳跃连接不同，因为它涉及主动删；除长度为一的连接并用更长的连接替换它们。以这种方式修改的单元被；迫在长时间尺度上运作。而通过时间跳跃连接是添加边。收到这种新连
          關鍵詞：这个想法与之前讨论的时间维度上的跳跃连接不同, 因为它涉及主动删, 收到这种新连, 迫在长时间尺度上运作, 而通过时间跳跃连接是添加边
        - 摘要：强制一组循环单元在不同时间尺度上运作有不同的方式。一种选择是使；循环单元变成渗漏单元，但不同的单元组关联不同的固定时间尺度。这；由Mozer（1992）提出，并被成功应用于Pascanu et al.  （2013a）。另一
          關鍵詞：但不同的单元组关联不同的固定时间尺度, 强制一组循环单元在不同时间尺度上运作有不同的方式, 提出, 另一, 并被成功应用于
        - 摘要：10.9.1　时间维度的跳跃；连接
          關鍵詞：时间维度的跳跃, 连接
        - 摘要：10.9.2　渗漏单元和一系；列不同时间尺度
          關鍵詞：渗漏单元和一系, 列不同时间尺度
        - 摘要：10.9.3　删除连接
          關鍵詞：删除连接
    10.10：长短期记忆和其他门控RNN
        - 摘要：10.10.1　LSTM
          關鍵詞：
        - 摘要：10.10.2　其他门控RNN
          關鍵詞：其他门控
        - 摘要：本书撰写之时，实际应用中最有效的序列模型称为门控RNN  （gated；RNN）。包括基于长短期记忆  （long  short-term  memory）和基于门控；循环单元 （gated recurrent unit）的网络。
          關鍵詞：本书撰写之时, 的网络, 循环单元, 和基于门控, 包括基于长短期记忆
        - 摘要：像渗漏单元一样，门控RNN想法也是基于生成通过时间的路径，其中导；数既不消失也不发生爆炸。渗漏单元通过手动选择常量的连接权重或参
          關鍵詞：数既不消失也不发生爆炸, 其中导, 想法也是基于生成通过时间的路径, 像渗漏单元一样, 门控
        - 摘要：数化的连接权重来达到这一目的。门控RNN将其推广为在每个时间步都；可能改变的连接权重。
          關鍵詞：将其推广为在每个时间步都, 门控, 数化的连接权重来达到这一目的, 可能改变的连接权重
        - 摘要：渗漏单元允许网络在较长持续时间内积累信息（诸如用于特定特征或类；的线索）。然而，一旦该信息被使用，让神经网络遗忘旧的状态可能是；有用的。例如，如果一个序列是由子序列组成，我们希望渗漏单元能在
          關鍵詞：渗漏单元允许网络在较长持续时间内积累信息, 如果一个序列是由子序列组成, 我们希望渗漏单元能在, 有用的, 然而
        - 摘要：10.10.1　LSTM
          關鍵詞：
        - 摘要：引入自循环的巧妙构思，以产生梯度长时间持续流动的路径是初始长短；期记忆 （long short-term memory，LSTM）模型的核心贡献（Hochreiter；and  Schmidhuber，1997）。其中一个关键扩展是使自循环的权重视上下
          關鍵詞：期记忆, 引入自循环的巧妙构思, 其中一个关键扩展是使自循环的权重视上下, 模型的核心贡献, 以产生梯度长时间持续流动的路径是初始长短
        - 摘要：LSTM块如图10.16所示。在浅循环网络的架构下，相应的前向传播公式；如下。更深的架构也被成功应用（Graves et al. ，2013；Pascanu et al. ，；2014a）。LSTM循环网络除了外部的RNN循环外，还具有内部
          關鍵詞：块如图, 循环网络除了外部的, 循环外, 还具有内部, 如下
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图10.16　 LSTM循环网络“细胞”的框图。细胞彼此循环连接，代替一般循环网络中普通的隐藏；单元。这里使用常规的人工神经元计算输入特征。如果sigmoid输入门允许，它的值可以累加到
          關鍵詞：如果, 这里使用常规的人工神经元计算输入特征, 的框图, 循环网络, 细胞
        - 摘要：其中 x  (t) 是当前输入向量，  h  t  是当前隐藏层向量， h  t  包含所有LSTM；细胞的输出。 b f 、 U f 、 W f 分别是偏置、输入权重和遗忘门的循环权；重。因此LSTM细胞内部状态以如下方式更新，其中有一个条件的自环
          關鍵詞：是当前隐藏层向量, 其中有一个条件的自环, 细胞的输出, 因此, 其中
        - 摘要：其中  b  、  U  、  W  分别是LSTM细胞中的偏置、输入权重和遗忘门的循；环权重。外部输入门  （external  input  gate）单元   以类似遗忘门（使；用sigmoid获得一个0和1之间的值）的方式更新，但有自身的参数：
          關鍵詞：细胞中的偏置, 之间的值, 以类似遗忘门, 外部输入门, 其中
        - 摘要：LSTM细胞的输出   也可以由输出门  （output  gate）   关闭（使用；sigmoid单元作为门控）：
          關鍵詞：也可以由输出门, 单元作为门控, 细胞的输出, 关闭, 使用
        - 摘要：其中 b o 、 U o 、 W o 分别是偏置、输入权重和遗忘门的循环权重。在这；些变体中，可以选择使用细胞状态   作为额外的输入（及其权重），；输入到第i个单元的3个门，如图10.16所示。这将需要3个额外的参数。
          關鍵詞：些变体中, 输入权重和遗忘门的循环权重, 在这, 作为额外的输入, 及其权重
        - 摘要：LSTM网络比简单的循环架构更易于学习长期依赖，先是用于测试长期；依赖学习能力的人工数据集（Bengio  et  al.  ，1994c；Hochreiter  and；Schmidhuber，1997；Hochreiter et al. ，2001），然后是在具有挑战性的
          關鍵詞：网络比简单的循环架构更易于学习长期依赖, 先是用于测试长期, 然后是在具有挑战性的, 依赖学习能力的人工数据集
        - 摘要：10.10.2　其他门控RNN
          關鍵詞：其他门控
        - 摘要：LSTM架构中哪些部分是真正必需的？还可以设计哪些其他成功架构允；许网络动态地控制时间尺度和不同单元的遗忘行为？
          關鍵詞：架构中哪些部分是真正必需的, 许网络动态地控制时间尺度和不同单元的遗忘行为, 还可以设计哪些其他成功架构允
        - 摘要：最近关于门控RNN的工作给出了这些问题的某些答案，其单元也被称为；门控循环单元或GRU（Cho  et  al.  ，2014c；Chung  et  al.  ，2014，；2015a；Jozefowicz  et  al.  ，2015；Chrupala  et  al.  ，2015）。与LSTM的
          關鍵詞：门控循环单元或, 其单元也被称为, 的工作给出了这些问题的某些答案, 最近关于门控
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；其中 u 代表“更新”门， r 表示“复位”门。它们的值就如通常所定义的：
          關鍵詞：表示, 代表, 更新, 其中, 它们的值就如通常所定义的
        - 摘要：和
          關鍵詞：
        - 摘要：复位和更新门能独立地“忽略”状态向量的一部分。更新门像条件渗漏累；积器一样可以线性门控任意维度，从而选择将它复制（在sigmoid的一；个极端）或完全由新的“目标状态”值（朝向渗漏累积器的收敛方向）替
          關鍵詞：复位和更新门能独立地, 忽略, 积器一样可以线性门控任意维度, 从而选择将它复制, 目标状态
        - 摘要：围绕这一主题可以设计更多的变种。例如复位门（或遗忘门）的输出可；以在多个隐藏单元间共享。或者，全局门的乘积（覆盖一整组的单元，；例如整一层）和一个局部门（每单元）可用于结合全局控制和局部控
          關鍵詞：以在多个隐藏单元间共享, 围绕这一主题可以设计更多的变种, 例如整一层, 每单元, 可用于结合全局控制和局部控
        - 摘要：10.10.1　LSTM
          關鍵詞：
        - 摘要：10.10.2　其他门控RNN
          關鍵詞：其他门控
    10.11：优化长期依赖
        - 摘要：10.11.1　截断梯度
          關鍵詞：截断梯度
        - 摘要：10.11.2　引导信息流的正则化
          關鍵詞：引导信息流的正则化
        - 摘要：我们已经在第8.2.5节和第10.7节中描述过在许多时间步上优化RNN时发；生的梯度消失和爆炸的问题。
          關鍵詞：节和第, 节中描述过在许多时间步上优化, 时发, 生的梯度消失和爆炸的问题, 我们已经在第
        - 摘要：由Martens and Sutskever（2011）提出了一个有趣的想法是，二阶导数可；能在一阶导数消失的同时消失。二阶优化算法可以大致被理解为将一阶；导数除以二阶导数（在更高维数，由梯度乘以Hessian的逆）。如果二阶
          關鍵詞：提出了一个有趣的想法是, 二阶导数可, 的逆, 能在一阶导数消失的同时消失, 如果二阶
        - 摘要：导数与一阶导数以类似的速率收缩，那么一阶和二阶导数的比率可保持；相对恒定。不幸的是，二阶方法有许多缺点，包括高的计算成本、需要；一个大的小批量并且倾向于被吸引到鞍点。Martens
          關鍵詞：二阶方法有许多缺点, 一个大的小批量并且倾向于被吸引到鞍点, 导数与一阶导数以类似的速率收缩, 相对恒定, 那么一阶和二阶导数的比率可保持
        - 摘要：10.11.1　截断梯度
          關鍵詞：截断梯度
        - 摘要：如第8.2.4节讨论，强非线性函数（如由许多时间步计算的循环网络）往；往倾向于非常大或非常小幅度的梯度。如图8.3和图10.17所示，我们可；以看到，目标函数（作为参数的函数）存在一个伴随“悬崖”的“地形”：
          關鍵詞：目标函数, 我们可, 和图, 强非线性函数, 所示
        - 摘要：这导致的困难是，当参数梯度非常大时，梯度下降的参数更新可以将参；数抛出很远，进入目标函数较大的区域，到达当前解所做的努力变成了；无用功。梯度告诉我们，围绕当前参数的无穷小区域内最速下降的方
          關鍵詞：这导致的困难是, 无用功, 数抛出很远, 梯度告诉我们, 当参数梯度非常大时
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图10.17　梯度截断在有两个参数 w 和 b 的循环网络中的效果示例。梯度截断可以使梯度下降在；极陡峭的悬崖附近更合理地执行。这些陡峭的悬崖通常发生在循环网络中，位于循环网络近似
          關鍵詞：梯度截断在有两个参数, 这些陡峭的悬崖通常发生在循环网络中, 极陡峭的悬崖附近更合理地执行, 梯度截断可以使梯度下降在, 的循环网络中的效果示例
        - 摘要：一个简单的解决方案已被从业者使用多年：截断梯度  （clipping；gradient）。此想法有不同实例（Mikolov，2012；Pascanu；2013a）。一种选择是在参数更新之前，逐元素地截断小批量产生的参
          關鍵詞：一个简单的解决方案已被从业者使用多年, 此想法有不同实例, 截断梯度, 一种选择是在参数更新之前, 逐元素地截断小批量产生的参
        - 摘要：the；et  al.  ，
          關鍵詞：
        - 摘要：其中ν是范数上界，  g  用来更新参数。因为所有参数（包括不同的参数；组，如权重和偏置）的梯度被单个缩放因子联合重整化，所以后一方法；具有的优点是保证了每个步骤仍然是在梯度方向上的，但实验表明两种
          關鍵詞：的梯度被单个缩放因子联合重整化, 具有的优点是保证了每个步骤仍然是在梯度方向上的, 所以后一方法, 其中, 因为所有参数
        - 摘要：10.11.2　引导信息流的正则化
          關鍵詞：引导信息流的正则化
        - 摘要：梯度截断有助于处理爆炸的梯度，但它无助于消失的梯度。为了解决消；失的梯度问题并更好地捕获长期依赖，我们讨论了如下想法：在展开循；环架构的计算图中，沿着与弧边相关联的梯度乘积接近1的部分创建路
          關鍵詞：我们讨论了如下想法, 梯度截断有助于处理爆炸的梯度, 环架构的计算图中, 沿着与弧边相关联的梯度乘积接近, 的部分创建路
        - 摘要：与
          關鍵詞：
        - 摘要：一样大。在这个目标下，Pascanu et al. （2013a）提出以下正则项：
          關鍵詞：一样大, 提出以下正则项, 在这个目标下
        - 摘要：计算这一梯度的正则项可能会出现困难，但Pascanu  et  al.  （2013a）提；出可以将后向传播向量；考虑为恒值作为近似（为了计算正则化
          關鍵詞：考虑为恒值作为近似, 为了计算正则化, 出可以将后向传播向量, 计算这一梯度的正则项可能会出现困难
        - 摘要：这种方法的一个主要弱点是，在处理数据冗余的任务时如语言模型，它；并不像LSTM一样有效。
          關鍵詞：这种方法的一个主要弱点是, 在处理数据冗余的任务时如语言模型, 并不像, 一样有效
        - 摘要：10.11.1　截断梯度
          關鍵詞：截断梯度
        - 摘要：10.11.2　引导信息流的正；则化
          關鍵詞：引导信息流的正, 则化
    10.12：外显记忆
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；智能需要知识并且可以通过学习获取知识，这已促使大型深度架构的发；展。然而，知识是不同的并且种类繁多。有些知识是隐含的、潜意识的
          關鍵詞：然而, 潜意识的, 知识是不同的并且种类繁多, 有些知识是隐含的, 这已促使大型深度架构的发
        - 摘要：神经网络擅长存储隐性知识，但是它们很难记住事实。被存储在神经网；络参数中之前，随机梯度下降需要多次提供相同的输入，即使如此，该；输入也不会被特别精确地存储。Graves  et  al.  （2014）推测这是因为神
          關鍵詞：络参数中之前, 随机梯度下降需要多次提供相同的输入, 即使如此, 被存储在神经网, 推测这是因为神
        - 摘要：为了解决这一难题，Weston  et  al.  （2014）引入了记忆网络  （memory；network），其中包括一组可以通过寻址机制来访问的记忆单元。记忆；网络原本需要监督信号指示它们如何使用自己的记忆单元。Graves et al.
          關鍵詞：记忆, 网络原本需要监督信号指示它们如何使用自己的记忆单元, 引入了记忆网络, 其中包括一组可以通过寻址机制来访问的记忆单元, 为了解决这一难题
        - 摘要：每个记忆单元可以被认为是LSTM和GRU中记忆单元的扩展。不同的；是，网络输出一个内部状态来选择从哪个单元读取或写入，正如数字计；算机读取或写入到特定地址的内存访问。
          關鍵詞：正如数字计, 中记忆单元的扩展, 网络输出一个内部状态来选择从哪个单元读取或写入, 算机读取或写入到特定地址的内存访问, 每个记忆单元可以被认为是
        - 摘要：产生确切整数地址的函数很难优化。为了缓解这一问题，NTM实际同；时从多个记忆单元写入或读取。读取时，它们采取许多单元的加权平均；值。写入时，它们对多个单元修改不同的数值。用于这些操作的系数被
          關鍵詞：写入时, 用于这些操作的系数被, 时从多个记忆单元写入或读取, 产生确切整数地址的函数很难优化, 它们采取许多单元的加权平均
        - 摘要：选择为集中在一个小数目的单元，如通过softmax函数产生它们。使用；这些具有非零导数的权重允许函数控制访问存储器，从而能使用梯度下；降法优化。关于这些系数的梯度指示着其中每个参数是应该增加还是减
          關鍵詞：选择为集中在一个小数目的单元, 关于这些系数的梯度指示着其中每个参数是应该增加还是减, 如通过, 降法优化, 这些具有非零导数的权重允许函数控制访问存储器
        - 摘要：这些记忆单元通常扩充为包含向量，而不是由LSTM或GRU存储单元所；存储的单个标量。增加记忆单元大小的原因有两个。原因之一是，我们；已经增加了访问记忆单元的成本。我们为产生用于许多单元的系数付出
          關鍵詞：这些记忆单元通常扩充为包含向量, 已经增加了访问记忆单元的成本, 我们, 增加记忆单元大小的原因有两个, 存储的单个标量
        - 摘要：如果一个存储单元的内容在大多数时间步上会被复制（不被忘记），则；它包含的信息可以在时间上向前传播，随时间向后传播的梯度也不会消；失或爆炸。
          關鍵詞：不被忘记, 如果一个存储单元的内容在大多数时间步上会被复制, 随时间向后传播的梯度也不会消, 失或爆炸, 它包含的信息可以在时间上向前传播
        - 摘要：外显记忆的方法在图10.18说明，其中我们可以看到与存储器耦接的“任；务神经网络”。虽然这一任务神经网络可以是前馈或循环的，但整个系；统是一个循环网络。任务网络可以选择读取或写入的特定内存地址。外
          關鍵詞：但整个系, 其中我们可以看到与存储器耦接的, 任务网络可以选择读取或写入的特定内存地址, 务神经网络, 外显记忆的方法在图
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图10.18　具有外显记忆网络的示意图，具备神经网络图灵机的一些关键设计元素。在此图中，；我们将模型的“表示”部分（“任务网络”，这里是底部的循环网络）与存储事实的模型（记忆单
          關鍵詞：表示, 部分, 我们将模型的, 记忆单, 与存储事实的模型
        - 摘要：作为存储器单元的加权平均值反向传播的替代，我们可以将存储器寻址；系数解释为概率，并随机从一个单元读取（Zaremba  and  Sutskever，；2015）。优化离散决策的模型需要专门的优化算法，这将在第20.9.1节
          關鍵詞：作为存储器单元的加权平均值反向传播的替代, 我们可以将存储器寻址, 这将在第, 并随机从一个单元读取, 优化离散决策的模型需要专门的优化算法
        - 摘要：无论是软（允许反向传播）或随机硬性的，用于选择一个地址的机制与；先前在机器翻译的背景下引入的注意力机制形式相同（Bahdanau  et  al.；，2015），这在第12.4.5.1节中也有讨论。甚至更早之前，注意力机制
          關鍵詞：用于选择一个地址的机制与, 无论是软, 节中也有讨论, 或随机硬性的, 甚至更早之前
        - 摘要：循环神经网络提供了将深度学习扩展到序列数据的一种方法。它们是我；们的深度学习工具箱中最后一个主要的工具。现在我们的讨论将转移到；如何选择和使用这些工具，以及如何在真实世界的任务中应用这些工
          關鍵詞：现在我们的讨论将转移到, 如何选择和使用这些工具, 以及如何在真实世界的任务中应用这些工, 循环神经网络提供了将深度学习扩展到序列数据的一种方法, 它们是我
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)    给定这些变量的父变量，其条件分布是确定性的。尽管设计具有这样确定性的隐藏单元的；图模型是很少见的，但这是完全合理的。
          關鍵詞：图模型是很少见的, 其条件分布是确定性的, 尽管设计具有这样确定性的隐藏单元的, 给定这些变量的父变量, 但这是完全合理的
        - 摘要：(2)  我们建议不要将“递归神经网络”缩写为“RNN”，以免与“循环神经网络”混淆。
          關鍵詞：循环神经网络, 缩写为, 递归神经网络, 混淆, 以免与
第11章：实践方法论
    10.12：外显记忆
        - 摘要：要成功地使用深度学习技术，仅仅知道存在哪些算法和解释它们为何有；效的原理是不够的。一个优秀的机器学习实践者还需要知道如何针对具；体应用挑选一个合适的算法以及如何监控，并根据实验反馈改进机器学
          關鍵詞：要成功地使用深度学习技术, 一个优秀的机器学习实践者还需要知道如何针对具, 体应用挑选一个合适的算法以及如何监控, 仅仅知道存在哪些算法和解释它们为何有, 效的原理是不够的
        - 摘要：本书的大部分内容都是关于不同的机器学习模型、训练算法和目标函；数，这可能给人一种印象——成为机器学习专家的最重要因素是了解各；种各样的机器学习技术，并熟悉各种不同的数学。在实践中，正确使用
          關鍵詞：这可能给人一种印象, 正确使用, 训练算法和目标函, 成为机器学习专家的最重要因素是了解各, 种各样的机器学习技术
        - 摘要：我们建议参考以下几个实践设计流程：
          關鍵詞：我们建议参考以下几个实践设计流程
        - 摘要：确定目标——使用什么样的误差度量，并为此误差度量指定目标；值。这些目标和误差度量取决于该应用旨在解决的问题。；尽快建立一个端到端的工作流程，包括估计合适的性能度量。
          關鍵詞：使用什么样的误差度量, 尽快建立一个端到端的工作流程, 确定目标, 这些目标和误差度量取决于该应用旨在解决的问题, 包括估计合适的性能度量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；根据具体观察反复地进行增量式的改动，如收集新数据、调整超参；数或改进算法。
          關鍵詞：如收集新数据, 数或改进算法, 调整超参, 根据具体观察反复地进行增量式的改动
        - 摘要：我们将使用街景地址号码转录系统（Goodfellow et  al.  ，2014d）作为一；个运行示例。该应用的目标是将建筑物添加到谷歌地图。街景车拍摄建；筑物，并记录与每张建筑照片相关的GPS坐标。卷积网络识别每张照片
          關鍵詞：筑物, 街景车拍摄建, 卷积网络识别每张照片, 坐标, 并记录与每张建筑照片相关的
        - 摘要：我们现在描述这个过程中的每一个步骤。
          關鍵詞：我们现在描述这个过程中的每一个步骤
    11.1：性能度量
        - 摘要：确定目标，即使用什么误差度量，是必要的第一步，因为误差度量将指；导接下来的所有工作。同时我们也应该了解大概能得到什么级别的目标；性能。
          關鍵詞：因为误差度量将指, 性能, 即使用什么误差度量, 导接下来的所有工作, 确定目标
        - 摘要：值得注意的是，对于大多数应用而言，不可能实现绝对零误差。即使你；有无限的训练数据，并且恢复了真正的概率分布，贝叶斯误差仍定义了；能达到的最小错误率。这是因为输入特征可能无法包含输出变量的完整
          關鍵詞：对于大多数应用而言, 不可能实现绝对零误差, 值得注意的是, 能达到的最小错误率, 有无限的训练数据
        - 摘要：训练数据的数量会因为各种原因受到限制。当目标是打造现实世界中最；好的产品或服务时，我们通常需要收集更多的数据，但必须确定进一步；减少误差的价值，并与收集更多数据的成本做权衡。数据收集会耗费时
          關鍵詞：并与收集更多数据的成本做权衡, 数据收集会耗费时, 好的产品或服务时, 当目标是打造现实世界中最, 训练数据的数量会因为各种原因受到限制
        - 摘要：如何确定合理的性能期望？在学术界，通常我们可以根据先前公布的基；准结果来估计预期错误率。在现实世界中，一个应用的错误率有必要是；安全的、具有成本效益的或吸引消费者的。一旦你确定了想要达到的错
          關鍵詞：准结果来估计预期错误率, 安全的, 一旦你确定了想要达到的错, 通常我们可以根据先前公布的基, 在学术界
        - 摘要：除了需要考虑性能度量之外，另一个需要考虑的是度量的选择。我们有
          關鍵詞：除了需要考虑性能度量之外, 我们有, 另一个需要考虑的是度量的选择
        - 摘要：几种不同的性能度量，可以用来度量一个含有机器学习组件的完整应用；的有效性。这些性能度量通常不同于训练模型的代价函数。如第5.1.2节；所述，我们通常会度量一个系统的准确率，或等价地，错误率。
          關鍵詞：可以用来度量一个含有机器学习组件的完整应用, 这些性能度量通常不同于训练模型的代价函数, 的有效性, 或等价地, 所述
        - 摘要：然而，许多应用需要更高级的度量。
          關鍵詞：许多应用需要更高级的度量, 然而
        - 摘要：有时，一种错误可能会比另一种错误更严重。例如，垃圾邮件检测系统；会有两种错误：将正常邮件错误地归为垃圾邮件，将垃圾邮件错误地归；为正常邮件。阻止正常消息比允许可疑消息通过糟糕得多。我们希望度
          關鍵詞：将正常邮件错误地归为垃圾邮件, 将垃圾邮件错误地归, 垃圾邮件检测系统, 有时, 一种错误可能会比另一种错误更严重
        - 摘要：有时，我们需要训练检测某些罕见事件的二元分类器。例如，我们可能；会为一种罕见疾病设计医疗测试。假设每一百万人中只有一人患病。我；们只需要让分类器一直报告没有患者，就能轻易地在检测任务上实现
          關鍵詞：会为一种罕见疾病设计医疗测试, 就能轻易地在检测任务上实现, 有时, 我们需要训练检测某些罕见事件的二元分类器, 例如
        - 摘要：x
          關鍵詞：
        - 摘要：另一种方法是报告PR曲线下方的总面积。
          關鍵詞：曲线下方的总面积, 另一种方法是报告
        - 摘要：在一些应用中，机器学习系统可能会拒绝作出判断。如果机器学习算法
          關鍵詞：如果机器学习算法, 机器学习系统可能会拒绝作出判断, 在一些应用中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；能够估计所作判断的置信度，这将会非常有用，特别是在错误判断会导；致严重危害，而人工操作员能够偶尔接管的情况下。街景转录系统可以
          關鍵詞：而人工操作员能够偶尔接管的情况下, 能够估计所作判断的置信度, 致严重危害, 这将会非常有用, 特别是在错误判断会导
        - 摘要：还有许多其他的性能度量。例如，我们可以度量点击率、收集用户满意；度调查等。许多专业的应用领域也有特定的标准。
          關鍵詞：许多专业的应用领域也有特定的标准, 我们可以度量点击率, 例如, 还有许多其他的性能度量, 度调查等
        - 摘要：最重要的是首先要确定改进哪个性能度量，然后专心提高性能度量。如；果没有明确的目标，那么我们很难判断机器学习系统上的改动是否有所；改进。
          關鍵詞：最重要的是首先要确定改进哪个性能度量, 然后专心提高性能度量, 那么我们很难判断机器学习系统上的改动是否有所, 改进, 果没有明确的目标
    11.2：默认的基准模型
        - 摘要：确定性能度量和目标后，任何实际应用的下一步是尽快建立一个合理的；端到端的系统。在本节中，我们提供了关于不同情况下使用哪种算法作；为第一基准方法的推荐。值得注意的是，深度学习研究进展迅速，所以
          關鍵詞：在本节中, 端到端的系统, 任何实际应用的下一步是尽快建立一个合理的, 深度学习研究进展迅速, 我们提供了关于不同情况下使用哪种算法作
        - 摘要：根据问题的复杂性，项目开始时可能无须使用深度学习。如果只需正确；地选择几个线性权重就可能解决问题，那么项目可以开始于一个简单的；统计模型，如逻辑回归。
          關鍵詞：地选择几个线性权重就可能解决问题, 统计模型, 如逻辑回归, 如果只需正确, 项目开始时可能无须使用深度学习
        - 摘要：如果问题属于“AI-完全”类的，如对象识别、语音识别、机器翻译等，；那么项目开始于一个合适的深度学习模型，效果会比较好。
          關鍵詞：语音识别, 效果会比较好, 完全, 那么项目开始于一个合适的深度学习模型, 机器翻译等
        - 摘要：首先，根据数据的结构选择一类合适的模型。如果项目是以固定大小的
          關鍵詞：首先, 根据数据的结构选择一类合适的模型, 如果项目是以固定大小的
        - 摘要：向量作为输入的监督学习，那么可以使用全连接的前馈网络。如果输入；已知的拓扑结构（例如，输入的是图像），那么可以使用卷积网络。在；这些情况下，刚开始可以使用某些分段线性单元（ReLU或者其扩展，
          關鍵詞：刚开始可以使用某些分段线性单元, 这些情况下, 输入的是图像, 那么可以使用卷积网络, 例如
        - 摘要：具有衰减学习率以及动量的SGD是优化算法一个合理的选择（流行的衰；减方法有，衰减到固定最低学习率的线性衰减、指数衰减，或每次发生；验证错误停滞时将学习率降低2～10倍，这些衰减方法在不同问题上好
          關鍵詞：是优化算法一个合理的选择, 这些衰减方法在不同问题上好, 减方法有, 衰减到固定最低学习率的线性衰减, 验证错误停滞时将学习率降低
        - 摘要：除非训练集包含数千万以及更多的样本，否则项目应该在一开始就包含；一些温和的正则化。提前终止也被普遍采用。Dropout也是一个很容易；实现，且兼容很多模型和训练算法的出色正则化项。批标准化有时也能
          關鍵詞：一些温和的正则化, 批标准化有时也能, 也是一个很容易, 实现, 除非训练集包含数千万以及更多的样本
        - 摘要：如果我们的任务和另一个被广泛研究的任务相似，那么通过复制先前研；究中已知性能良好的模型和算法，可能会得到很好的效果，甚至可以从；该任务中复制一个训练好的模型。例如，通常会使用在ImageNet上训练
          關鍵詞：该任务中复制一个训练好的模型, 可能会得到很好的效果, 甚至可以从, 通常会使用在, 如果我们的任务和另一个被广泛研究的任务相似
        - 摘要：一个常见问题是项目开始时是否使用无监督学习，我们将在第三部分进；一步探讨这个问题。这个问题和特定领域有关。在某些领域，比如自然；语言处理，能够大大受益于无监督学习技术，如学习无监督词嵌入。在
          關鍵詞：我们将在第三部分进, 比如自然, 这个问题和特定领域有关, 如学习无监督词嵌入, 语言处理
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；11.3　决定是否收集更多数据
          關鍵詞：决定是否收集更多数据
        - 摘要：在建立第一个端到端的系统后，就可以度量算法性能并决定如何改进算；法。许多机器学习新手都忍不住尝试很多不同的算法来进行改进。然；而，收集更多的数据往往比改进学习算法要有用得多。
          關鍵詞：许多机器学习新手都忍不住尝试很多不同的算法来进行改进, 收集更多的数据往往比改进学习算法要有用得多, 在建立第一个端到端的系统后, 就可以度量算法性能并决定如何改进算
        - 摘要：怎样判断是否要收集更多的数据？首先，确定训练集上的性能是否可接；受。如果模型在训练集上的性能就很差，学习算法都不能在训练集上学；习出良好的模型，那么就没必要收集更多的数据。反之，可以尝试增加
          關鍵詞：如果模型在训练集上的性能就很差, 怎样判断是否要收集更多的数据, 确定训练集上的性能是否可接, 学习算法都不能在训练集上学, 习出良好的模型
        - 摘要：如果训练集上的性能是可接受的，那么我们开始度量测试集上的性能。；如果测试集上的性能也是可以接受的，那么就顺利完成了。如果测试集；上的性能比训练集的要差得多，那么收集更多的数据是最有效的解决方
          關鍵詞：那么我们开始度量测试集上的性能, 上的性能比训练集的要差得多, 如果训练集上的性能是可接受的, 那么收集更多的数据是最有效的解决方, 如果测试集
        - 摘要：在决定是否收集更多的数据时，也需要确定收集多少数据。如图5.4所；示，绘制曲线显示训练集规模和泛化误差之间的关系是很有帮助的。根；据走势延伸曲线，可以预测还需要多少训练数据来达到一定的性能。通
          關鍵詞：也需要确定收集多少数据, 据走势延伸曲线, 可以预测还需要多少训练数据来达到一定的性能, 如图, 绘制曲线显示训练集规模和泛化误差之间的关系是很有帮助的
        - 摘要：如果收集更多的数据是不可行的，那么改进泛化误差的唯一方法是改进；学习算法本身。这属于研究领域，并非对应用实践者的建议。
          關鍵詞：这属于研究领域, 学习算法本身, 那么改进泛化误差的唯一方法是改进, 如果收集更多的数据是不可行的, 并非对应用实践者的建议
    11.4：选择超参数
        - 摘要：11.4.1　手动调整超参数
          關鍵詞：手动调整超参数
        - 摘要：11.4.2　自动超参数优化算法
          關鍵詞：自动超参数优化算法
        - 摘要：11.4.3　网格搜索
          關鍵詞：网格搜索
        - 摘要：11.4.4　随机搜索
          關鍵詞：随机搜索
        - 摘要：11.4.5　基于模型的超参数优化
          關鍵詞：基于模型的超参数优化
        - 摘要：大部分深度学习算法都有许多超参数来控制不同方面的算法表现。有些；超参数会影响算法运行的时间和存储成本，有些超参数会影响学习到的；模型质量以及在新输入上推断正确结果的能力。
          關鍵詞：超参数会影响算法运行的时间和存储成本, 模型质量以及在新输入上推断正确结果的能力, 有些超参数会影响学习到的, 大部分深度学习算法都有许多超参数来控制不同方面的算法表现, 有些
        - 摘要：有两种选择超参数的基本方法：手动选择和自动选择。手动选择超参数；需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛；化。自动选择超参数算法大大减少了了解这些想法的需要，但它们往往
          關鍵詞：自动选择超参数算法大大减少了了解这些想法的需要, 手动选择和自动选择, 以及机器学习模型如何才能取得良好的泛, 需要了解超参数做了些什么, 手动选择超参数
        - 摘要：11.4.1　手动调整超参数
          關鍵詞：手动调整超参数
        - 摘要：手动设置超参数，我们必须了解超参数、训练误差、泛化误差和计算资；源（内存和运行时间）之间的关系。这需要切实了解一个学习算法有效；容量的基础概念，如第5章所描述的。
          關鍵詞：训练误差, 手动设置超参数, 容量的基础概念, 内存和运行时间, 章所描述的
        - 摘要：手动搜索超参数的目标通常是最小化受限于运行时间和内存预算的泛化；误差。我们不去探讨如何确定各种超参数对运行时间和内存的影响，因；为这高度依赖于平台。
          關鍵詞：手动搜索超参数的目标通常是最小化受限于运行时间和内存预算的泛化, 为这高度依赖于平台, 误差, 我们不去探讨如何确定各种超参数对运行时间和内存的影响
        - 摘要：手动搜索超参数的主要目标是调整模型的有效容量以匹配任务的复杂；性。有效容量受限于3个因素：模型的表示容量、学习算法成功最小化；训练模型代价函数的能力，以及代价函数和训练过程正则化模型的程
          關鍵詞：以及代价函数和训练过程正则化模型的程, 训练模型代价函数的能力, 手动搜索超参数的主要目标是调整模型的有效容量以匹配任务的复杂, 个因素, 有效容量受限于
        - 摘要：当泛化误差以某个超参数为变量，作为函数绘制出来时，通常会表现为；U形曲线，如图5.3所示。在某个极端情况下，超参数对应着低容量，并；且泛化误差由于训练误差较大而很高。这便是欠拟合的情况。另一种极
          關鍵詞：通常会表现为, 超参数对应着低容量, 在某个极端情况下, 且泛化误差由于训练误差较大而很高, 这便是欠拟合的情况
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；之间的差距较大而很高。最优的模型容量位于曲线中间的某个位置，能；够达到最低可能的泛化误差，由某个中等的泛化误差和某个中等的训练
          關鍵詞：由某个中等的泛化误差和某个中等的训练, 够达到最低可能的泛化误差, 最优的模型容量位于曲线中间的某个位置, 之间的差距较大而很高
        - 摘要：对于某些超参数，当超参数数值太大时，会发生过拟合。例如中间层隐；藏单元的数量，增加数量能提高模型的容量，容易发生过拟合。对于某；些超参数，当超参数数值太小时，也会发生过拟合。例如，最小的权重
          關鍵詞：当超参数数值太大时, 会发生过拟合, 些超参数, 例如, 最小的权重
        - 摘要：并非每个超参数都能对应着完整的U形曲线。很多超参数是离散的，如；中间层单元数目或是maxout单元中线性元件的数目，这种情况只能沿曲；线探索一些点。有些超参数是二值的。通常这些超参数用来指定是否使
          關鍵詞：单元中线性元件的数目, 很多超参数是离散的, 这种情况只能沿曲, 线探索一些点, 通常这些超参数用来指定是否使
        - 摘要：学习率可能是最重要的超参数。如果你只有时间调整一个超参数，那就；调整学习率。相比其他超参数，它以一种更复杂的方式控制模型的有效；容量——当学习率适合优化问题时，模型的有效容量最高，此时学习率
          關鍵詞：相比其他超参数, 它以一种更复杂的方式控制模型的有效, 那就, 容量, 调整学习率
        - 摘要：图11.1　训练误差和学习率之间的典型关系。注意，当学习率大于最优值时，误差会有显著的；提升。此图针对固定的训练时间，越小的学习率有时候可以以一个正比于学习率减小量的因素；来减慢训练过程。泛化误差也会得到类似的曲线，由于正则项作用在学习率过大或过小处比较
          關鍵詞：由于正则项作用在学习率过大或过小处比较, 来减慢训练过程, 注意, 当学习率大于最优值时, 提升
        - 摘要：调整学习率外的其他参数时，需要同时监测训练误差和测试误差，以判；断模型是否过拟合或欠拟合，然后适当调整其容量。
          關鍵詞：调整学习率外的其他参数时, 然后适当调整其容量, 断模型是否过拟合或欠拟合, 需要同时监测训练误差和测试误差, 以判
        - 摘要：如果训练集错误率大于目标错误率，那么只能增加模型容量以改进模；型。如果没有使用正则化，并且确信优化算法正确运行，那么有必要添；加更多的网络层或隐藏单元。然而，令人遗憾的是，这增加了模型的计
          關鍵詞：那么有必要添, 加更多的网络层或隐藏单元, 这增加了模型的计, 然而, 那么只能增加模型容量以改进模
        - 摘要：如果测试集错误率大于目标错误率，那么可以采取两个方法。测试误差；是训练误差和测试误差之间差距与训练误差的总和。寻找最佳的测试误；差需要权衡这些数值。当训练误差较小（因此容量较大），测试误差主
          關鍵詞：当训练误差较小, 寻找最佳的测试误, 差需要权衡这些数值, 测试误差, 测试误差主
        - 摘要：效果最好。此时目标是缩小这一差距，使训练误差的增长速率不快于差；距减小的速率。要减少这个差距，我们可以改变正则化超参数，以减少；有效的模型容量，如添加Dropout或权重衰减策略。通常，最佳性能来
          關鍵詞：此时目标是缩小这一差距, 通常, 如添加, 有效的模型容量, 以减少
        - 摘要：大部分超参数可以通过推理其是否增加或减少模型容量来设置。部分示；例如表11.1所示。
          關鍵詞：大部分超参数可以通过推理其是否增加或减少模型容量来设置, 所示, 例如表, 部分示
        - 摘要：表11.1　各种超参数对模型容量的影响
          關鍵詞：各种超参数对模型容量的影响
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；注意事项
          關鍵詞：注意事项
        - 摘要：几乎模型每个操作；所需的时间和内存代；价都会随隐藏单元数
          關鍵詞：所需的时间和内存代, 价都会随隐藏单元数, 几乎模型每个操作
        - 摘要：较宽的卷积核导致；较窄的输出尺寸，除；非使用隐式零填充减
          關鍵詞：较窄的输出尺寸, 非使用隐式零填充减, 较宽的卷积核导致
        - 摘要：大多数操作的时间；和内存代价会增加
          關鍵詞：和内存代价会增加, 大多数操作的时间
        - 摘要：超参数
          關鍵詞：超参数
        - 摘要：容量何时增；加
          關鍵詞：容量何时增
        - 摘要：原因
          關鍵詞：原因
        - 摘要：隐藏单元数；量
          關鍵詞：隐藏单元数
        - 摘要：增加
          關鍵詞：增加
        - 摘要：学习率
          關鍵詞：学习率
        - 摘要：调至最优
          關鍵詞：调至最优
        - 摘要：增加隐藏单元数量；会增加模型的表示能；力
          關鍵詞：会增加模型的表示能, 增加隐藏单元数量
        - 摘要：不正确的学习速；率，不管是太高还是；太低都会由于优化失
          關鍵詞：不管是太高还是, 不正确的学习速, 太低都会由于优化失
        - 摘要：卷积核宽度     增加
          關鍵詞：卷积核宽度, 增加
        - 摘要：增加卷积核宽度会；增加模型的参数数；量
          關鍵詞：增加模型的参数数, 增加卷积核宽度会
        - 摘要：隐式零填充     增加
          關鍵詞：隐式零填充, 增加
        - 摘要：权重衰减系；数
          關鍵詞：权重衰减系
        - 摘要：降低
          關鍵詞：降低
        - 摘要：Dropout比
          關鍵詞：
        - 摘要：率
          關鍵詞：
        - 摘要：降低
          關鍵詞：降低
        - 摘要：在卷积之前隐式添；加零能保持较大尺寸；的表示
          關鍵詞：的表示, 在卷积之前隐式添, 加零能保持较大尺寸
        - 摘要：手动调整超参数时，不要忘记最终目标：提升测试集性能。加入正则化；只是实现这个目标的一种方法。只要训练误差低，随时都可以通过收集
          關鍵詞：只要训练误差低, 随时都可以通过收集, 手动调整超参数时, 只是实现这个目标的一种方法, 加入正则化
        - 摘要：更多的训练数据来减少泛化误差。实践中能够确保学习有效的暴力方法；就是不断提高模型容量和训练集的大小，直到解决问题。这种做法增加；了训练和推断的计算代价，所以只有在拥有足够资源时才是可行的。原
          關鍵詞：就是不断提高模型容量和训练集的大小, 所以只有在拥有足够资源时才是可行的, 更多的训练数据来减少泛化误差, 实践中能够确保学习有效的暴力方法, 直到解决问题
        - 摘要：11.4.2　自动超参数优化算法
          關鍵詞：自动超参数优化算法
        - 摘要：理想的学习算法应该是只需要输入一个数据集，就可以输出学习的函；数，而不需要手动调整超参数。一些流行的学习算法，如逻辑回归和支；持向量机，流行的部分原因是这类算法只有一到两个超参数需要调整，
          關鍵詞：理想的学习算法应该是只需要输入一个数据集, 而不需要手动调整超参数, 流行的部分原因是这类算法只有一到两个超参数需要调整, 就可以输出学习的函, 如逻辑回归和支
        - 摘要：如果仔细想想使用者搜索学习算法合适超参数的方式，我们会意识到这；其实是一种优化：我们在试图寻找超参数来优化目标函数，例如验证误；差，有时还会有一些约束（如训练时间、内存或识别时间的预算）。因
          關鍵詞：我们在试图寻找超参数来优化目标函数, 我们会意识到这, 其实是一种优化, 如训练时间, 有时还会有一些约束
        - 摘要：11.4.3　网格搜索
          關鍵詞：网格搜索
        - 摘要：当有3个或更少的超参数时，常见的超参数搜索方法是网格搜索  （grid；search）。对于每个超参数，使用者选择一个较小的有限值集去探索。；然后，这些超参数笛卡儿乘积得到一组组超参数，网格搜索使用每组超
          關鍵詞：个或更少的超参数时, 这些超参数笛卡儿乘积得到一组组超参数, 使用者选择一个较小的有限值集去探索, 当有, 对于每个超参数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；11.2所示是超参数值的网络。
          關鍵詞：所示是超参数值的网络
        - 摘要：图11.2　网格搜索和随机搜索的比较。为了便于说明，我们只展示两个超参数的例子，但是我；们关注的问题中超参数个数通常会更多。（左）为了实现网格搜索，我们为每个超参数提供了；一个值的集合。搜索算法对每一种在这些集合的交叉积中的超参数组合进行训练。（右）为了
          關鍵詞：我们为每个超参数提供了, 但是我, 为了, 搜索算法对每一种在这些集合的交叉积中的超参数组合进行训练, 我们只展示两个超参数的例子
        - 摘要：应该如何选择搜索集合的范围呢？在超参数是数值（有序）的情况下，；每个列表的最小和最大的元素可以基于先前相似实验的经验保守地挑选；出来，以确保最优解非常可能在所选范围内。通常，网格搜索大约会在
          關鍵詞：有序, 通常, 网格搜索大约会在, 每个列表的最小和最大的元素可以基于先前相似实验的经验保守地挑选, 应该如何选择搜索集合的范围呢
        - 摘要：通常重复进行网格搜索时，效果会最好。例如，假设我们在集合{-1，；0，1}上网格搜索超参数α。如果找到的最佳值是1，那么说明我们低估；了最优值α所在的范围，应该改变搜索格点，例如在集合{1，2，3}中搜
          關鍵詞：例如在集合, 假设我们在集合, 通常重复进行网格搜索时, 了最优值, 所在的范围
        - 摘要：网格搜索带来的一个明显问题是，计算代价会随着超参数数量呈指数级
          關鍵詞：网格搜索带来的一个明显问题是, 计算代价会随着超参数数量呈指数级
        - 摘要：增长。如果有m个超参数，每个最多取n个值，那么训练和估计所需的；试验数将是O(n  m  )。我们可以并行地进行实验，并且并行要求十分宽松；（进行不同搜索的机器之间几乎没有必要进行通信）。令人遗憾的是，
          關鍵詞：试验数将是, 我们可以并行地进行实验, 那么训练和估计所需的, 如果有, 并且并行要求十分宽松
        - 摘要：11.4.4　随机搜索
          關鍵詞：随机搜索
        - 摘要：幸运的是，有一个替代网格搜索的方法，并且编程简单，使用更方便，；能更快地收敛到超参数的良好取值——随机搜索（Bergstra；and
          關鍵詞：能更快地收敛到超参数的良好取值, 随机搜索, 并且编程简单, 幸运的是, 使用更方便
        - 摘要：随机搜索过程如下。首先，我们为每个超参数定义一个边缘分布，例；如，Bernoulli分布或范畴分布（分别对应着二元超参数或离散超参；数），或者对数尺度上的均匀分布（对应着正实值超参数）。例如，
          關鍵詞：或者对数尺度上的均匀分布, 例如, 分布或范畴分布, 对应着正实值超参数, 随机搜索过程如下
        - 摘要：其中，u(a,b)表示区间(a,b)上均匀采样的样本。类似；地，log_number_of_hidden_units 可以从u(log(50),log(2000))上采样。
          關鍵詞：上采样, 其中, 类似, 可以从, 上均匀采样的样本
        - 摘要：与网格搜索不同，我们不需要离散化超参数的值。这允许我们在一个更；大的集合上进行搜索，而不产生额外的计算代价。实际上，如图11.2所；示，当有几个超参数对性能度量没有显著影响时，随机搜索相比于网格
          關鍵詞：我们不需要离散化超参数的值, 这允许我们在一个更, 而不产生额外的计算代价, 大的集合上进行搜索, 随机搜索相比于网格
        - 摘要：与网格搜索一样，我们通常会重复运行不同版本的随机搜索，以基于前；一次运行的结果改进下一次搜索。
          關鍵詞：以基于前, 一次运行的结果改进下一次搜索, 与网格搜索一样, 我们通常会重复运行不同版本的随机搜索
        - 摘要：随机搜索能比网格搜索更快地找到良好超参数的原因是，没有浪费的实；验，不像网格搜索有时会对一个超参数的两个不同值（给定其他超参数；值不变）给出相同结果。在网格搜索中，其他超参数将在这两次实验中
          關鍵詞：没有浪费的实, 值不变, 随机搜索能比网格搜索更快地找到良好超参数的原因是, 在网格搜索中, 给定其他超参数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；果这两个值的变化所对应的验证集误差没有明显区别的话，网格搜索没；有必要重复两个等价的实验，而随机搜索仍然会对其他超参数进行两次
          關鍵詞：网格搜索没, 有必要重复两个等价的实验, 果这两个值的变化所对应的验证集误差没有明显区别的话, 而随机搜索仍然会对其他超参数进行两次
        - 摘要：11.4.5　基于模型的超参数优化
          關鍵詞：基于模型的超参数优化
        - 摘要：超参数搜索问题可以转化为一个优化问题，决策变量是超参数，优化的；代价是超参数训练出来的模型在验证集上的误差。在简化的设定下，可；以计算验证集上可导误差函数关于超参数的梯度，然后我们遵循这个梯
          關鍵詞：代价是超参数训练出来的模型在验证集上的误差, 优化的, 超参数搜索问题可以转化为一个优化问题, 然后我们遵循这个梯, 决策变量是超参数
        - 摘要：为了弥补梯度的缺失，我们可以对验证集误差建模，然后通过优化该模；型来提出新的超参数猜想。大部分基于模型的超参数搜索算法，都是使；用贝叶斯回归模型来估计每个超参数的验证集误差期望和该期望的不确
          關鍵詞：都是使, 为了弥补梯度的缺失, 型来提出新的超参数猜想, 然后通过优化该模, 用贝叶斯回归模型来估计每个超参数的验证集误差期望和该期望的不确
        - 摘要：et
          關鍵詞：
        - 摘要：目前，我们无法明确确定，贝叶斯超参数优化是否是一个能够实现更好；深度学习结果或是能够事半功倍的成熟工具。贝叶斯超参数优化有时表；现得像人类专家，能够在有些问题上取得很好的效果，但有时又会在某
          關鍵詞：贝叶斯超参数优化有时表, 我们无法明确确定, 目前, 深度学习结果或是能够事半功倍的成熟工具, 能够在有些问题上取得很好的效果
        - 摘要：大部分超参数优化算法比随机搜索更复杂，并且具有一个共同的缺点，；在它们能够从实验中提取任何信息之前，它们需要运行完整的训练实；验。相比于人类实践者手动搜索，考虑实验早期可以收集的信息量，这
          關鍵詞：它们需要运行完整的训练实, 大部分超参数优化算法比随机搜索更复杂, 相比于人类实践者手动搜索, 考虑实验早期可以收集的信息量, 并且具有一个共同的缺点
        - 摘要：的早期版本算法。在不同的时间点，超参数优化算法可以选择开启一个；新实验，“冻结”正在运行但希望不大的实验，或是“解冻”并恢复早期被；冻结的，但现在根据更多信息后又有希望的实验。
          關鍵詞：在不同的时间点, 解冻, 但现在根据更多信息后又有希望的实验, 新实验, 的早期版本算法
        - 摘要：11.4.1　手动调整超参数
          關鍵詞：手动调整超参数
        - 摘要：11.4.2　自动超参数优化；算法
          關鍵詞：自动超参数优化, 算法
        - 摘要：11.4.3　网格搜索
          關鍵詞：网格搜索
        - 摘要：11.4.4　随机搜索
          關鍵詞：随机搜索
        - 摘要：11.4.5　基于模型的超参；数优化
          關鍵詞：基于模型的超参, 数优化
    11.5：调试策略
        - 摘要：当一个机器学习系统效果不好时，通常很难判断效果不好的原因是算法；本身，还是算法实现错误。由于各种原因，机器学习系统很难调试。
          關鍵詞：本身, 通常很难判断效果不好的原因是算法, 由于各种原因, 机器学习系统很难调试, 还是算法实现错误
        - 摘要：在大多数情况下，我们不能提前知道算法的行为。事实上，使用机器学；习的整个出发点是，它会发现一些我们自己无法发现的有用行为。如果；我们在一个新的分类任务上训练一个神经网络，它达到5％的测试误
          關鍵詞：它会发现一些我们自己无法发现的有用行为, 如果, 我们在一个新的分类任务上训练一个神经网络, 在大多数情况下, 事实上
        - 摘要：另一个难点是，大部分机器学习模型有多个自适应的部分。如果一个部；分失效了，其他部分仍然可以自适应，并获得大致可接受的性能。例；如，假设我们正在训练多层神经网络，其中参数为权重  W  和偏置  b  。
          關鍵詞：和偏置, 另一个难点是, 假设我们正在训练多层神经网络, 其中参数为权重, 其他部分仍然可以自适应
        - 摘要：其中α是学习率。这个错误更新没有使用梯度。它会导致偏置在整个学；习中不断变为负值，对于一个学习算法来说这显然是错误的。然而只是；检查模型输出的话，该错误可能并不是显而易见的。根据输入的分布，
          關鍵詞：习中不断变为负值, 这个错误更新没有使用梯度, 它会导致偏置在整个学, 其中, 根据输入的分布
        - 摘要：大部分神经网络的调试策略都是解决这两个难题中的一个或两个。我们；可以设计一种足够简单的情况，能够提前得到正确结果，判断模型预测；是否与之相符；我们也可以设计一个测试，独立检查神经网络实现的各
          關鍵詞：能够提前得到正确结果, 是否与之相符, 我们, 我们也可以设计一个测试, 可以设计一种足够简单的情况
        - 摘要：一些重要的调试检测如下所述。
          關鍵詞：一些重要的调试检测如下所述
        - 摘要：可视化计算中模型的行为：当训练模型检测图像中的对象时，查看一些；模型检测到部分重叠的图像。在训练语音生成模型时，试听一些生成的
          關鍵詞：查看一些, 当训练模型检测图像中的对象时, 试听一些生成的, 在训练语音生成模型时, 可视化计算中模型的行为
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；语音样本。这似乎是显而易见的，但在实际中很容易只注意量化性能度；量，如准确率或对数似然。直接观察机器学习模型运行其任务，有助于
          關鍵詞：语音样本, 但在实际中很容易只注意量化性能度, 如准确率或对数似然, 直接观察机器学习模型运行其任务, 这似乎是显而易见的
        - 摘要：可视化最严重的错误：大多数模型能够输出运行任务时的某种置信度；量。例如，基于softmax函数输出层的分类器给每个类分配一个概率。；因此，分配给最有可能的类的概率给出了模型在其分类决定上的置信估
          關鍵詞：函数输出层的分类器给每个类分配一个概率, 大多数模型能够输出运行任务时的某种置信度, 因此, 例如, 分配给最有可能的类的概率给出了模型在其分类决定上的置信估
        - 摘要：根据训练和测试误差检测软件：我们往往很难确定底层软件是否正确实；现。训练和测试误差能够提供一些线索。如果训练误差较低，但是测试；误差较高，那么很有可能训练过程是在正常运行，但模型由于算法原因
          關鍵詞：但模型由于算法原因, 根据训练和测试误差检测软件, 如果训练误差较低, 那么很有可能训练过程是在正常运行, 训练和测试误差能够提供一些线索
        - 摘要：拟合极小的数据集：当训练集上有很大的误差时，我们需要确定问题是；真正的欠拟合，还是软件错误。通常，即使是小模型也可以保证很好地；拟合一个足够小的数据集。例如，只有一个样本的分类数据可以通过正
          關鍵詞：只有一个样本的分类数据可以通过正, 真正的欠拟合, 通常, 拟合极小的数据集, 例如
        - 摘要：比较反向传播导数和数值导数：如果读者正在使用一个需要实现梯度计；算的软件框架，或者在添加一个新操作到求导库中，必须定义它的；bprop  方法，那么常见的错误原因是没能正确地实现梯度表达。验证这
          關鍵詞：那么常见的错误原因是没能正确地实现梯度表达, 比较反向传播导数和数值导数, 验证这, 算的软件框架, 必须定义它的
        - 摘要：我们可以使用小的、有限的  近似导数：
          關鍵詞：我们可以使用小的, 有限的, 近似导数
        - 摘要：我们可以使用中心差分 （centered difference）提高近似的准确率：
          關鍵詞：我们可以使用中心差分, 提高近似的准确率
        - 摘要：扰动大小   必须足够大，以确保该扰动不会由于数值计算的有限精度；问题产生舍入误差。
          關鍵詞：必须足够大, 扰动大小, 问题产生舍入误差, 以确保该扰动不会由于数值计算的有限精度
        - 摘要：通常，我们会测试向量值函数；的梯度或Jacobian矩；阵。令人遗憾的是，有限差分只允许我们每次计算一个导数。我们可以
          關鍵詞：我们可以, 通常, 我们会测试向量值函数, 的梯度或, 令人遗憾的是
        - 摘要：如果我们可以在复数上进行数值计算，那么使用复数作为函数的输入会；有非常高效的数值方法估算梯度（Squire  and  Trapp，1998）。该方法基；于如下观察：
          關鍵詞：如果我们可以在复数上进行数值计算, 那么使用复数作为函数的输入会, 于如下观察, 有非常高效的数值方法估算梯度, 该方法基
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；其中；我们对f在不同点上计
          關鍵詞：我们对, 在不同点上计, 其中
        - 摘要：。和上面的实值情况不同，这里不存在消除影响，因为
          關鍵詞：因为, 这里不存在消除影响, 和上面的实值情况不同
        - 摘要：算差分。因此我们可以使用很小的   ，比如   ＝10  -150  ，其中误差
          關鍵詞：比如, 算差分, 因此我们可以使用很小的, 其中误差
        - 摘要：对所有实用目标都是微不足道的。
          關鍵詞：对所有实用目标都是微不足道的
        - 摘要：监控激活函数值和梯度的直方图：可视化神经网络在大量训练迭代后；（也许是一个轮）收集到的激活函数值和梯度的统计量往往是有用的。；隐藏单元的预激活值可以告诉我们该单元是否饱和，或者它们饱和的频
          關鍵詞：监控激活函数值和梯度的直方图, 收集到的激活函数值和梯度的统计量往往是有用的, 或者它们饱和的频, 可视化神经网络在大量训练迭代后, 也许是一个轮
        - 摘要：最后，许多深度学习算法为每一步产生的结果提供了某种保证。例如，；在第3部分，我们将看到一些使用代数解决优化问题的近似推断算法。；通常，这些可以通过测试它们的每个保
          關鍵詞：许多深度学习算法为每一步产生的结果提供了某种保证, 这些可以通过测试它们的每个保, 部分, 通常, 我们将看到一些使用代数解决优化问题的近似推断算法
        - 摘要：证来调试。某些优化算法提供的保证包括，目标函数值在算法的迭代步；中不会增加，某些变量的导数在算法的每一步中都是零，所有变量的梯；度在收敛时会变为零。通常，由于舍入误差，这些条件不会在数字计算
          關鍵詞：通常, 这些条件不会在数字计算, 证来调试, 目标函数值在算法的迭代步, 中不会增加
    11.6：示例：多位数字识别
        - 摘要：为了端到端地说明如何在实践中应用我们的设计方法论，我们从设计深；度学习组件出发，简单地介绍一下街景转录系统。显然，整个系统的许
          關鍵詞：显然, 整个系统的许, 度学习组件出发, 为了端到端地说明如何在实践中应用我们的设计方法论, 简单地介绍一下街景转录系统
        - 摘要：多其他组件，如街景车、数据库设施等，也是极其重要的。
          關鍵詞：也是极其重要的, 如街景车, 多其他组件, 数据库设施等
        - 摘要：从机器学习任务的视角出发，首先这个过程要采集数据。街景车收集原；始数据，然后操作员手动提供标签。转录任务开始前有大量的数据处理；工作，包括在转录前使用其他机器学习技术探测房屋号码。
          關鍵詞：然后操作员手动提供标签, 首先这个过程要采集数据, 转录任务开始前有大量的数据处理, 包括在转录前使用其他机器学习技术探测房屋号码, 始数据
        - 摘要：转录项目开始于性能度量的选择和对这些度量的期望值。一个重要的总；原则是度量的选择要符合项目的业务目标。因为地图只有是高准确率时；才有用，所以为这个项目设置高准确率的要求非常重要。具体地，目标
          關鍵詞：原则是度量的选择要符合项目的业务目标, 转录项目开始于性能度量的选择和对这些度量的期望值, 目标, 因为地图只有是高准确率时, 才有用
        - 摘要：在选择量化目标后，我们推荐方法的下一步是要快速建立一个合理的基；准系统。对于视觉任务而言，基准系统是带有整流线性单元的卷积网；络。转录项目开始于一个这样的模型。当时，使用卷积网络输出预测序
          關鍵詞：准系统, 对于视觉任务而言, 转录项目开始于一个这样的模型, 使用卷积网络输出预测序, 在选择量化目标后
        - 摘要：我们建议反复细化这些基准，并测试每个变化是否都有改进。街景转录；系统的第一个变化受激励于覆盖指标的理论理解和数据结构。具体地，；当输出序列的概率低于某个值t即p( y ｜ x )＜t时，网络拒绝为输入 x 分
          關鍵詞：系统的第一个变化受激励于覆盖指标的理论理解和数据结构, 网络拒绝为输入, 并测试每个变化是否都有改进, 当输出序列的概率低于某个值, 我们建议反复细化这些基准
        - 摘要：此时，覆盖仍低于90％，但该方法没有明显的理论问题了。因此，我们；的方法论建议综合训练集和测试集性能，以确定问题是否欠拟合或过拟；合。在这种情况下，训练和测试集误差几乎是一样的。事实上，这个项
          關鍵詞：训练和测试集误差几乎是一样的, 事实上, 因此, 的方法论建议综合训练集和测试集性能, 在这种情况下
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；的错误。在这种情况下，这意味着可视化不正确而模型给了最高置信度；的训练集转录结果。结果显示，主要是输入图像裁剪得太紧，有些和地
          關鍵詞：这意味着可视化不正确而模型给了最高置信度, 的错误, 在这种情况下, 有些和地, 主要是输入图像裁剪得太紧
        - 摘要：最后，性能提升的最后几个百分点来自调整超参数。这主要包括在保持；一些计算代价限制的同时加大模型的规模。因为训练误差和测试误差保；持几乎相等，所以明确表明性能不足是由欠拟合造成的，数据集本身也
          關鍵詞：所以明确表明性能不足是由欠拟合造成的, 性能提升的最后几个百分点来自调整超参数, 持几乎相等, 这主要包括在保持, 数据集本身也
        - 摘要：总体来说，转录项目是非常成功的，可以比人工速度更快、代价更低地；转录数以亿计的地址。
          關鍵詞：转录项目是非常成功的, 可以比人工速度更快, 总体来说, 转录数以亿计的地址, 代价更低地
        - 摘要：我们希望本章中介绍的设计原则能带来其他更多类似的成功。
          關鍵詞：我们希望本章中介绍的设计原则能带来其他更多类似的成功
第12章：应用
    11.6：示例：多位数字识别
        - 摘要：在本章中，我们将介绍如何使用深度学习来解决计算机视觉、语音识；别、自然语言处理以及其他商业领域中的应用。首先我们将讨论在许多；最重要的AI应用中所需的大规模神经网络的实现。接着，我们将回顾深
          關鍵詞：我们将介绍如何使用深度学习来解决计算机视觉, 最重要的, 我们将回顾深, 语音识, 接着
    12.1：大规模深度学习
        - 摘要：12.1.1　快速的CPU实现
          關鍵詞：实现, 快速的
        - 摘要：12.1.2　GPU实现
          關鍵詞：实现
        - 摘要：12.1.3　大规模的分布式实现
          關鍵詞：大规模的分布式实现
        - 摘要：12.1.4　模型压缩
          關鍵詞：模型压缩
        - 摘要：12.1.5　动态结构
          關鍵詞：动态结构
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；12.1.6　深度网络的专用硬件实现
          關鍵詞：深度网络的专用硬件实现
        - 摘要：深度学习的基本思想基于联结主义：尽管机器学习模型中单个生物性的；神经元或者说是单个特征不是智能的，但是大量的神经元或者特征作用；在一起往往能够表现出智能。我们必须着重强调神经元数量必须很大这
          關鍵詞：但是大量的神经元或者特征作用, 我们必须着重强调神经元数量必须很大这, 在一起往往能够表现出智能, 神经元或者说是单个特征不是智能的, 尽管机器学习模型中单个生物性的
        - 摘要：个事实。相比20世纪80年代，如今神经网络的精度以及处理任务的复杂；度都有一定提升，其中一个关键的因素就是网络规模的巨大提升。正如；我们在第1.2.3节中看到的一样，在过去的30年内，网络规模是以指数级
          關鍵詞：我们在第, 个事实, 在过去的, 世纪, 其中一个关键的因素就是网络规模的巨大提升
        - 摘要：由于规模的大小对于神经网络来说至关重要，因此深度学习需要高性能；的硬件设施和软件实现。
          關鍵詞：由于规模的大小对于神经网络来说至关重要, 因此深度学习需要高性能, 的硬件设施和软件实现
        - 摘要：12.1.1　快速的CPU实现
          關鍵詞：实现, 快速的
        - 摘要：传统的神经网络是用单台机器的CPU来训练的。如今，这种做法通常被；视为是不可取的。现在，我们通常使用GPU或者许多台机器的CPU连接；在一起进行计算。在使用这种昂贵配置之前，为论证CPU无法承担神经
          關鍵詞：视为是不可取的, 连接, 无法承担神经, 或者许多台机器的, 传统的神经网络是用单台机器的
        - 摘要：描述如何实现高效的数值CPU代码已经超出了本书的讨论范围，但是我；们在这里还是要强调通过设计一些特定的CPU上的操作可以大大提升效；率。例如，在2011年，最好的CPU在训练神经网络时使用定点运算能够
          關鍵詞：但是我, 在训练神经网络时使用定点运算能够, 上的操作可以大大提升效, 最好的, 例如
        - 摘要：12.1.2　GPU实现
          關鍵詞：实现
        - 摘要：许多现代神经网络的实现基于图形处理器  （Graphics  Processing  Unit，；GPU）。图形处理器最初是为图形应用而开发的专用硬件组件。视频游；戏系统的消费市场刺激了图形处理硬件的发展。GPU为视频游戏所设计
          關鍵詞：图形处理器最初是为图形应用而开发的专用硬件组件, 为视频游戏所设计, 戏系统的消费市场刺激了图形处理硬件的发展, 许多现代神经网络的实现基于图形处理器, 视频游
        - 摘要：视频游戏的渲染要求许多操作能够快速并行地执行。环境和角色模型通
          關鍵詞：环境和角色模型通, 视频游戏的渲染要求许多操作能够快速并行地执行
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；过一系列顶点的3D坐标确定。为了将大量的3D坐标转化为2D显示器上；的坐标，显卡必须并行地对许多顶点执行矩阵乘法与除法。之后，显卡
          關鍵詞：之后, 的坐标, 显示器上, 坐标转化为, 过一系列顶点的
        - 摘要：与上述的实时图形算法相比，神经网络算法所需要的性能特性是相同；的。神经网络算法通常涉及大量参数、激活值、梯度值的缓冲区，其中；每个值在每一次训练迭代中都要被完全更新。这些缓冲太大，会超出传
          關鍵詞：神经网络算法所需要的性能特性是相同, 每个值在每一次训练迭代中都要被完全更新, 神经网络算法通常涉及大量参数, 这些缓冲太大, 其中
        - 摘要：GPU硬件最初专为图形任务而设计。随着时间的推移，GPU也变得更灵；活，允许定制的子程序处理转化顶点坐标或者计算像素颜色的任务。原；则上，GPU不要求这些像素值实际基于渲染任务。只要将计算的输出值
          關鍵詞：也变得更灵, 硬件最初专为图形任务而设计, 则上, 允许定制的子程序处理转化顶点坐标或者计算像素颜色的任务, 不要求这些像素值实际基于渲染任务
        - 摘要：et
          關鍵詞：
        - 摘要：在通用GPU发布以后，使用显卡训练神经网络的热度开始爆炸性地增；长。这种通用GPU可以执行任意的代码，而并非仅仅渲染子程序。；NVIDIA的CUDA编程语言使得我们可以用一种像C一样的语言实现任意
          關鍵詞：发布以后, 这种通用, 编程语言使得我们可以用一种像, 而并非仅仅渲染子程序, 使用显卡训练神经网络的热度开始爆炸性地增
        - 摘要：2009b；Ciresan et al. ，2010）。
          關鍵詞：
        - 摘要：如何在通用GPU上写高效的代码依然是一个难题。在GPU上获得良好表；现所需的技术与CPU上的技术非常不同。比如说，基于CPU的良好代码；通常被设计为尽可能从高速缓存中读取更多的信息。然而在GPU中，大
          關鍵詞：上写高效的代码依然是一个难题, 如何在通用, 基于, 比如说, 通常被设计为尽可能从高速缓存中读取更多的信息
        - 摘要：由于实现高效GPU代码的困难性，研究人员应该组织好他们的工作流；程，避免对每一个新的模型或算法都编写新的GPU代码。通常来讲，人；们会选择建立一个包含高效操作（如卷积和矩阵乘法）的软件库解决这
          關鍵詞：的软件库解决这, 代码的困难性, 由于实现高效, 避免对每一个新的模型或算法都编写新的, 们会选择建立一个包含高效操作
        - 摘要：12.1.3　大规模的分布式实现
          關鍵詞：大规模的分布式实现
        - 摘要：在许多情况下，单个机器的计算资源是有限的。因此，我们希望把训练；或者推断的任务分摊到多个机器上进行。
          關鍵詞：在许多情况下, 因此, 或者推断的任务分摊到多个机器上进行, 单个机器的计算资源是有限的, 我们希望把训练
        - 摘要：分布式的推断是容易实现的，因为每一个输入的样本都可以在单独的机
          關鍵詞：因为每一个输入的样本都可以在单独的机, 分布式的推断是容易实现的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；器上运行。这也被称为数据并行 （data parallelism）。
          關鍵詞：这也被称为数据并行, 器上运行
        - 摘要：同样地，模型并行  （model  parallelism）也是可行的，其中多个机器共；同运行一个数据点，每一个机器负责模型的一个部分。对于推断和训；练，这都是可行的。
          關鍵詞：模型并行, 每一个机器负责模型的一个部分, 这都是可行的, 也是可行的, 同样地
        - 摘要：在训练过程中，数据并行从某种程度上来说更加困难。对于随机梯度下；降的单步来说，我们可以增加小批量的大小，但是从优化性能的角度来；说，我们得到的回报通常并不会线性增长。使用多个机器并行地计算多
          關鍵詞：使用多个机器并行地计算多, 我们得到的回报通常并不会线性增长, 降的单步来说, 我们可以增加小批量的大小, 在训练过程中
        - 摘要：这个问题可以使用异步随机梯度下降  （Asynchoronous；Stochasitc；Gradient Descent）（Bengio et al. ，2001b；Recht et al. ，2011）解决。
          關鍵詞：这个问题可以使用异步随机梯度下降, 解决
        - 摘要：12.1.4　模型压缩
          關鍵詞：模型压缩
        - 摘要：在许多商业应用的机器学习模型中，一个时间和内存开销较小的推断算；法比一个时间和内存开销较小的训练算法要更为重要。对于那些不需要；个性化设计的应用来说，我们只需要一次性地训练模型，然后它就可以
          關鍵詞：一个时间和内存开销较小的推断算, 我们只需要一次性地训练模型, 法比一个时间和内存开销较小的训练算法要更为重要, 然后它就可以, 个性化设计的应用来说
        - 摘要：减少推断所需开销的一个关键策略是模型压缩  （model  compression）；（Buciluă  et  al.  ，2006）。模型压缩的基本思想是用一个更小的模型取；代替原始耗时的模型，从而使得用来存储与评估所需的内存与运行时间
          關鍵詞：减少推断所需开销的一个关键策略是模型压缩, 模型压缩的基本思想是用一个更小的模型取, 代替原始耗时的模型, 从而使得用来存储与评估所需的内存与运行时间
        - 摘要：当原始模型的规模很大，且我们需要防止过拟合时，模型压缩就可以起；到作用。在许多情况下，拥有最小泛化误差的模型往往是多个独立训练；而成的模型的集成。评估所有n个集成成员的成本很高。有时候，当单
          關鍵詞：当原始模型的规模很大, 而成的模型的集成, 个集成成员的成本很高, 在许多情况下, 模型压缩就可以起
        - 摘要：这些巨大的模型能够学习到某个函数f( x )，但选用的参数数量超过了任；务所需的参数数量。只是因为训练样本数是有限的，所以模型的规模才；变得必要。只要我们拟合了这个函数f(  x  )，我们就可以通过将f作用于
          關鍵詞：我们就可以通过将, 所以模型的规模才, 但选用的参数数量超过了任, 只要我们拟合了这个函数, 这些巨大的模型能够学习到某个函数
        - 摘要：此外，我们还可以仅在原始训练数据上训练一个更小的模型，但只是为；了复制模型的其他特征，比如在不正确的类上的后验分布（Hinton et al.；，2014，2015）。
          關鍵詞：但只是为, 了复制模型的其他特征, 此外, 我们还可以仅在原始训练数据上训练一个更小的模型, 比如在不正确的类上的后验分布
        - 摘要：12.1.5　动态结构
          關鍵詞：动态结构
        - 摘要：一般来说，加速数据处理系统的一种策略是构造一个系统，这个系统用；动态结构  （dynamic  structure）描述图中处理输入所需的计算过程。在；给定一个输入的情况中，数据处理系统可以动态地决定运行神经网络系
          關鍵詞：数据处理系统可以动态地决定运行神经网络系, 给定一个输入的情况中, 描述图中处理输入所需的计算过程, 加速数据处理系统的一种策略是构造一个系统, 这个系统用
        - 摘要：动态结构计算是一种基础的计算机科学方法，广泛应用于软件工程项
          關鍵詞：广泛应用于软件工程项, 动态结构计算是一种基础的计算机科学方法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；目。应用于神经网络的最简单的动态结构基于决定神经网络（或者其他；机器学习模型）中的哪些子集需要应用于特定的输入。
          關鍵詞：机器学习模型, 中的哪些子集需要应用于特定的输入, 或者其他, 应用于神经网络的最简单的动态结构基于决定神经网络
        - 摘要：在分类器中加速推断的可行策略是使用级联  （cascade）的分类器。当；目标是检测罕见对象（或事件）是否存在时，可以应用级联策略。要确；定对象是否存在，我们必须使用具有高容量、运行成本高的复杂分类
          關鍵詞：我们必须使用具有高容量, 可以应用级联策略, 运行成本高的复杂分类, 或事件, 的分类器
        - 摘要：决策树本身是动态结构的一个例子，因为树中的每个节点决定应该使用；哪个子树来评估输入。一个结合深度学习和动态结构的简单方法是训练；一个决策树，其中每个节点使用神经网络作出决策（Guo  and  Gelfand，
          關鍵詞：其中每个节点使用神经网络作出决策, 一个决策树, 哪个子树来评估输入, 决策树本身是动态结构的一个例子, 因为树中的每个节点决定应该使用
        - 摘要：类似地，我们可以使用称为选通器  （gater）的神经网络来选择在给定；当前输入的情况下将使用几个专家网络  （expert  network）中的哪一个；来计算输出。这个想法的第一个版本被称为专家混合体  （mixture  of
          關鍵詞：中的哪一个, 来计算输出, 我们可以使用称为选通器, 的神经网络来选择在给定, 这个想法的第一个版本被称为专家混合体
        - 摘要：专家输出一个概率或权重（通过非线性的softmax函数获得），并且最；终输出由各个专家输出的加权组合获得。在这种情况下，使用选通器不；会降低计算成本，但如果每个样本的选通器选择单个专家，我们就会获
          關鍵詞：通过非线性的, 并且最, 在这种情况下, 但如果每个样本的选通器选择单个专家, 专家输出一个概率或权重
        - 摘要：另一种动态结构是开关，其中隐藏单元可以根据具体情况从不同单元接；收输入。这种动态路由方法可以理解为注意力机制；（attention
          關鍵詞：收输入, 这种动态路由方法可以理解为注意力机制, 另一种动态结构是开关, 其中隐藏单元可以根据具体情况从不同单元接
        - 摘要：使用动态结构化系统的主要障碍是由于系统针对不同输入的不同代码分；支导致的并行度降低。这意味着网络中只有很少的操作可以被描述为对；样本小批量的矩阵乘法或批量卷积。我们可以写更多的专用子程序，用
          關鍵詞：我们可以写更多的专用子程序, 使用动态结构化系统的主要障碍是由于系统针对不同输入的不同代码分, 样本小批量的矩阵乘法或批量卷积, 这意味着网络中只有很少的操作可以被描述为对, 支导致的并行度降低
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；12.1.6　深度网络的专用硬件实现
          關鍵詞：深度网络的专用硬件实现
        - 摘要：自从早期的神经网络研究以来，硬件设计者就已经致力于可以加速神经；网络算法的训练和/或推断的专用硬件实现。读者可以查看早期的和更；近的专用硬件深度网络的评论（Lindsey and Lindblad，1994；Beiu et al.
          關鍵詞：网络算法的训练和, 自从早期的神经网络研究以来, 硬件设计者就已经致力于可以加速神经, 或推断的专用硬件实现, 近的专用硬件深度网络的评论
        - 摘要：不同形式的专用硬件（Graf and Jackel，1989；Mead and Ismail，2012；；Kim et al. ，2009；Pham et al. ，2012；Chen et al. ，2014b，a）的研究；已经持续了好几十年，比如专用集成电路
          關鍵詞：的研究, 比如专用集成电路, 已经持续了好几十年, 不同形式的专用硬件
        - 摘要：虽然CPU和GPU上的软件实现通常使用32位或64位的精度来表示浮点；数，但是长期以来使用较低的精度在更短的时间内完成推断也是可行的；（Holt and Baker，1991；Holi and Hwang，1993；Presley and Haggard，
          關鍵詞：上的软件实现通常使用, 虽然, 位的精度来表示浮点, 但是长期以来使用较低的精度在更短的时间内完成推断也是可行的, 位或
        - 摘要：最近对基于反向传播神经网络的低精度实现的工作（Vanhoucke et al. ，；2011；Courbariaux et al. ，2015；Gupta et al. ，2015）表明，8位和16位；之间的精度足以满足使用或训练基于反向传播的深度神经网络的要求。
          關鍵詞：之间的精度足以满足使用或训练基于反向传播的深度神经网络的要求, 最近对基于反向传播神经网络的低精度实现的工作, 位和, 表明
        - 摘要：形式的动态定点表示能够减少每个数需要的存储空间。传统的定点数被；限制在一个固定范围之内（其对应于浮点表示中的给定指数）。而动态；定点表示在一组数字（例如一个层中的所有权重）之间共享该范围。使
          關鍵詞：之间共享该范围, 形式的动态定点表示能够减少每个数需要的存储空间, 其对应于浮点表示中的给定指数, 限制在一个固定范围之内, 而动态
        - 摘要：12.1.1　快速的CPU实现
          關鍵詞：实现, 快速的
        - 摘要：12.1.2　GPU实现
          關鍵詞：实现
        - 摘要：12.1.3　大规模的分布式；实现
          關鍵詞：大规模的分布式, 实现
        - 摘要：12.1.4　模型压缩
          關鍵詞：模型压缩
        - 摘要：12.1.5　动态结构
          關鍵詞：动态结构
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；12.1.6　深度网络的专用；硬件实现
          關鍵詞：硬件实现, 深度网络的专用
    12.2：计算机视觉
        - 摘要：12.2.1　预处理
          關鍵詞：预处理
        - 摘要：12.2.2　数据集增强
          關鍵詞：数据集增强
        - 摘要：长久以来，计算机视觉就是深度学习应用中几个最活跃的研究方向之；一。因为视觉是一个对人类以及许多动物毫不费力，但对计算机却充满；挑战的任务（Ballard  et  al.  ，1983）。深度学习中许多流行的标准基准
          關鍵詞：长久以来, 但对计算机却充满, 因为视觉是一个对人类以及许多动物毫不费力, 挑战的任务, 深度学习中许多流行的标准基准
        - 摘要：计算机视觉是一个非常广阔的发展领域，其中包括多种多样的处理图片；的方式以及应用方向。计算机视觉的应用广泛：从复现人类视觉能力；（比如识别人脸）到创造全新的视觉能力。举个后者的例子，近期一个
          關鍵詞：到创造全新的视觉能力, 举个后者的例子, 计算机视觉是一个非常广阔的发展领域, 近期一个, 其中包括多种多样的处理图片
        - 摘要：12.2.1　预处理
          關鍵詞：预处理
        - 摘要：由于原始输入往往以深度学习架构难以表示的形式出现，许多应用领域；需要复杂精细的预处理。计算机视觉通常只需要相对少的这种预处理。；图像应该被标准化，从而使得它们的像素都在相同并且合理的范围内，
          關鍵詞：图像应该被标准化, 需要复杂精细的预处理, 由于原始输入往往以深度学习架构难以表示的形式出现, 计算机视觉通常只需要相对少的这种预处理, 从而使得它们的像素都在相同并且合理的范围内
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图像混合，通常会导致失败。将图像格式化为具有相同的比例，严格上；说是唯一一种必要的预处理。许多计算机视觉架构需要标准尺寸的图
          關鍵詞：许多计算机视觉架构需要标准尺寸的图, 说是唯一一种必要的预处理, 图像混合, 将图像格式化为具有相同的比例, 严格上
        - 摘要：数据集增强可以被看作一种只对训练集做预处理的方式。数据集增强是；减少大多数计算机视觉模型泛化误差的一种极好方法。在测试时可用的；一个相关想法是将同一输入的许多不同版本传给模型（例如，在稍微不
          關鍵詞：在稍微不, 在测试时可用的, 减少大多数计算机视觉模型泛化误差的一种极好方法, 数据集增强可以被看作一种只对训练集做预处理的方式, 数据集增强是
        - 摘要：其他种类的预处理需要同时应用于训练集和测试集，其目的是将每个样；本置于更规范的形式，以便减少模型需要考虑的变化量。减少数据中的；变化量既能够减少泛化误差，也能够减小拟合训练集所需模型的大小。
          關鍵詞：其他种类的预处理需要同时应用于训练集和测试集, 本置于更规范的形式, 也能够减小拟合训练集所需模型的大小, 变化量既能够减少泛化误差, 减少数据中的
        - 摘要：12.2.1.1　对比度归一化
          關鍵詞：对比度归一化
        - 摘要：在许多任务中，对比度是能够安全移除的最为明显的变化源之一。简单；地说，对比度指的是图像中亮像素和暗像素之间差异的大小。量化图像；对比度有许多方式。在深度学习中，对比度通常指的是图像或图像区域
          關鍵詞：对比度通常指的是图像或图像区域, 地说, 简单, 对比度是能够安全移除的最为明显的变化源之一, 对比度有许多方式
        - 摘要：其中X 是整个图片的平均强度，满足
          關鍵詞：满足, 是整个图片的平均强度, 其中
        - 摘要：全局对比度归一化  （global  contrast  normalization，GCN）旨在通过从；每个图像中减去其平均值，然后重新缩放使其像素上的标准差等于某个；常数s来防止图像具有变化的对比度。这种方法非常复杂，因为没有缩
          關鍵詞：因为没有缩, 旨在通过从, 每个图像中减去其平均值, 常数, 全局对比度归一化
        - 摘要：从大图像中剪切感兴趣的对象所组成的数据集不可能包含任何强度几乎；恒定的图像。在这些情况下，通过设置λ＝0来忽略小分母问题是安全；的，并且在非常罕见的情况下为了避免除以0，通过将   设置为一个非
          關鍵詞：恒定的图像, 来忽略小分母问题是安全, 并且在非常罕见的情况下为了避免除以, 通过将, 设置为一个非
        - 摘要：尺度参数s通常可以设置为1（如Coates et al. （2011）所采用的），或选；择使所有样本上每个像素的标准差接近1（如Goodfellow；al.
          關鍵詞：所采用的, 择使所有样本上每个像素的标准差接近, 通常可以设置为, 尺度参数, 或选
        - 摘要：et
          關鍵詞：
        - 摘要：式（12.3）中的标准差仅仅是对图片L 2 范数的重新缩放（假设图像的平；2  范数来定义；均值已经被移除）。我们更偏向于根据标准差而不是L
          關鍵詞：中的标准差仅仅是对图片, 我们更偏向于根据标准差而不是, 均值已经被移除, 范数的重新缩放, 假设图像的平
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；GCN，因为标准差包括除以像素数量这一步，从而基于标准差的GCN；能够使用与图像大小无关的固定的s。然而，观察到L  2  范数与标准差成
          關鍵詞：因为标准差包括除以像素数量这一步, 能够使用与图像大小无关的固定的, 然而, 从而基于标准差的, 观察到
        - 摘要：图12.1　 GCN将样本投影到一个球上。（左）原始的输入数据可能拥有任意的范数。（中）λ＝；＝10 -8 。由于我们；0时，GCN可以完美地将所有的非零样本投影到球上。这里我们令s＝1，
          關鍵詞：原始的输入数据可能拥有任意的范数, 可以完美地将所有的非零样本投影到球上, 将样本投影到一个球上, 由于我们, 这里我们令
        - 摘要：与直觉相反的是，存在被称为sphering  的预处理操作，并且它不同于；GCN。sphering并不会使数据位于球形壳上，而是将主成分重新缩放以；具有相等方差，使得PCA使用的多变量正态分布具有球形等高线。
          關鍵詞：与直觉相反的是, 并且它不同于, 具有相等方差, 的预处理操作, 而是将主成分重新缩放以
        - 摘要：全局对比度归一化常常不能突出我们想要突出的图像特征，例如边缘和；角。如果我们有一个场景，包含了一个大的黑暗区域和一个大的明亮区；域（例如一个城市广场有一半的区域处于建筑物的阴影之中），则全局
          關鍵詞：包含了一个大的黑暗区域和一个大的明亮区, 全局对比度归一化常常不能突出我们想要突出的图像特征, 例如一个城市广场有一半的区域处于建筑物的阴影之中, 如果我们有一个场景, 则全局
        - 摘要：这催生了局部对比度归一化  （local  contrast  normalization，LCN）。局
          關鍵詞：这催生了局部对比度归一化
        - 摘要：部对比度归一化确保对比度在每个小窗口上被归一化，而不是作为整体；在图像上被归一化。关于局部对比度归一化和全局对比度归一化的比较；可以参考图12.2。
          關鍵詞：在图像上被归一化, 关于局部对比度归一化和全局对比度归一化的比较, 可以参考图, 而不是作为整体, 部对比度归一化确保对比度在每个小窗口上被归一化
        - 摘要：图12.2　全局对比度归一化和局部对比度归一化的比较。直观上说，全局对比度归一化的效果；很巧妙。它使得所有图片的尺度都差不多，这减轻了学习算法处理多个尺度的负担。局部对比；度归一化更多地改变了图像，丢弃了所有相同强度的区域。这使模型能够只关注于边缘。较好
          關鍵詞：它使得所有图片的尺度都差不多, 这使模型能够只关注于边缘, 丢弃了所有相同强度的区域, 直观上说, 局部对比
        - 摘要：局部对比度归一化的各种定义都是可行的。在所有情况下，我们可以通；过减去邻近像素的平均值并除以邻近像素的标准差来修改每个像素。在；一些情况下，要计算以当前要修改的像素为中心的矩形窗口中所有像素
          關鍵詞：过减去邻近像素的平均值并除以邻近像素的标准差来修改每个像素, 我们可以通, 局部对比度归一化的各种定义都是可行的, 在所有情况下, 要计算以当前要修改的像素为中心的矩形窗口中所有像素
        - 摘要：局部对比度归一化通常可以通过使用可分离卷积（参考第9.8节）来计；算特征映射的局部平均值和局部标准差，然后在不同的特征映射上使用；逐元素的减法和除法。
          關鍵詞：逐元素的减法和除法, 参考第, 算特征映射的局部平均值和局部标准差, 然后在不同的特征映射上使用, 局部对比度归一化通常可以通过使用可分离卷积
        - 摘要：局部对比度归一化是可微分的操作，并且还可以作为一种非线性作用应；用于网络隐藏层，以及应用于输入的预处理操作。
          關鍵詞：并且还可以作为一种非线性作用应, 用于网络隐藏层, 以及应用于输入的预处理操作, 局部对比度归一化是可微分的操作
        - 摘要：与全局对比度归一化一样，我们通常需要正则化局部对比度归一化来避；免出现除以零的情况。事实上，因为局部对比度归一化通常作用于较小
          關鍵詞：免出现除以零的情况, 事实上, 我们通常需要正则化局部对比度归一化来避, 因为局部对比度归一化通常作用于较小, 与全局对比度归一化一样
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；的窗口，所以正则化更加重要。较小的窗口更可能包含彼此几乎相同的；值，因此更可能具有零标准差。
          關鍵詞：较小的窗口更可能包含彼此几乎相同的, 因此更可能具有零标准差, 所以正则化更加重要, 的窗口
        - 摘要：12.2.2　数据集增强
          關鍵詞：数据集增强
        - 摘要：如第7.4节中讲到的一样，我们很容易通过增加训练集的额外副本来增；加训练集的大小，进而改进分类器的泛化能力。这些额外副本可以通过；对原始图像进行一些变化来生成，但是并不改变其类别。对象识别这个
          關鍵詞：进而改进分类器的泛化能力, 但是并不改变其类别, 加训练集的大小, 对原始图像进行一些变化来生成, 我们很容易通过增加训练集的额外副本来增
        - 摘要：12.2.1　预处理
          關鍵詞：预处理
        - 摘要：12.2.2　数据集增强
          關鍵詞：数据集增强
    12.3：语音识别
        - 摘要：语音识别任务是将一段包括了自然语言发音的声学信号投影到对应说话；人的词序列上。令 X ＝( x (1) , x (2) ,…, x (T) )表示语音的输入向量（传统；做法以20ms为一帧分割信号）。许多语音识别的系统通过特殊的手工设
          關鍵詞：表示语音的输入向量, 语音识别任务是将一段包括了自然语言发音的声学信号投影到对应说话, 为一帧分割信号, 传统, 许多语音识别的系统通过特殊的手工设
        - 摘要：recognition，ASR）任务指的是构造一个函数；，使得它能够在给定声学序列 X 的情况下计算最有可能的语言序
          關鍵詞：的情况下计算最有可能的语言序, 任务指的是构造一个函数, 使得它能够在给定声学序列
        - 摘要：列 y ：
          關鍵詞：
        - 摘要：其中P * 是给定输入值 X 时对应目标 y 的真实条件分布。
          關鍵詞：时对应目标, 其中, 是给定输入值, 的真实条件分布
        - 摘要：从20世纪80年代直到2009～2012年，最先进的语音识别系统是隐马尔可；夫模型  （hidden  markov  model，HMM）和高斯混合模型  （gaussian；mixture  model，GMM）的结合。GMM对声学特征和音素  （phoneme）
          關鍵詞：的结合, 对声学特征和音素, 世纪, 夫模型, 最先进的语音识别系统是隐马尔可
        - 摘要：HMM模型将语音信号视作由如下过程生成：首先，一个HMM生成了一；个音素的序列以及离散的子音素状态（比如每一个音素的开始、中间、；结尾），然后GMM把每一个离散的状态转化为一个简短的声音信号。
          關鍵詞：把每一个离散的状态转化为一个简短的声音信号, 个音素的序列以及离散的子音素状态, 模型将语音信号视作由如下过程生成, 比如每一个音素的开始, 中间
        - 摘要：之后，随着更大更深的模型以及更大的数据集的出现，通过使用神经网；络代替GMM来实现将声学特征转化为音素（或者子音素状态）的过程；可以大大地提高识别的精度。从2009年开始，语音识别的研究者们将一
          關鍵詞：之后, 的过程, 络代替, 通过使用神经网, 或者子音素状态
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；发展到了使用诸如整流线性单元和Dropout这样的技术（Zeiler  et  al.  ，；2013；Dahl  et  al.  ，2013）。从那时开始，工业界的几个语音研究组开
          關鍵詞：从那时开始, 发展到了使用诸如整流线性单元和, 这样的技术, 工业界的几个语音研究组开
        - 摘要：随后，当研究组使用了越来越大的带标签的数据集，加入了各种初始；化、训练方法以及调试深度神经网络的结构之后，他们发现这种无监督；的预训练方式是没有必要的，或者说不能带来任何显著的改进。
          關鍵詞：当研究组使用了越来越大的带标签的数据集, 加入了各种初始, 随后, 或者说不能带来任何显著的改进, 的预训练方式是没有必要的
        - 摘要：用语音识别中词错误率来衡量，在语音识别性能上的这些突破是史无前；例的（大约30％的提高）。在这之前的长达十年左右的时间内，尽管数；据集的规模是随时间增长的（见Deng  and  Yu（2014）的图2.4），但基
          關鍵詞：但基, 据集的规模是随时间增长的, 例的, 尽管数, 在这之前的长达十年左右的时间内
        - 摘要：其中的一个创新点是卷积网络的应用（Sainath  et  al.  ，2013）。卷积网；络在时域与频域上复用了权重，改进了之前的仅在时域上使用重复权值；的时延神经网络。这种新的二维卷积模型并不是将输入的频谱当作一个
          關鍵詞：其中的一个创新点是卷积网络的应用, 的时延神经网络, 这种新的二维卷积模型并不是将输入的频谱当作一个, 卷积网, 改进了之前的仅在时域上使用重复权值
        - 摘要：et
          關鍵詞：
        - 摘要：完全抛弃HMM并转向研究端到端的深度学习语音识别系统是至今仍然；活跃的另一个重要推动。这个领域第一个主要突破是Graves；（2013），他训练了一个深度的长短期记忆循环神经网络（见第10.10
          關鍵詞：活跃的另一个重要推动, 这个领域第一个主要突破是, 完全抛弃, 他训练了一个深度的长短期记忆循环神经网络, 并转向研究端到端的深度学习语音识别系统是至今仍然
        - 摘要：al.
          關鍵詞：
        - 摘要：另一个端到端深度学习语音识别方向的最新方法是，让系统学习如何利；用语音（phonetic）层级的信息“排列”声学；（acoustic）层级的信息
          關鍵詞：层级的信息, 让系统学习如何利, 声学, 另一个端到端深度学习语音识别方向的最新方法是, 排列
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；12.4　自然语言处理
          關鍵詞：自然语言处理
        - 摘要：自然语言处理  （natural  language  processing，NLP）是让计算机能够使；用人类语言，例如英语或法语。为了让简单的程序能够高效明确地解；析，计算机程序通常读取和发出特殊化的语言。而自然语言通常是模糊
          關鍵詞：是让计算机能够使, 自然语言处理, 例如英语或法语, 为了让简单的程序能够高效明确地解, 计算机程序通常读取和发出特殊化的语言
        - 摘要：与本章讨论的其他应用一样，非常通用的神经网络技术可以成功地应用；于自然语言处理。然而，为了实现卓越的性能并扩展到大型应用程序，；一些领域特定的策略也很重要。为了构建自然语言的有效模型，通常必
          關鍵詞：与本章讨论的其他应用一样, 为了实现卓越的性能并扩展到大型应用程序, 通常必, 为了构建自然语言的有效模型, 然而
        - 摘要：12.4.1　n-gram
          關鍵詞：
        - 摘要：语言模型  （language  model）定义了自然语言中标记序列的概率分布。；根据模型的设计，标记可以是词、字符甚至是字节。标记总是离散的实；体。最早成功的语言模型基于固定长度序列的标记模型，称为n-gram。
          關鍵詞：标记总是离散的实, 称为, 标记可以是词, 字符甚至是字节, 最早成功的语言模型基于固定长度序列的标记模型
        - 摘要：基于n-gram的模型定义一个条件概率——给定前n-1个标记后的第n个标；记的条件概率。该模型使用这些条件分布的乘积定义较长序列的概率分；布：
          關鍵詞：给定前, 的模型定义一个条件概率, 个标, 记的条件概率, 该模型使用这些条件分布的乘积定义较长序列的概率分
        - 摘要：这个分解可以由概率的链式法则证明。初始序列P(x 1 ,…,x  n-1 )的概率分；布可以通过带有较小n值的不同模型建模。
          關鍵詞：初始序列, 这个分解可以由概率的链式法则证明, 的概率分, 值的不同模型建模, 布可以通过带有较小
        - 摘要：训练n-gram模型是简单的，因为最大似然估计可以通过简单地统计每个；可能的n-gram在训练集中出现的次数来获得。几十年来，基于n-gram的；模型都是统计语言模型的核心模块（Jelinek  and  Mercer，1980；Katz，
          關鍵詞：训练, 在训练集中出现的次数来获得, 模型都是统计语言模型的核心模块, 可能的, 基于
        - 摘要：对于小的n值，模型有特定的名称：n＝1称为一元语法  （unigram），n；＝2称为二元语法 （bigram），n＝3称为三元语法  （trigram）。这些名；称源于相应数字的拉丁前缀和希腊后缀“-gram”，分别表示所写之物。
          關鍵詞：分别表示所写之物, 称为三元语法, 模型有特定的名称, 称为二元语法, 对于小的
        - 摘要：通常我们同时训练n-gram模型和n-1  gram模型。这使得下式可以简单地；通过查找两个存储的概率来计算。
          關鍵詞：这使得下式可以简单地, 通常我们同时训练, 模型, 通过查找两个存储的概率来计算, 模型和
        - 摘要：为了在P  n  中精确地再现推断，我们训练P  n-1  时必须省略每个序列最后；一个字符。
          關鍵詞：一个字符, 为了在, 时必须省略每个序列最后, 中精确地再现推断, 我们训练
        - 摘要：举个例子，我们演示三元模型如何计算句子“THE DOG RAN AWAY．；”的概率。句子的第一个词不能通过上述条件概率的公式计算，因为句；子的开头没有上下文。取而代之，在句子的开头我们必须使用词的边缘
          關鍵詞：取而代之, 在句子的开头我们必须使用词的边缘, 因为句, 举个例子, 句子的第一个词不能通过上述条件概率的公式计算
        - 摘要：n-gram模型最大似然的基本限制是，在许多情况下从训练集计数估计得；到的P n 很可能为零（即使元组(x t-n+1 ,…,x t )可能出现在测试集中）。这；可能会导致两种不同的灾难性后果。当P  n-1  为零时，该比率是未定义
          關鍵詞：模型最大似然的基本限制是, 到的, 该比率是未定义, 可能会导致两种不同的灾难性后果, 很可能为零
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；先验的贝叶斯推断。另一个非常流行的想法是包含高阶和低阶n-gram模；型的混合模型，其中高阶模型提供更多的容量，而低阶模型尽可能地避
          關鍵詞：先验的贝叶斯推断, 其中高阶模型提供更多的容量, 型的混合模型, 而低阶模型尽可能地避, 另一个非常流行的想法是包含高阶和低阶
        - 摘要：经典的n-gram模型特别容易引起维数灾难。因为存在；可能的n-；gram，而且   通常很大。即使有大量训练数据和适当的n，大多数n-
          關鍵詞：即使有大量训练数据和适当的, 因为存在, 大多数, 而且, 模型特别容易引起维数灾难
        - 摘要：为了提高n-gram模型的统计效率，基于类的语言模型；（class-based；language model）（Brown et al. ，1992；Ney and Kneser，1993；Niesler
          關鍵詞：基于类的语言模型, 为了提高, 模型的统计效率
        - 摘要：12.4.2　神经语言模型
          關鍵詞：神经语言模型
        - 摘要：神经语言模型  （neural  language  model，NLM）是一类用来克服维数灾；难的语言模型，它使用词的分布式表示对自然语言序列建模（Bengio  et；al.  ，2001b）。不同于基于类的n-gram模型，神经语言模型在能够识别
          關鍵詞：它使用词的分布式表示对自然语言序列建模, 是一类用来克服维数灾, 神经语言模型在能够识别, 神经语言模型, 模型
        - 摘要：性的表示，则包含词cat  的句子可以告知模型对包含词dog  的句子做出；预测，反之亦然。因为这样的属性很多，所以存在许多泛化的方式，可；以将信息从每个训练语句传递到指数数量的语义相关语句。维数灾难需
          關鍵詞：维数灾难需, 反之亦然, 的句子做出, 性的表示, 因为这样的属性很多
        - 摘要：我们有时将这些词表示称为词嵌入  （word  embedding）。在这个解释；下，我们将原始符号视为维度等于词表大小的空间中的点。词表示将这；些点嵌入到较低维的特征空间中。在原始空间中，每个词由一个one-hot
          關鍵詞：在这个解释, 我们有时将这些词表示称为词嵌入, 些点嵌入到较低维的特征空间中, 词表示将这, 在原始空间中
        - 摘要：图12.3　从神经机器翻译模型获得的词嵌入的二维可视化（Bahdanau et al. ，2015）。此图在语；义相关词的特定区域放大，它们具有彼此接近的嵌入向量。国家在左图，数字在右图。注意，；这些嵌入是为了可视化才表示为二维。在实际应用中，嵌入通常具有更高的维度并且可以同时
          關鍵詞：从神经机器翻译模型获得的词嵌入的二维可视化, 数字在右图, 嵌入通常具有更高的维度并且可以同时, 注意, 在实际应用中
        - 摘要：其他领域的神经网络也可以定义嵌入。例如，卷积网络的隐藏层提；供“图像嵌入”。因为自然语言最初不在实值向量空间上，所以NLP从业；者通常对嵌入的这个想法更感兴趣。隐藏层在表示数据的方式上提供了
          關鍵詞：其他领域的神经网络也可以定义嵌入, 例如, 从业, 者通常对嵌入的这个想法更感兴趣, 所以
        - 摘要：使用分布式表示来改进自然语言处理模型的基本思想不必局限于神经网；络。它还可以用于图模型，其中分布式表示是多个潜变量的形式（Mnih
          關鍵詞：它还可以用于图模型, 其中分布式表示是多个潜变量的形式, 使用分布式表示来改进自然语言处理模型的基本思想不必局限于神经网
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；and Hinton，2007）。
          關鍵詞：
        - 摘要：12.4.3　高维输出
          關鍵詞：高维输出
        - 摘要：在许多自然语言应用中，通常希望我们的模型产生词（而不是字符）作；为输出的基本单位。对于大词汇表，由于词汇量很大，在词的选择上表；示输出分布的计算成本可能非常高。在许多应用中，   包含数十万
          關鍵詞：为输出的基本单位, 示输出分布的计算成本可能非常高, 对于大词汇表, 而不是字符, 包含数十万
        - 摘要：假设 h 是用于预测输出概率  的顶部隐藏层。如果我们使用学到的权重；W  和学到的偏置 b  参数化从  h  到   的变换，则仿射softmax输出层执行；以下计算：
          關鍵詞：如果我们使用学到的权重, 是用于预测输出概率, 输出层执行, 和学到的偏置, 的顶部隐藏层
        - 摘要：。在n  h  为数千；如果 h 包含n h 个元素，则上述操作复杂度是；和  数十万的情况下，这个操作占据了神经语言模型的大多数计算。
          關鍵詞：如果, 则上述操作复杂度是, 包含, 数十万的情况下, 这个操作占据了神经语言模型的大多数计算
        - 摘要：12.4.3.1　使用短列表
          關鍵詞：使用短列表
        - 摘要：第一个神经语言模型（Bengio et al. ，2001b，2003）通过将词汇量限制；为10  000或20  000来减轻大词汇表上softmax的高成本。Schwenk  and；Gauvain（2002）和Schwenk（2007）在这种方法的基础上建立新的方
          關鍵詞：的高成本, 来减轻大词汇表上, 在这种方法的基础上建立新的方, 第一个神经语言模型, 通过将词汇量限制
        - 摘要：分为最常见词汇（由神经网络处理）的短列表；式，将词汇表；（shortlist）   和较稀有词汇的尾列表
          關鍵詞：和较稀有词汇的尾列表, 分为最常见词汇, 将词汇表, 由神经网络处理, 的短列表
        - 摘要：实现这个预测。额外输出则可以用来估计   中所有词
          關鍵詞：额外输出则可以用来估计, 实现这个预测, 中所有词
        - 摘要：的概率分布，如下：
          關鍵詞：的概率分布, 如下
        - 摘要：其中；由n-gram模型提供。稍作修改，这种方法也可以在神经语言模型模型的；softmax层中使用额外的输出值，而不是单独的sigmoid单元。
          關鍵詞：稍作修改, 这种方法也可以在神经语言模型模型的, 模型提供, 其中, 层中使用额外的输出值
        - 摘要：由神经语言模型提供
          關鍵詞：由神经语言模型提供
        - 摘要：短列表方法的一个明显缺点是，神经语言模型的潜在泛化优势仅限于最；常用的词，这大概是最没用的。这个缺点引发了处理高维输出替代方法；的探索，如下所述。
          關鍵詞：的探索, 短列表方法的一个明显缺点是, 如下所述, 神经语言模型的潜在泛化优势仅限于最, 这个缺点引发了处理高维输出替代方法
        - 摘要：12.4.3.2　分层Softmax
          關鍵詞：分层
        - 摘要：上高维输出层计算负担的经典方法（Goodman，；减少大词汇表；一样低，而无须
          關鍵詞：一样低, 上高维输出层计算负担的经典方法, 减少大词汇表, 而无须
        - 摘要：我们可以认为这种层次结构是先建立词的类别，然后是词类别的类别，；然后是词类别的类别的类别等。这些嵌套类别构成一棵树，其叶子为；词。在平衡树中，树的深度为log
          關鍵詞：其叶子为, 然后是词类别的类别, 我们可以认为这种层次结构是先建立词的类别, 这些嵌套类别构成一棵树, 树的深度为
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图12.4　词类别简单层次结构的示意图，其中8个词w 0 ,…,w 7 组织成三级层次结构。树的叶子；表示实际特定的词。内部节点表示词的组别。任何节点都可以通过二值决策序列（0＝左，1＝
          關鍵詞：个词, 任何节点都可以通过二值决策序列, 树的叶子, 内部节点表示词的组别, 其中
        - 摘要：次操作（从根开始的路径上的每个节点一次操作）。在该
          關鍵詞：次操作, 在该, 从根开始的路径上的每个节点一次操作
        - 摘要：的对数同阶：从
          關鍵詞：的对数同阶
        - 摘要：为了预测树的每个节点所需的条件概率，我们通常在树的每个节点处使；用逻辑回归模型，并且为所有这些模型提供与输入相同的上下文C。因；为正确的输出编码在训练集中，我们可以使用监督学习训练逻辑回归模
          關鍵詞：我们可以使用监督学习训练逻辑回归模, 为了预测树的每个节点所需的条件概率, 用逻辑回归模型, 为正确的输出编码在训练集中, 我们通常在树的每个节点处使
        - 摘要：因为可以高效地计算输出对数似然（低至log
          關鍵詞：低至, 因为可以高效地计算输出对数似然
        - 摘要：而不是  ），所以也
          關鍵詞：所以也, 而不是
        - 摘要：可以高效地计算梯度。这不仅包括关于输出参数的梯度，而且还包括关；于隐藏层激活的梯度。
          關鍵詞：可以高效地计算梯度, 于隐藏层激活的梯度, 这不仅包括关于输出参数的梯度, 而且还包括关
        - 摘要：优化树结构最小化期望的计算数量是可能的，但通常不切实际。给定词；的相对频率，信息理论的工具可以指定如何选择最佳的二进制编码。为；此，我们可以构造树，使得与词相关联的位数量近似等于该词频率的对
          關鍵詞：的相对频率, 优化树结构最小化期望的计算数量是可能的, 给定词, 信息理论的工具可以指定如何选择最佳的二进制编码, 但通常不切实际
        - 摘要：，而输出计算增长为O(n h n b )。只要n b ≤ln h ，我们可以通过收；缩n h 比收缩n b 减少更多的计算量。事实上，n  b  通常很小。因为词汇表；的大小很少超过一百万，而
          關鍵詞：通常很小, 事实上, 的大小很少超过一百万, 而输出计算增长为, 比收缩
        - 摘要：一个仍然有点开放的问题是如何最好地定义这些词类，或者如何定义一；般的词层次结构。早期工作使用现有的层次结构（Morin  and  Bengio，；2005），但也可以理想地与神经语言模型联合学习层次结构。学习层次
          關鍵詞：但也可以理想地与神经语言模型联合学习层次结构, 或者如何定义一, 学习层次, 早期工作使用现有的层次结构, 一个仍然有点开放的问题是如何最好地定义这些词类
        - 摘要：分层softmax的一个重要优点是，它在训练期间和测试期间（如果在测；试时我们想计算特定词的概率）都带来了计算上的好处。
          關鍵詞：的一个重要优点是, 都带来了计算上的好处, 分层, 它在训练期间和测试期间, 如果在测
        - 摘要：当然即使使用分层softmax，计算所有  个词概率的成本仍是很高的。；另一个重要的操作是在给定上下文中选择最可能的词。不幸的是，树结；构不能为这个问题提供高效精确的解决方案。
          關鍵詞：另一个重要的操作是在给定上下文中选择最可能的词, 树结, 构不能为这个问题提供高效精确的解决方案, 个词概率的成本仍是很高的, 不幸的是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；其缺点是在实践中，分层softmax倾向于更差的测试结果（相对基于采；样的方法），我们将在下文描述。这可能是因为词类选择得不好。
          關鍵詞：我们将在下文描述, 倾向于更差的测试结果, 样的方法, 分层, 相对基于采
        - 摘要：12.4.3.3　重要采样
          關鍵詞：重要采样
        - 摘要：加速神经语言模型训练的一种方式是，避免明确地计算所有未出现在下；一位置的词对梯度的贡献。每个不正确的词在此模型下具有低概率。枚；举所有这些词的计算成本可能会很高。相反，我们可以仅采样词的子
          關鍵詞：加速神经语言模型训练的一种方式是, 避免明确地计算所有未出现在下, 举所有这些词的计算成本可能会很高, 我们可以仅采样词的子, 每个不正确的词在此模型下具有低概率
        - 摘要：phase）项，推动a
          關鍵詞：推动
        - 摘要：其中  a  是presoftmax激活（或得分）向量，每个词对应一个元素。第一；y  向上；而第二项是负相；项是正相  （positive
          關鍵詞：向上, 而第二项是负相, 或得分, 第一, 激活
        - 摘要：and
          關鍵詞：
        - 摘要：我们可以从另一个分布中采样，而不是从模型中采样，这个分布称为提；议分布 （proposal distribution）（记为q），并通过适当的权重校正从错；误分布采样引入的偏差（Bengio
          關鍵詞：记为, 议分布, 这个分布称为提, 我们可以从另一个分布中采样, 并通过适当的权重校正从错
        - 摘要：Sénécal，2003；Bengio
          關鍵詞：
        - 摘要：这些权重用于对来自q的m个负样本给出适当的重要性，以形成负相估；计对梯度的贡献：
          關鍵詞：个负样本给出适当的重要性, 计对梯度的贡献, 以形成负相估, 这些权重用于对来自
        - 摘要：一元语法或二元语法分布与提议分布q工作得一样好。从数据估计这种；分布的参数是很容易。在估计参数之后，也可以非常高效地从这样的分；布采样。
          關鍵詞：一元语法或二元语法分布与提议分布, 也可以非常高效地从这样的分, 布采样, 从数据估计这种, 分布的参数是很容易
        - 摘要：重要采样 （Importance Sampling）不仅可以加速具有较大softmax输出的；模型。更一般地，它可以加速具有大稀疏输出层的训练，其中输出是稀；疏向量而不是n选1。其中一个例子是词袋  （bag  of  words）。词袋具有
          關鍵詞：其中一个例子是词袋, 疏向量而不是, 更一般地, 不仅可以加速具有较大, 它可以加速具有大稀疏输出层的训练
        - 摘要：在所有这些情况下，输出层梯度估计的计算复杂度被减少为与负样本数；量成比例，而不是与输出向量的大小成比例。
          關鍵詞：输出层梯度估计的计算复杂度被减少为与负样本数, 量成比例, 在所有这些情况下, 而不是与输出向量的大小成比例
        - 摘要：12.4.3.4　噪声对比估计和排名损失
          關鍵詞：噪声对比估计和排名损失
        - 摘要：为减少训练大词汇表的神经语言模型的计算成本，研究者也提出了其他；基于采样的方法。早期的例子是Collobert  and  Weston（2008a）提出的；排名损失，将神经语言模型每个词的输出视为一个得分，并试图使正确
          關鍵詞：将神经语言模型每个词的输出视为一个得分, 基于采样的方法, 研究者也提出了其他, 排名损失, 早期的例子是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；词的得分a y 比其他词a i 排名更高。提出的排名损失则是
          關鍵詞：比其他词, 排名更高, 提出的排名损失则是, 词的得分
        - 摘要：如果观察到词的得分a y 远超过负词的得分a i （相差大于1），则第i项梯；度为零。这个准则的一个问题是它不提供估计的条件概率，条件概率在；很多应用中是有用的，包括语音识别和文本生成（包括诸如翻译的条件
          關鍵詞：则第, 项梯, 度为零, 包括语音识别和文本生成, 相差大于
        - 摘要：最近用于神经语言模型的训练目标是噪声对比估计，将在第18.6节中介；绍。这种方法已成功应用于神经语言模型（Mnih  and  Teh，2012；Mnih；and Kavukcuoglu，2013）。
          關鍵詞：将在第, 这种方法已成功应用于神经语言模型, 节中介, 最近用于神经语言模型的训练目标是噪声对比估计
        - 摘要：12.4.4　结合n-gram和神经语言模型
          關鍵詞：和神经语言模型, 结合
        - 摘要：n-gram模型相对神经网络的主要优点是n-gram模型具有更高的模型容量；（通过存储非常多的元组的频率），并且处理样本只需非常少的计算量；（通过查找只匹配当前上下文的几个元组）。如果我们使用哈希表或树
          關鍵詞：模型相对神经网络的主要优点是, 并且处理样本只需非常少的计算量, 模型具有更高的模型容量, 通过查找只匹配当前上下文的几个元组, 如果我们使用哈希表或树
        - 摘要：因此，增加容量的一种简单方法是将两种方法结合，由神经语言模型和；n-gram语言模型组成集成（Bengio et al. ，2001b，2003）。
          關鍵詞：由神经语言模型和, 因此, 语言模型组成集成, 增加容量的一种简单方法是将两种方法结合
        - 摘要：对于任何集成，如果集成成员产生独立的错误，这种技术可以减少测试；误差。集成学习领域提供了许多方法来组合集成成员的预测，包括统一；加权和在验证集上选择权重。Mikolov  et  al.  （2011a）扩展了集成，不
          關鍵詞：这种技术可以减少测试, 包括统一, 如果集成成员产生独立的错误, 对于任何集成, 加权和在验证集上选择权重
        - 摘要：接到模型的任何其他部分。额外输入是输入上下文中特定n-gram是否存；在的指示器，因此这些变量是非常高维且非常稀疏的。
          關鍵詞：在的指示器, 接到模型的任何其他部分, 额外输入是输入上下文中特定, 因此这些变量是非常高维且非常稀疏的, 是否存
        - 摘要：模型容量的增加是巨大的（架构的新部分包含高达｜sV｜  n  个参数），；但是处理输入所需的额外计算量是很小的（因为额外输入非常稀疏）。
          關鍵詞：但是处理输入所需的额外计算量是很小的, 因为额外输入非常稀疏, 个参数, 模型容量的增加是巨大的, 架构的新部分包含高达
        - 摘要：12.4.5　神经机器翻译
          關鍵詞：神经机器翻译
        - 摘要：机器翻译以一种自然语言读取句子并产生等同含义的另一种语言的句；子。机器翻译系统通常涉及许多组件。在高层次，一个组件通常会提出；许多候选翻译。由于语言之间的差异，这些翻译中的许多翻译是不符合
          關鍵詞：机器翻译以一种自然语言读取句子并产生等同含义的另一种语言的句, 这些翻译中的许多翻译是不符合, 一个组件通常会提出, 机器翻译系统通常涉及许多组件, 在高层次
        - 摘要：最早的机器翻译神经网络探索中已经纳入了编码器和解码器的想法；（Allen  1987；Chris-man  1991；Forcada  and  Ñeco  1997），而翻译中神；经网络的第一个大规模有竞争力的用途是通过神经语言模型升级翻译系
          關鍵詞：最早的机器翻译神经网络探索中已经纳入了编码器和解码器的想法, 经网络的第一个大规模有竞争力的用途是通过神经语言模型升级翻译系, 而翻译中神
        - 摘要：and
          關鍵詞：
        - 摘要：传统语言模型仅仅报告自然语言句子的概率。因为机器翻译涉及给定输；入句子产生输出句子，所以将自然语言模型扩展为条件的是有意义的。；如第6.2.1.1节所述，可以直接地扩展一个模型，该模型定义某些变量的
          關鍵詞：因为机器翻译涉及给定输, 可以直接地扩展一个模型, 如第, 节所述, 入句子产生输出句子
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；基于MLP方法的缺点是需要将序列预处理为固定长度。为了使翻译更加；灵活，我们希望模型允许可变的输入长度和输出长度。RNN具备这种能
          關鍵詞：我们希望模型允许可变的输入长度和输出长度, 方法的缺点是需要将序列预处理为固定长度, 为了使翻译更加, 灵活, 具备这种能
        - 摘要：图12.5　编码器-解码器架构在直观表示（例如词序列或图像）和语义表示之间来回映射。使用；来自一种模态数据的编码器输出（例如从法语句子到捕获句子含义的隐藏表示的编码器映射）；作为用于另一模态的解码器输入（如解码器将捕获句子含义的隐藏表示映射到英语），我们可
          關鍵詞：例如从法语句子到捕获句子含义的隐藏表示的编码器映射, 解码器架构在直观表示, 作为用于另一模态的解码器输入, 来自一种模态数据的编码器输出, 我们可
        - 摘要：为生成以源句为条件的整句，模型必须具有表示整个源句的方式。早期
          關鍵詞：为生成以源句为条件的整句, 早期, 模型必须具有表示整个源句的方式
        - 摘要：模型只能表示单个词或短语。从表示学习的观点来看，具有相同含义的；句子具有类似表示是有用的，无论它们是以源语言还是以目标语言书；写。研究者首先使用卷积和RNN的组合探索该策略（Kalchbrenner
          關鍵詞：研究者首先使用卷积和, 无论它们是以源语言还是以目标语言书, 句子具有类似表示是有用的, 从表示学习的观点来看, 具有相同含义的
        - 摘要：12.4.5.1　使用注意力机制并对齐数据片段
          關鍵詞：使用注意力机制并对齐数据片段
        - 摘要：使用固定大小的表示概括非常长的句子（例如60个词）的所有语义细节；是非常困难的。这需要使用足够大的RNN，并且用足够长的时间训练得；很好才能实现，如Cho et al. （2014b）和Sutskever et al. （2014）所表明
          關鍵詞：个词, 这需要使用足够大的, 例如, 使用固定大小的表示概括非常长的句子, 的所有语义细节
        - 摘要：图12.6　由Bahdanau et al. （2015）引入的现代注意力机制，本质上是加权平均。注意力机制对；具有权重α (t) 的特征向量 h (t) 进行加权平均形成上下文向量 c 。在一些应用中，特征向量 h 是神
          關鍵詞：注意力机制对, 进行加权平均形成上下文向量, 特征向量, 具有权重, 引入的现代注意力机制
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；经网络的隐藏单元，但它们也可以是模型的原始输入。权重α (t) 由模型本身产生。它们通常是；区间［0，1］中的值，并且旨在仅仅集中在单个 h (t) 周围，使得加权平均精确地读取接近一个
          關鍵詞：由模型本身产生, 周围, 中的值, 并且旨在仅仅集中在单个, 经网络的隐藏单元
        - 摘要：我们可以认为基于注意力机制的系统有三个组件：
          關鍵詞：我们可以认为基于注意力机制的系统有三个组件
        - 摘要：读取器读取原始数据（例如源语句中的源词）并将其转换为分布式；表示，其中一个特征向量与每个词的位置相关联。；存储器存储读取器输出的特征向量列表。这可以被理解为包含事实
          關鍵詞：表示, 读取器读取原始数据, 其中一个特征向量与每个词的位置相关联, 存储器存储读取器输出的特征向量列表, 并将其转换为分布式
        - 摘要：第三组件可以生成翻译语句。
          關鍵詞：第三组件可以生成翻译语句
        - 摘要：当用一种语言书写的句子中的词与另一种语言的翻译语句中的相应词对；齐时，可以使对应的词嵌入相关联。早期的工作表明，我们可以学习将；一种语言中的词嵌入与另一种语言中的词嵌入相关联的翻译矩阵
          關鍵詞：我们可以学习将, 一种语言中的词嵌入与另一种语言中的词嵌入相关联的翻译矩阵, 当用一种语言书写的句子中的词与另一种语言的翻译语句中的相应词对, 早期的工作表明, 齐时
        - 摘要：12.4.6　历史展望
          關鍵詞：历史展望
        - 摘要：在对反向传播的第一次探索中，Rumelhart et al. （1986a）等人提出了分；布式表示符号的思想，其中符号对应于族成员的身份，而神经网络捕获；族成员之间的关系，训练样本形成三元组如（Colin、Mother、
          關鍵詞：等人提出了分, 布式表示符号的思想, 在对反向传播的第一次探索中, 训练样本形成三元组如, 而神经网络捕获
        - 摘要：母亲。
          關鍵詞：母亲
        - 摘要：Deerwester et al. （1990）将符号嵌入的想法扩展到对词的嵌入。这些嵌；入使用SVD学习。之后，嵌入将通过神经网络学习。
          關鍵詞：之后, 嵌入将通过神经网络学习, 学习, 将符号嵌入的想法扩展到对词的嵌入, 入使用
        - 摘要：自然语言处理的历史是由流行表示（对模型输入不同方式的表示）的变；化为标志的。在早期对符号和词建模的工作之后，神经网络在NLP上一；些最早的应用（Miikkulainen  and  Dyer，1991；Schmidhuber，1996）将
          關鍵詞：对模型输入不同方式的表示, 些最早的应用, 上一, 自然语言处理的历史是由流行表示, 在早期对符号和词建模的工作之后
        - 摘要：Bengio et al. （2001b）将焦点重新引到对词建模并引入神经语言模型，；能产生可解释的词嵌入。这些神经模型已经从在一小组符号上的定义表；示（20世纪80年代）扩展到现代应用中的数百万字（包括专有名词和拼
          關鍵詞：能产生可解释的词嵌入, 将焦点重新引到对词建模并引入神经语言模型, 世纪, 扩展到现代应用中的数百万字, 年代
        - 摘要：最初，使用词作为语言模型的基本单元可以改进语言建模的性能；（Bengio et al.  ，2001b）。而今，新技术不断推动基于字符（Sutskever；et  al. ，2011）和基于词的模型向前发展，最近的工作（Gillick  et  al.  ，
          關鍵詞：使用词作为语言模型的基本单元可以改进语言建模的性能, 最近的工作, 新技术不断推动基于字符, 和基于词的模型向前发展, 最初
        - 摘要：神经语言模型背后的思想已经扩展到多个自然语言处理应用，如解析；（Henderson，2003，2004；Collobert，2011）、词性标注、语义角色标；注、分块等，有时使用共享词嵌入的单一多任务学习架构（Collobert
          關鍵詞：语义角色标, 有时使用共享词嵌入的单一多任务学习架构, 词性标注, 分块等, 神经语言模型背后的思想已经扩展到多个自然语言处理应用
        - 摘要：随着t-SNE降维算法的发展（van  der  Maaten  and  Hinton，2008）以及；Joseph  Turian在2009年引入的专用于可视化词嵌入的应用，用于分析语；言模型嵌入的二维可视化成为一种流行的工具。
          關鍵詞：用于分析语, 言模型嵌入的二维可视化成为一种流行的工具, 随着, 以及, 降维算法的发展
    12.4：自然语言处理
        - 摘要：12.4.1　n-gram
          關鍵詞：
        - 摘要：12.4.2　神经语言模型
          關鍵詞：神经语言模型
        - 摘要：12.4.3　高维输出
          關鍵詞：高维输出
        - 摘要：12.4.4　结合n-gram和神经语言模型
          關鍵詞：和神经语言模型, 结合
        - 摘要：12.4.5　神经机器翻译
          關鍵詞：神经机器翻译
        - 摘要：12.4.6　历史展望
          關鍵詞：历史展望
        - 摘要：12.4.1　n-gram
          關鍵詞：
        - 摘要：12.4.2　神经语言模型
          關鍵詞：神经语言模型
        - 摘要：12.4.3　高维输出
          關鍵詞：高维输出
        - 摘要：12.4.4　结合n-gram和神；经语言模型
          關鍵詞：和神, 结合, 经语言模型
        - 摘要：12.4.5　神经机器翻译
          關鍵詞：神经机器翻译
        - 摘要：12.4.6　历史展望
          關鍵詞：历史展望
    12.5：其他应用
        - 摘要：12.5.1　推荐系统
          關鍵詞：推荐系统
        - 摘要：12.5.2　知识表示、推理和回答
          關鍵詞：知识表示, 推理和回答
        - 摘要：第3部分　深度学习研究
          關鍵詞：部分, 深度学习研究
        - 摘要：在本节中，我们介绍深度学习一些其他类型的应用，它们与上面讨论的；标准对象识别、语音识别和自然语言处理任务不同。本书的第3部分将；扩大这个范围，甚至进一步扩展到仍是目前主要研究领域的任务。
          關鍵詞：在本节中, 本书的第, 扩大这个范围, 语音识别和自然语言处理任务不同, 部分将
        - 摘要：12.5.1　推荐系统
          關鍵詞：推荐系统
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；信息技术部门中机器学习的主要应用之一是向潜在用户或客户推荐项；目。这可以分为两种主要的应用：在线广告和项目建议（通常这些建议
          關鍵詞：这可以分为两种主要的应用, 通常这些建议, 信息技术部门中机器学习的主要应用之一是向潜在用户或客户推荐项, 在线广告和项目建议
        - 摘要：通常，这种关联问题可以作为监督学习问题来处理：给出一些关于项目；和关于用户的信息，预测感兴趣的行为（用户点击广告、输入评级、点；击“喜欢”按钮、购买产品，在产品上花钱、花时间访问产品页面等）。
          關鍵詞：在产品上花钱, 通常, 预测感兴趣的行为, 喜欢, 给出一些关于项目
        - 摘要：早期推荐系统的工作依赖于这些预测输入的最小信息：用户ID和项目；ID。在这种情况下，唯一的泛化方式依赖于不同用户或不同项目的目标；变量值之间的模式相似性。假设用户1和用户2都喜欢项目A，B和C．由
          關鍵詞：用户, 在这种情况下, 都喜欢项目, 唯一的泛化方式依赖于不同用户或不同项目的目标, 和用户
        - 摘要：通常，人们希望最小化预测评级  和实际评级  之间的平方误差。；当用户嵌入和项目嵌入首次缩小到低维度（两个或三个）时，它们就可；以方便地可视化，或者可以将用户或项目彼此进行比较（就像词嵌
          關鍵詞：当用户嵌入和项目嵌入首次缩小到低维度, 通常, 以方便地可视化, 它们就可, 两个或三个
        - 摘要：除了这些具有分布式表示的双线性模型之外，第一次用于协同过滤的神；al.  ，；经网络之一是基于RBM的无向概率模型（Salakhutdinov
          關鍵詞：第一次用于协同过滤的神, 经网络之一是基于, 的无向概率模型, 除了这些具有分布式表示的双线性模型之外
        - 摘要：et
          關鍵詞：
        - 摘要：然而，协同过滤系统有一个基本限制：当引入新项目或新用户时，缺乏；评级历史意味着无法评估其与其他项目或用户的相似性，或者说无法评；估新的用户和现有项目的联系。这被称为冷启动推荐问题。解决冷启动
          關鍵詞：解决冷启动, 缺乏, 估新的用户和现有项目的联系, 或者说无法评, 然而
        - 摘要：专用的深度学习架构，如卷积网络已经应用于从丰富内容中提取特征，；如提取用于音乐推荐的音乐音轨（van den Oörd et al. ，2013）。在该工；作中，卷积网络将声学特征作为输入并计算相关歌曲的嵌入。该歌曲嵌
          關鍵詞：如提取用于音乐推荐的音乐音轨, 在该工, 作中, 该歌曲嵌, 专用的深度学习架构
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；12.5.1.1　探索与开发
          關鍵詞：探索与开发
        - 摘要：当向用户推荐时，会产生超出普通监督学习范围的问题，并进入强化学；习的领域。理论上，许多推荐问题最准确的描述是contextual；bandit（Langford  and  Zhang，2008；Lu  et  al.  ，2010）。问题是，当我
          關鍵詞：当我, 习的领域, 理论上, 问题是, 许多推荐问题最准确的描述是
        - 摘要：强化学习需要权衡探索 （exploration）与开发  （exploitation）。开发指；的是从目前学到的最好策略采取动作，也就是我们所知的将获得高奖励；的动作。探索  是指采取行动以获得更多的训练数据。如果我们知道给
          關鍵詞：强化学习需要权衡探索, 开发指, 是指采取行动以获得更多的训练数据, 如果我们知道给, 与开发
        - 摘要：无论如何，我们至少获得了一些知识。
          關鍵詞：我们至少获得了一些知识, 无论如何
        - 摘要：探索  可以以许多方式实现，从覆盖可能动作的整个空间的随机动作到；基于模型的方法（基于预期回报和模型对该回报不确定性的量来计算动；作的选择）。
          關鍵詞：基于模型的方法, 从覆盖可能动作的整个空间的随机动作到, 探索, 基于预期回报和模型对该回报不确定性的量来计算动, 作的选择
        - 摘要：许多因素决定了我们喜欢探索或开发的程度。最突出的因素之一是我们；感兴趣的时间尺度。如果代理只有短暂的时间积累奖励，那么我们喜欢；更多的开发。如果代理有很长时间积累奖励，那么我们开始更多的探
          關鍵詞：最突出的因素之一是我们, 许多因素决定了我们喜欢探索或开发的程度, 如果代理只有短暂的时间积累奖励, 更多的开发, 如果代理有很长时间积累奖励
        - 摘要：监督学习在探索或开发之间没有权衡，因为监督信号总是指定哪个输出；对于每个输入是正确的。我们总是知道标签是最好的输出，没有必要尝；试不同的输出来确定是否优于模型当前的输出。
          關鍵詞：我们总是知道标签是最好的输出, 因为监督信号总是指定哪个输出, 监督学习在探索或开发之间没有权衡, 试不同的输出来确定是否优于模型当前的输出, 没有必要尝
        - 摘要：除了权衡探索和开发之外，强化学习背景下出现的另一个困难是难以评；估和比较不同的策略。强化学习包括学习者和环境之间的相互作用。这；个反馈回路意味着使用固定的测试集输入评估学习者的表现不是直接
          關鍵詞：估和比较不同的策略, 除了权衡探索和开发之外, 强化学习背景下出现的另一个困难是难以评, 个反馈回路意味着使用固定的测试集输入评估学习者的表现不是直接, 强化学习包括学习者和环境之间的相互作用
        - 摘要：12.5.2　知识表示、推理和回答
          關鍵詞：知识表示, 推理和回答
        - 摘要：因为使用符号（Rumelhart  et  al.  ，1986a）和词嵌入（Deerwester  et  al.；，1990；Bengio et al. ，2001b），深度学习方法在语言模型、机器翻译；和自然语言处理方面非常成功。这些嵌入表示关于单个词或概念的语义
          關鍵詞：深度学习方法在语言模型, 机器翻译, 和自然语言处理方面非常成功, 和词嵌入, 这些嵌入表示关于单个词或概念的语义
        - 摘要：12.5.2.1　知识、联系和回答
          關鍵詞：知识, 联系和回答
        - 摘要：一个有趣的研究方向是确定如何训练分布式表示才能捕获两个实体之间；的关系 （relation）。
          關鍵詞：一个有趣的研究方向是确定如何训练分布式表示才能捕获两个实体之间, 的关系
        - 摘要：数学中，二元关系是一组有序的对象对。集合中的对具有这种关系，而
          關鍵詞：数学中, 二元关系是一组有序的对象对, 集合中的对具有这种关系
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；那些不在集合中的对则没有。例如，我们可以在实体集{1，2，3}上定；。一旦
          關鍵詞：那些不在集合中的对则没有, 一旦, 例如, 我们可以在实体集, 上定
        - 摘要：在AI的背景下，我们将关系看作句法上简单且高度结构化的语言。关系；起到动词的作用，而关系的两个参数发挥着主体和客体的作用。这些句；子是一个三元组标记的形式：
          關鍵詞：子是一个三元组标记的形式, 而关系的两个参数发挥着主体和客体的作用, 我们将关系看作句法上简单且高度结构化的语言, 起到动词的作用, 的背景下
        - 摘要：其值是
          關鍵詞：其值是
        - 摘要：我们还可以定义属性  （attribute），类似于关系的概念，但只需要一个；参数：
          關鍵詞：参数, 类似于关系的概念, 我们还可以定义属性, 但只需要一个
        - 摘要：例如，我们可以定义has_fur 属性，并将其应用于像狗这样的实体。
          關鍵詞：并将其应用于像狗这样的实体, 属性, 我们可以定义, 例如
        - 摘要：许多应用中需要表示关系和推理。我们如何在神经网络中做到这一点？
          關鍵詞：许多应用中需要表示关系和推理, 我们如何在神经网络中做到这一点
        - 摘要：机器学习模型当然需要训练数据。我们可以推断非结构化自然语言组成；的训练数据集中实体之间的关系，也可以使用明确定义关系的结构化数；据库。这些数据库的共同结构是关系型数据库，它存储这种相同类型的
          關鍵詞：也可以使用明确定义关系的结构化数, 据库, 我们可以推断非结构化自然语言组成, 的训练数据集中实体之间的关系, 它存储这种相同类型的
        - 摘要：et
          關鍵詞：
        - 摘要：除了训练数据，我们还需定义训练的模型族。一种常见的方法是将神经；语言模型扩展到模型实体和关系。神经语言模型学习提供每个词分布式；表示的向量。他们还通过学习这些向量的函数来学习词之间的相互作
          關鍵詞：一种常见的方法是将神经, 语言模型扩展到模型实体和关系, 他们还通过学习这些向量的函数来学习词之间的相互作, 我们还需定义训练的模型族, 神经语言模型学习提供每个词分布式
        - 摘要：et
          關鍵詞：
        - 摘要：这种模型的实际短期应用是链接预测 （link prediction）：预测知识图谱；中缺失的弧。这是基于旧事实推广新事实的一种形式。目前存在的大多；数知识库都是通过人力劳动构建的，这往往使知识库缺失许多并且可能
          關鍵詞：目前存在的大多, 这是基于旧事实推广新事实的一种形式, 这往往使知识库缺失许多并且可能, 这种模型的实际短期应用是链接预测, 预测知识图谱
        - 摘要：我们很难评估链接预测任务上模型的性能，因为我们的数据集只有正样；本（已知是真实的事实）。如果模型提出了不在数据集中的事实，我们；不确定模型是犯了错误还是发现了一个新的以前未知的事实。度量基于
          關鍵詞：不确定模型是犯了错误还是发现了一个新的以前未知的事实, 因为我们的数据集只有正样, 如果模型提出了不在数据集中的事实, 我们, 我们很难评估链接预测任务上模型的性能
        - 摘要：知识库和分布式表示的另一个应用是词义消歧；（word-sense；disambiguation）（Navigli  and  Velardi，2005；Bordes  et  al.  ，2012），
          關鍵詞：知识库和分布式表示的另一个应用是词义消歧
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；最后，知识的关系结合一个推理过程和对自然语言的理解可以让我们建；立一个一般的问答系统。一般的问答系统必须能处理输入信息并记住重
          關鍵詞：知识的关系结合一个推理过程和对自然语言的理解可以让我们建, 一般的问答系统必须能处理输入信息并记住重, 立一个一般的问答系统, 最后
        - 摘要：深度学习已经应用于其他许多应用（除了这里描述的应用以外），并且；肯定会在此之后应用于更多的场景。我们不可能全面描述与此主题相关；的所有应用。本项调查尽可能地提供了在本文写作之时的代表性样本。
          關鍵詞：深度学习已经应用于其他许多应用, 除了这里描述的应用以外, 的所有应用, 肯定会在此之后应用于更多的场景, 我们不可能全面描述与此主题相关
        - 摘要：本书第2部分介绍了涉及深度学习的现代实践，囊括了所有非常成功的；方法。一般而言，这些方法使用代价函数的梯度寻找模型（近似于某些；所期望的函数）的参数。当具有足够的训练数据时，这种方法是非常强
          關鍵詞：的参数, 当具有足够的训练数据时, 所期望的函数, 本书第, 部分介绍了涉及深度学习的现代实践
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)  译者注：所有样本相似的距离。
          關鍵詞：所有样本相似的距离, 译者注
        - 摘要：分别可以在如下网址获取：freebase.com，cyc.com/opencyc，wordnet.princeton.edu，
          關鍵詞：分别可以在如下网址获取
        - 摘要：(2)；wikiba.se
          關鍵詞：
        - 摘要：(3)  geneontology.org
          關鍵詞：
        - 摘要：第3部分　深度学习研究
          關鍵詞：部分, 深度学习研究
        - 摘要：本书这一部分描述目前研究社群所追求的、更有远见和更先进的深度学；习方法。
          關鍵詞：习方法, 更有远见和更先进的深度学, 本书这一部分描述目前研究社群所追求的
        - 摘要：在本书的前两部分，我们已经展示了如何解决监督学习问题，即在给定；足够的映射样本的情况下，学习将一个向量映射到另一个。
          關鍵詞：学习将一个向量映射到另一个, 我们已经展示了如何解决监督学习问题, 在本书的前两部分, 足够的映射样本的情况下, 即在给定
        - 摘要：我们想要解决的问题并不全都属于这个类别。我们可能希望生成新的样；本、或确定一个点的似然性、或处理缺失值以及利用一组大量的未标记；样本或相关任务的样本。当前应用于工业的最先进技术的缺点是我们的
          關鍵詞：我们可能希望生成新的样, 当前应用于工业的最先进技术的缺点是我们的, 或确定一个点的似然性, 我们想要解决的问题并不全都属于这个类别, 样本或相关任务的样本
        - 摘要：许多深度学习算法被设计为处理无监督学习问题，但不像深度学习已经；在很大程度上解决了各种任务的监督学习问题，没有一个算法能以同样；的方式真正解决无监督学习问题。在本书这一部分，我们描述无监督学
          關鍵詞：在很大程度上解决了各种任务的监督学习问题, 在本书这一部分, 但不像深度学习已经, 没有一个算法能以同样, 的方式真正解决无监督学习问题
        - 摘要：无监督学习困难的核心原因是被建模的随机变量的高维度。这带来了两；个不同的挑战：统计挑战和计算挑战。统计挑战与泛化相关：我们可能；想要区分的配置数会随着感兴趣的维度数指数增长，并且这快速变得比
          關鍵詞：统计挑战和计算挑战, 这带来了两, 统计挑战与泛化相关, 个不同的挑战, 并且这快速变得比
        - 摘要：使用概率模型，这种计算挑战来自执行难解的推断或归一化分布。
          關鍵詞：使用概率模型, 这种计算挑战来自执行难解的推断或归一化分布
        - 摘要：难解的推断：推断主要在第19章讨论。推断关于捕获a、b和c上联；合分布的模型，给定其他变量b的情况下，猜测一些变量a的可能；值。为了计算这样的条件概率，我们需要对变量c的值求和，以及
          關鍵詞：难解的推断, 我们需要对变量, 上联, 章讨论, 推断主要在第
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；面对这些难以处理的计算的一种方法是近似它们，如在本书的第3部分；中讨论的，研究者已经提出了许多方法。这里还讨论另一种有趣的方式
          關鍵詞：中讨论的, 部分, 如在本书的第, 研究者已经提出了许多方法, 这里还讨论另一种有趣的方式
        - 摘要：第3部分对于研究者来说是最重要的，研究者想要了解深度学习领域的；广度，并将领域推向真正的人工智能。
          關鍵詞：并将领域推向真正的人工智能, 广度, 研究者想要了解深度学习领域的, 部分对于研究者来说是最重要的
        - 摘要：12.5.1　推荐系统
          關鍵詞：推荐系统
        - 摘要：12.5.2　知识表示、推理；和回答
          關鍵詞：推理, 知识表示, 和回答
        - 摘要：第3部分　深度学习研究
          關鍵詞：部分, 深度学习研究
第13章：线性因子模型
    12.5：其他应用
        - 摘要：许多深度学习的研究前沿均涉及构建输入的概率模型p  model  (  x  )。原则；上说，给定任何其他变量的情况下，这样的模型可以使用概率推断来预；h  ，其中
          關鍵詞：许多深度学习的研究前沿均涉及构建输入的概率模型, 其中, 给定任何其他变量的情况下, 原则, 上说
        - 摘要：在本章中，我们描述了一些基于潜变量的最简单的概率模型：线性因子；模型 （linear  factor  model）。这些模型有时被用来作为混合模型的组成；模块（Hinton et  al.  ，1995a；Ghahramani  and  Hinton，1996；Roweis  et
          關鍵詞：模块, 我们描述了一些基于潜变量的最简单的概率模型, 模型, 线性因子, 这些模型有时被用来作为混合模型的组成
        - 摘要：线性因子模型通过随机线性解码器函数来定义，该函数通过对 h 的线性；变换以及添加噪声来生成 x 。
          關鍵詞：线性因子模型通过随机线性解码器函数来定义, 该函数通过对, 变换以及添加噪声来生成, 的线性
        - 摘要：有趣的是，通过这些模型我们能够发现一些符合简单联合分布的解释性；因子。线性解码器的简单性使得它们成为了最早被广泛研究的潜变量模；型。
          關鍵詞：线性解码器的简单性使得它们成为了最早被广泛研究的潜变量模, 有趣的是, 通过这些模型我们能够发现一些符合简单联合分布的解释性, 因子
        - 摘要：线性因子模型描述如下的数据生成过程。首先，我们从一个分布中抽取；解释性因子 h ，
          關鍵詞：首先, 我们从一个分布中抽取, 解释性因子, 线性因子模型描述如下的数据生成过程
        - 摘要：其中p( h )是一个因子分布，满足p( h )＝∏ i p(h i )，所以易于从中采样。；接下来，在给定因子的情况下，我们对实值的可观察变量进行采样
          關鍵詞：接下来, 是一个因子分布, 满足, 其中, 我们对实值的可观察变量进行采样
        - 摘要：其中噪声通常是对角化的（在维度上是独立的）且服从高斯分布。这在；图13.1有具体说明。
          關鍵詞：这在, 且服从高斯分布, 有具体说明, 在维度上是独立的, 其中噪声通常是对角化的
        - 摘要：图13.1　描述线性因子模型族的有向图模型，其中我们假设观察到的数据向量 x 是通过独立的潜；在因子 h 的线性组合再加上一定噪声获得的。不同的模型，比如概率PCA、因子分析或者是；ICA，都是选择了不同形式的噪声以及先验p( h )
          關鍵詞：在因子, 都是选择了不同形式的噪声以及先验, 其中我们假设观察到的数据向量, 描述线性因子模型族的有向图模型, 的线性组合再加上一定噪声获得的
    13.1：概率PCA和因子分析
        - 摘要：概率  PCA（probabilistic  PCA）、因子分析和其他线性因子模型是上述；等式（式（13.1）和式（13.2））的特殊情况，并且仅在对观测到  x  之；前的噪声分布和潜变量 h 先验的选择上有所不同。
          關鍵詞：等式, 因子分析和其他线性因子模型是上述, 的特殊情况, 先验的选择上有所不同, 概率
        - 摘要：在因子分析  （factor；1994）中，潜变量的先验是一个方差为单位矩阵的高斯分布
          關鍵詞：潜变量的先验是一个方差为单位矩阵的高斯分布, 在因子分析
        - 摘要：analysis）（Bartholomew，1987；Basilevsky，
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；i  是条件独立  （conditionally；同时，假定在给定  h  的条件下观察值x
          關鍵詞：假定在给定, 同时, 是条件独立, 的条件下观察值
        - 摘要：σ
          關鍵詞：
        - 摘要：2
          關鍵詞：
        - 摘要：差。
          關鍵詞：
        - 摘要：因此，潜变量的作用是捕获不同观测变量x i 之间的依赖关系。实际上，；可以容易地看出 x 服从多维正态分布，并满足
          關鍵詞：潜变量的作用是捕获不同观测变量, 因此, 服从多维正态分布, 实际上, 可以容易地看出
        - 摘要：为了将PCA引入到概率框架中，我们可以对因子分析模型作轻微修改，；使条件方差   等于同一个值。在这种情况下，  x  的协方差简化为；，这里的σ  2  是一个标量。由此可以得到条件分布，如
          關鍵詞：是一个标量, 的协方差简化为, 由此可以得到条件分布, 使条件方差, 在这种情况下
        - 摘要：下：
          關鍵詞：
        - 摘要：或者等价地
          關鍵詞：或者等价地
        - 摘要：其中；出了一种迭代的EM算法来估计参数 W 和σ 2 。
          關鍵詞：其中, 算法来估计参数, 出了一种迭代的
        - 摘要：是高斯噪声。之后Tipping and Bishop（1999）提
          關鍵詞：是高斯噪声, 之后
        - 摘要：这个概率PCA  （probabilistic  PCA）模型利用了这样一种观察现象：除；了一些微小残余的重构误差  （reconstruction  error）（至多为σ  2  ），数；据中的大多数变化可以由潜变量
          關鍵詞：据中的大多数变化可以由潜变量, 至多为, 这个概率, 了一些微小残余的重构误差, 模型利用了这样一种观察现象
        - 摘要：描述。通过Tipping
          關鍵詞：通过, 描述
        - 摘要：h
          關鍵詞：
        - 摘要：当σ→0时，概率PCA所定义的密度函数在d维的  W  的列生成空间周围非
          關鍵詞：概率, 的列生成空间周围非, 维的, 所定义的密度函数在
        - 摘要：常尖锐。这导致模型会为没有在一个超平面附近聚集的数据分配非常低；的概率。
          關鍵詞：的概率, 常尖锐, 这导致模型会为没有在一个超平面附近聚集的数据分配非常低
    13.2：独立成分分析
        - 摘要：独立成分分析 （independent component analysis，ICA）是最古老的表示；学习算法之一（Herault  and  Ans，1984；Jutten  and  Herault，1991；；Comon，1994；Hyvärinen，1999；Hyvärinen  et  al.  ，2001a；Hinton  et
          關鍵詞：独立成分分析, 是最古老的表示, 学习算法之一
        - 摘要：许多不同的具体方法被称为ICA。与我们本书中描述的其他生成模型最；相似的ICA变种（Pham  et  al.  ，1992）训练了完全参数化的生成模型。；潜在因子  h  的先验p(  h  )，必须由用户提前给出并固定。接着模型确定
          關鍵詞：必须由用户提前给出并固定, 训练了完全参数化的生成模型, 潜在因子, 许多不同的具体方法被称为, 相似的
        - 摘要：这种方法的动机是，通过选择一个独立的p(  h  )，我们可以尽可能恢复；接近独立的潜在因子。这是一种常用的方法，它并不是用来捕捉高级别；的抽象因果因子，而是恢复已经混合在一起的低级别信号。在该设置
          關鍵詞：接近独立的潜在因子, 而是恢复已经混合在一起的低级别信号, 在该设置, 通过选择一个独立的, 的抽象因果因子
        - 摘要：如前所述，ICA存在许多变种。一些版本在  x  的生成中添加一些噪声，；而不是使用确定性的解码器。大多数方法不使用最大似然准则，而是旨
          關鍵詞：如前所述, 而是旨, 存在许多变种, 一些版本在, 的生成中添加一些噪声
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在使  h  ＝  W  -1  x  的元素彼此独立。许多准则能够达成这个目标。式；（3.47）需要用到  W  的行列式，这可能是代价很高且数值不稳定的操
          關鍵詞：这可能是代价很高且数值不稳定的操, 的元素彼此独立, 在使, 的行列式, 需要用到
        - 摘要：ICA的所有变种均要求p( h )是非高斯的。这是因为如果p( h )是具有高斯；分量的独立先验，则  W  是不可识别的。对于许多  W  值，我们可以在p(；x )上获得相同的分布。这与其他线性因子模型有很大的区别，例如概率
          關鍵詞：例如概率, 上获得相同的分布, 是具有高斯, 是非高斯的, 对于许多
        - 摘要：用
          關鍵詞：
        - 摘要：。这些非高斯分布的典型选择在
          關鍵詞：这些非高斯分布的典型选择在
        - 摘要：0附近具有比高斯分布更高的峰值，因此我们也可以看到独立成分分析；经常用于学习稀疏特征。
          關鍵詞：附近具有比高斯分布更高的峰值, 因此我们也可以看到独立成分分析, 经常用于学习稀疏特征
        - 摘要：按照我们对生成模型这个术语的定义，ICA的许多变种不是生成模型。；在本书中，生成模型可以直接表示p(  x  )，也可以认为是从p(  x  )中抽取；样本。ICA的许多变种仅知道如何在 x 和 h 之间变换，而没有任何表示
          關鍵詞：生成模型可以直接表示, 样本, 中抽取, 按照我们对生成模型这个术语的定义, 在本书中
        - 摘要：正如PCA可以推广到第14章中描述的非线性自编码器，ICA也可以推广；到非线性生成模型，其中我们使用非线性函数f来生成观测数据。关于；非线性ICA最初的工作可以参考Hyvärinen and Pajunen（1999），它和集
          關鍵詞：到非线性生成模型, 可以推广到第, 非线性, 它和集, 也可以推广
        - 摘要：ICA的另一个推广是通过鼓励组内统计依赖关系、抑制组间依赖关系来
          關鍵詞：抑制组间依赖关系来, 的另一个推广是通过鼓励组内统计依赖关系
        - 摘要：学习特征组（Hyvärinen and Hoyer，1999；Hyvärinen et al. ，2001b）。；当相关单元的组被选为不重叠时，这被称为独立子空间分析；（independent  subspace  analysis）。我们还可以向每个隐藏单元分配空
          關鍵詞：我们还可以向每个隐藏单元分配空, 当相关单元的组被选为不重叠时, 这被称为独立子空间分析, 学习特征组
        - 摘要：ICA）方法可以学习Gabor滤波器，从而使得相邻特征具
          關鍵詞：从而使得相邻特征具, 方法可以学习, 滤波器
    13.3：慢特征分析
        - 摘要：慢特征分析  （slow  feature  analysis，SFA）是使用来自时间信号的信息；学习不变特征的线性因子模型（Wiskott and Sejnowski，2002）。
          關鍵詞：慢特征分析, 是使用来自时间信号的信息, 学习不变特征的线性因子模型
        - 摘要：慢特征分析的想法源于所谓的慢性原则  （slowness  principle）。其基本；思想是，与场景中起描述作用的单个量度相比，场景的重要特性通常变；化得非常缓慢。例如，在计算机视觉中，单个像素值可以非常快速地改
          關鍵詞：单个像素值可以非常快速地改, 场景的重要特性通常变, 思想是, 例如, 其基本
        - 摘要：慢性原则早于慢特征分析，并已被应用于各种模型（Hinton，1989；；Földiák，1989；Mobahi et al. ，2009；Bergstra and Bengio，2009）。一；般来说，我们可以将慢性原则应用于可以使用梯度下降训练的任何可微
          關鍵詞：我们可以将慢性原则应用于可以使用梯度下降训练的任何可微, 并已被应用于各种模型, 般来说, 慢性原则早于慢特征分析
        - 摘要：其中λ是确定慢度正则化强度的超参数项，t是样本时间序列的索引，f是；需要正则化的特征提取器，L是测量f（ x  (t)  ）和f(  x  (t+1)  )之间的距离的；损失函数。L的一个常见选择是均方误差。
          關鍵詞：需要正则化的特征提取器, 之间的距离的, 的一个常见选择是均方误差, 其中, 是样本时间序列的索引
        - 摘要：慢特征分析是慢性原则中一个特别高效的应用。由于它被应用于线性特
          關鍵詞：慢特征分析是慢性原则中一个特别高效的应用, 由于它被应用于线性特
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；征提取器，并且可以通过闭式解训练，所以它是高效的。像ICA的一些；变种一样，SFA本身并不是生成模型，只是在输入空间和特征空间之间
          關鍵詞：的一些, 本身并不是生成模型, 并且可以通过闭式解训练, 变种一样, 只是在输入空间和特征空间之间
        - 摘要：SFA算法（Wiskott and Sejnowski，2002）先将f( x ;θ)定义为线性变换，；然后求解如下优化问题：
          關鍵詞：先将, 定义为线性变换, 算法, 然后求解如下优化问题
        - 摘要：并且满足下面的约束：
          關鍵詞：并且满足下面的约束
        - 摘要：以及
          關鍵詞：以及
        - 摘要：学习特征具有零均值的约束对于使问题具有唯一解是必要的，否则我们；可以向所有特征值添加一个常数，并获得具有相等慢度目标值的不同；解。特征具有单位方差的约束对于防止所有特征趋近于0的病态解是必
          關鍵詞：特征具有单位方差的约束对于防止所有特征趋近于, 否则我们, 学习特征具有零均值的约束对于使问题具有唯一解是必要的, 可以向所有特征值添加一个常数, 的病态解是必
        - 摘要：这要求学习的特征必须彼此线性去相关。没有这个约束，所有学习到的；特征将简单地捕获一个最慢的信号。可以想象使用其他机制，如最小化；重构误差，也可以迫使特征多样化。但是由于SFA特征的线性，这种去
          關鍵詞：但是由于, 所有学习到的, 特征将简单地捕获一个最慢的信号, 重构误差, 这要求学习的特征必须彼此线性去相关
        - 摘要：在运行SFA之前，SFA通常通过对  x  使用非线性的基扩充来学习非线性；特征。例如，通常用 x 的二次基扩充来代替原来的 x ，得到一个包含所；有x i x j 的向量。由此，我们可以通过反复地学习一个线性SFA特征提取
          關鍵詞：在运行, 的二次基扩充来代替原来的, 例如, 特征提取, 由此
        - 摘要：SFA特征提取器的方式来组合线性SFA模块，从而学习深度非线性慢特；征提取器。
          關鍵詞：模块, 特征提取器的方式来组合线性, 从而学习深度非线性慢特, 征提取器
        - 摘要：当在自然场景视频的小块空间部分上训练时，使用二次基扩展的SFA所；学习到的特征与V1皮层中那些复杂细胞的特征有许多共同特性（Berkes；and  Wiskott，2005）。当在计算机渲染的3D环境内随机运动的视频上训
          關鍵詞：当在计算机渲染的, 皮层中那些复杂细胞的特征有许多共同特性, 当在自然场景视频的小块空间部分上训练时, 学习到的特征与, 环境内随机运动的视频上训
        - 摘要：SFA的一个主要优点是，即使在深度非线性条件下，它依然能够在理论；上预测SFA能够学习哪些特征。为了做出这样的理论预测，必须知道关；于配置空间的环境动力（例如，在3D渲染环境中随机运动的例子中，
          關鍵詞：能够学习哪些特征, 必须知道关, 为了做出这样的理论预测, 例如, 于配置空间的环境动力
        - 摘要：深度SFA也已经被用于学习用在对象识别和姿态估计的特征（Franzius；et  al.  ，2008）。到目前为止，慢性原则尚未成为任何最先进应用的基；础。究竟是什么因素限制了其性能仍有待研究。我们推测，或许慢度先
          關鍵詞：究竟是什么因素限制了其性能仍有待研究, 也已经被用于学习用在对象识别和姿态估计的特征, 到目前为止, 或许慢度先, 慢性原则尚未成为任何最先进应用的基
    13.4：稀疏编码
        - 摘要：稀疏编码 （sparse coding）（Olshausen and Field，1996）是一个线性因；子模型，已作为一种无监督特征学习和特征提取机制得到了广泛研究。；严格来说，术语“稀疏编码”是指在该模型中推断  h  值的过程，而“稀疏
          關鍵詞：已作为一种无监督特征学习和特征提取机制得到了广泛研究, 是指在该模型中推断, 稀疏编码, 子模型, 术语
        - 摘要：像大多数其他线性因子模型一样，它使用了线性的解码器加上噪声的方
          關鍵詞：它使用了线性的解码器加上噪声的方, 像大多数其他线性因子模型一样
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；式获得一个  x  的重构，就像式（13.2）描述的一样。更具体地说，稀疏；编码模型通常假设线性因子有一个各向同性精度为β的高斯噪声：
          關鍵詞：编码模型通常假设线性因子有一个各向同性精度为, 式获得一个, 描述的一样, 就像式, 的重构
        - 摘要：分布p(  h  )通常选取为一个峰值很尖锐且接近0的分布（Olshausen  and；Field，1996）。常见的选择包括可分解的Laplace、Cauchy或者可分解；的Student-t分布。例如，以稀疏惩罚系数λ为参数的Laplace先验可以表
          關鍵詞：为参数的, 或者可分解, 例如, 先验可以表, 常见的选择包括可分解的
        - 摘要：相应地，Student-t先验分布可以表示为
          關鍵詞：相应地, 先验分布可以表示为
        - 摘要：使用最大似然的方法来训练稀疏编码模型是不可行的。相反，为了在给；定编码的情况下更好地重构数据，训练过程在编码数据和训练解码器之；间交替进行。稍后在第19.3节中，这种方法将被进一步证明为是解决最
          關鍵詞：定编码的情况下更好地重构数据, 稍后在第, 为了在给, 这种方法将被进一步证明为是解决最, 节中
        - 摘要：对于诸如PCA的模型，我们已经看到使用了预测  h 的参数化的编码器函；数，并且该函数仅包括乘以权重矩阵。稀疏编码中的编码器不是参数化；的编码器。相反，编码器是一个优化算法，在这个优化问题中，我们寻
          關鍵詞：编码器是一个优化算法, 稀疏编码中的编码器不是参数化, 在这个优化问题中, 的模型, 的参数化的编码器函
        - 摘要：结合式（13.13）和式（13.12），我们得到如下的优化问题：
          關鍵詞：结合式, 我们得到如下的优化问题, 和式
        - 摘要：其中，我们扔掉了与  h  无关的项，并除以一个正的缩放因子来简化表；达。
          關鍵詞：并除以一个正的缩放因子来简化表, 其中, 无关的项, 我们扔掉了与
        - 摘要：由于在  h  上施加L  1  范数，这个过程将产生稀疏的  h  ∗  （详见第7.1.2；节）。
          關鍵詞：这个过程将产生稀疏的, 上施加, 由于在, 范数, 详见第
        - 摘要：为了训练模型而不仅仅是进行推断，我们交替迭代关于  h  和  W  的最小；化过程。在这里，我们将β视为超参数。我们通常将其设置为1，因为它；在此优化问题的作用与λ类似，没有必要使用两个超参数。原则上，我
          關鍵詞：我们交替迭代关于, 在这里, 我们将, 因为它, 没有必要使用两个超参数
        - 摘要：不是所有的稀疏编码方法都显式地构建了一个p( h )和一个p( x  ｜  h  )。；通常我们只是对学习一个带有激活值的特征的字典感兴趣，当特征是由；这个推断过程提取时，这个激活值通常为0。
          關鍵詞：不是所有的稀疏编码方法都显式地构建了一个, 当特征是由, 通常我们只是对学习一个带有激活值的特征的字典感兴趣, 和一个, 这个激活值通常为
        - 摘要：如果我们从Laplace先验中采样 h ， h 的元素实际上为0是一个零概率事；件。生成模型本身并不稀疏，只有特征提取器是稀疏的。Goodfellow  et；al.  （2013f）描述了不同模型族中的近似推断，如尖峰和平板稀疏编码
          關鍵詞：如尖峰和平板稀疏编码, 先验中采样, 是一个零概率事, 如果我们从, 描述了不同模型族中的近似推断
        - 摘要：与非参数编码器结合的稀疏编码方法原则上可以比任何特定的参数化编；码器更好地最小化重构误差和对数先验的组合。另一个优点是编码器没；有泛化误差。参数化的编码器必须泛化地学习如何将 x 映射到 h 。对于
          關鍵詞：与非参数编码器结合的稀疏编码方法原则上可以比任何特定的参数化编, 另一个优点是编码器没, 有泛化误差, 参数化的编码器必须泛化地学习如何将, 对于
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；预测编码值时，基于优化的稀疏编码模型的编码过程中较小的泛化误差；可以得到更好的泛化能力。Coates  and  Ng（2011）证明了在对象识别任
          關鍵詞：可以得到更好的泛化能力, 基于优化的稀疏编码模型的编码过程中较小的泛化误差, 证明了在对象识别任, 预测编码值时
        - 摘要：al.
          關鍵詞：
        - 摘要：et
          關鍵詞：
        - 摘要：非参数编码器的主要缺点是在给定 x 的情况下需要大量的时间来计算  h；，因为非参数方法需要运行迭代算法。在第14章中讲到的参数化自编码；器方法仅使用固定数量的层，通常只有一层。另一个缺点是它不直接通
          關鍵詞：的情况下需要大量的时间来计算, 器方法仅使用固定数量的层, 另一个缺点是它不直接通, 章中讲到的参数化自编码, 在第
        - 摘要：像其他线性因子模型一样，稀疏编码经常产生糟糕的样本，如图13.2所；示。即使当模型能够很好地重构数据并为分类器提供有用的特征时，也；会发生这种情况。这种现象发生的原因是每个单独的特征可以很好地被
          關鍵詞：像其他线性因子模型一样, 即使当模型能够很好地重构数据并为分类器提供有用的特征时, 会发生这种情况, 如图, 稀疏编码经常产生糟糕的样本
        - 摘要：图13.2　尖峰和平板稀疏编码模型上在MNIST数据集训练的样例和权重。（左）这个模型中的；样本和训练样本相差很大。第一眼看来，我们可能认为模型拟合得很差。（右）这个模型的权；重向量已经学习到了如何表示笔迹，有时候还能写完整的数字。因此这个模型也学习到了有用
          關鍵詞：这个模型的权, 第一眼看来, 样本和训练样本相差很大, 重向量已经学习到了如何表示笔迹, 有时候还能写完整的数字
        - 摘要：Goodfellow et al. （2013f）允许转载
          關鍵詞：允许转载
    13.5：PCA的流形解释
        - 摘要：线性因子模型，包括PCA和因子分析，可以理解为学习一个流形；（Hinton  et  al.  ，1997）。我们可以将概率PCA定义为高概率的薄饼状；区域，即一个高斯分布，沿着某些轴非常窄，就像薄饼沿着其垂直轴非
          關鍵詞：区域, 和因子分析, 沿着某些轴非常窄, 我们可以将概率, 线性因子模型
        - 摘要：图13.3　平坦的高斯能够描述一个低维流形附近的概率密度。此图表示了“流形平面”上“馅饼”的；上半部分，并且这个平面穿过了馅饼的中心。正交于流形方向（指向平面外的箭头方向）的方；差非常小，可以被视作“噪声”，其他方向（平面内的箭头）的方差则很大，对应了“信号”以及
          關鍵詞：流形平面, 平坦的高斯能够描述一个低维流形附近的概率密度, 馅饼, 的方, 的方差则很大
        - 摘要：编码器表示为
          關鍵詞：编码器表示为
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；编码器计算h的低维表示。从自编码器的角度来看，解码器负责计算重；构：
          關鍵詞：解码器负责计算重, 编码器计算, 的低维表示, 从自编码器的角度来看
        - 摘要：能够最小化重构误差
          關鍵詞：能够最小化重构误差
        - 摘要：的线性编码器和解码器的选择对应着 V ＝ W ，；的列形成一组标准正交基，这组基生成的子空间与协方差矩阵 C
          關鍵詞：这组基生成的子空间与协方差矩阵, 的列形成一组标准正交基, 的线性编码器和解码器的选择对应着
        - 摘要：， W
          關鍵詞：
        - 摘要：的主特征向量所生成的子空间相同。在PCA中，  W  的列是按照对应特；征值（其全部是实数和非负数）幅度大小排序所对应的特征向量。
          關鍵詞：其全部是实数和非负数, 的列是按照对应特, 的主特征向量所生成的子空间相同, 征值, 幅度大小排序所对应的特征向量
        - 摘要：我们还可以发现 C 的特征值λ i 对应了 x 在特征向量 ν (i) 方向上的方差。；如果 x ∈；， h ∈  并且满足d＜D，则（给定上述的 µ ， b ， V
          關鍵詞：给定上述的, 如果, 我们还可以发现, 的特征值, 方向上的方差
        - 摘要：因此，如果协方差矩阵的秩为d，则特征值λ  d+1  到λ  D  都为0，并且重构；误差为0。
          關鍵詞：则特征值, 都为, 因此, 并且重构, 如果协方差矩阵的秩为
        - 摘要：此外，我们还可以证明上述解可以通过在给定正交矩阵  W  的情况下最；大化 h 元素的方差，而不是最小化重构误差来获得。
          關鍵詞：此外, 大化, 的情况下最, 而不是最小化重构误差来获得, 我们还可以证明上述解可以通过在给定正交矩阵
        - 摘要：从某种程度上说，线性因子模型是最简单的生成模型和学习数据表示的；最简单模型。许多模型如线性分类器和线性回归模型可以扩展到深度前；馈网络，而这些线性因子模型可以扩展到自编码器网络和深度概率模
          關鍵詞：馈网络, 线性因子模型是最简单的生成模型和学习数据表示的, 最简单模型, 许多模型如线性分类器和线性回归模型可以扩展到深度前, 而这些线性因子模型可以扩展到自编码器网络和深度概率模
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)  第3.8节讨论了不相关变量和独立变量之间的差异。
          關鍵詞：节讨论了不相关变量和独立变量之间的差异
第14章：自编码器
    13.5：PCA的流形解释
        - 摘要：自编码器  （autoencoder）是神经网络的一种，经过训练后能尝试将输；h  ，可以产生编码；入复制到输出。自编码器  内部有一个隐藏层
          關鍵詞：经过训练后能尝试将输, 内部有一个隐藏层, 是神经网络的一种, 自编码器, 可以产生编码
        - 摘要：现代自编码器将编码器和解码器的概念推而广之，将其中的确定函数推；广为随机映射p encoder ( h ｜ x )和p decoder ( x ｜ h )。
          關鍵詞：将其中的确定函数推, 广为随机映射, 现代自编码器将编码器和解码器的概念推而广之
        - 摘要：数十年间，自编码器的想法一直是神经网络历史景象的一部分；（LeCun，1987；Bourlard  and  Kamp，1988；Hinton  and  Zemel，；1994）。传统自编码器被用于降维或特征学习。近年来，自编码器与潜
          關鍵詞：数十年间, 自编码器的想法一直是神经网络历史景象的一部分, 近年来, 自编码器与潜, 传统自编码器被用于降维或特征学习
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图14.1　自编码器的一般结构，通过内部表示或编码 h 将输入 x 映射到输出（称为重构） r 。自；编码器具有两个组件：编码器f(将 x 映射到 h )和解码器g(将 h 映射到 r )
          關鍵詞：编码器, 将输入, 映射到输出, 映射到, 自编码器的一般结构
    14.1：欠完备自编码器
        - 摘要：将输入复制到输出听起来没什么用，但我们通常不关心解码器的输出。；相反，我们希望通过训练自编码器对输入进行复制而使 h 获得有用的特；性。
          關鍵詞：获得有用的特, 将输入复制到输出听起来没什么用, 但我们通常不关心解码器的输出, 我们希望通过训练自编码器对输入进行复制而使, 相反
        - 摘要：从自编码器获得有用特征的一种方法是限制 h 的维度比 x 小，这种编码；维度小于输入维度的自编码器称为欠完备  （undercomplete）自编码；器。学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。
          關鍵詞：自编码, 维度小于输入维度的自编码器称为欠完备, 这种编码, 从自编码器获得有用特征的一种方法是限制, 的维度比
        - 摘要：学习过程可以简单地描述为最小化一个损失函数
          關鍵詞：学习过程可以简单地描述为最小化一个损失函数
        - 摘要：其中L是一个损失函数，惩罚g(f( x ))与 x 的差异，如均方误差。
          關鍵詞：其中, 惩罚, 是一个损失函数, 如均方误差, 的差异
        - 摘要：当解码器是线性的且L是均方误差，欠完备的自编码器会学习出与PCA；相同的生成子空间。这种情况下，自编码器在训练来执行复制任务的同；时学到了训练数据的主元子空间。
          關鍵詞：是均方误差, 欠完备的自编码器会学习出与, 时学到了训练数据的主元子空间, 这种情况下, 相同的生成子空间
        - 摘要：因此，拥有非线性编码器函数f和非线性解码器函数g的自编码器能够学；习出更强大的PCA非线性推广。不幸的是，如果编码器和解码器被赋予；过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的
          關鍵詞：和非线性解码器函数, 因此, 过大的容量, 习出更强大的, 如果编码器和解码器被赋予
    14.2：正则自编码器
        - 摘要：14.2.1　稀疏自编码器
          關鍵詞：稀疏自编码器
        - 摘要：14.2.2　去噪自编码器
          關鍵詞：去噪自编码器
        - 摘要：14.2.3　惩罚导数作为正则
          關鍵詞：惩罚导数作为正则
        - 摘要：编码维数小于输入维数的欠完备自编码器可以学习数据分布最显著的特；征。我们已经知道，如果赋予这类自编码器过大的容量，它就不能学到；任何有用的信息。
          關鍵詞：如果赋予这类自编码器过大的容量, 编码维数小于输入维数的欠完备自编码器可以学习数据分布最显著的特, 我们已经知道, 它就不能学到, 任何有用的信息
        - 摘要：如果隐藏编码的维数允许与输入相等，或隐藏编码维数大于输入的过完；备  （overcomplete）情况下，会发生类似的问题。在这些情况下，即使；是线性编码器和线性解码器也可以学会将输入复制到输出，而学不到任
          關鍵詞：或隐藏编码维数大于输入的过完, 而学不到任, 即使, 是线性编码器和线性解码器也可以学会将输入复制到输出, 情况下
        - 摘要：理想情况下，根据要建模的数据分布的复杂性，选择合适的编码维数和；编码器、解码器容量，就可以成功训练任意架构的自编码器。正则自编；码器提供这样的能力。正则自编码器使用的损失函数可以鼓励模型学习
          關鍵詞：根据要建模的数据分布的复杂性, 正则自编码器使用的损失函数可以鼓励模型学习, 码器提供这样的能力, 解码器容量, 就可以成功训练任意架构的自编码器
        - 摘要：除了这里所描述的方法（正则化自编码器最自然的解释），几乎任何带；有潜变量并配有一个推断过程（计算给定输入的潜在表示）的生成模；型，都可以看作自编码器的一种特殊形式。强调与自编码器联系的两个
          關鍵詞：强调与自编码器联系的两个, 几乎任何带, 有潜变量并配有一个推断过程, 除了这里所描述的方法, 计算给定输入的潜在表示
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；入数据中有用的结构信息，并且也无须对模型进行正则化。这些编码显；然是有用的，因为这些模型被训练为近似训练数据的概率分布而不是将
          關鍵詞：因为这些模型被训练为近似训练数据的概率分布而不是将, 入数据中有用的结构信息, 然是有用的, 并且也无须对模型进行正则化, 这些编码显
        - 摘要：14.2.1　稀疏自编码器
          關鍵詞：稀疏自编码器
        - 摘要：稀疏自编码器简单地在训练时结合编码层的稀疏惩罚Ω(  h；差：
          關鍵詞：稀疏自编码器简单地在训练时结合编码层的稀疏惩罚
        - 摘要：)和重构误
          關鍵詞：和重构误
        - 摘要：其中g( h )是解码器的输出，通常 h 是编码器的输出，即 h ＝f( x )。
          關鍵詞：是编码器的输出, 通常, 其中, 是解码器的输出
        - 摘要：稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。稀疏正；则化的自编码器必须反映训练数据集的独特统计特征，而不是简单地充；当恒等函数。以这种方式训练，执行附带稀疏惩罚的复制任务可以得到
          關鍵詞：而不是简单地充, 执行附带稀疏惩罚的复制任务可以得到, 以便用于像分类这样的任务, 稀疏自编码器一般用来学习特征, 以这种方式训练
        - 摘要：我们可以简单地将惩罚项Ω(  h  )视为加到前馈网络的正则项，这个前馈；网络的主要任务是将输入复制到输出（无监督学习的目标），并尽可能；地根据这些稀疏特征执行一些监督学习任务（根据监督学习的目标）。
          關鍵詞：并尽可能, 根据监督学习的目标, 我们可以简单地将惩罚项, 视为加到前馈网络的正则项, 地根据这些稀疏特征执行一些监督学习任务
        - 摘要：我们可以认为整个稀疏自编码器框架是对带有潜变量的生成模型的近似；最大似然训练，而不将稀疏惩罚视为复制任务的正则化。假如我们有一；个带有可见变量 x 和潜变量 h 的模型，且具有明确的联合分布p model ( x
          關鍵詞：和潜变量, 的模型, 我们可以认为整个稀疏自编码器框架是对带有潜变量的生成模型的近似, 且具有明确的联合分布, 假如我们有一
        - 摘要：方式不同，之前指分布p( θ )在我们看到数据前就对模型参数的先验进行；编码。对数似然函数可分解为
          關鍵詞：编码, 方式不同, 在我们看到数据前就对模型参数的先验进行, 之前指分布, 对数似然函数可分解为
        - 摘要：我们可以认为自编码器使用一个高似然值 h 的点估计近似这个总和。这；类似于稀疏编码生成模型（第13.4节），但  h  是参数编码器的输出，而；不是从优化结果推断出的最可能的h。从这个角度来看，我们根据这个
          關鍵詞：不是从优化结果推断出的最可能的, 我们根据这个, 类似于稀疏编码生成模型, 是参数编码器的输出, 的点估计近似这个总和
        - 摘要：log p model ( h )项能被稀疏诱导。如Laplace先验，
          關鍵詞：先验, 项能被稀疏诱导
        - 摘要：对应于绝对值稀疏惩罚。将对数先验表示为绝对值惩罚，我们得到
          關鍵詞：将对数先验表示为绝对值惩罚, 对应于绝对值稀疏惩罚, 我们得到
        - 摘要：这里的常数项只跟λ有关。通常我们将λ视为超参数，因此可以丢弃不影；响参数学习的常数项。其他如Student-t先验也能诱导稀疏性。从稀疏性；导致p  model  (  h  )学习成近似最大似然的结果看，稀疏惩罚完全不是一个
          關鍵詞：稀疏惩罚完全不是一个, 因此可以丢弃不影, 其他如, 从稀疏性, 有关
        - 摘要：稀疏自编码器的早期工作（Ranzato  et  al.  ，2007a，2008）探讨了各种；形式的稀疏性，并提出了稀疏惩罚和log Z项（将最大似然应用到无向概
          關鍵詞：稀疏自编码器的早期工作, 探讨了各种, 将最大似然应用到无向概, 形式的稀疏性, 并提出了稀疏惩罚和
        - 摘要：率模型
          關鍵詞：率模型
        - 摘要：时产生）之间的联系。这个想
          關鍵詞：之间的联系, 时产生, 这个想
        - 摘要：法是最小化log Z防止概率模型处处具有高概率，同理强制稀疏可以防止
          關鍵詞：同理强制稀疏可以防止, 法是最小化, 防止概率模型处处具有高概率
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；自编码器处处具有低的重构误差。这种情况下，这种联系是对通用机制；的直观理解而不是数学上的对应。在数学上更容易解释稀疏惩罚对应于
          關鍵詞：这种情况下, 这种联系是对通用机制, 自编码器处处具有低的重构误差, 在数学上更容易解释稀疏惩罚对应于, 的直观理解而不是数学上的对应
        - 摘要：Glorot et  al. （2011b）提出了一种在稀疏（和去噪）自编码器的 h 中实；现真正为零的方式。该想法是使用整流线性单元产生编码层。基于将表；示真正推向零（如绝对值惩罚）的先验，可以间接控制表示中零的平均
          關鍵詞：中实, 示真正推向零, 该想法是使用整流线性单元产生编码层, 提出了一种在稀疏, 基于将表
        - 摘要：14.2.2　去噪自编码器
          關鍵詞：去噪自编码器
        - 摘要：除了向代价函数增加一个惩罚项，我们也可以通过改变重构误差项来获；得一个能学到有用信息的自编码器。
          關鍵詞：除了向代价函数增加一个惩罚项, 我们也可以通过改变重构误差项来获, 得一个能学到有用信息的自编码器
        - 摘要：传统的自编码器最小化以下目标
          關鍵詞：传统的自编码器最小化以下目标
        - 摘要：其中L是一个损失函数，惩罚g(f( x ))与 x 的差异，如它们彼此差异的L 2；范数。如果模型被赋予过大的容量，L仅仅使得g◦f学成一个恒等函数。
          關鍵詞：如果模型被赋予过大的容量, 其中, 惩罚, 仅仅使得, 是一个损失函数
        - 摘要：相反，去噪自编码器 （denoising autoencoder，DAE）最小化
          關鍵詞：最小化, 去噪自编码器, 相反
        - 摘要：其中   是被某种噪声损坏的  x  的副本。因此去噪自编码器必须撤销这；些损坏，而不是简单地复制输入。
          關鍵詞：是被某种噪声损坏的, 的副本, 其中, 些损坏, 因此去噪自编码器必须撤销这
        - 摘要：Alain and Bengio（2013）和Bengio et al. （2013c）指出去噪训练过程强；制f和g隐式地学习p  data  (  x  )的结构。因此，去噪自编码器也是一个通过；最小化重构误差获取有用特性的例子。这也是将过完备、高容量的模型
          關鍵詞：去噪自编码器也是一个通过, 因此, 这也是将过完备, 指出去噪训练过程强, 最小化重构误差获取有用特性的例子
        - 摘要：14.2.3　惩罚导数作为正则
          關鍵詞：惩罚导数作为正则
        - 摘要：另一正则化自编码器的策略是使用一个类似稀疏自编码器中的惩罚项；Ω，
          關鍵詞：另一正则化自编码器的策略是使用一个类似稀疏自编码器中的惩罚项
        - 摘要：但Ω的形式不同：
          關鍵詞：的形式不同
        - 摘要：这迫使模型学习一个在 x  变化小时目标也没有太大变化的函数。因为这；个惩罚只对训练数据适用，它迫使自编码器学习可以反映训练数据分布；信息的特征。
          關鍵詞：信息的特征, 它迫使自编码器学习可以反映训练数据分布, 因为这, 变化小时目标也没有太大变化的函数, 个惩罚只对训练数据适用
        - 摘要：这样正则化的自编码器被称为收缩自编码器 （contractive  autoencoder，；CAE）。这种方法与去噪自编码器、流形学习和概率模型存在一定理论；联系。收缩自编码器将在第14.7节更详细地描述。
          關鍵詞：这种方法与去噪自编码器, 这样正则化的自编码器被称为收缩自编码器, 节更详细地描述, 流形学习和概率模型存在一定理论, 联系
        - 摘要：14.2.1　稀疏自编码器
          關鍵詞：稀疏自编码器
        - 摘要：14.2.2　去噪自编码器
          關鍵詞：去噪自编码器
        - 摘要：14.2.3　惩罚导数作为正；则
          關鍵詞：惩罚导数作为正
    14.3：表示能力、层的大小和深度
        - 摘要：自编码器通常只有单层的编码器和解码器，但这不是必然的。实际上深；度编码器和解码器能提供更多优势。
          關鍵詞：但这不是必然的, 实际上深, 自编码器通常只有单层的编码器和解码器, 度编码器和解码器能提供更多优势
        - 摘要：回忆第6.4.1节，其中提到加深前馈网络有很多优势。这些优势也同样适；用于自编码器，因为它也属于前馈网络。此外，编码器和解码器各自都；是一个前馈网络，因此这两个部分也能各自从深度结构中获得好处。
          關鍵詞：因为它也属于前馈网络, 回忆第, 此外, 编码器和解码器各自都, 用于自编码器
        - 摘要：万能近似定理保证至少有一层隐藏层且隐藏单元足够多的前馈神经网络；能以任意精度近似任意函数（在很大范围里），这是非平凡深度（至少；有一层隐藏层）的一个主要优点。这意味着具有单隐藏层的自编码器在
          關鍵詞：在很大范围里, 这意味着具有单隐藏层的自编码器在, 有一层隐藏层, 的一个主要优点, 至少
        - 摘要：深度可以指数地降低表示某些函数的计算成本。深度也能指数地减少学；习一些函数所需的训练数据量。读者可以参考第6.4.1节巩固深度在前馈
          關鍵詞：深度也能指数地减少学, 习一些函数所需的训练数据量, 节巩固深度在前馈, 读者可以参考第, 深度可以指数地降低表示某些函数的计算成本
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；网络中的优势。
          關鍵詞：网络中的优势
        - 摘要：实验中，深度自编码器能比相应的浅层或线性自编码器产生更好的压缩；效率（Hinton and Salakhutdinov，2006）。
          關鍵詞：深度自编码器能比相应的浅层或线性自编码器产生更好的压缩, 实验中, 效率
        - 摘要：训练深度自编码器的普遍策略是训练一堆浅层的自编码器来贪心地预训；练相应的深度架构。所以即使最终目标是训练深度自编码器，我们也经；常会遇到浅层自编码器。
          關鍵詞：练相应的深度架构, 训练深度自编码器的普遍策略是训练一堆浅层的自编码器来贪心地预训, 常会遇到浅层自编码器, 我们也经, 所以即使最终目标是训练深度自编码器
    14.4：随机编码器和解码器
        - 摘要：自编码器本质上是一个前馈网络，可以使用与传统前馈网络相同的损失；函数和输出单元。
          關鍵詞：函数和输出单元, 可以使用与传统前馈网络相同的损失, 自编码器本质上是一个前馈网络
        - 摘要：如第6.2.2.4节中描述，设计前馈网络的输出单元和损失函数普遍策略是；定义一个输出分布p( y ｜ x  )并最小化负对数似然−log p( y ｜ x )。在这；种情况下， y 是关于目标的向量（如类标）。
          關鍵詞：节中描述, 在这, 定义一个输出分布, 如类标, 是关于目标的向量
        - 摘要：在自编码器中， x 既是输入也是目标。然而，我们仍然可以使用与之前；相同的架构。给定一个隐藏编码 h ，我们可以认为解码器提供了一个条；件分布p model ( x ｜ h )。接着我们根据最小化−log p decoder ( x ｜ h )来训
          關鍵詞：既是输入也是目标, 接着我们根据最小化, 来训, 在自编码器中, 相同的架构
        - 摘要：为了更彻底地与我们之前了解到的前馈网络相区别，我们也可以将编码；函数  （encoding  function）f(  x  )的概念推广为编码分布  （encoding；distribution）p encoder ( h ｜ x )，如图14.2所示。
          關鍵詞：为了更彻底地与我们之前了解到的前馈网络相区别, 函数, 所示, 的概念推广为编码分布, 如图
        - 摘要：任何潜变量模型p model ( h , x )定义一个随机编码器
          關鍵詞：定义一个随机编码器, 任何潜变量模型
        - 摘要：以及一个随机解码器
          關鍵詞：以及一个随机解码器
        - 摘要：图14.2　随机自编码器的结构，其中编码器和解码器包括一些噪声注入，而不是简单的函数。；这意味着可以将它们的输出视为来自分布的采样（对于编码器是p encoder ( h ｜ x )，对于解码器；是p decoder ( x ｜ h )）
          關鍵詞：对于解码器, 随机自编码器的结构, 而不是简单的函数, 对于编码器是, 其中编码器和解码器包括一些噪声注入
        - 摘要：通常情况下，编码器和解码器的分布没有必要是与唯一一个联合分布p；model ( x , h )相容的条件分布。Alain et al. （2015）指出，在保证足够的；容量和样本的情况下，将编码器和解码器作为去噪自编码器训练，能使
          關鍵詞：指出, 通常情况下, 容量和样本的情况下, 能使, 将编码器和解码器作为去噪自编码器训练
    14.5：去噪自编码器详解
        - 摘要：14.5.1　得分估计
          關鍵詞：得分估计
        - 摘要：14.5.2　历史展望
          關鍵詞：历史展望
        - 摘要：去噪自编码器  （denoising  autoencoder，DAE）是一类接受损坏数据作；为输入，并训练来预测原始未被损坏数据作为输出的自编码器。
          關鍵詞：并训练来预测原始未被损坏数据作为输出的自编码器, 为输入, 去噪自编码器, 是一类接受损坏数据作
        - 摘要：DAE的训练过程如图14.3所示。我们引入一个损坏过程；，这；个条件分布代表给定数据样本x  产生损坏样本   的概率。自编码器则
          關鍵詞：产生损坏样本, 我们引入一个损坏过程, 的训练过程如图, 的概率, 所示
        - 摘要：x
          關鍵詞：
        - 摘要：,
          關鍵詞：
        - 摘要：（1）从训练数据中采一个训练样本 x 。
          關鍵詞：从训练数据中采一个训练样本
        - 摘要：（2）从 C (
          關鍵詞：
        - 摘要：｜ x ＝ x )采一个损坏样本  。
          關鍵詞：采一个损坏样本
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；（3）将（ x ,；x ｜  )＝p decoder ( x ｜ h )，其中 h 是编码器f(
          關鍵詞：是编码器, 其中
        - 摘要：）作为训练样本来估计自编码器的重构分布p  reconstruct (；)的输出，p  decoder 根
          關鍵詞：的输出, 作为训练样本来估计自编码器的重构分布
        - 摘要：通常我们可以简单地对负对数似然−log p decoder ( x ｜ h )进行基于梯度法；（如小批量梯度下降）的近似最小化。只要编码器是确定性的，去噪自；编码器就是一个前馈网络，并且可以使用与其他前馈网络完全相同的方
          關鍵詞：如小批量梯度下降, 只要编码器是确定性的, 编码器就是一个前馈网络, 通常我们可以简单地对负对数似然, 的近似最小化
        - 摘要：图14.3　去噪自编码器代价函数的计算图。去噪自编码器被训练为从损坏的版本；))实现，其中；据点 x 。这可以通过最小化损失L＝−log p decoder ( x ｜ h ＝f(
          關鍵詞：实现, 其中, 去噪自编码器被训练为从损坏的版本, 据点, 这可以通过最小化损失
        - 摘要：｜ x )后得到的损坏版本。通常，分布p decoder 是因子的分布（平均参数由前馈网
          關鍵詞：通常, 是因子的分布, 平均参数由前馈网, 分布, 后得到的损坏版本
        - 摘要：重构干净数；是样本 x 经过损
          關鍵詞：是样本, 重构干净数, 经过损
        - 摘要：因此我们可以认为DAE是在以下期望下进行随机梯度下降：
          關鍵詞：是在以下期望下进行随机梯度下降, 因此我们可以认为
        - 摘要：其中
          關鍵詞：其中
        - 摘要：是训练数据的分布。
          關鍵詞：是训练数据的分布
        - 摘要：14.5.1　得分估计
          關鍵詞：得分估计
        - 摘要：得分匹配（Hyvärinen，2005a）是最大似然的代替。它提供了概率分布；的一致估计，促使模型在各个数据点  x  上获得与数据分布相同的得分；（score）。在这种情况下，得分是一个特定的梯度场：
          關鍵詞：它提供了概率分布, 的一致估计, 在这种情况下, 促使模型在各个数据点, 得分匹配
        - 摘要：我们将在第18.4节中更详细地讨论得分匹配。对于现在讨论的自编码；器，理解学习log p data 的梯度场是学习p data 结构的一种方式就足够了。
          關鍵詞：节中更详细地讨论得分匹配, 我们将在第, 理解学习, 的梯度场是学习, 对于现在讨论的自编码
        - 摘要：DAE的训练准则（条件高斯p(  x  ｜  h  )）能让自编码器学到能估计数据；分布得分的向量场（g(f( x ))− x ），这是DAE的一个重要特性，具体如；图14.4所示。
          關鍵詞：能让自编码器学到能估计数据, 具体如, 这是, 的训练准则, 分布得分的向量场
        - 摘要：图14.4　去噪自编码器被训练为将损坏的数据点；为位于低维流形（粗黑线）附近的红叉。我们用灰色圆圈表示等概率的损坏过程 C (；灰色箭头演示了如何将一个训练样本转换为经过此损坏过程的样本。当训练去噪自编码器最小
          關鍵詞：粗黑线, 灰色箭头演示了如何将一个训练样本转换为经过此损坏过程的样本, 去噪自编码器被训练为将损坏的数据点, 为位于低维流形, 我们用灰色圆圈表示等概率的损坏过程
        - 摘要：映射回原始数据点 x 。我们将训练样本 x 表示
          關鍵詞：表示, 我们将训练样本, 映射回原始数据点
        - 摘要：的平均值时，重构g(f(
          關鍵詞：的平均值时, 重构
        - 摘要：｜ x )。
          關鍵詞：
        - 摘要：))估计；。对可能产生
          關鍵詞：估计, 对可能产生
        - 摘要：的原始点 x 的质心进行估
          關鍵詞：的原始点, 的质心进行估
        - 摘要：计，所以向量g(f(
          關鍵詞：所以向量
        - 摘要：))−
          關鍵詞：
        - 摘要：近似指向流形上最近的点。因此自编码器可以学习由绿色箭头表示的
          關鍵詞：近似指向流形上最近的点, 因此自编码器可以学习由绿色箭头表示的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；向量场g(f( x ))− x 。该向量场将得分；方根的平均
          關鍵詞：方根的平均, 该向量场将得分, 向量场
        - 摘要：估计为一个乘性因子，即重构误差均
          關鍵詞：估计为一个乘性因子, 即重构误差均
        - 摘要：对一类采用高斯噪声和均方误差作为重构误差的特定去噪自编码器（具；有sigmoid隐藏单元和线性重构单元）的去噪训练过程，与训练一类特；定的被称为RBM的无向概率模型是等价的（Vincent，2011）。这类模
          關鍵詞：与训练一类特, 的无向概率模型是等价的, 这类模, 对一类采用高斯噪声和均方误差作为重构误差的特定去噪自编码器, 定的被称为
        - 摘要：自编码器和RBM还存在其他联系。在RBM上应用得分匹配后，其代价；函数将等价于重构误差结合类似CAE惩罚的正则项（Swersky  et  al.  ，；2011）。Bengio  and  Delalleau（2009）指出自编码器的梯度是对RBM对
          關鍵詞：函数将等价于重构误差结合类似, 还存在其他联系, 其代价, 自编码器和, 上应用得分匹配后
        - 摘要：对于连续的 x ，高斯损坏和重构分布的去噪准则得到的得分估计适用于；一般编码器和解码器的参数化（Alain and Bengio，2013）。这意味着一；个使用平方误差准则
          關鍵詞：一般编码器和解码器的参数化, 个使用平方误差准则, 对于连续的, 这意味着一, 高斯损坏和重构分布的去噪准则得到的得分估计适用于
        - 摘要：和噪声方差为σ 2 的损坏
          關鍵詞：的损坏, 和噪声方差为
        - 摘要：的通用编码器-解码器架构可以用来训练估计得分。图14.5展示了其中的；工作原理。
          關鍵詞：展示了其中的, 解码器架构可以用来训练估计得分, 的通用编码器, 工作原理
        - 摘要：图14.5　由去噪自编码器围绕一维弯曲流形学习的向量场，其中数据集中在二维空间中。每个；箭头与重构向量减去自编码器的输入向量后的向量成比例，并且根据隐式估计的概率分布指向；较高的概率。向量场在估计的密度函数的最大值处（在数据流形上）和密度函数的最小值处都
          關鍵詞：箭头与重构向量减去自编码器的输入向量后的向量成比例, 其中数据集中在二维空间中, 由去噪自编码器围绕一维弯曲流形学习的向量场, 在数据流形上, 并且根据隐式估计的概率分布指向
        - 摘要：一般情况下，不能保证重构函数g(f( x ))减去输入 x 后对应于某个函数的；梯度，更不用说得分。这是早期工作（Vincent，2011）专用于特定参数；x  能通过另一个函数的导数获得）。
          關鍵詞：减去输入, 梯度, 这是早期工作, 能通过另一个函数的导数获得, 专用于特定参数
        - 摘要：))−
          關鍵詞：
        - 摘要：x
          關鍵詞：
        - 摘要：目前为止我们所讨论的仅限于去噪自编码器如何学习表示一个概率分；布。更一般的，我们可能希望使用自编码器作为生成模型，并从其分布；中进行采样。这将在第20.11节中讨论。
          關鍵詞：更一般的, 目前为止我们所讨论的仅限于去噪自编码器如何学习表示一个概率分, 我们可能希望使用自编码器作为生成模型, 这将在第, 并从其分布
        - 摘要：14.5.2　历史展望
          關鍵詞：历史展望
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；et
          關鍵詞：
        - 摘要：采用MLP去噪的想法可以追溯到LeCun（1987）和Gallinari；al.；（1987）的工作。Behnke（2001）也曾使用循环网络对图像去噪。在某
          關鍵詞：采用, 在某, 去噪的想法可以追溯到, 也曾使用循环网络对图像去噪, 的工作
        - 摘要：在引入现代DAE之前，Inayoshi and Kurita（2005）探索了其中一些相同；的方法和目标。他们除了在监督目标的情况下最小化重构误差之外，还；在监督MLP的隐藏层注入噪声，通过引入重构误差和注入噪声提升泛化
          關鍵詞：探索了其中一些相同, 在引入现代, 在监督, 通过引入重构误差和注入噪声提升泛化, 他们除了在监督目标的情况下最小化重构误差之外
        - 摘要：14.5.1　得分估计
          關鍵詞：得分估计
        - 摘要：14.5.2　历史展望
          關鍵詞：历史展望
    14.6：使用自编码器学习流形
        - 摘要：如第5.11.3节描述，自编码器跟其他很多机器学习算法一样，也利用了；数据集中在一个低维流形或者一小组这样的流形的思想。其中一些机器；学习算法仅能学习到在流形上表现良好但给定不在流形上的输入会导致
          關鍵詞：节描述, 数据集中在一个低维流形或者一小组这样的流形的思想, 自编码器跟其他很多机器学习算法一样, 也利用了, 学习算法仅能学习到在流形上表现良好但给定不在流形上的输入会导致
        - 摘要：要了解自编码器如何做到这一点，我们必须介绍流形的一些重要特性。
          關鍵詞：要了解自编码器如何做到这一点, 我们必须介绍流形的一些重要特性
        - 摘要：流形的一个重要特征是切平面 （tangent plane）的集合。d维流形上的一；点  x  ，切平面由能张成流形上允许变动的局部方向的d维基向量给出。；如图14.6所示，这些局部方向决定了我们能如何微小地变动  x  而保持于
          關鍵詞：维基向量给出, 而保持于, 所示, 这些局部方向决定了我们能如何微小地变动, 维流形上的一
        - 摘要：所有自编码器的训练过程涉及两种推动力的折衷：
          關鍵詞：所有自编码器的训练过程涉及两种推动力的折衷
        - 摘要：（1）学习训练样本  x  的表示  h  使得  x  能通过解码器近似地从  h  中恢；复。 x 是从训练数据挑出的这一事实很关键，因为这意味着在自编码器；不需要成功重构不属于数据生成分布下的输入。
          關鍵詞：的表示, 学习训练样本, 能通过解码器近似地从, 中恢, 是从训练数据挑出的这一事实很关键
        - 摘要：（2）满足约束或正则惩罚。这既可以是限制自编码器容量的架构约；束，也可以是加入到重构代价的一个正则项。这些技术一般倾向那些对；输入较不敏感的解。
          關鍵詞：满足约束或正则惩罚, 输入较不敏感的解, 也可以是加入到重构代价的一个正则项, 这些技术一般倾向那些对, 这既可以是限制自编码器容量的架构约
        - 摘要：显然，单一的推动力是无用的——从它本身将输入复制到输出是无用；的，同样忽略输入也是没用的。相反，两种推动力结合是有用的，因为；它们驱使隐藏的表示能捕获有关数据分布结构的信息。重要的原则是，
          關鍵詞：显然, 两种推动力结合是有用的, 因为, 重要的原则是, 单一的推动力是无用的
        - 摘要：图14.7中一维的例子说明，我们可以通过构建对数据点周围的输入扰动；不敏感的重构函数，使得自编码器恢复流形结构。
          關鍵詞：我们可以通过构建对数据点周围的输入扰动, 使得自编码器恢复流形结构, 不敏感的重构函数, 中一维的例子说明
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图14.6　正切超平面概念的图示。我们在784维空间中创建了一维流形。我们使用一张784像素；的MNIST图像，并通过垂直平移来转换它。垂直平移的量定义沿着一维流形的坐标，轨迹为通
          關鍵詞：维空间中创建了一维流形, 正切超平面概念的图示, 像素, 图像, 轨迹为通
        - 摘要：图14.7　如果自编码器学习到对数据点附近的小扰动不变的重构函数，它就能捕获数据的流形；结构。这里，流形结构是0维流形的集合。虚线对角线表示重构的恒等函数目标。最佳重构函数；会在存在数据点的任意处穿过恒等函数。图底部的水平箭头表示在输入空间中基于箭头的 r ( x )
          關鍵詞：图底部的水平箭头表示在输入空间中基于箭头的, 流形结构是, 最佳重构函数, 如果自编码器学习到对数据点附近的小扰动不变的重构函数, 这里
        - 摘要：为了理解自编码器可用于流形学习的原因，我们可以将自编码器和其他；方法进行对比。学习表征流形最常见的是流形上（或附近）数据点的表；示  （representation）。对于特定的实例，这样的表示也被称为嵌入。它
          關鍵詞：学习表征流形最常见的是流形上, 我们可以将自编码器和其他, 或附近, 数据点的表, 方法进行对比
        - 摘要：流形学习大多专注于试图捕捉到这些流形的无监督学习过程。最初始的；学习非线性流形的机器学习研究专注基于最近邻图  （nearest  neighbor；graph）的非参数  （non-parametric）方法。该图中每个训练样例对应一
          關鍵詞：该图中每个训练样例对应一, 的非参数, 流形学习大多专注于试图捕捉到这些流形的无监督学习过程, 学习非线性流形的机器学习研究专注基于最近邻图, 最初始的
        - 摘要：al. ，1998b；Roweis and Saul，2000；Tenenbaum et al. ，2000；Brand，；2003b；Belkin；and  Grimes，2003；
          關鍵詞：
        - 摘要：and  Niyogi，2003a；Donoho
          關鍵詞：
        - 摘要：图14.8　非参数流形学习过程构建的最近邻图，其中节点表示训练样本，有向边指示最近邻关；系。因此，各种过程可以获得与图的邻域相关联的切平面以及将每个训练样本与实值向量位置；或 嵌入 （embedding）相关联的坐标系。我们可以通过插值将这种表示概括为新的样本。只要
          關鍵詞：其中节点表示训练样本, 相关联的坐标系, 因此, 嵌入, 有向边指示最近邻关
        - 摘要：全局坐标系则可以通过优化或求解线性系统获得。图14.9展示了如何通；过大量局部线性的类高斯样平铺（或“薄煎饼”，因为高斯块在切平面方；向是扁平的）得到一个流形。
          關鍵詞：过大量局部线性的类高斯样平铺, 展示了如何通, 向是扁平的, 薄煎饼, 因为高斯块在切平面方
        - 摘要：然而，Bengio  and  Monperrus（2005）指出了这些局部非参数方法应用；于流形学习的根本困难：如果流形不是很光滑（它们有许多波峰、波谷；和曲折），为覆盖其中的每一个变化，我们可能需要非常多的训练样
          關鍵詞：和曲折, 指出了这些局部非参数方法应用, 我们可能需要非常多的训练样, 然而, 它们有许多波峰
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；能具有非常复杂的结构，难以仅从局部插值捕获特征。考虑图14.6转换；所得的流形样例。如果我们只观察输入向量内的一个坐标x i ，当平移图
          關鍵詞：能具有非常复杂的结构, 难以仅从局部插值捕获特征, 所得的流形样例, 考虑图, 转换
        - 摘要：图14.9　如果每个位置处的切平面（见图14.6）是已知的，则它们可以平铺后形成全局坐标系或；密度函数。每个局部块可以被认为是局部欧几里德坐标系，或者是局部平面高斯或“薄饼”，在；与薄饼正交的方向上具有非常小的方差而在定义坐标系的方向上具有非常大的方差。这些高斯
          關鍵詞：是已知的, 这些高斯, 密度函数, 如果每个位置处的切平面, 薄饼
    14.7：收缩自编码器
        - 摘要：收缩自编码器（Rifai et  al. ，2011a，b）在编码 h ＝f( x )的基础上添加；了显式的正则项，鼓励f的导数尽可能小：
          關鍵詞：收缩自编码器, 了显式的正则项, 在编码, 的导数尽可能小, 的基础上添加
        - 摘要：惩罚项Ω(  h  )为平方Frobenius范数（元素平方之和），作用于与编码器
          關鍵詞：作用于与编码器, 惩罚项, 范数, 为平方, 元素平方之和
        - 摘要：的函数相关偏导数的的Jacobian矩阵。
          關鍵詞：的函数相关偏导数的的, 矩阵
        - 摘要：去噪自编码器和收缩自编码器之间存在一定联系：Alain；and；Bengio（2013）指出在小高斯噪声的限制下，当重构函数将  x  映射到  r
          關鍵詞：指出在小高斯噪声的限制下, 去噪自编码器和收缩自编码器之间存在一定联系, 当重构函数将, 映射到
        - 摘要：分类任务中，基于Jacobian的收缩惩罚预训练特征函数f(  x  )，将收缩惩；罚应用在f( x )而不是g(f(  x  ))可以产生最好的分类精度。如第14.5.1节所；讨论的，应用于f( x )的收缩惩罚与得分匹配也有紧密的联系。
          關鍵詞：应用于, 可以产生最好的分类精度, 讨论的, 的收缩惩罚与得分匹配也有紧密的联系, 而不是
        - 摘要：收缩  （contractive）源于CAE弯曲空间的方式。具体来说，由于CAE训；练为抵抗输入扰动，鼓励将输入点邻域映射到输出点处更小的邻域。我；们能认为这是将输入的邻域收缩到更小的输出邻域。
          關鍵詞：们能认为这是将输入的邻域收缩到更小的输出邻域, 弯曲空间的方式, 收缩, 鼓励将输入点邻域映射到输出点处更小的邻域, 由于
        - 摘要：说得更清楚一点，CAE只在局部收缩——一个训练样本 x 的所有扰动都；映射到f( x )的附近。全局来看，两个不同的点 x 和 x ＇会分别被映射到；远离原点的两个点f(  x  )和f(  x  ＇)。f扩展到数据流形的中间或远处是合
          關鍵詞：一个训练样本, 远离原点的两个点, 两个不同的点, 的所有扰动都, 只在局部收缩
        - 摘要：我们可以认为点  x  处的Jacobian矩阵  J  能将非线性编码器近似为线性算；子。这允许我们更形式地使用“收缩”这个词。在线性理论中，当  Jx  的；范数对于所有单位  x  都小于等于1时，  J  被称为收缩的。换句话说，如
          關鍵詞：换句话说, 收缩, 这个词, 在线性理论中, 这允许我们更形式地使用
        - 摘要：如第14.6节中描述，正则自编码器基于两种相反的推动力学习流形。在；CAE的情况下，这两种推动力是重构误差和收缩惩罚Ω(  h  )。单独的重；构误差鼓励CAE学习一个恒等函数。单独的收缩惩罚将鼓励CAE学习关
          關鍵詞：单独的重, 学习一个恒等函数, 单独的收缩惩罚将鼓励, 正则自编码器基于两种相反的推动力学习流形, 这两种推动力是重构误差和收缩惩罚
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；于  x  是恒定的特征。这两种推动力的的折衷产生导数
          關鍵詞：是恒定的特征, 这两种推动力的的折衷产生导数
        - 摘要：大多是
          關鍵詞：大多是
        - 摘要：微小的自编码器。只有少数隐藏单元，对应于一小部分输入数据的方；向，可能有显著的导数。
          關鍵詞：微小的自编码器, 对应于一小部分输入数据的方, 只有少数隐藏单元, 可能有显著的导数
        - 摘要：CAE的目标是学习数据的流形结构。使  Jx  很大的方向  x  ，会快速改变；h  ，因此很可能是近似流形切平面的方向。Rifai  et  al.  （2011a，b）的；实验显示训练CAE会导致  J  中大部分奇异值（幅值）比1小，因此是收
          關鍵詞：实验显示训练, 的目标是学习数据的流形结构, 会快速改变, 因此是收, 会导致
        - 摘要：图14.10　通过局部PCA和收缩自编码器估计的流形切向量的图示。流形的位置由来自CIFAR-10
          關鍵詞：流形的位置由来自, 通过局部, 和收缩自编码器估计的流形切向量的图示
        - 摘要：数据集中狗的输入图像定义。切向量通过输入到代码映射的Jacobian矩阵
          關鍵詞：数据集中狗的输入图像定义, 矩阵, 切向量通过输入到代码映射的
        - 摘要：的前导奇异向量
          關鍵詞：的前导奇异向量
        - 摘要：估计。虽然局部PCA和CAE都可以捕获局部切方向，但CAE能够从有限训练数据形成更准确的；估计，因为它利用了不同位置的参数共享（共享激活的隐藏单元子集）。CAE切方向通常对应；于物体的移动或改变部分（例如头或腿）。经Rifai et al. （2011c）许可转载此图
          關鍵詞：估计, 许可转载此图, 于物体的移动或改变部分, 共享激活的隐藏单元子集, 例如头或腿
        - 摘要：收缩自编码器正则化准则的一个实际问题是，尽管它在单一隐藏层的自；编码器情况下是容易计算的，但在更深的自编码器情况下会变得难以计；算。根据Rifai  et  al.  （2011a）的策略，分别训练一系列单层的自编码
          關鍵詞：收缩自编码器正则化准则的一个实际问题是, 根据, 的策略, 编码器情况下是容易计算的, 但在更深的自编码器情况下会变得难以计
        - 摘要：度自编码器自然也是收缩的。这个结果与联合训练深度模型完整架构；（带有关于Jacobian的惩罚项）获得的结果是不同的，但它抓住了许多；理想的定性特征。
          關鍵詞：的惩罚项, 带有关于, 这个结果与联合训练深度模型完整架构, 度自编码器自然也是收缩的, 但它抓住了许多
        - 摘要：另一个实际问题是，如果我们不对解码器强加一些约束，收缩惩罚可能；导致无用的结果。例如，编码器将输入乘一个小常数   ，解码器将编；码除以一个小常数  。随着  趋向于0，编码器会使收缩惩罚项Ω( h )
          關鍵詞：导致无用的结果, 如果我们不对解码器强加一些约束, 趋向于, 码除以一个小常数, 编码器将输入乘一个小常数
    14.8：预测稀疏分解
        - 摘要：预测稀疏分解 （predictive sparse decomposition，PSD）是稀疏编码和参；数化自编码器（Kavukcuoglu et al. ，2008）的混合模型。参数化编码器；被训练为能预测迭代推断的输出。PSD被应用于图片和视频中对象识别
          關鍵詞：的混合模型, 预测稀疏分解, 数化自编码器, 被训练为能预测迭代推断的输出, 被应用于图片和视频中对象识别
        - 摘要：就像稀疏编码，训练算法交替地相对 h 和模型的参数最小化上述目标。；相对 h 最小化较快，因为f( x )提供 h 的良好初始值以及损失函数将 h 约；束在f( x )附近。简单的梯度下降算法只需10步左右就能获得理想的 h 。
          關鍵詞：相对, 训练算法交替地相对, 最小化较快, 因为, 附近
        - 摘要：PSD所使用的训练程序不是先训练稀疏编码模型，然后训练f(  x  )来预测；稀疏编码的特征。PSD训练过程正则化解码器，使用f(  x  )可以推断出良；好编码的参数。
          關鍵詞：稀疏编码的特征, 所使用的训练程序不是先训练稀疏编码模型, 训练过程正则化解码器, 然后训练, 可以推断出良
        - 摘要：预测稀疏分解是学习近似推断 （learned appro x imate inference）的一个；例子。在第19.5节中，这个话题将会进一步展开。第19章中展示的工具；能让我们了解到，PSD能够被解释为通过最大化模型的对数似然下界训
          關鍵詞：章中展示的工具, 的一个, 能让我们了解到, 节中, 预测稀疏分解是学习近似推断
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在PSD的实际应用中，迭代优化仅在训练过程中使用。模型被部署后，；参数编码器f用于计算已经习得的特征。相比通过梯度下降推断  h  ，计
          關鍵詞：相比通过梯度下降推断, 的实际应用中, 模型被部署后, 迭代优化仅在训练过程中使用, 参数编码器
    14.9：自编码器的应用
        - 摘要：自编码器已成功应用于降维和信息检索任务。降维是表示学习和深度学；习的第一批应用之一。它是研究自编码器早期驱动力之一。例如，；Hinton  and  Salakhutdinov（2006）训练了一个栈式RBM，然后利用它们
          關鍵詞：习的第一批应用之一, 然后利用它们, 降维是表示学习和深度学, 例如, 它是研究自编码器早期驱动力之一
        - 摘要：低维表示可以提高许多任务的性能，例如分类。小空间的模型消耗更少；的内存和运行时间。据Salakhutdinov  and  Hinton（2007b）和Torralba  et；al.  （2008）观察，许多降维的形式会将语义上相关的样本置于彼此邻
          關鍵詞：低维表示可以提高许多任务的性能, 许多降维的形式会将语义上相关的样本置于彼此邻, 例如分类, 的内存和运行时间, 小空间的模型消耗更少
        - 摘要：相比普通任务，信息检索  （information  retrieval）从降维中获益更多，；此任务需要找到数据库中类似查询的条目。此任务不仅和其他任务一样；从降维中获得一般益处，还使某些低维空间中的搜索变得极为高效。特
          關鍵詞：从降维中获得一般益处, 相比普通任务, 还使某些低维空间中的搜索变得极为高效, 此任务需要找到数据库中类似查询的条目, 从降维中获益更多
        - 摘要：通常在最终层上使用sigmoid编码函数产生语义哈希的二值编码。；sigmoid单元必须被训练为到达饱和，对所有输入值都接近0或接近1。；能做到这一点的窍门就是训练时在sigmoid非线性单元前简单地注入加
          關鍵詞：能做到这一点的窍门就是训练时在, 单元必须被训练为到达饱和, 编码函数产生语义哈希的二值编码, 非线性单元前简单地注入加, 或接近
        - 摘要：学习哈希函数的思想已在其他多个方向进一步探讨，包括改变损失训练；表示的想法，其中所需优化的损失与哈希表中查找附近样本的任务有更；直接的联系（Norouzi and Fleet，2011）。
          關鍵詞：表示的想法, 学习哈希函数的思想已在其他多个方向进一步探讨, 直接的联系, 包括改变损失训练, 其中所需优化的损失与哈希表中查找附近样本的任务有更
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；第15章　表示学习
          關鍵詞：表示学习
    15.1：贪心逐层无监督预训练
        - 摘要：15.1.1　何时以及为何无；监督预训练有效有效
          關鍵詞：何时以及为何无, 监督预训练有效有效
第15章：表示学习
    14.9：自编码器的应用
        - 摘要：在本章中，首先我们会讨论表示学习是什么意思，以及表示的概念如何；有助于深度框架的设计。我们探讨学习算法如何在不同任务中共享统计；信息，包括使用无监督任务中的信息来完成监督任务。共享表示有助于
          關鍵詞：首先我们会讨论表示学习是什么意思, 有助于深度框架的设计, 信息, 包括使用无监督任务中的信息来完成监督任务, 共享表示有助于
        - 摘要：很多信息处理任务可能非常容易，也可能非常困难，这取决于信息是如；何表示的。这是一个广泛适用于日常生活、计算机科学及机器学习的基；本原则。例如，对于人而言，可以直接使用长除法计算210除以6。但如
          關鍵詞：可以直接使用长除法计算, 除以, 这取决于信息是如, 但如, 何表示的
        - 摘要：在机器学习中，到底是什么因素决定了一种表示比另一种表示更好呢？；一般而言，一个好的表示可以使后续的学习任务更容易。选择什么表示；通常取决于后续的学习任务。
          關鍵詞：一个好的表示可以使后续的学习任务更容易, 到底是什么因素决定了一种表示比另一种表示更好呢, 在机器学习中, 通常取决于后续的学习任务, 选择什么表示
        - 摘要：我们可以将监督学习训练的前馈网络视为表示学习的一种形式。具体；地，网络的最后一层通常是线性分类器，如softmax回归分类器。网络；的其余部分学习出该分类器的表示。监督学习训练模型，一般会使得模
          關鍵詞：网络, 具体, 监督学习训练模型, 的其余部分学习出该分类器的表示, 我们可以将监督学习训练的前馈网络视为表示学习的一种形式
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；最后一层的类型学习不同的性质。
          關鍵詞：最后一层的类型学习不同的性质
        - 摘要：前馈网络的监督训练并没有给学成的中间特征明确强加任何条件。其他；的表示学习算法往往会以某种特定的方式明确设计表示。例如，我们想；要学习一种使得密度估计更容易的表示。具有更多独立性的分布会更容
          關鍵詞：要学习一种使得密度估计更容易的表示, 我们想, 具有更多独立性的分布会更容, 前馈网络的监督训练并没有给学成的中间特征明确强加任何条件, 例如
        - 摘要：大多数表示学习算法都会在尽可能多地保留与输入相关的信息和追求良；好的性质（如独立性）之间作出权衡。
          關鍵詞：好的性质, 如独立性, 大多数表示学习算法都会在尽可能多地保留与输入相关的信息和追求良, 之间作出权衡
        - 摘要：表示学习特别有趣，因为它提供了进行无监督学习和半监督学习的一种；方法。我们通常会有巨量的未标注训练数据和相对较少的标注训练数；据。在非常有限的标注数据集上监督学习通常会导致严重的过拟合。半
          關鍵詞：因为它提供了进行无监督学习和半监督学习的一种, 在非常有限的标注数据集上监督学习通常会导致严重的过拟合, 我们通常会有巨量的未标注训练数据和相对较少的标注训练数, 表示学习特别有趣, 方法
        - 摘要：人类和动物能够从非常少的标注样本中学习。我们至今仍不知道这是如；何做到的。有许多假说解释人类的卓越学习能力——例如，大脑可能使；用了大量的分类器或者贝叶斯推断技术的集成。一种流行的假说是，大
          關鍵詞：有许多假说解释人类的卓越学习能力, 人类和动物能够从非常少的标注样本中学习, 例如, 何做到的, 我们至今仍不知道这是如
    15.1：贪心逐层无监督预训练
        - 摘要：15.1.1　何时以及为何无监督预训练有效有效
          關鍵詞：何时以及为何无监督预训练有效有效
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；15.2　迁移学习和领域自适应
          關鍵詞：迁移学习和领域自适应
        - 摘要：无监督学习在深度神经网络的复兴上起到了关键的、历史性的作用，它；使研究者首次可以训练不含诸如卷积或者循环这类特殊结构的深度监督；（unsupervised
          關鍵詞：无监督学习在深度神经网络的复兴上起到了关键的, 历史性的作用, 使研究者首次可以训练不含诸如卷积或者循环这类特殊结构的深度监督
        - 摘要：贪心逐层无监督预训练依赖于单层表示学习算法，例如RBM、单层自；编码器、稀疏编码模型或其他学习潜在表示的模型。每一层使用无监督；学习预训练，将前一层的输出作为输入，输出数据的新的表示。这个新
          關鍵詞：这个新, 贪心逐层无监督预训练依赖于单层表示学习算法, 例如, 学习预训练, 输出数据的新的表示
        - 摘要：算法15.1 贪心逐层无监督预训练的协定。
          關鍵詞：贪心逐层无监督预训练的协定, 算法
        - 摘要：给定如下：无监督特征学习算法；使用训练集样本并返回编码；器或特征函数f。原始输入数据是 X ，每行一个样本，并且f (1)  ( x )是第
          關鍵詞：原始输入数据是, 器或特征函数, 使用训练集样本并返回编码, 无监督特征学习算法, 给定如下
        - 摘要：目标 Y ），并返回细调好函数。阶段数为m。
          關鍵詞：目标, 阶段数为, 并返回细调好函数
        - 摘要：基于无监督标准的贪心逐层训练过程，早已被用来规避监督问题中深度；神经网络难以联合训练多层的问题。这种方法至少可以追溯神经认知机；（Fukushima，1975）。深度学习的复兴始于2006年，源于发现这种贪
          關鍵詞：源于发现这种贪, 这种方法至少可以追溯神经认知机, 早已被用来规避监督问题中深度, 基于无监督标准的贪心逐层训练过程, 神经网络难以联合训练多层的问题
        - 摘要：al.  ，2006b；Hinton
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；贪心逐层无监督预训练被称为贪心  （greedy）的，是因为它是一个贪心；算法  （greedy  algo-rithm），这意味着它独立地优化解决方案的每一个
          關鍵詞：这意味着它独立地优化解决方案的每一个, 贪心逐层无监督预训练被称为贪心, 算法, 是因为它是一个贪心
        - 摘要：通常而言，“预训练”不仅单指预训练阶段，也指结合预训练和监督学习；的两阶段学习过程。监督学习阶段可能会使用预训练阶段得到的顶层特；征训练一个简单分类器，或者可能会对预训练阶段得到的整个网络进行
          關鍵詞：或者可能会对预训练阶段得到的整个网络进行, 的两阶段学习过程, 征训练一个简单分类器, 不仅单指预训练阶段, 通常而言
        - 摘要：贪心逐层无监督预训练也能用作其他无监督学习算法的初始化，比如深；度自编码器（Hin-ton  and  Salakhutdinov，2006）和具有很多潜变量层的；概率模型。这些模型包括深度信念网络（Hinton et  al.  ，2006b）和深度
          關鍵詞：度自编码器, 贪心逐层无监督预训练也能用作其他无监督学习算法的初始化, 这些模型包括深度信念网络, 和深度, 和具有很多潜变量层的
        - 摘要：正如第8.7.4节所探讨的，我们也可以进行贪心逐层监督预训练。这是建；立在训练浅层模型比深度模型更容易的前提下，而该前提似乎在一些情；况下已被证实（Erhan et al. ，2010）。
          關鍵詞：节所探讨的, 正如第, 立在训练浅层模型比深度模型更容易的前提下, 况下已被证实, 我们也可以进行贪心逐层监督预训练
        - 摘要：15.1.1　何时以及为何无监督预训练有效有效
          關鍵詞：何时以及为何无监督预训练有效有效
        - 摘要：在很多分类任务中，贪心逐层无监督预训练能够在测试误差上获得重大；提升。这一观察结果始于2006年对深度神经网络的重新关注（Hinton  et；al. ，2006b；Bengio et al. ，2007d；Ranzato et al. ，2007a）。然而，在
          關鍵詞：然而, 这一观察结果始于, 提升, 在很多分类任务中, 贪心逐层无监督预训练能够在测试误差上获得重大
        - 摘要：测上的影响。结果发现，平均而言预训练是有轻微负面影响的，但在有；些问题上会有显著帮助。由于无监督预训练有时有效，但经常也会带来；负面效果，因此很有必要了解它何时有效以及有效的原因，以确定它是
          關鍵詞：但在有, 测上的影响, 以确定它是, 结果发现, 些问题上会有显著帮助
        - 摘要：首先，要注意的是这个讨论大部分都是针对贪心无监督预训练而言。还；有很多其他完全不同的方法使用半监督学习来训练神经网络，比如第；7.13节介绍的虚拟对抗训练。我们还可以在训练监督模型的同时训练自
          關鍵詞：要注意的是这个讨论大部分都是针对贪心无监督预训练而言, 有很多其他完全不同的方法使用半监督学习来训练神经网络, 我们还可以在训练监督模型的同时训练自, 比如第, 首先
        - 摘要：无监督预训练结合了两种不同的想法。第一，它利用了深度神经网络对；初始参数的选择，可以对模型有着显著的正则化效果（在较小程度上，；可以改进优化）的想法。第二，它利用了更一般的想法——学习输入分
          關鍵詞：可以对模型有着显著的正则化效果, 第一, 在较小程度上, 它利用了更一般的想法, 可以改进优化
        - 摘要：这两个想法都涉及机器学习算法中多个未能完全理解的部分之间复杂的；相互作用。
          關鍵詞：这两个想法都涉及机器学习算法中多个未能完全理解的部分之间复杂的, 相互作用
        - 摘要：第一个想法，即深度神经网络初始参数的选择对其性能具有很强的正则；化效果，很少有关于这个想法的理解。在预训练变得流行时，在一个位；置初始化模型被认为会使其接近某一个局部极小点，而不是另一个局部
          關鍵詞：在预训练变得流行时, 而不是另一个局部, 即深度神经网络初始参数的选择对其性能具有很强的正则, 很少有关于这个想法的理解, 第一个想法
        - 摘要：另一个想法有更好的理解，即学习算法可以使用无监督阶段学习的信
          關鍵詞：另一个想法有更好的理解, 即学习算法可以使用无监督阶段学习的信
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；息，在监督学习的阶段表现得更好。其基本想法是，对于无监督任务有；用的一些特征对于监督学习任务也可能是有用的。例如，如果我们训练
          關鍵詞：用的一些特征对于监督学习任务也可能是有用的, 如果我们训练, 例如, 在监督学习的阶段表现得更好, 对于无监督任务有
        - 摘要：从无监督预训练作为学习一个表示的角度来看，我们可以期望无监督预；训练在初始表示较差的情况下更有效。一个重要的例子是词嵌入。使用；one-hot向量表示的词并不具有很多信息，因为任意两个不同的one-hot向
          關鍵詞：训练在初始表示较差的情况下更有效, 向量表示的词并不具有很多信息, 因为任意两个不同的, 从无监督预训练作为学习一个表示的角度来看, 一个重要的例子是词嵌入
        - 摘要：从无监督预训练作为正则化项的角度来看，我们可以期望无监督预训练；在标注样本数量非常小时很有帮助。因为无监督预训练添加的信息来源；于未标注数据，所以当未标注样本的数量非常大时，我们也可以期望无
          關鍵詞：从无监督预训练作为正则化项的角度来看, 因为无监督预训练添加的信息来源, 我们可以期望无监督预训练, 于未标注数据, 所以当未标注样本的数量非常大时
        - 摘要：还可能涉及一些其他的因素。例如，当我们要学习的函数非常复杂时，；无监督预训练可能会非常有用。无监督学习不同于权重衰减这样的正则；化项，它不偏向于学习一个简单的函数，而是学习对无监督学习任务有
          關鍵詞：无监督预训练可能会非常有用, 无监督学习不同于权重衰减这样的正则, 例如, 还可能涉及一些其他的因素, 当我们要学习的函数非常复杂时
        - 摘要：除了这些注意事项外，我们现在分析一些无监督预训练改善性能的成功；示例，并解释这种改进发生的已知原因。无监督预训练通常用来改进分；类器，并且从减少测试集误差的观点来看是很有意思的。然而，无监督
          關鍵詞：并且从减少测试集误差的观点来看是很有意思的, 无监督, 然而, 除了这些注意事项外, 无监督预训练通常用来改进分
        - 摘要：Erhan et  al. （2010）进行了许多实验来解释无监督预训练的几个成功原；因。对训练误差和测试误差的改进都可以解释为，无监督预训练将参数；引入到了其他方法可能探索不到的区域。神经网络训练是非确定性的，
          關鍵詞：进行了许多实验来解释无监督预训练的几个成功原, 无监督预训练将参数, 对训练误差和测试误差的改进都可以解释为, 神经网络训练是非确定性的, 引入到了其他方法可能探索不到的区域
        - 摘要：图15.1　在函数空间（并非参数空间，避免从参数向量到函数的多对一映射）不同神经网络学；习轨迹的非线性映射的可视化。不同网络采用不同的随机初始化，并且有的使用了无监督预训；练，有的没有。每个点对应着训练过程中一个特定时间的神经网络。经Erhan et al. （2010）许
          關鍵詞：避免从参数向量到函数的多对一映射, 不同网络采用不同的随机初始化, 习轨迹的非线性映射的可视化, 并且有的使用了无监督预训, 不同神经网络学
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Isomap（Tenenbaum et al. ，2000）进行进一步的非线性投影并投到二维空间。颜色表示时间。；所有的网络初始化在图15.1的中心点附近（对应的函数区域在不多数输入上具有近似均匀分布
          關鍵詞：进行进一步的非线性投影并投到二维空间, 所有的网络初始化在图, 的中心点附近, 颜色表示时间, 对应的函数区域在不多数输入上具有近似均匀分布
        - 摘要：Erhan et  al. （2010）也回答了何时预训练效果最好——预训练的网络越；深，测试误差的均值和方差下降得越多。值得注意的是，这些实验是在；训练非常深层网络的现代方法发明和流行（整流线性单元、Dropout和
          關鍵詞：训练非常深层网络的现代方法发明和流行, 预训练的网络越, 测试误差的均值和方差下降得越多, 这些实验是在, 值得注意的是
        - 摘要：一个重要的问题是无监督预训练是如何起到正则化项作用的。一个假设；是，预训练鼓励学习算法发现那些与生成观察数据的潜在原因相关的特；征。这也是启发除无监督预训练之外许多其他算法的重要思想，将会在
          關鍵詞：将会在, 预训练鼓励学习算法发现那些与生成观察数据的潜在原因相关的特, 一个假设, 这也是启发除无监督预训练之外许多其他算法的重要思想, 一个重要的问题是无监督预训练是如何起到正则化项作用的
        - 摘要：与无监督学习的其他形式相比，无监督预训练的缺点是其使用了两个单；独的训练阶段。很多正则化技术都具有一个优点，允许用户通过调整单；一超参数的值来控制正则化的强度。无监督预训练没有一种明确的方法
          關鍵詞：无监督预训练的缺点是其使用了两个单, 一超参数的值来控制正则化的强度, 无监督预训练没有一种明确的方法, 允许用户通过调整单, 很多正则化技术都具有一个优点
        - 摘要：具有两个单独的训练阶段的另一个缺点是每个阶段都具有各自的超参；数。第二阶段的性能通常不能在第一阶段期间预测，因此在第一阶段提；出超参数和第二阶段根据反馈来更新之间存在较长的延迟。最通用的方
          關鍵詞：最通用的方, 具有两个单独的训练阶段的另一个缺点是每个阶段都具有各自的超参, 因此在第一阶段提, 出超参数和第二阶段根据反馈来更新之间存在较长的延迟, 第二阶段的性能通常不能在第一阶段期间预测
        - 摘要：如今，大部分算法已经不使用无监督预训练了，除了在自然语言处理领；域中单词作为one-hot向量的自然表示不能传达相似性信息，并且有非常；多的未标注数据集可用。在这种情况下，预训练的优点是可以对一个巨
          關鍵詞：向量的自然表示不能传达相似性信息, 大部分算法已经不使用无监督预训练了, 除了在自然语言处理领, 多的未标注数据集可用, 在这种情况下
        - 摘要：基于监督学习的深度学习技术，通过Dropout或批标准化来正则化，能；够在很多任务上达到人类级别的性能，但仅仅是在极大的标注数据集；上。在中等大小的数据集（例如CIFAR-10和MNIST，每个类大约有
          關鍵詞：够在很多任务上达到人类级别的性能, 例如, 在中等大小的数据集, 但仅仅是在极大的标注数据集, 每个类大约有
    15.2：迁移学习和领域自适应
        - 摘要：迁移学习和领域自适应指的是利用一个情景（例如，分布P  1  ）中已经；学到的内容去改善另一个情景（比如分布P  2  ）中的泛化情况。这点概；括了上一节提出的想法，即在无监督学习任务和监督学习任务之间转移
          關鍵詞：括了上一节提出的想法, 学到的内容去改善另一个情景, 比如分布, 中的泛化情况, 这点概
        - 摘要：在迁移学习 （transfer  learning）中，学习器必须执行两个或更多个不同；的任务，但是我们假设能够解释P  1  变化的许多因素和学习P  2  需要抓住；的变化相关。这通常能够在监督学习中解释，输入是相同的，但是输出
          關鍵詞：但是输出, 学习器必须执行两个或更多个不同, 的任务, 输入是相同的, 但是我们假设能够解释
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；如猫和狗，然后在第二种情景中学习一组不同的视觉类别，比如蚂蚁和；黄蜂。如果第一种情景（从P  1  采样）中具有非常多的数据，那么这有
          關鍵詞：如果第一种情景, 如猫和狗, 那么这有, 采样, 黄蜂
        - 摘要：然而，有时不同任务之间共享的不是输入的语义，而是输出的语义。例；如，语音识别系统需要在输出层产生有效的句子，但是输入附近的较低；层可能需要识别相同音素或子音素发音的非常不同的版本（这取决于说
          關鍵詞：语音识别系统需要在输出层产生有效的句子, 但是输入附近的较低, 有时不同任务之间共享的不是输入的语义, 然而, 而是输出的语义
        - 摘要：图15.2　多任务学习或者迁移学习的架构示例。输出变量 y 在所有的任务上具有相同的语义，输；入变量 x 在每个任务（或者，比如每个用户）上具有不同的意义（甚至可能具有不同的维；度）。图上3个任务为 x (1) 、 x (2) 、 x （3） 。底层结构（决定了选择方向）是面向任务的，上
          關鍵詞：决定了选择方向, 多任务学习或者迁移学习的架构示例, 上具有不同的意义, 比如每个用户, 在每个任务
        - 摘要：层结构是共享的。底层结构学习将面向特定任务的输入转化为通用特征
          關鍵詞：层结构是共享的, 底层结构学习将面向特定任务的输入转化为通用特征
        - 摘要：在领域自适应 （domain adaption）的相关情况下，在每个情景之间任务；（和最优的输入到输出的映射）都是相同的，但是输入分布稍有不同。；例如，考虑情感分析的任务，如判断一条评论是表达积极的还是消极的
          關鍵詞：如判断一条评论是表达积极的还是消极的, 和最优的输入到输出的映射, 例如, 在领域自适应, 但是输入分布稍有不同
        - 摘要：一个相关的问题是概念漂移  （concept  drift），我们可以将其视为一种；迁移学习，因为数据分布随时间而逐渐变化。概念漂移和迁移学习都可；以被视为多任务学习的特定形式。“多任务学习”这个术语通常指监督学
          關鍵詞：概念漂移和迁移学习都可, 我们可以将其视为一种, 因为数据分布随时间而逐渐变化, 迁移学习, 以被视为多任务学习的特定形式
        - 摘要：在所有这些情况下，我们的目标是利用第一个情景下的数据，提取那些；在第二种情景中学习时或直接进行预测时可能有用的信息。表示学习的；核心思想是相同的表示可能在两种情景中都是有用的。两个情景使用相
          關鍵詞：在第二种情景中学习时或直接进行预测时可能有用的信息, 核心思想是相同的表示可能在两种情景中都是有用的, 表示学习的, 在所有这些情况下, 两个情景使用相
        - 摘要：如前所述，迁移学习中无监督深度学习已经在一些机器学习比赛中取得；了成功（Mesnil et al. ，2011；Goodfellow et al.  ，2011）。这些比赛中；的某一个实验配置如下。首先每个参与者获得一个第一种情景（来自分
          關鍵詞：如前所述, 这些比赛中, 来自分, 了成功, 的某一个实验配置如下
        - 摘要：迁移学习的两种极端形式是一次学习  （one-shot  learning）和零次学习
          關鍵詞：和零次学习, 迁移学习的两种极端形式是一次学习
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；（zero-shot；learning）。只有一个标注样本的迁移任务被称为一次学习；没有标注
          關鍵詞：没有标注, 只有一个标注样本的迁移任务被称为一次学习
        - 摘要：learning），有时也被称为零数据学习
          關鍵詞：有时也被称为零数据学习
        - 摘要：（zero-data
          關鍵詞：
        - 摘要：因为第一阶段学习出的表示就可以清楚地分离出潜在的类别，所以一次；学习（Fei-Fei  et  al.  ，2006）是可能的。在迁移学习阶段，仅需要一个；标注样本来推断表示空间中聚集在相同点周围许多可能测试样本的标
          關鍵詞：标注样本来推断表示空间中聚集在相同点周围许多可能测试样本的标, 在迁移学习阶段, 是可能的, 所以一次, 学习
        - 摘要：考虑一个零次学习情景的例子，学习器已经读取了大量文本，然后要解；决对象识别的问题。如果文本足够好地描述了对象，那么即使没有看到；某对象的图像，也能识别出该对象的类别。例如，已知猫有4条腿和尖
          關鍵詞：已知猫有, 那么即使没有看到, 学习器已经读取了大量文本, 某对象的图像, 也能识别出该对象的类别
        - 摘要：只有在训练时使用了额外信息，零数据学习（Larochelle et  al.  ，2008）；和零次学习（Palatucci et al. ，2009；Socher et al. ，2013b）才是有可能；的。我们可以认为零数据学习场景包含3个随机变量：传统输入  x  ，传
          關鍵詞：只有在训练时使用了额外信息, 才是有可能, 零数据学习, 和零次学习, 我们可以认为零数据学习场景包含
        - 摘要：零次学习要求T被表示为某种形式的泛化。例如，T不能仅是指示对象；类别的one-hot编码。通过使用每个类别词的词嵌入表示，Socher  et  al.；（2013b）提出了对象类别的分布式表示。
          關鍵詞：类别的, 提出了对象类别的分布式表示, 被表示为某种形式的泛化, 编码, 通过使用每个类别词的词嵌入表示
        - 摘要：我们还可以在机器翻译中发现一种类似的现象（Klementiev  et  al.  ，；2012；Mikolov et al. ，2013b；Gouws et al. ，2014）：我们已经知道一；种语言中的单词，还可以学到单一语言语料库中词与词之间的关系；另
          關鍵詞：我们还可以在机器翻译中发现一种类似的现象, 还可以学到单一语言语料库中词与词之间的关系, 我们已经知道一, 种语言中的单词
        - 摘要：的句子。即使我们可能没有将语言X中的单词A翻译成语言Y中的单词B；的标注样本，我们也可以泛化并猜出单词A的翻译，这是由于我们已经；学习了语言X和Y单词的分布式表示，并且通过两种语言句子的匹配对
          關鍵詞：翻译成语言, 的翻译, 我们也可以泛化并猜出单词, 学习了语言, 这是由于我们已经
        - 摘要：零次学习是迁移学习的一种特殊形式。同样的原理可以解释如何能执行；多模态学习  （multimodal  learning），学习两种模态的表示，和一种模；态中的观察结果 x 与另一种模态中的观察结果y组成的对（ x  ，  y  ）之
          關鍵詞：学习两种模态的表示, 组成的对, 与另一种模态中的观察结果, 和一种模, 多模态学习
        - 摘要：and
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图15.3　两个域 x 和 y 之间的迁移学习能够进行零次学习。标注或未标注样本 x 可以学习表示函；数 f x 。同样地，样本 y 也可以学习表示函数 f y 。图中 f x 和 f y 旁都有一个向上的箭头，不同的
          關鍵詞：可以学习表示函, 也可以学习表示函数, 样本, 同样地, 之间的迁移学习能够进行零次学习
    15.3：半监督解释因果关系
        - 摘要：表示学习的一个重要问题是“什么原因能够使一个表示比另一个表示更；好？”一种假设是，理想表示中的特征对应到观测数据的潜在成因，特；征空间中不同的特征或方向对应着不同的原因，从而表示能够区分这些
          關鍵詞：从而表示能够区分这些, 什么原因能够使一个表示比另一个表示更, 理想表示中的特征对应到观测数据的潜在成因, 征空间中不同的特征或方向对应着不同的原因, 一种假设是
        - 摘要：et
          關鍵詞：
        - 摘要：在表示学习的其他方法中，我们大多关注易于建模的表示——例如，数；据稀疏或是各项之间相互独立的情况。能够清楚地分离出潜在因素的表；示可能并不一定易于建模。然而，该假设促使半监督学习使用无监督表
          關鍵詞：在表示学习的其他方法中, 我们大多关注易于建模的表示, 据稀疏或是各项之间相互独立的情况, 例如, 然而
        - 摘要：首先，让我们看看p(x  )的无监督学习无助于学习p(y  ｜x  )时，半监督学；习为何失败。例如，考虑一种情况，p(x  )是均匀分布的，我们希望学习
          關鍵詞：的无监督学习无助于学习, 我们希望学习, 考虑一种情况, 例如, 半监督学
        - 摘要：p(y ｜x )的任何信息。
          關鍵詞：的任何信息
        - 摘要：。显然，仅仅观察训练集的值  x  不能给我们关于
          關鍵詞：显然, 不能给我们关于, 仅仅观察训练集的值
        - 摘要：接下来，让我们看看半监督学习成功的一个简单例子。考虑这样的情；况，x  来自一个混合分布，每个y  值具有一个混合分量，如图15.4所；示。如果混合分量很好地分出来了，那么建模p(x  )可以精确地指出每个
          關鍵詞：接下来, 可以精确地指出每个, 考虑这样的情, 值具有一个混合分量, 让我们看看半监督学习成功的一个简单例子
        - 摘要：图15.4　混合模型。具有3个混合分量的 x 上混合密度示例。混合分量的内在本质是潜在解释因；子 y 。因为混合分量（例如，图像数据中的自然对象类别）在统计学上是显著的，所以仅仅使；用未标注样本无监督建模p( x )也能揭示解释因子 y
          關鍵詞：图像数据中的自然对象类别, 所以仅仅使, 个混合分量的, 混合模型, 混合分量的内在本质是潜在解释因
        - 摘要：如果y 与x 的成因之一非常相关，那么p(x )和p(y ｜x )也会紧密关联，试；图找到变化潜在因素的无监督表示学习可能像半监督学习一样有用。
          關鍵詞：如果, 图找到变化潜在因素的无监督表示学习可能像半监督学习一样有用, 也会紧密关联, 那么, 的成因之一非常相关
        - 摘要：假设y  是x  的成因之一，让h  代表所有这些成因。真实的生成过程可以；被认为是根据这个有向图模型结构化出来的，其中h 是x 的父节点：
          關鍵詞：真实的生成过程可以, 代表所有这些成因, 其中, 的父节点, 被认为是根据这个有向图模型结构化出来的
        - 摘要：因此，数据的边缘概率是
          關鍵詞：数据的边缘概率是, 因此
        - 摘要：从这个直观的观察中，我们得出结论，x  最好可能的模型（从广义的观
          關鍵詞：最好可能的模型, 我们得出结论, 从这个直观的观察中, 从广义的观
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；点）是会表示上述“真实”结构的，其中 h 作为潜变量解释 x 中可观察的；变化。上文讨论的“理想”的表示学习应该能够反映出这些潜在因子。如
          關鍵詞：结构的, 的表示学习应该能够反映出这些潜在因子, 真实, 其中, 理想
        - 摘要：因此边缘概率p(x  )和条件概率p(y  ｜x  )密切相关，前者的结构信息应该；有助于学习后者。因此，在这些假设情况下，半监督学习应该能提高性；能。
          關鍵詞：密切相关, 前者的结构信息应该, 因此边缘概率, 因此, 半监督学习应该能提高性
        - 摘要：关于这个事实的一个重要的研究问题是，大多数观察是由极其大量的潜；在成因形成的。假设y ＝h i ，但是无监督学习器并不知道是哪一个h i 。；对于一个无监督学习器暴力求解就是学习一种表示，这种表示能够捕获
          關鍵詞：大多数观察是由极其大量的潜, 但是无监督学习器并不知道是哪一个, 对于一个无监督学习器暴力求解就是学习一种表示, 这种表示能够捕获, 假设
        - 摘要：在实践中，暴力求解是不可行的，因为不可能捕获影响观察的所有或大；多数变化因素。例如，在视觉场景中，表示是否应该对背景中的所有最；小对象进行编码？根据一个有据可查的心理学现象，人们不会察觉到环
          關鍵詞：因为不可能捕获影响观察的所有或大, 人们不会察觉到环, 在视觉场景中, 多数变化因素, 暴力求解是不可行的
        - 摘要：无监督学习的另一个思路是选择一个更好的确定哪些潜在因素最为关键；的定义。之前，自编码器和生成模型被训练来优化一个类似于均方误差；的固定标准。这些固定标准确定了哪些因素是重要的。例如，图像像素
          關鍵詞：这些固定标准确定了哪些因素是重要的, 自编码器和生成模型被训练来优化一个类似于均方误差, 图像像素, 例如, 无监督学习的另一个思路是选择一个更好的确定哪些潜在因素最为关键
        - 摘要：著）。
          關鍵詞：
        - 摘要：图15.5　机器人任务上，基于均方误差训练的自编码器不能重构乒乓球。乒乓球的存在及其所；有空间坐标，是生成图像且与机器人任务相关的重要潜在因素。不幸的是，自编码器具有有限；的容量，基于均方误差的训练没能将乒乓球作为显著物体识别出来编码。以上图像由Chelsea
          關鍵詞：机器人任务上, 自编码器具有有限, 以上图像由, 的容量, 乒乓球的存在及其所
        - 摘要：还有一些其他的显著性的定义。例如，如果一组像素具有高度可识别的；模式，那么即使该模式不涉及极端的亮度或暗度，该模式还是会被认为；非常显著。实现这样一种定义显著的方法是使用最近提出的生成式对抗
          關鍵詞：实现这样一种定义显著的方法是使用最近提出的生成式对抗, 如果一组像素具有高度可识别的, 例如, 非常显著, 那么即使该模式不涉及极端的亮度或暗度
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图15.6　预测生成网络是一个学习哪些特征显著的例子。在这个例子中，预测生成网络已被训；练成在特定视角预测人头的3D模型。（左）真实情况。这是一张网络应该生成的正确图片。
          關鍵詞：预测生成网络是一个学习哪些特征显著的例子, 在这个例子中, 模型, 预测生成网络已被训, 练成在特定视角预测人头的
        - 摘要：正如Schölkopf  et  al.  （2012）指出，学习潜在因素的好处是，如果真实；的生成过程中x 是结果，y 是原因，那么建模p(x ｜y )对于p(y )的变化是；鲁棒的。如果因果关系被逆转，这是不对的，因为根据贝叶斯规则，
          關鍵詞：指出, 学习潜在因素的好处是, 是原因, 是结果, 的变化是
    15.4：分布式表示
        - 摘要：分布式表示的概念（由很多元素组合的表示，这些元素之间可以设置成；可分离的）是表示学习最重要的工具之一。分布式表示非常强大，因为；他们能用具有k个值的n个特征去描述k  n  个不同的概念。正如我们在本
          關鍵詞：因为, 他们能用具有, 可分离的, 个特征去描述, 个不同的概念
        - 摘要：n维二元向量是一个分布式表示的示例，有2  n  种配置，每一种都对应输；入空间中的一个不同区域，如图15.7所示。这可以与符号表示相比较，；其中输入关联到单一符号或类别。如果字典中有n个符号，那么可以想
          關鍵詞：入空间中的一个不同区域, 那么可以想, 所示, 其中输入关联到单一符号或类别, 个符号
        - 摘要：图15.7　基于分布式表示的学习算法如何将输入空间分割成多个区域的图示。这个例子具有二；元变量h 1 、h 2 、h 3 。每个特征通过为学成的线性变换设定输出阈值而定义。每个特征将；分成两个半平面。令
          關鍵詞：分成两个半平面, 基于分布式表示的学习算法如何将输入空间分割成多个区域的图示, 这个例子具有二, 元变量, 每个特征将
        - 摘要：表示输入点h i ＝0的集合。在这个图示；区域。整个表示在这些半平；对应着区域
          關鍵詞：区域, 表示输入点, 在这个图示, 整个表示在这些半平, 的集合
        - 摘要：表示输入点h i ＝1的集合；
          關鍵詞：表示输入点, 的集合
        - 摘要：。可以将以上表示和图15.8中的非分布式表示进行比较。在输入维度是d的一；。具有n个特征的分布式表
          關鍵詞：个特征的分布式表, 在输入维度是, 的一, 具有, 中的非分布式表示进行比较
        - 摘要：般情况下，分布式表示通过半空间（而不是半平面）的交叉分割；示给O(n d )个不同区域分配唯一的编码，而具有n个样本的最近邻算法只能给n个不同区域分配
          關鍵詞：个不同区域分配唯一的编码, 示给, 个不同区域分配, 个样本的最近邻算法只能给, 的交叉分割
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；唯一的编码。因此，分布式表示能够比非分布式表示多分配指数级的区域。注意并非所有的 h；值都是可取的（这个例子中没有 h ＝ 0 ），在分布式表示上的线性分类器不能向每个相邻区域分
          關鍵詞：唯一的编码, 分布式表示能够比非分布式表示多分配指数级的区域, 注意并非所有的, 因此, 在分布式表示上的线性分类器不能向每个相邻区域分
        - 摘要：以下是基于非分布式表示的学习算法的示例：
          關鍵詞：以下是基于非分布式表示的学习算法的示例
        - 摘要：聚类算法，包含k-means算法：每个输入点恰好分配到一个类别。；k-最近邻算法：给定一个输入，一个或几个模板或原型样本与之关；联。在k＞1的情况下，每个输入都使用多个值来描述，但是它们不
          關鍵詞：每个输入点恰好分配到一个类别, 包含, 算法, 聚类算法, 一个或几个模板或原型样本与之关
        - 摘要：图15.8　最近邻算法如何将输入空间分成不同区域的图示。最近邻算法是一个基于非分布式表；示的学习算法的示例。不同的非分布式算法可以具有不同的几何形状，但是它们通常将输入空；间分成区域，每个区域具有不同的参数。非分布式方法的优点是，给定足够的参数，它能够拟
          關鍵詞：但是它们通常将输入空, 最近邻算法是一个基于非分布式表, 不同的非分布式算法可以具有不同的几何形状, 每个区域具有不同的参数, 间分成区域
        - 摘要：对于部分非分布式算法而言，有些输出并非是恒定的，而是在相邻区域；之间内插。参数（或样本）的数量和它们能够定义区域的数量之间仍保；持线性关系。
          關鍵詞：的数量和它们能够定义区域的数量之间仍保, 对于部分非分布式算法而言, 参数, 而是在相邻区域, 或样本
        - 摘要：将分布式表示和符号表示区分开来的一个重要概念是，由不同概念之间；的共享属性而产生的泛化。作为纯符号，“猫”和“狗”之间的距离和任意；其他两种符号的距离一样。然而，如果将它们与有意义的分布式表示相
          關鍵詞：由不同概念之间, 将分布式表示和符号表示区分开来的一个重要概念是, 然而, 如果将它们与有意义的分布式表示相, 之间的距离和任意
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；操作的模型泛化得更好。分布式表示具有丰富的相似性空间，语义上相；近的概念（或输入）在距离上接近，这是纯粹的符号表示所缺少的特
          關鍵詞：或输入, 在距离上接近, 近的概念, 分布式表示具有丰富的相似性空间, 这是纯粹的符号表示所缺少的特
        - 摘要：在学习算法中使用分布式表示何时以及为什么具有统计优势？当一个明；显复杂的结构可以用较少参数紧致地表示时，分布式表示具有统计上的；优点。一些传统的非分布式学习算法仅仅在平滑假设的情况下能够泛
          關鍵詞：当一个明, 在学习算法中使用分布式表示何时以及为什么具有统计优势, 优点, 一些传统的非分布式学习算法仅仅在平滑假设的情况下能够泛, 显复杂的结构可以用较少参数紧致地表示时
        - 摘要：如果我们幸运的话，除了平滑之外，目标函数可能还有一些其他规律。；例如，具有最大池化的卷积网络可以在不考虑对象在图像中位置（即使；对象的空间变换不对应输入空间的平滑变换）的情况下识别出对象。
          關鍵詞：除了平滑之外, 即使, 对象的空间变换不对应输入空间的平滑变换, 目标函数可能还有一些其他规律, 具有最大池化的卷积网络可以在不考虑对象在图像中位置
        - 摘要：让我们检查分布式表示学习算法的一个特殊情况，它通过对输入的线性；函数进行阈值处理来提取二元特征。该表示中的每个二元特征将  分；成一对半空间，如图15.7所示。n个相应半空间的指数级数量的交集确
          關鍵詞：函数进行阈值处理来提取二元特征, 该表示中的每个二元特征将, 个相应半空间的指数级数量的交集确, 所示, 它通过对输入的线性
        - 摘要：因此，我们会发现关于输入大小呈指数级增长，关于隐藏单元的数量呈；多项式级增长。
          關鍵詞：因此, 我们会发现关于输入大小呈指数级增长, 多项式级增长, 关于隐藏单元的数量呈
        - 摘要：这提供了分布式表示泛化能力的一种几何解释：O(nd)个参数（空间
          關鍵詞：这提供了分布式表示泛化能力的一种几何解释, 空间, 个参数
        - 摘要：中的n个线性阈值特征）能够明确表示输入空间中O(n  d  )个不同区；域。如果我们没有对数据做任何假设，并且每个区域使用唯一的符号来；表示，每个符号使用单独的参数去识别   中的对应区域，那么指定
          關鍵詞：表示, 能够明确表示输入空间中, 个不同区, 如果我们没有对数据做任何假设, 那么指定
        - 摘要：另一个解释基于分布式表示的模型泛化能力更好的说法是，尽管能够明；确地编码这么多不同的区域，但它们的容量仍然是很有限的。例如，线；性阈值单元神经网络的VC维仅为O(w
          關鍵詞：维仅为, 尽管能够明, 另一个解释基于分布式表示的模型泛化能力更好的说法是, 例如, 性阈值单元神经网络的
        - 摘要：到目前为止讨论的想法都是抽象的，但是它们可以通过实验验证。Zhou；et  al.  （2015）发现，在ImageNet和Places基准数据集上训练的深度卷积；网络中的隐藏单元学成的特征通常是可以解释的，对应人类自然分配的
          關鍵詞：网络中的隐藏单元学成的特征通常是可以解释的, 对应人类自然分配的, 到目前为止讨论的想法都是抽象的, 发现, 基准数据集上训练的深度卷积
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；是女性，而另一个方向对应着该人是否戴着眼镜。这些特征都是自动发；现的，而非先验固定的。我们没有必要为隐藏单元分类器提供标签：只
          關鍵詞：我们没有必要为隐藏单元分类器提供标签, 现的, 而非先验固定的, 这些特征都是自动发, 是女性
        - 摘要：图15.9　生成模型学到了分布式表示，能够从戴眼镜的概念中区分性别的概念。如果我们从一；个戴眼镜的男人的概念表示向量开始，然后减去一个没戴眼镜的男人的概念表示向量，最后加；上一个没戴眼镜的女人的概念表示向量，那么我们会得到一个戴眼镜的女人的概念表示向量。
          關鍵詞：个戴眼镜的男人的概念表示向量开始, 上一个没戴眼镜的女人的概念表示向量, 生成模型学到了分布式表示, 那么我们会得到一个戴眼镜的女人的概念表示向量, 然后减去一个没戴眼镜的男人的概念表示向量
    15.5：得益于深度的指数增益
        - 摘要：我们已经在第6.4.1节中看到，多层感知机是万能近似器，相比于浅层网；络，一些函数能够用指数级小的深度网络表示。缩小模型规模能够提高；统计效率。在本节中，我们描述如何将类似结果更一般地应用于其他具
          關鍵詞：一些函数能够用指数级小的深度网络表示, 在本节中, 我们描述如何将类似结果更一般地应用于其他具, 统计效率, 缩小模型规模能够提高
        - 摘要：在第15.4节中，我们看到了一个生成模型的示例，能够学习人脸图像的；潜在解释因子，包括性别以及是否佩戴眼镜。完成这个任务的生成模型；是基于一个深度神经网络的。浅层网络例如线性网络不能学习出这些抽
          關鍵詞：是基于一个深度神经网络的, 我们看到了一个生成模型的示例, 浅层网络例如线性网络不能学习出这些抽, 节中, 在第
        - 摘要：输入的函数）或因子（被视为生成原因）。
          關鍵詞：或因子, 输入的函数, 被视为生成原因
        - 摘要：在许多不同情景中已经证明，非线性和重用特征层次结构的组合来组织；计算，可以使分布式表示获得指数级加速之外，还可以获得统计效率的；指数级提升。许多种类的只有一个隐藏层的网络（例如，具有饱和非线
          關鍵詞：在许多不同情景中已经证明, 可以使分布式表示获得指数级加速之外, 还可以获得统计效率的, 许多种类的只有一个隐藏层的网络, 例如
        - 摘要：在第6.4.1节中，我们看到确定性前馈网络是函数的万能近似器。许多具；有单个隐藏层（潜变量）的结构化概率模型（包括受限玻尔兹曼机、深；度信念网络）是概率分布的万能近似器（Le  Roux  and  Bengio，2008，
          關鍵詞：是概率分布的万能近似器, 潜变量, 节中, 的结构化概率模型, 度信念网络
        - 摘要：在第6.4.1节中，我们看到足够深的前馈网络会比深度不够的网络具有指；数级优势。这样的结果也能从诸如概率模型的其他模型中获得。和—积；网络 （sum-product network，SPN）（Poon and Domingos，2011）是这
          關鍵詞：网络, 这样的结果也能从诸如概率模型的其他模型中获得, 数级优势, 节中, 我们看到足够深的前馈网络会比深度不够的网络具有指
        - 摘要：另一个有趣的进展是，一系列和卷积网络相关的深度回路族表达能力的；理论结果，即使让浅度回路只去近似深度回路计算的函数，也能突出反；映深度回路的指数级优势（Cohen et al. ，2015）。相比之下，以前的理
          關鍵詞：也能突出反, 以前的理, 映深度回路的指数级优势, 一系列和卷积网络相关的深度回路族表达能力的, 理论结果
    15.6：提供发现潜在原因的线索
        - 摘要：我们回到最初的问题之一来结束本章：什么原因能够使一个表示比另一
          關鍵詞：我们回到最初的问题之一来结束本章, 什么原因能够使一个表示比另一
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；个表示更好？首先在第15.3节中介绍的一个答案是，一个理想的表示能；够区分生成数据变化的潜在因果因子，特别是那些与我们的应用相关的
          關鍵詞：够区分生成数据变化的潜在因果因子, 节中介绍的一个答案是, 个表示更好, 一个理想的表示能, 特别是那些与我们的应用相关的
        - 摘要：在此，我们提供了一些通用正则化策略的列表。该列表显然是不详尽；的，但是给出了一些学习算法是如何发现对应潜在因素的特征的具体示；例。该列表在Bengio et al. （2013d）的第3.1节中提出，这里进行了部分
          關鍵詞：的第, 但是给出了一些学习算法是如何发现对应潜在因素的特征的具体示, 该列表在, 在此, 我们提供了一些通用正则化策略的列表
        - 摘要：平滑：假设对于单位  d  和小量   有；。这；个假设允许学习器从训练样本泛化到输入空间中附近的点。许多机
          關鍵詞：个假设允许学习器从训练样本泛化到输入空间中附近的点, 许多机, 假设对于单位, 平滑, 和小量
        - 摘要：是有利的，当潜在成因上的分布发生改变，或者我们应用模型到一；个新的任务上时，学成的模型都会更加鲁棒。；深度，或者解释因子的层次组织：高级抽象概念能够通过将简单概
          關鍵詞：高级抽象概念能够通过将简单概, 学成的模型都会更加鲁棒, 当潜在成因上的分布发生改变, 或者我们应用模型到一, 是有利的
        - 摘要：表示学习的概念将许多深度学习形式联系在了一起。前馈网络和循环网
          關鍵詞：前馈网络和循环网, 表示学习的概念将许多深度学习形式联系在了一起
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；络，自编码器和深度概率模型都在学习和使用表示。学习最佳表示仍然；是一个令人兴奋的研究方向。
          關鍵詞：自编码器和深度概率模型都在学习和使用表示, 是一个令人兴奋的研究方向, 学习最佳表示仍然
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)；一般来说，我们可能会想要学习一个函数，这个函数在指数级数量区域的表现都是不同；的：在d-维空间中，为了区分每一维，至少有两个不同的值。我们想要函数f区分这2  d  个不同
          關鍵詞：我们想要函数, 这个函数在指数级数量区域的表现都是不同, 至少有两个不同的值, 区分这, 我们可能会想要学习一个函数
第16章：深度学习中的结构化概率模型
    15.6：提供发现潜在原因的线索
        - 摘要：深度学习为研究者们提供了许多建模方式，用以设计以及描述算法。其；中一种形式是结构化概率模型  （structured  probabilistic  model）的思；想。我们曾经在第3.14节中简要讨论过结构化概率模型。此前简要的介
          關鍵詞：深度学习为研究者们提供了许多建模方式, 中一种形式是结构化概率模型, 我们曾经在第, 此前简要的介, 的思
        - 摘要：结构化概率模型使用图来描述概率分布中随机变量之间的直接相互作；用，从而描述一个概率分布。在这里我们使用了图论（一系列结点通过；一系列边来连接）中“图”的概念，由于模型结构是由图定义的，所以这
          關鍵詞：由于模型结构是由图定义的, 一系列边来连接, 在这里我们使用了图论, 一系列结点通过, 所以这
        - 摘要：图模型的研究社群是巨大的，并提出过大量的模型、训练算法和推断算；法。在本章中，我们将介绍图模型中几个核心方法的基本背景，并且重；点描述已被证明对深度学习社群最有用的观点。如果你已经熟知图模
          關鍵詞：我们将介绍图模型中几个核心方法的基本背景, 点描述已被证明对深度学习社群最有用的观点, 训练算法和推断算, 图模型的研究社群是巨大的, 如果你已经熟知图模
        - 摘要：我们首先介绍了构建大规模概率模型时面临的挑战。之后，我们介绍如
          關鍵詞：之后, 我们首先介绍了构建大规模概率模型时面临的挑战, 我们介绍如
        - 摘要：何使用一个图来描述概率分布的结构。尽管这个方法能够帮助我们解决；许多挑战和问题，它本身仍有很多缺陷。图模型中的一个主要难点就是；判断哪些变量之间存在直接的相互作用关系，也就是对于给定的问题哪
          關鍵詞：判断哪些变量之间存在直接的相互作用关系, 许多挑战和问题, 也就是对于给定的问题哪, 它本身仍有很多缺陷, 图模型中的一个主要难点就是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；16.1　非结构化建模的挑战
          關鍵詞：非结构化建模的挑战
        - 摘要：深度学习的目标是使得机器学习能够解决许多人工智能中亟需解决的挑；战。这也意味着它们能够理解具有丰富结构的高维数据。举个例子，我；们希望AI的算法能够理解自然图片 (1) ，表示语音的声音信号和包含许多
          關鍵詞：表示语音的声音信号和包含许多, 举个例子, 这也意味着它们能够理解具有丰富结构的高维数据, 深度学习的目标是使得机器学习能够解决许多人工智能中亟需解决的挑, 们希望
        - 摘要：分类问题可以把这样一个来自高维分布的数据作为输入，然后使用一个；类别的标签来概括它——这个标签既可以是照片中有什么物品，一段语；音中说的是哪个单词，也可以是一段文档描述的是哪个话题。这个分类
          關鍵詞：一段语, 这个分类, 然后使用一个, 类别的标签来概括它, 分类问题可以把这样一个来自高维分布的数据作为输入
        - 摘要：我们也可以使用概率模型完成许多其他的任务。这些任务通常相比于分；类成本更高。其中的一些任务需要产生多个输出。大部分任务需要对输；入数据整个结构的完整理解，所以并不能舍弃数据的一部分。这些任务
          關鍵詞：这些任务, 入数据整个结构的完整理解, 这些任务通常相比于分, 所以并不能舍弃数据的一部分, 类成本更高
        - 摘要：估计密度函数：  给定一个输入  x  ，机器学习系统返回一个对数据；生成分布的真实密度函数p( x )的估计。这只需要一个输出，但它需；要完全理解整个输入。即使向量中只有一个元素不太正常，系统也
          關鍵詞：的估计, 估计密度函数, 系统也, 要完全理解整个输入, 机器学习系统返回一个对数据
        - 摘要：采样：  模型从分布p(  x  )中抽取新的样本。其应用包括语音合成，；即产生一个听起来很像人说话的声音。这个模型也需要多个输出以；及对输入整体的良好建模。即使样本只有一个从错误分布中产生的
          關鍵詞：这个模型也需要多个输出以, 模型从分布, 采样, 及对输入整体的良好建模, 其应用包括语音合成
        - 摘要：图16.1中描述了一个使用较小的自然图片的采样任务。
          關鍵詞：中描述了一个使用较小的自然图片的采样任务
        - 摘要：对上千甚至是上百万随机变量的分布建模，无论从计算上还是从统计意；义上说，都是一个极具挑战性的任务。假设我们只想对二值的随机变量；建模。这是一个最简单的例子，但是我们仍然无能为力。对一个只有
          關鍵詞：对上千甚至是上百万随机变量的分布建模, 无论从计算上还是从统计意, 建模, 但是我们仍然无能为力, 对一个只有
        - 摘要：通常意义上讲，如果我们希望对一个包含n个离散变量并且每个变量都；能取k个值的 x  的分布建模，那么最简单的表示P( x  )的方法需要存储一；个可以查询的表格。这个表格记录了每一种可能值的概率，则需要k
          關鍵詞：这个表格记录了每一种可能值的概率, 则需要, 的分布建模, 那么最简单的表示, 能取
        - 摘要：基于下述几个原因，这种方式是不可行的。
          關鍵詞：基于下述几个原因, 这种方式是不可行的
        - 摘要：内存：存储参数的开销。除了极小的n和k的值，用表格的形式来表；示这样一个分布需要太多的存储空间。；统计的高效性：当模型中的参数个数增加时，使用统计估计器估计
          關鍵詞：内存, 用表格的形式来表, 统计的高效性, 除了极小的, 存储参数的开销
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；素。最差情况下，这个操作需要读取整个表格，所以和其他操作一；样，它也需要指数级别的时间。
          關鍵詞：这个操作需要读取整个表格, 所以和其他操作一, 它也需要指数级别的时间, 最差情况下
        - 摘要：图16.1　自然图片的概率建模。（上）CIFAR-10数据集（Krizhevsky and Hinton，2009）中的；32×32像素的样例图片。（下）从这个数据集上训练的结构化概率模型中抽出的样本。每一个样；本都出现在与其欧式距离最近的训练样本的格点中。这种比较使得我们发现这个模型确实能够
          關鍵詞：像素的样例图片, 每一个样, 本都出现在与其欧式距离最近的训练样本的格点中, 这种比较使得我们发现这个模型确实能够, 从这个数据集上训练的结构化概率模型中抽出的样本
        - 摘要：基于表格操作的方法的主要问题是我们显式地对每一种可能的变量子集；所产生的每一种可能类型的相互作用建模。在实际问题中我们遇到的概；率分布远比这个简单。通常，许多变量只是间接地相互作用。
          關鍵詞：许多变量只是间接地相互作用, 通常, 在实际问题中我们遇到的概, 率分布远比这个简单, 基于表格操作的方法的主要问题是我们显式地对每一种可能的变量子集
        - 摘要：例如，我们想要对接力跑步比赛中一个队伍完成比赛的时间进行建模。；假设这个队伍有3名成员：Alice、Bob和Carol。在比赛开始时，Alice拿；着接力棒，开始跑第一段距离。在跑完她的路程以后，她把棒递给了
          關鍵詞：名成员, 我们想要对接力跑步比赛中一个队伍完成比赛的时间进行建模, 她把棒递给了, 开始跑第一段距离, 例如
        - 摘要：Bob。然后Bob开始跑，再把棒给Carol，Carol跑最后一棒。我们可以用；连续变量来建模他们每个人完成的时间。因为Alice第一个跑，所以她的；完成时间并不依赖于其他的人。Bob的完成时间依赖于Alice的完成时
          關鍵詞：因为, 完成时间并不依赖于其他的人, 的完成时间依赖于, 第一个跑, 再把棒给
        - 摘要：结构化概率模型为随机变量之间的直接作用提供了一个正式的建模框；架。这种方式大大减少了模型的参数个数，以至于模型只需要更少的数；据来进行有效的估计。这些更小的模型大大减小了在模型存储、模型推
          關鍵詞：结构化概率模型为随机变量之间的直接作用提供了一个正式的建模框, 模型推, 这些更小的模型大大减小了在模型存储, 以至于模型只需要更少的数, 这种方式大大减少了模型的参数个数
    16.2：使用图描述模型结构
        - 摘要：16.2.1　有向模型
          關鍵詞：有向模型
        - 摘要：16.2.2　无向模型
          關鍵詞：无向模型
        - 摘要：16.2.3　配分函数
          關鍵詞：配分函数
        - 摘要：16.2.4　基于能量的模型
          關鍵詞：基于能量的模型
        - 摘要：16.2.5　分离和d-分离
          關鍵詞：分离, 分离和
        - 摘要：16.2.6　在有向模型和无向模型中转换
          關鍵詞：在有向模型和无向模型中转换
        - 摘要：16.2.7　因子图
          關鍵詞：因子图
        - 摘要：结构化概率模型使用图（在图论中“结点”是通过“边”来连接的）来表示；随机变量之间的相互作用。每一个结点代表一个随机变量。每一条边代；表一个直接相互作用。这些直接相互作用隐含着其他的间接相互作用，
          關鍵詞：结点, 在图论中, 每一条边代, 结构化概率模型使用图, 表一个直接相互作用
        - 摘要：使用图来描述概率分布中相互作用的方法不止一种。在下文中我们会介；绍几种最为流行和有用的方法。图模型可以被大致分为两类：基于有向；无环图的模型和基于无向图的模型。
          關鍵詞：绍几种最为流行和有用的方法, 图模型可以被大致分为两类, 使用图来描述概率分布中相互作用的方法不止一种, 在下文中我们会介, 无环图的模型和基于无向图的模型
        - 摘要：16.2.1　有向模型
          關鍵詞：有向模型
        - 摘要：有向图模型  （directed  graphical  model）是一种结构化概率模型，也被；称为信念网络 （belief network）或者贝叶斯网络  （Bayesian  network）；(2) （Pearl，1985）。
          關鍵詞：有向图模型, 是一种结构化概率模型, 也被, 或者贝叶斯网络, 称为信念网络
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；之所以命名为有向图模型，是因为所有的边都是有方向的，即从一个结；点指向另一个结点。这个方向可以通过画一个箭头来表示。箭头所指的
          關鍵詞：点指向另一个结点, 箭头所指的, 这个方向可以通过画一个箭头来表示, 之所以命名为有向图模型, 是因为所有的边都是有方向的
        - 摘要：我们继续第16.1节所讲的接力赛的例子，我们假设Alice的完成时间为t  0；，Bob的完成时间为t 1 ，Carol的完成时间为t 2 。就像我们之前看到的一；样，t 1 的估计是依赖于t 0 的，t 2 的估计是直接依赖于t 1 的，但是仅仅间
          關鍵詞：的估计是直接依赖于, 但是仅仅间, 的完成时间为, 我们继续第, 我们假设
        - 摘要：图16.2　描述接力赛例子的有向图模型。Alice的完成时间t 0 影响了Bob的完成时间t 1 ，因为；Bob只能在Alice完成比赛后才开始。类似地，Carol也只会在Bob完成之后才开始，所以Bob的完；成时间t 1 直接影响了Carol的完成时间t 2
          關鍵詞：因为, 影响了, 成时间, 完成比赛后才开始, 完成之后才开始
        - 摘要：正式地说，变量x 的有向概率模型是通过有向无环图  （每个结点都是；模型中的随机变量）和一系列局部条件概率分布  （local；conditional
          關鍵詞：和一系列局部条件概率分布, 每个结点都是, 模型中的随机变量, 正式地说, 的有向概率模型是通过有向无环图
        - 摘要：distribution）
          關鍵詞：
        - 摘要：表示结点x i 的所有父结点。x 的概率分布可以表示为
          關鍵詞：的所有父结点, 表示结点, 的概率分布可以表示为
        - 摘要：在之前所述的接力赛的例子中，参考图16.2，这意味着概率分布可以被；表示为
          關鍵詞：在之前所述的接力赛的例子中, 参考图, 表示为, 这意味着概率分布可以被
        - 摘要：这是我们看到的第一个结构化概率模型的实际例子。我们能够检查这样；建模的计算开销，为了验证相比于非结构化建模，结构化建模为什么有；那么多的优势。
          關鍵詞：建模的计算开销, 我们能够检查这样, 结构化建模为什么有, 那么多的优势, 为了验证相比于非结构化建模
        - 摘要：假设我们采用从第0分钟到第10分钟每6秒一块的方式离散化地表示时；间。这使得t  0  、t  1  和t  2  都是一个有100个取值可能的离散变量。如果我；们尝试着用一个表来表示p(t  0  ,t  1  ,t  2  )，那么我们需要存储999  999个值
          關鍵詞：这使得, 秒一块的方式离散化地表示时, 如果我, 那么我们需要存储, 分钟到第
        - 摘要：通常意义上说，对每个变量都能取k个值的n个变量建模，基于建表的方；法需要的复杂度是O(k  n  )，就像我们之前观察到的一样。现在假设我们；用一个有向图模型来对这些变量建模。如果m代表图模型的单个条件概
          關鍵詞：如果, 通常意义上说, 对每个变量都能取, 代表图模型的单个条件概, 用一个有向图模型来对这些变量建模
        - 摘要：，那么复杂度就会被大大地减小。
          關鍵詞：那么复杂度就会被大大地减小
        - 摘要：换一句话说，只要图中的每个变量都只有少量的父结点，那么这个分布；就可以用较少的参数来表示。图结构上的一些限制条件，比如说要求这；个图为一棵树，也可以保证一些操作（例如求一小部分变量的边缘或者
          關鍵詞：那么这个分布, 换一句话说, 个图为一棵树, 也可以保证一些操作, 图结构上的一些限制条件
        - 摘要：决定哪些信息需要被包含在图中而哪些不需要是很重要的。如果变量之；间可以被假设为是条件独立的，那么这个图可以包含这种简化假设。当；然也存在其他类型的简化图模型的假设。例如，我们可以假设无论Alice
          關鍵詞：然也存在其他类型的简化图模型的假设, 间可以被假设为是条件独立的, 我们可以假设无论, 例如, 决定哪些信息需要被包含在图中而哪些不需要是很重要的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；计算Bob的完成时间时需要加上Alice的时间。这个假设使得我们所需要；的参数量从O(k  2  )  降到了O(k)。然而，值得注意的是，在这个假设下t  0
          關鍵詞：的完成时间时需要加上, 然而, 的参数量从, 值得注意的是, 在这个假设下
        - 摘要：16.2.2　无向模型
          關鍵詞：无向模型
        - 摘要：有向图模型为我们提供了一种描述结构化概率模型的语言。而另一种常；见的语言则是无向模型  （undirected  model），也被称为马尔可夫随机；field，MRF）或者是马尔可夫网络  （Markov
          關鍵詞：见的语言则是无向模型, 也被称为马尔可夫随机, 有向图模型为我们提供了一种描述结构化概率模型的语言, 而另一种常, 或者是马尔可夫网络
        - 摘要：random
          關鍵詞：
        - 摘要：当存在很明显的理由画出每一个指向特定方向的箭头时，有向模型显然；最适用。有向模型中，经常存在我们理解的具有因果关系以及因果关系；有明确方向的情况。接力赛的例子就是一个这样的情况。之前运动员的
          關鍵詞：有向模型中, 有向模型显然, 当存在很明显的理由画出每一个指向特定方向的箭头时, 接力赛的例子就是一个这样的情况, 有明确方向的情况
        - 摘要：然而并不是所有情况的相互作用都有一个明确的方向关系。当相互的作；用并没有本质性的指向，或者是明确的双向相互作用时，使用无向模型；更加合适。
          關鍵詞：当相互的作, 或者是明确的双向相互作用时, 然而并不是所有情况的相互作用都有一个明确的方向关系, 用并没有本质性的指向, 更加合适
        - 摘要：作为一个这种情况的例子，假设我们希望对3个二值随机变量建模：你；是否生病，你的同事是否生病以及你的室友是否生病。就像在接力赛的；例子中所作的简化假设一样，我们可以在这里做一些关于相互作用的简
          關鍵詞：作为一个这种情况的例子, 我们可以在这里做一些关于相互作用的简, 例子中所作的简化假设一样, 是否生病, 就像在接力赛的
        - 摘要：另一个人。我们通过对你的同事传染给你以及你传染给你的室友建模来；对这种间接的从你的同事到你的室友的感冒传染建模。
          關鍵詞：对这种间接的从你的同事到你的室友的感冒传染建模, 另一个人, 我们通过对你的同事传染给你以及你传染给你的室友建模来
        - 摘要：在这种情况下，你传染给你的室友和你的室友传染给你都是非常容易；的，所以模型不存在一个明确的单向箭头。这启发我们使用无向模型。；其中随机变量对应着图中的相互作用的结点。与有向模型相同的是，如
          關鍵詞：所以模型不存在一个明确的单向箭头, 这启发我们使用无向模型, 你传染给你的室友和你的室友传染给你都是非常容易, 其中随机变量对应着图中的相互作用的结点, 在这种情况下
        - 摘要：我们把对应你健康状况的随机变量记作h  y  ，对应你的室友健康状况的；随机变量记作h r ，你的同事健康的变量记作h c 。图16.3表示这种关系。
          關鍵詞：对应你的室友健康状况的, 表示这种关系, 你的同事健康的变量记作, 随机变量记作, 我们把对应你健康状况的随机变量记作
        - 摘要：图16.3　表示你室友健康状况的h r 、你健康状况的h y 和你同事健康状况的h c 之间如何相互影；响的一个无向图。你和你的室友可能会相互传染感冒，你和你的同事之间也是如此，但是假设；你室友和同事之间相互不认识，他们只能通过你来间接传染
          關鍵詞：你健康状况的, 之间如何相互影, 你室友和同事之间相互不认识, 你和你的室友可能会相互传染感冒, 你和你的同事之间也是如此
        - 摘要：正式地说，一个无向模型是一个定义在无向模型   上的结构化概率模；型。对于图中的每一个团 (3)；，一个因子 （factor）φ（  ）（也称为
          關鍵詞：一个无向模型是一个定义在无向模型, 也称为, 一个因子, 正式地说, 上的结构化概率模
        - 摘要：只要所有团中的结点数都不大，那么我们就能够高效地处理这些未归一；化概率函数。它包含了这样的思想，密切度越高的状态有越大的概率。；然而，不像贝叶斯网络，几乎不存在团定义的结构，所以不能保证把它
          關鍵詞：几乎不存在团定义的结构, 它包含了这样的思想, 化概率函数, 所以不能保证把它, 然而
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；型中读取分解信息的例子。
          關鍵詞：型中读取分解信息的例子
        - 摘要：图16.4　这个图说明通过选择适当的φ，函数p(a,b,c,d,e,f)可以写作
          關鍵詞：函数, 这个图说明通过选择适当的, 可以写作
        - 摘要：在你、你的室友和同事之间感冒传染的例子中包含了两个团。一个团包；含了h  y  和h  c  。这个团的因子可以通过一个表来定义，可能取到下面的；值。
          關鍵詞：一个团包, 含了, 在你, 可能取到下面的, 你的室友和同事之间感冒传染的例子中包含了两个团
        - 摘要：状态为1代表了健康的状态，相对的状态为0则表示不好的健康状态（即；感染了感冒）。你们两个通常都是健康的，所以对应的状态拥有最高的
          關鍵詞：相对的状态为, 感染了感冒, 则表示不好的健康状态, 代表了健康的状态, 你们两个通常都是健康的
        - 摘要：密切程度。两个人中只有一个人是生病的密切程度是最低的，因为这是；一个很罕见的状态。两个人都生病的状态（通过一个人来传染给了另一；个人）有一个稍高的密切程度，尽管仍然不及两个人都健康的密切程
          關鍵詞：两个人都生病的状态, 个人, 有一个稍高的密切程度, 尽管仍然不及两个人都健康的密切程, 因为这是
        - 摘要：为了完整地定义这个模型，我们需要对包含h  y  和h  r  的团定义类似的因；子。
          關鍵詞：我们需要对包含, 的团定义类似的因, 为了完整地定义这个模型
        - 摘要：16.2.3　配分函数
          關鍵詞：配分函数
        - 摘要：尽管这个未归一化概率函数处处不为零，我们仍然无法保证它的概率之；和或者积分为1。为了得到一个有效的概率分布，我们需要使用对应的；归一化的概率分布 (4) ：
          關鍵詞：为了得到一个有效的概率分布, 尽管这个未归一化概率函数处处不为零, 我们仍然无法保证它的概率之, 我们需要使用对应的, 归一化的概率分布
        - 摘要：其中，Z是使得所有的概率之和或者积分为1的常数，并且满足：
          關鍵詞：并且满足, 是使得所有的概率之和或者积分为, 的常数, 其中
        - 摘要：当函数φ固定时，我们可以把Z当成是一个常数。值得注意的是，如果函；数φ带有参数时，那么Z是这些参数的一个函数。在相关文献中为了节省；空间忽略控制Z的变量而直接写Z是一个常用的方式。归一化常数Z被称
          關鍵詞：在相关文献中为了节省, 的变量而直接写, 我们可以把, 带有参数时, 是一个常用的方式
        - 摘要：由于Z通常是由对所有可能的x  状态的联合分布空间求和或者求积分得；到的，它通常是很难计算的。为了获得一个无向模型的归一化概率分；布，模型的结构和函数φ的定义通常需要设计为有助于高效地计算Z。在
          關鍵詞：状态的联合分布空间求和或者求积分得, 到的, 它通常是很难计算的, 由于, 模型的结构和函数
        - 摘要：在设计无向模型时，我们必须牢记于心的一个要点是设定一些使得Z不；存在的因子也是有可能的。当模型中的一些变量是连续的，且  在其定；义域上的积分发散时这种情况就会发生。例如，当我们需要对一个单独
          關鍵詞：我们必须牢记于心的一个要点是设定一些使得, 义域上的积分发散时这种情况就会发生, 存在的因子也是有可能的, 例如, 在其定
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；的标量变量；这种情况下，
          關鍵詞：的标量变量, 这种情况下
        - 摘要：建模，并且单个团势能定义为φ(x)＝x  2  时。在
          關鍵詞：并且单个团势能定义为, 建模
        - 摘要：由于这个积分是发散的，所以不存在一个对应着这个势能函数φ(x)的概；率分布。有时候φ函数某些参数的选择可以决定相应的概率分布是否能；够被定义。例如，对φ函数φ(x;β)＝exp(−βx 2 )来说，参数β决定了归一化
          關鍵詞：够被定义, 参数, 决定了归一化, 率分布, 函数
        - 摘要：有向建模和无向建模之间一个重要的区别就是有向模型是通过从起始点；的概率分布直接定义的，反之无向模型的定义显得更加宽松，通过φ函；数转化为概率分布而定义。这改变了我们处理这些建模问题的直觉。当
          關鍵詞：数转化为概率分布而定义, 这改变了我们处理这些建模问题的直觉, 有向建模和无向建模之间一个重要的区别就是有向模型是通过从起始点, 的概率分布直接定义的, 反之无向模型的定义显得更加宽松
        - 摘要：i
          關鍵詞：
        - 摘要：i
          關鍵詞：
        - 摘要：)。如果x
          關鍵詞：如果
        - 摘要：＝1)＝sigmoid(b
          關鍵詞：
        - 摘要：，那么p(x )可以被分解成n个独立的分布，并且满足p(x；的定义域是基本单位向量；的集合，
          關鍵詞：可以被分解成, 个独立的分布, 那么, 的集合, 并且满足
        - 摘要：16.2.4　基于能量的模型
          關鍵詞：基于能量的模型
        - 摘要：这个假；无向模型中许多有趣的理论结果都依赖于∀  x  ，；设。使这个条件满足的一种简单方式是使用基于能量的模型  （Energy-
          關鍵詞：这个假, 无向模型中许多有趣的理论结果都依赖于, 使这个条件满足的一种简单方式是使用基于能量的模型
        - 摘要：based model，EBM），其中
          關鍵詞：其中
        - 摘要：E(x  )被称作是能量函数  （energy  function）。对所有的z，exp(z)都是正；的，这保证了没有一个能量函数会使得某一个状态x  的概率为0。我们；可以完全自由地选择那些能够简化学习过程的能量函数。如果我们直接
          關鍵詞：可以完全自由地选择那些能够简化学习过程的能量函数, 被称作是能量函数, 我们, 如果我们直接, 的概率为
        - 摘要：（Boltzmann
          關鍵詞：
        - 摘要：服从式（16.7）形式的任意分布都是玻尔兹曼分布；distribution）的一个实例。正是基于这个原因，我们把许多基于能量的；模型称为玻尔兹曼机 （Boltzmann  Machine）（Fahlman  et  al.  ，1983；
          關鍵詞：模型称为玻尔兹曼机, 形式的任意分布都是玻尔兹曼分布, 服从式, 我们把许多基于能量的, 的一个实例
        - 摘要：无向模型中的团对应于未归一化概率函数中的因子。通过exp(a＋b)＝；exp(a)exp(b)，我们发现无向模型中的不同团对应于能量函数的不同项。；换句话说，基于能量的模型只是一种特殊的马尔可夫网络：求幂使能量
          關鍵詞：无向模型中的团对应于未归一化概率函数中的因子, 我们发现无向模型中的不同团对应于能量函数的不同项, 求幂使能量, 基于能量的模型只是一种特殊的马尔可夫网络, 通过
        - 摘要：of
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图16.5　这个图说明通过为每个团选择适当的能量函数E(a,b,c,d,e,f)可以写作E a，b （a,b）＋E；b,c (b,c)＋E a,d (a,d)＋E b,e (b,e)＋E e,f (e,f)。值得注意的是，我们令φ等于对应负能量的指数，
          關鍵詞：我们令, 可以写作, 值得注意的是, 等于对应负能量的指数, 这个图说明通过为每个团选择适当的能量函数
        - 摘要：基于能量的模型定义的一部分无法用机器学习观点来解释：即式；（16.7）中的“-”符号。这个“-”符号可以被包含在E的定义之中。对于很；多E函数的选择来说，学习算法可以自由地决定能量的符号。这个负号
          關鍵詞：这个负号, 符号, 的定义之中, 对于很, 函数的选择来说
        - 摘要：许多对概率模型进行操作的算法不需要计算p  model  (  x  )，而只需要计算；。对于具有潜变量 h 的基于能量的模型，这些算法有
          關鍵詞：的基于能量的模型, 而只需要计算, 许多对概率模型进行操作的算法不需要计算, 对于具有潜变量, 这些算法有
        - 摘要：时会将该量的负数称为自由能 （free energy）：
          關鍵詞：时会将该量的负数称为自由能
        - 摘要：在本书中，我们更倾向于更为通用的基于
          關鍵詞：在本书中, 我们更倾向于更为通用的基于
        - 摘要：的定义。
          關鍵詞：的定义
        - 摘要：16.2.5　分离和d-分离
          關鍵詞：分离, 分离和
        - 摘要：图模型中的边告诉我们哪些变量直接相互作用。我们经常需要知道哪些；变量间接相互作用。某些间接相互作用可以通过观察其他变量来启用或；禁用。更正式地，我们想知道在给定其他变量子集的值时，哪些变量子
          關鍵詞：变量间接相互作用, 哪些变量子, 某些间接相互作用可以通过观察其他变量来启用或, 我们经常需要知道哪些, 禁用
        - 摘要：在无向模型中，识别图中的条件独立性是非常简单的。在这种情况下，；图中隐含的条件独立性称为分离  （separation）。如果图结构显示给定；变量集   的情况下变量集   与变量集   无关，那么我们声称给定变
          關鍵詞：识别图中的条件独立性是非常简单的, 在无向模型中, 变量集, 与变量集, 在这种情况下
        - 摘要：当我们画图时，我们可以通过加阴影来表示观察到的变量。图16.6用于；描述当以这种方式绘图时无向模型中的活跃和非活跃路径的样子。图；16.7描述了一个从无向模型中读取分离信息的例子。
          關鍵詞：当我们画图时, 描述当以这种方式绘图时无向模型中的活跃和非活跃路径的样子, 用于, 我们可以通过加阴影来表示观察到的变量, 描述了一个从无向模型中读取分离信息的例子
        - 摘要：图16.6　（a）随机变量a和随机变量b之间穿过s的路径是活跃的，因为s是观察不到的。这意味；着a和b之间不是分离的。（b）图中s用阴影填充，表示它是可观察的。因为a和b之间的唯一路；径通过s，并且这条路径是不活跃的，我们可以得出结论，在给定s的条件下a和b是分离的
          關鍵詞：因为, 径通过, 的条件下, 这意味, 是分离的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图16.7　从一个无向图中读取分离性质的一个例子。这里b用阴影填充，表示它是可观察的。由；于b挡住了从a到c的唯一路径，我们说在给定b的情况下a和c是相互分离的。观察值b同样挡住了
          關鍵詞：观察值, 的唯一路径, 同样挡住了, 从一个无向图中读取分离性质的一个例子, 表示它是可观察的
        - 摘要：类似的概念适用于有向模型，只是在有向模型中，这些概念被称为d-分；离  （d-separation）。“d”代表“依赖”的意思。有向图中d-分离的定义与；无向模型中分离的定义相同：如果图结构显示给定变量集   时，变量
          關鍵詞：这些概念被称为, 有向图中, 类似的概念适用于有向模型, 的意思, 只是在有向模型中
        - 摘要：与无向模型一样，我们可以通过查看图中存在的活跃路径来检查图中隐；含的独立性。如前所述，如果两个变量之间存在活跃路径，则两个变量；是依赖的。如果没有活跃路径，则为d-分离。在有向网络中，确定路径
          關鍵詞：如前所述, 是依赖的, 如果两个变量之间存在活跃路径, 我们可以通过查看图中存在的活跃路径来检查图中隐, 则两个变量
        - 摘要：尤其重要的是，要记住分离和d-分离只能告诉我们图中隐含的条件独立；性。图并不需要表示所有存在的独立性。进一步的，使用完全图（具有；所有可能的边的图）来表示任何分布总是合法的。事实上，一些分布包
          關鍵詞：一些分布包, 事实上, 来表示任何分布总是合法的, 尤其重要的是, 分离只能告诉我们图中隐含的条件独立
        - 摘要：的，但是当a是1时，b确定地等于c。当a＝1时，图模型需要连接b和c的；边。但是图不能说明当a＝0时，b和c不是独立的。
          關鍵詞：不是独立的, 确定地等于, 但是图不能说明当, 图模型需要连接, 但是当
        - 摘要：一般来说，当独立性不存在时，图不会显示独立性。然而，图可能无法；编码独立性。
          關鍵詞：编码独立性, 然而, 图不会显示独立性, 一般来说, 当独立性不存在时
        - 摘要：图16.8　两个随机变量a和b之间存在长度为2的所有种类的活跃路径。（a）箭头方向从a指向b的；任何路径，反过来也一样。如果s可以被观察到，这种路径就是阻塞的。在接力赛的例子中，我；们已经看到过这种类型的路径。（b）变量a和b通过共因s相连。举个例子，假设s是一个表示是
          關鍵詞：如果, 在接力赛的例子中, 们已经看到过这种类型的路径, 指向, 是一个表示是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图16.9　从这张图中，我们可以发现一些d-分离的性质。它包括了以下几点
          關鍵詞：它包括了以下几点, 我们可以发现一些, 分离的性质, 从这张图中
        - 摘要：给定空集的情况下，a和b是d-分离的。；给定c的情况下，a和e是d-分离的。；给定c的情况下，d和e是d-分离的。
          關鍵詞：分离的, 给定空集的情况下, 给定, 的情况下
        - 摘要：我们还可以发现当我们观察到一些变量时，一些变量不再是d-分离的。
          關鍵詞：分离的, 一些变量不再是, 我们还可以发现当我们观察到一些变量时
        - 摘要：给定c的情况下，a和b不是d-分离的。；给定d的情况下，a和b不是d-分离的
          關鍵詞：的情况下, 给定, 不是, 分离的
        - 摘要：16.2.6　在有向模型和无向模型中转换
          關鍵詞：在有向模型和无向模型中转换
        - 摘要：我们经常将特定的机器学习模型称为无向模型或有向模型。例如，我们；通常将受限玻尔兹曼机称为无向模型，而稀疏编码则被称为有向模型。
          關鍵詞：通常将受限玻尔兹曼机称为无向模型, 我们, 例如, 而稀疏编码则被称为有向模型, 我们经常将特定的机器学习模型称为无向模型或有向模型
        - 摘要：这种措辞的选择可能有点误导，因为没有概率模型本质上是有向或无向；的。但是，一些模型很适合使用有向图描述，而另一些模型很适合使用；无向模型描述。
          關鍵詞：一些模型很适合使用有向图描述, 但是, 无向模型描述, 因为没有概率模型本质上是有向或无向, 而另一些模型很适合使用
        - 摘要：有向模型和无向模型都有其优点和缺点。这两种方法都不是明显优越和；普遍优选的。相反，我们根据具体的每个任务来决定使用哪一种模型。；这个选择部分取决于我们希望描述的概率分布。根据哪种方法可以最大
          關鍵詞：普遍优选的, 这两种方法都不是明显优越和, 根据哪种方法可以最大, 这个选择部分取决于我们希望描述的概率分布, 有向模型和无向模型都有其优点和缺点
        - 摘要：每个概率分布可以由有向模型或由无向模型表示。在最坏的情况下，我；们可以使用“完全图”来表示任何分布。在有向模型的情况下，完全图是；任意有向无环图，其中我们对随机变量排序，并且每个变量在排序中位
          關鍵詞：任意有向无环图, 在最坏的情况下, 完全图, 来表示任何分布, 完全图是
        - 摘要：图16.10　完全图的例子，完全图能够描述任何的概率分布。这里我们展示了一个带有4个随机；变量的例子。（左）完全无向图。在无向图中，完全图是唯一的。（右）一个完全有向图。在；有向图中，并不存在唯一的完全图。我们选择一种变量的排序，然后对每一个变量，从它本身
          關鍵詞：有向图中, 完全图的例子, 个随机, 这里我们展示了一个带有, 完全图是唯一的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；种完全图。在这个例子中，我们从左到右、从上到下地排序变量
          關鍵詞：在这个例子中, 种完全图, 从上到下地排序变量, 我们从左到右
        - 摘要：当然，图模型的优势在于图能够包含一些变量不直接相互作用的信息。；完全图并不是很有用，因为它并不隐含任何独立性。
          關鍵詞：当然, 因为它并不隐含任何独立性, 完全图并不是很有用, 图模型的优势在于图能够包含一些变量不直接相互作用的信息
        - 摘要：当我们用图表示概率分布时，我们想要选择一个包含尽可能多独立性的；图，但是并不会假设任何实际上不存在的独立性。
          關鍵詞：但是并不会假设任何实际上不存在的独立性, 当我们用图表示概率分布时, 我们想要选择一个包含尽可能多独立性的
        - 摘要：从这个角度来看，一些分布可以使用有向模型更高效地表示，而其他分；布可以使用无向模型更高效地表示。换句话说，有向模型可以编码一些；无向模型所不能编码的独立性，反之亦然。
          關鍵詞：反之亦然, 而其他分, 无向模型所不能编码的独立性, 布可以使用无向模型更高效地表示, 有向模型可以编码一些
        - 摘要：有向模型能够使用一种无向模型无法完美表示的特定类型的子结构。这；个子结构被称为不道德  （immorality）。这种结构出现在当两个随机变；量a和b都是第三个随机变量c的父结点，并且不存在任一方向上直接连
          關鍵詞：的父结点, 个子结构被称为不道德, 有向模型能够使用一种无向模型无法完美表示的特定类型的子结构, 并且不存在任一方向上直接连, 这种结构出现在当两个随机变
        - 摘要：图16.11　通过构造道德图将有向模型（上一行）转化为无向模型（下一行）的例子。（左）只；需要把有向边替换成无向边就可以把这个简单的链转化为一个道德图。得到的无向模型包含了；完全相同的独立关系和条件独立关系。（中）是在不丢失独立性的情况下无法转化为无向模型
          關鍵詞：转化为无向模型, 通过构造道德图将有向模型, 完全相同的独立关系和条件独立关系, 上一行, 是在不丢失独立性的情况下无法转化为无向模型
        - 摘要：同样地，无向模型可以包括有向模型不能完美表示的子结构。具体来；说，如果  包含长度大于3的环 （loop），则有向图  不能捕获无向；模型  所包含的所有条件独立性，除非该环还包含弦 （chord）。环指
          關鍵詞：如果, 具体来, 不能捕获无向, 环指, 所包含的所有条件独立性
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；graph）或者三角形化图 （triangulated  graph），因为我们现在可以用更；小的、三角的环来描述所有的环。要从弦图构建有向图   ，我们还需
          關鍵詞：要从弦图构建有向图, 因为我们现在可以用更, 或者三角形化图, 我们还需, 小的
        - 摘要：图16.12　将一个无向模型转化为一个有向模型。（左）这个无向模型无法转化为有向模型，因；为它有一个长度为4且不带有弦的环。具体说来，这个无向模型包含了两种不同的独立性，并且；不存在一个有向模型可以同时描述这两种性质：a⊥c｜{b,d}和b⊥d｜{a,c}。（中）为了将无向
          關鍵詞：不存在一个有向模型可以同时描述这两种性质, 为了将无向, 具体说来, 为它有一个长度为, 将一个无向模型转化为一个有向模型
        - 摘要：16.2.7　因子图
          關鍵詞：因子图
        - 摘要：因子图 （factor graph）是从无向模型中抽样的另一种方法，它可以解决；标准无向模型语法中图表达的模糊性。在无向模型中，每个φ函数的范；围必须是图中某个团的子集。我们无法确定每一个团是否含有一个作用
          關鍵詞：在无向模型中, 我们无法确定每一个团是否含有一个作用, 因子图, 每个, 它可以解决
        - 摘要：变量。图16.13给出了一个例子来说明因子图如何解决无向网络中的模；糊性。
          關鍵詞：给出了一个例子来说明因子图如何解决无向网络中的模, 糊性, 变量
        - 摘要：图16.13　因子图如何解决无向网络中模糊性的一个例子。（左）一个包含3个变量（a、b和c）；的团组成的无向网络。（中）对应这个无向模型的因子图。这个因子图有一个包含3个变量的因；子。（右）对应这个无向模型的另一种有效的因子图。这个因子图包含了3个因子，每个因子只
          關鍵詞：因子图如何解决无向网络中模糊性的一个例子, 的团组成的无向网络, 对应这个无向模型的因子图, 每个因子只, 个变量
        - 摘要：16.2.1　有向模型
          關鍵詞：有向模型
        - 摘要：16.2.2　无向模型
          關鍵詞：无向模型
        - 摘要：16.2.3　配分函数
          關鍵詞：配分函数
        - 摘要：16.2.4　基于能量的模型
          關鍵詞：基于能量的模型
        - 摘要：16.2.5　分离和d-分离
          關鍵詞：分离, 分离和
        - 摘要：16.2.6　在有向模型和无；向模型中转换
          關鍵詞：向模型中转换, 在有向模型和无
        - 摘要：16.2.7　因子图
          關鍵詞：因子图
    16.3：从图模型中采样
        - 摘要：图模型同样简化了从模型中采样的过程。
          關鍵詞：图模型同样简化了从模型中采样的过程
        - 摘要：有向图模型的一个优点是，可以通过一个简单高效的过程从模型所表示；的联合分布中产生样本，这个过程被称为原始采样；（ancestral
          關鍵詞：有向图模型的一个优点是, 可以通过一个简单高效的过程从模型所表示, 的联合分布中产生样本, 这个过程被称为原始采样
        - 摘要：原始采样的基本思想是将图中的变量x  i  使用拓扑排序，使得对于所有i；和j，如果x i 是x j 的一个父亲结点，则j大于i。然后可以按此顺序对变量；)，然后采
          關鍵詞：如果, 然后可以按此顺序对变量, 的一个父亲结点, 使用拓扑排序, 原始采样的基本思想是将图中的变量
        - 摘要：∼P(x
          關鍵詞：
        - 摘要：1
          關鍵詞：
        - 摘要：有些图可能存在多个拓扑排序。原始采样可以使用这些拓扑排序中的任；何一个。
          關鍵詞：何一个, 有些图可能存在多个拓扑排序, 原始采样可以使用这些拓扑排序中的任
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；原始采样通常非常快（假设从每个条件分布中采样都是很容易的），并；且非常简便。
          關鍵詞：假设从每个条件分布中采样都是很容易的, 原始采样通常非常快, 且非常简便
        - 摘要：原始采样的一个缺点是其仅适用于有向图模型。另一个缺点是它并不是；每次采样都是条件采样操作。当我们希望从有向图模型中变量的子集中；采样时，给定一些其他变量，我们经常要求所有给定的条件变量在顺序
          關鍵詞：当我们希望从有向图模型中变量的子集中, 原始采样的一个缺点是其仅适用于有向图模型, 采样时, 给定一些其他变量, 另一个缺点是它并不是
        - 摘要：不幸的是，原始采样仅适用于有向模型。我们可以通过将无向模型转换；为有向模型来实现从无向模型中采样，但是这通常需要解决棘手的推断；问题（要确定新有向图的根节点上的边缘分布），或者需要引入许多
          關鍵詞：原始采样仅适用于有向模型, 要确定新有向图的根节点上的边缘分布, 或者需要引入许多, 问题, 不幸的是
    16.4：结构化建模的优势
        - 摘要：使用结构化概率模型的主要优点是，它们能够显著降低表示概率分布、；学习和推断的成本。有向模型中采样还可以被加速，但是对于无向模型；情况则较为复杂。选择不对某些变量的相互作用进行建模是允许所有这
          關鍵詞：它们能够显著降低表示概率分布, 但是对于无向模型, 使用结构化概率模型的主要优点是, 学习和推断的成本, 情况则较为复杂
        - 摘要：些操作使用较少的运行时间和内存的主要机制。图模型通过省略某些边；来传达信息。在没有边的情况下，模型假设不对变量之间直接的相互作；用建模。
          關鍵詞：用建模, 在没有边的情况下, 来传达信息, 模型假设不对变量之间直接的相互作, 图模型通过省略某些边
        - 摘要：结构化概率模型允许我们明确地将给定的现有知识与知识的学习或者推；断分开，这是一个不容易量化的益处。这使我们的模型更容易开发和调；试。我们可以设计、分析和评估适用于更广范围的图的学习算法和推断
          關鍵詞：这使我们的模型更容易开发和调, 结构化概率模型允许我们明确地将给定的现有知识与知识的学习或者推, 这是一个不容易量化的益处, 我们可以设计, 断分开
    16.5：学习依赖关系
        - 摘要：良好的生成模型需要准确地捕获所观察到的或“可见”变量v  上的分布。；通常v  的不同元素彼此高度依赖。在深度学习中，最常用于建模这些依；赖关系的方法是引入几个潜在或“隐藏”变量h  。然后，该模型可以捕获
          關鍵詞：可见, 的不同元素彼此高度依赖, 通常, 隐藏, 然后
        - 摘要：如果一个良好的关于v  的模型不包含任何潜变量，那么它在贝叶斯网络；中的每个节点需要具有大量父节点或在马尔可夫网络中具有非常大的；团。仅仅表示这些高阶相互作用的成本就很高了，首先从计算角度考
          關鍵詞：那么它在贝叶斯网络, 中的每个节点需要具有大量父节点或在马尔可夫网络中具有非常大的, 仅仅表示这些高阶相互作用的成本就很高了, 如果一个良好的关于, 首先从计算角度考
        - 摘要：当模型旨在描述直接连接的可见变量之间的依赖关系时，通常不可能连；接所有变量，因此设计图模型时需要连接那些紧密相关的变量，并忽略；其他变量之间的作用。机器学习中有一个称为结构学习
          關鍵詞：通常不可能连, 因此设计图模型时需要连接那些紧密相关的变量, 并忽略, 当模型旨在描述直接连接的可见变量之间的依赖关系时, 接所有变量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；使用潜变量而不是自适应结构避免了离散搜索和多轮训练的需要。可见；变量和潜变量之间的固定结构可以使用可见单元和隐藏单元之间的直接
          關鍵詞：可见, 变量和潜变量之间的固定结构可以使用可见单元和隐藏单元之间的直接, 使用潜变量而不是自适应结构避免了离散搜索和多轮训练的需要
        - 摘要：潜变量除了发挥本来的作用，即能够高效地描述p( v )以外，还具有另外；的优势。新变量h  还提供了v  的替代表示。例如，如第3.9.6节所示，高；斯混合模型学习了一个潜变量，这个潜变量对应于输入样本是从哪一个
          關鍵詞：还提供了, 即能够高效地描述, 的替代表示, 潜变量除了发挥本来的作用, 的优势
    16.6：推断和近似推断
        - 摘要：解决变量之间如何相互关联的问题是我们使用概率模型的一个主要方；式。给定一组医学测试，我们可以询问患者可能患有什么疾病。在一个；的特征
          關鍵詞：的特征, 在一个, 我们可以询问患者可能患有什么疾病, 给定一组医学测试, 解决变量之间如何相互关联的问题是我们使用概率模型的一个主要方
        - 摘要：用最大似然的准则来训练我们的模型。由于
          關鍵詞：用最大似然的准则来训练我们的模型, 由于
        - 摘要：)。所有这些都是推断；学习过程中，我们经常需要计算p(h  ｜；（inference）问题的例子，其中我们必须预测给定其他变量的情况下一
          關鍵詞：学习过程中, 问题的例子, 我们经常需要计算, 其中我们必须预测给定其他变量的情况下一, 所有这些都是推断
        - 摘要：ν
          關鍵詞：
        - 摘要：不幸的是，对于大多数有趣的深度模型来说，即使我们使用结构化图模；型来简化这些推断问题，它们仍然是难以处理的。图结构允许我们用合；理数量的参数来表示复杂的高维分布，但是用于深度学习的图并不满足
          關鍵詞：对于大多数有趣的深度模型来说, 理数量的参数来表示复杂的高维分布, 型来简化这些推断问题, 不幸的是, 图结构允许我们用合
        - 摘要：我们可以直接看出，计算一般图模型的边缘概率是#P-hard的。复杂性类；别#P是复杂性类别NP的泛化。NP中的问题只需确定其中一个问题是否；有解决方案，并找到一个解决方案（如果存在）就可以解决。#P中的问
          關鍵詞：就可以解决, 复杂性类, 是复杂性类别, 中的问题只需确定其中一个问题是否, 计算一般图模型的边缘概率是
        - 摘要：这促使我们使用近似推断。在深度学习中，这通常涉及变分推断，其中；通过寻求尽可能接近真实分布的近似分布q(h  ｜v  )来逼近真实分布p(h；｜ ν )。这个技术将在第19章中深入讨论。
          關鍵詞：这通常涉及变分推断, 其中, 在深度学习中, 通过寻求尽可能接近真实分布的近似分布, 来逼近真实分布
    16.7：结构化概率模型的深度学习方法
        - 摘要：16.7.1　实例：受限玻尔兹曼机
          關鍵詞：受限玻尔兹曼机, 实例
        - 摘要：深度学习从业者通常与其他从事结构化概率模型研究的机器学习研究者；使用相同的基本计算工具。然而，在深度学习中，我们通常对如何组合；这些工具作出不同的设计决定，导致总体算法、模型与更传统的图模型
          關鍵詞：模型与更传统的图模型, 深度学习从业者通常与其他从事结构化概率模型研究的机器学习研究者, 使用相同的基本计算工具, 导致总体算法, 然而
        - 摘要：深度学习并不总是涉及特别深的图模型。在图模型中，我们可以根据图；模型的图而不是计算图来定义模型的深度。如果从潜变量h i 到可观察变；量的最短路径是j步，我们可以认为潜变量h  j  处于深度j。我们通常将模
          關鍵詞：在图模型中, 深度学习并不总是涉及特别深的图模型, 我们可以根据图, 如果从潜变量, 到可观察变
        - 摘要：深度学习基本上总是利用分布式表示的思想。即使是用于深度学习目的；的浅层模型（例如预训练浅层模型，稍后将形成深层模型），也几乎总；是具有单个大的潜变量层。深度学习模型通常具有比可观察变量更多的
          關鍵詞：也几乎总, 深度学习基本上总是利用分布式表示的思想, 即使是用于深度学习目的, 的浅层模型, 例如预训练浅层模型
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；潜变量。变量之间复杂的非线性相互作用通过多个潜变量的间接连接来；实现。
          關鍵詞：变量之间复杂的非线性相互作用通过多个潜变量的间接连接来, 实现, 潜变量
        - 摘要：相比之下，传统的图模型通常包含至少是偶尔观察到的变量，即使一些；训练样本中的许多变量随机地丢失。传统模型大多使用高阶项和结构学；习来捕获变量之间复杂的非线性相互作用。如果有潜变量，则它们的数
          關鍵詞：则它们的数, 即使一些, 习来捕获变量之间复杂的非线性相互作用, 如果有潜变量, 传统模型大多使用高阶项和结构学
        - 摘要：潜变量的设计方式在深度学习中也有所不同。深度学习从业者通常不希；望潜变量提前包含了任何特定的含义——训练算法可以自由地开发对特；定数据集建模所需要的概念。在事后解释潜变量通常是很困难的，但是
          關鍵詞：潜变量的设计方式在深度学习中也有所不同, 但是, 深度学习从业者通常不希, 望潜变量提前包含了任何特定的含义, 在事后解释潜变量通常是很困难的
        - 摘要：另一个明显的区别是深度学习方法中经常使用的连接类型。深度图模型；通常具有大的与其他单元组全连接的单元组，使得两个组之间的相互作；用可以由单个矩阵描述。传统的图模型具有非常少的连接，并且每个变
          關鍵詞：另一个明显的区别是深度学习方法中经常使用的连接类型, 深度图模型, 并且每个变, 用可以由单个矩阵描述, 传统的图模型具有非常少的连接
        - 摘要：最后，图模型的深度学习方法的一个主要特征在于对未知量的较高容忍；度。与简化模型直到它的每一个量都可以被精确计算不同的是，我们仅；仅直接使用数据运行或者是训练，以增强模型的能力。一般我们使用边
          關鍵詞：我们仅, 图模型的深度学习方法的一个主要特征在于对未知量的较高容忍, 一般我们使用边, 以增强模型的能力, 与简化模型直到它的每一个量都可以被精确计算不同的是
        - 摘要：16.7.1　实例：受限玻尔兹曼机
          關鍵詞：受限玻尔兹曼机, 实例
        - 摘要：Boltzmann
          關鍵詞：
        - 摘要：（Restricted
          關鍵詞：
        - 摘要：Machine，RBM）；受限玻尔兹曼机；（Smolensky，1986）或者簧风琴  （harmonium）是图模型如何用于深
          關鍵詞：受限玻尔兹曼机, 或者簧风琴, 是图模型如何用于深
        - 摘要：标准的RBM是具有二值的可见和隐藏单元的基于能量的模型。其能量；函数为
          關鍵詞：函数为, 其能量, 是具有二值的可见和隐藏单元的基于能量的模型, 标准的
        - 摘要：其中 b、c 和 W 都是无约束、实值的可学习参数。我们可以看到，模型；被分成两组单元： ν  和  h  ，它们之间的相互作用由矩阵  W  来描述。该；模型在图16.14中以图的形式描绘。该图能够使我们更清楚地发现，该
          關鍵詞：模型在图, 来描述, 其中, 我们可以看到, 被分成两组单元
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图16.14　一个画成马尔可夫网络形式的RBM
          關鍵詞：一个画成马尔可夫网络形式的
        - 摘要：对RBM结构的限制产生了良好的属性
          關鍵詞：结构的限制产生了良好的属性
        - 摘要：以及
          關鍵詞：以及
        - 摘要：独立的条件分布很容易计算。对于二元的受限玻尔兹曼机，我们可以得；到
          關鍵詞：独立的条件分布很容易计算, 对于二元的受限玻尔兹曼机, 我们可以得
        - 摘要：结合这些属性可以得到高效的块吉布斯采样；Gibbs；Sampling），它在同时采样所有 h 和同时采样所有 ν 之间交替。RBM模
          關鍵詞：和同时采样所有, 之间交替, 结合这些属性可以得到高效的块吉布斯采样, 它在同时采样所有
        - 摘要：（block
          關鍵詞：
        - 摘要：图16.15　训练好的RBM的样本及其权重。（左）用MNIST训练模型，然后用Gibbs采样进行采；样。每一列是一个单独的Gibbs采样过程。每一行表示另一个1000步后Gibbs采样的输出。连续；的样本之间彼此高度相关。（右）对应的权重向量。将本图结果与图13.2中描述的线性因子模
          關鍵詞：采样进行采, 每一行表示另一个, 连续, 中描述的线性因子模, 的样本之间彼此高度相关
        - 摘要：由于能量函数本身只是参数的线性函数，很容易获取能量函数的导数。；例如，
          關鍵詞：很容易获取能量函数的导数, 例如, 由于能量函数本身只是参数的线性函数
        - 摘要：这两个属性，高效的Gibbs采样和导数计算，使训练过程变得非常方；便。在第18章中，我们将看到，可以通过计算应用于这种来自模型样本；的导数来训练无向模型。
          關鍵詞：的导数来训练无向模型, 章中, 我们将看到, 高效的, 使训练过程变得非常方
        - 摘要：训练模型可以得到数据 v 的表示 h 。我们经常使用；一组描述 v 的特征。
          關鍵詞：的表示, 的特征, 一组描述, 训练模型可以得到数据, 我们经常使用
        - 摘要：作为
          關鍵詞：作为
        - 摘要：总的来说，RBM展示了典型的图模型深度学习方法：使用多层潜变；量，并由矩阵参数化层之间的高效相互作用来完成表示学习。
          關鍵詞：并由矩阵参数化层之间的高效相互作用来完成表示学习, 展示了典型的图模型深度学习方法, 总的来说, 使用多层潜变
        - 摘要：图模型为描述概率模型提供了一种优雅、灵活、清晰的语言。在后续的；章节中，我们将使用这种语言，以其他视角来描述各种各样的深度概率；模型。
          關鍵詞：章节中, 在后续的, 灵活, 我们将使用这种语言, 图模型为描述概率模型提供了一种优雅
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；(1)    自然图片指的是能够在正常的环境下被照相机拍摄的图片，不同于合成的图片，或者一个；网页的截图等。
          關鍵詞：网页的截图等, 不同于合成的图片, 或者一个, 自然图片指的是能够在正常的环境下被照相机拍摄的图片
        - 摘要：(2)  当我们希望“强调”从网络中计算出的值的“推断”本质，即强调这些值代表的是置信程度大小；而不是事件的频率时，Judea Pearl建议使用“贝叶斯网络”这个术语
          關鍵詞：强调, 推断, 建议使用, 这个术语, 即强调这些值代表的是置信程度大小
        - 摘要：(3)  图的一个团是图中结点的一个子集，并且其中的点是全连接的。
          關鍵詞：图的一个团是图中结点的一个子集, 并且其中的点是全连接的
        - 摘要：(4)  一个通过归一化团势能乘积定义的分布也被称作 吉布斯分布 （Gibbs distribution）。
          關鍵詞：吉布斯分布, 一个通过归一化团势能乘积定义的分布也被称作
        - 摘要：(5)  对于某些模型，我们可以仍然使用约束优化方法来确保Z存在。
          關鍵詞：对于某些模型, 我们可以仍然使用约束优化方法来确保, 存在
        - 摘要：16.7.1　实例：受限玻尔；兹曼机
          關鍵詞：实例, 受限玻尔, 兹曼机
第17章：蒙特卡罗方法
    16.7：结构化概率模型的深度学习方法
        - 摘要：Vegas算法和蒙特卡罗算法。Las
          關鍵詞：算法和蒙特卡罗算法
        - 摘要：随机算法可以粗略地分为两类：Las；Vegas算法总是精确地返回一个正确答案（或者返回算法失败了）。这；类方法通常需要占用随机量的计算资源（一般指内存或运行时间）。与
          關鍵詞：一般指内存或运行时间, 随机算法可以粗略地分为两类, 或者返回算法失败了, 算法总是精确地返回一个正确答案, 类方法通常需要占用随机量的计算资源
        - 摘要：对于机器学习中的许多问题来说，我们很难得到精确的答案。这类问题；很难用精确的确定性算法如Las Vegas算法解决。取而代之的是确定性的；近似算法或蒙特卡罗近似方法。这两种方法在机器学习中都非常普遍。
          關鍵詞：取而代之的是确定性的, 这两种方法在机器学习中都非常普遍, 对于机器学习中的许多问题来说, 近似算法或蒙特卡罗近似方法, 这类问题
    17.1：采样和蒙特卡罗方法
        - 摘要：17.1.1　为什么需要采样
          關鍵詞：为什么需要采样
        - 摘要：17.1.2　蒙特卡罗采样的基础
          關鍵詞：蒙特卡罗采样的基础
        - 摘要：机器学习中的许多重要工具都基于从某种分布中采样，以及用这些样本；对目标量做一个蒙特卡罗估计。
          關鍵詞：机器学习中的许多重要工具都基于从某种分布中采样, 对目标量做一个蒙特卡罗估计, 以及用这些样本
        - 摘要：17.1.1　为什么需要采样
          關鍵詞：为什么需要采样
        - 摘要：有许多原因使我们希望从某个分布中采样。当我们需要以较小的代价近；似许多项的和或某个积分时，采样是一种很灵活的选择。有时候，我们；使用它加速一些很费时却易于处理的求和估计，就像我们使用小批量对
          關鍵詞：就像我们使用小批量对, 采样是一种很灵活的选择, 当我们需要以较小的代价近, 我们, 有时候
        - 摘要：整个训练代价进行子采样一样。在其他情况下，我们需要近似一个难以；处理的求和或积分，例如估计一个无向模型中配分函数对数的梯度时。；在许多其他情况下，抽样实际上是我们的目标，例如我们想训练一个可
          關鍵詞：例如估计一个无向模型中配分函数对数的梯度时, 我们需要近似一个难以, 整个训练代价进行子采样一样, 处理的求和或积分, 抽样实际上是我们的目标
        - 摘要：17.1.2　蒙特卡罗采样的基础
          關鍵詞：蒙特卡罗采样的基础
        - 摘要：当无法精确计算和或积分（例如，和具有指数数量个项，且无法被精确；简化）时，通常可以使用蒙特卡罗采样来近似它。这种想法把和或者积；分视作某分布下的期望，然后通过估计对应的平均值来近似这个期望。
          關鍵詞：当无法精确计算和或积分, 这种想法把和或者积, 分视作某分布下的期望, 然后通过估计对应的平均值来近似这个期望, 简化
        - 摘要：或者
          關鍵詞：或者
        - 摘要：为我们所需要估计的和或者积分，写成期望的形式，p是一个关于随机；变量x 的概率分布（求和时）或者概率密度函数（求积分时）。
          關鍵詞：的概率分布, 或者概率密度函数, 为我们所需要估计的和或者积分, 写成期望的形式, 是一个关于随机
        - 摘要：我们可以通过从p中抽取n个样本 x  (1)  ，…，  x  (n)  来近似s并得到一个经；验平均值
          關鍵詞：我们可以通过从, 并得到一个经, 个样本, 中抽取, 验平均值
        - 摘要：下面几个性质表明了这种近似的合理性。首先很容易观察到   这个估计；是无偏的，由于
          關鍵詞：这个估计, 下面几个性质表明了这种近似的合理性, 由于, 首先很容易观察到, 是无偏的
        - 摘要：此外，根据大数定理 （Law of large number），如果样本 x  (i)  是独立同；分布的，那么其平均值几乎必然收敛到期望值，即
          關鍵詞：那么其平均值几乎必然收敛到期望值, 是独立同, 此外, 根据大数定理, 如果样本
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；只需要满足各个单项的方差Var［f(  x  (i)  )］有界。详细地说，我们考虑；当n增大时  的方差。只要满足Var［f( x  (i) )］＜∞，方差
          關鍵詞：有界, 详细地说, 我们考虑, 只需要满足各个单项的方差, 方差
        - 摘要：这个简单有用的结果启迪我们如何估计蒙特卡罗均值中的不确定性，或；者等价地说是蒙特卡罗估计的期望误差。我们计算了f( x  (i)  )的经验均值；和方差 (1) ，然后将估计的方差除以样本数n来得到
          關鍵詞：这个简单有用的结果启迪我们如何估计蒙特卡罗均值中的不确定性, 和方差, 者等价地说是蒙特卡罗估计的期望误差, 来得到, 然后将估计的方差除以样本数
        - 摘要：值以
          關鍵詞：值以
        - 摘要：为方差的正态分布。这使得我们可以利用正态分布
          關鍵詞：为方差的正态分布, 这使得我们可以利用正态分布
        - 摘要：的累积函数来估计  的置信区间。
          關鍵詞：的置信区间, 的累积函数来估计
        - 摘要：以上的所有结论都依赖于我们可以从基准分布p(x  )中轻易地采样，但是；这个假设并不是一直成立的。当我们无法从p中采样时，一个备选方案；是用第17.2节讲到的重要采样。一种更加通用的方式是构建一个收敛到
          關鍵詞：一个备选方案, 是用第, 但是, 当我们无法从, 节讲到的重要采样
        - 摘要：17.1.1　为什么需要采样
          關鍵詞：为什么需要采样
        - 摘要：17.1.2　蒙特卡罗采样的；基础
          關鍵詞：基础, 蒙特卡罗采样的
    17.2：重要采样
        - 摘要：如方程（17.2）所示，在蒙特卡罗方法中，对积分（或者和）分解，确；定积分中哪一部分作为概率分布p( x )以及哪一部分作为被积的函数f( x )；（我们感兴趣的是估计f(  x  )在概率分布p(  x  )下的期望）是很关键的一
          關鍵詞：分解, 我们感兴趣的是估计, 定积分中哪一部分作为概率分布, 下的期望, 是很关键的一
        - 摘要：在这里，我们从q分布中采样，然后估计   在此分布下的均值。许多
          關鍵詞：在这里, 在此分布下的均值, 许多, 我们从, 然后估计
        - 摘要：情况中，我们希望在给定p和f的情况下计算某个期望，这个问题既然是；求期望，那么很自然地p和f是一种分解选择。然而，如果考虑达到某给；定精度所需要的样本数量，这个问题最初的分解选择不是最优的选择。
          關鍵詞：这个问题既然是, 求期望, 如果考虑达到某给, 然而, 的情况下计算某个期望
        - 摘要：从式（17.8）所示的关系中可以发现，任意蒙特卡罗估计
          關鍵詞：所示的关系中可以发现, 任意蒙特卡罗估计, 从式
        - 摘要：可以被转化为一个重要采样的估计
          關鍵詞：可以被转化为一个重要采样的估计
        - 摘要：我们可以容易地发现估计的期望与q分布无关：
          關鍵詞：分布无关, 我们可以容易地发现估计的期望与
        - 摘要：然而，重要采样的方差可能对q的选择非常敏感。这个方差可以表示为
          關鍵詞：这个方差可以表示为, 然而, 的选择非常敏感, 重要采样的方差可能对
        - 摘要：方差想要取到最小值，q需要满足
          關鍵詞：方差想要取到最小值, 需要满足
        - 摘要：在这里Z表示归一化常数，选择适当的Z使得q  ∗  (  x  )之和或者积分为1。；一个更好的重要采样分布会把更多的权重放在被积函数较大的地方。事；实上，当f(  x  )的正负符号不变时，
          關鍵詞：在这里, 之和或者积分为, 的正负符号不变时, 选择适当的, 实上
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；∗  时已经解决了原问题。所以在实践中这种只需要采样一个样本的方法；往往是无法实现的。
          關鍵詞：时已经解决了原问题, 往往是无法实现的, 所以在实践中这种只需要采样一个样本的方法
        - 摘要：对于重要采样来说，任意q分布都是可行的（从得到一个期望上正确的；值的角度来说），q  ∗  指的是最优的q分布（从得到最小方差的角度上考；虑）。从q  ∗  中采样往往是不可行的，但是其他仍然能降低方差的q的选
          關鍵詞：对于重要采样来说, 从得到一个期望上正确的, 但是其他仍然能降低方差的, 分布都是可行的, 中采样往往是不可行的
        - 摘要：另一种方法是采用有偏重要采样  （biased  importance  sampling），这种；方法有一个优势，即不需要归一化的p或q分布。在处理离散变量时，有；偏重要采样估计可以表示为
          關鍵詞：这种, 另一种方法是采用有偏重要采样, 偏重要采样估计可以表示为, 分布, 方法有一个优势
        - 摘要：其中   和   分别是分布p和q的未经归一化的形式，  x  (i)  是从分布q中抽；取的样本。这种估计是有偏的，因为；，只有当n→∞且方
          關鍵詞：因为, 只有当, 中抽, 取的样本, 这种估计是有偏的
        - 摘要：一个好的q分布的选择可以显著地提高蒙特卡罗估计的效率，而一个糟；糕的q分布选择则会使效率更糟糕。我们回过头来看看方程式（17.12）
          關鍵詞：一个好的, 糕的, 我们回过头来看看方程式, 分布选择则会使效率更糟糕, 而一个糟
        - 摘要：会发现，如果存在一个q使得
          關鍵詞：如果存在一个, 使得, 会发现
        - 摘要：很大，那么这个估计的方差
          關鍵詞：那么这个估计的方差, 很大
        - 摘要：也会很大。当q( x )很小，而f( x )和p( x )都较大并且无法抵消q时，这种；情况会非常明显。q分布经常会取一些简单常用的分布使得我们能够从q；分布中容易地采样。当 x  是高维数据时，q分布的简单性使得它很难与p
          關鍵詞：分布中容易地采样, 这种, 也会很大, 情况会非常明显, 都较大并且无法抵消
        - 摘要：到了很多无用的样本（很小的数或零相加）。另一种相对少见的情况是；，相应的比值会非常大。正因为后一个
          關鍵詞：另一种相对少见的情况是, 正因为后一个, 到了很多无用的样本, 很小的数或零相加, 相应的比值会非常大
        - 摘要：事件是很少发生的，这种样本很难被采到，通常使得对s的估计出现了；典型的欠估计，很难被整体的过估计抵消。这样的不均匀情况在高维数；据屡见不鲜，因为在高维度分布中联合分布的动态域可能非常大。
          關鍵詞：这样的不均匀情况在高维数, 事件是很少发生的, 因为在高维度分布中联合分布的动态域可能非常大, 典型的欠估计, 这种样本很难被采到
        - 摘要：尽管存在上述的风险，但是重要采样及其变种在机器学习的应用中仍然；扮演着重要的角色，包括深度学习算法。例如，重要采样被应用于加速；训练具有大规模词表的神经网络语言模型的过程中（见第12.4.3.3节）
          關鍵詞：尽管存在上述的风险, 但是重要采样及其变种在机器学习的应用中仍然, 例如, 包括深度学习算法, 扮演着重要的角色
    17.3：马尔可夫链蒙特卡罗方法
        - 摘要：在许多实例中，我们希望采用蒙特卡罗方法，然而往往又不存在一种简；单的方法可以直接从目标分布p model (x )中精确采样或者一个好的（方差；较小的）重要采样分布q(  x  )。在深度学习中，当分布p  model  (x  )表示成
          關鍵詞：然而往往又不存在一种简, 单的方法可以直接从目标分布, 较小的, 在许多实例中, 在深度学习中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；立。
          關鍵詞：
        - 摘要：为了解释从基于能量的模型中采样困难的原因，我们考虑一个包含两个；变量的EBM的例子，记p(a,b)为其分布。为了采a，我们必须先从p(a｜b)；中采样；为了采b，我们又必须从p(b｜a)中采样。这似乎成了棘手的先
          關鍵詞：为了解释从基于能量的模型中采样困难的原因, 为了采, 为其分布, 我们必须先从, 的例子
        - 摘要：在EBM中，我们通过使用马尔可夫链来采样，从而避免了先有鸡还是先；有蛋的问题。马尔可夫链的核心思想是从某个可取任意值的状态  x  出；发。随着时间的推移，我们随机地反复更新状态 x 。最终 x 成为了一个
          關鍵詞：有蛋的问题, 马尔可夫链的核心思想是从某个可取任意值的状态, 我们通过使用马尔可夫链来采样, 成为了一个, 从而避免了先有鸡还是先
        - 摘要：为了给出MCMC方法为何有效的一些理论解释，重参数化这个问题是很；有用的。首先我们关注一些简单的情况，其中随机变量x  有可数个状；态。我们将这种状态简单地记作正整数x。不同的整数x的大小对应着原
          關鍵詞：首先我们关注一些简单的情况, 为了给出, 有用的, 我们将这种状态简单地记作正整数, 有可数个状
        - 摘要：接下来我们考虑如果并行地运行无穷多个马尔可夫链的情况。不同马尔；可夫链的所有状态都采样自某一个分布q (t) (x)，在这里t表示消耗的时间；数。开始时，对每个马尔可夫链，我们采用一个分布q  0  来任意地初始
          關鍵詞：接下来我们考虑如果并行地运行无穷多个马尔可夫链的情况, 在这里, 可夫链的所有状态都采样自某一个分布, 来任意地初始, 不同马尔
        - 摘要：因为我们已经用正整数x重参数化了这个问题，我们可以用一个向量；来描述这个概率分布q，
          關鍵詞：我们可以用一个向量, 来描述这个概率分布, 因为我们已经用正整数, 重参数化了这个问题
        - 摘要：ν
          關鍵詞：
        - 摘要：然后我们考虑更新单一的马尔可夫链，从状态x到新状态x＇。单一状态；转移到x＇的概率可以表示为
          關鍵詞：然后我们考虑更新单一的马尔可夫链, 转移到, 的概率可以表示为, 从状态, 单一状态
        - 摘要：根据状态为整数的参数化设定，我们可以将转移算子T表示成一个矩阵；A 。矩阵 A 的定义如下：
          關鍵詞：根据状态为整数的参数化设定, 矩阵, 的定义如下, 表示成一个矩阵, 我们可以将转移算子
        - 摘要：使用这一定义，我们可以改写式（17.18）。不同于之前使用q和T来理；解单个状态的更新，我们现在可以使用 ν 和 A 来描述当我们更新时（并；行运行的）不同马尔可夫链上整个分布是如何变化的：
          關鍵詞：使用这一定义, 不同马尔可夫链上整个分布是如何变化的, 来描述当我们更新时, 不同于之前使用, 我们可以改写式
        - 摘要：重复地使用马尔可夫链更新相当于重复地与矩阵 A 相乘。换言之，我们；可以认为这一过程就是关于 A 的幂乘：
          關鍵詞：相乘, 可以认为这一过程就是关于, 换言之, 重复地使用马尔可夫链更新相当于重复地与矩阵, 我们
        - 摘要：矩阵 A 有一种特殊的结构，因为它的每一列都代表一个概率分布。这样；的矩阵被称为随机矩阵  （Stochastic  Matrix）。如果对于任意状态x到任；意其他状态x＇存在一个t使得转移概率不为0，那么Perron-Frobenius定
          關鍵詞：的矩阵被称为随机矩阵, 使得转移概率不为, 到任, 如果对于任意状态, 矩阵
        - 摘要：这个过程导致了所有不等于1的特征值都衰减到0。在一些额外的较为宽；松的假设下，我们可以保证矩阵  A  只有一个对应特征值为1的特征向；量。所以这个过程收敛到平稳分布  （Stationary  Distribution），有时也
          關鍵詞：我们可以保证矩阵, 只有一个对应特征值为, 的特征向, 所以这个过程收敛到平稳分布, 有时也
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；这个条件也适用于收敛之后的每一步。这就是特征向量方程。作为收敛；的稳定点，  ν  一定是特征值为1所对应的特征向量。这个条件保证收敛
          關鍵詞：这个条件保证收敛, 作为收敛, 这个条件也适用于收敛之后的每一步, 所对应的特征向量, 的稳定点
        - 摘要：如果我们正确地选择了转移算子T，那么最终的平稳分布q将会等于我们；所希望采样的分布p。我们会将第17.4节介绍如何选择T。
          關鍵詞：所希望采样的分布, 那么最终的平稳分布, 我们会将第, 节介绍如何选择, 将会等于我们
        - 摘要：可数状态马尔可夫链的大多数性质可以被推广到连续状态的马尔可夫链；中。在这种情况下，一些研究者把这种马尔可夫链称为哈里斯链；（Harris Chain），但是我们将这两种情况都称为马尔可夫链。通常在一
          關鍵詞：但是我们将这两种情况都称为马尔可夫链, 在这种情况下, 通常在一, 一些研究者把这种马尔可夫链称为哈里斯链, 可数状态马尔可夫链的大多数性质可以被推广到连续状态的马尔可夫链
        - 摘要：这个方程的离散版本就相当于重新改写方程式（17.23）。当x  是离散值；时，这个期望对应着求和，而当x  是连续值时，这个期望对应的是积；分。
          關鍵詞：而当, 这个方程的离散版本就相当于重新改写方程式, 这个期望对应的是积, 是连续值时, 是离散值
        - 摘要：无论状态是连续的还是离散的，所有的马尔可夫链方法都包括重复、随；机地更新直到最后状态开始从均衡分布中采样。运行马尔可夫链直到它；达到均衡分布的过程通常被称为马尔可夫链的磨合  （Burning-in）过
          關鍵詞：所有的马尔可夫链方法都包括重复, 运行马尔可夫链直到它, 达到均衡分布的过程通常被称为马尔可夫链的磨合, 机地更新直到最后状态开始从均衡分布中采样, 无论状态是连续的还是离散的
        - 摘要：为100。
          關鍵詞：
        - 摘要：另一个难点是我们无法预先知道马尔可夫链需要运行多少步才能到达均；衡分布。这段时间通常被称为混合时间  （Mixing  Time）。检测一个马；尔可夫链是否达到平衡是很困难的。我们并没有足够完善的理论来解决
          關鍵詞：检测一个马, 我们并没有足够完善的理论来解决, 衡分布, 另一个难点是我们无法预先知道马尔可夫链需要运行多少步才能到达均, 这段时间通常被称为混合时间
    17.4：Gibbs采样
        - 摘要：目前为止我们已经了解了如何通过反复更新 x ←− x ＇∼T( x ＇｜ x  )从；一个分布q(  x  )中采样，然而我们还没有介绍过如何确定q(  x  )是否是一；个有效的分布。本书中将会描述两种基本的方法。第一种方法是从已经
          關鍵詞：是否是一, 个有效的分布, 本书中将会描述两种基本的方法, 第一种方法是从已经, 目前为止我们已经了解了如何通过反复更新
        - 摘要：在深度学习中，我们通常使用马尔可夫链从定义为基于能量的模型的分；布p  model ( x  )中采样。在这种情况下，我们希望马尔可夫链的q(  x )分布；就是p  model ( x )。为了得到所期望的q( x  )分布，我们必须选取合适的T(
          關鍵詞：我们通常使用马尔可夫链从定义为基于能量的模型的分, 就是, 我们必须选取合适的, 在这种情况下, 在深度学习中
        - 摘要：Gibbs采样  （Gibbs  Sampling）是一种概念简单而又有效的方法。它构；造一个从p  model  (  x  )中采样的马尔可夫链，其中在基于能量的模型中从；T(x ＇|x )采样是通过选择一个变量x i ，然后从p model 中该点关于在无向
          關鍵詞：是一种概念简单而又有效的方法, 其中在基于能量的模型中从, 然后从, 采样, 它构
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图   （定义了基于能量的模型结构）中邻接点的条件分布中采样。只；要一些变量在给定相邻变量时是条件独立的，那么这些变量就可以被同
          關鍵詞：定义了基于能量的模型结构, 中邻接点的条件分布中采样, 要一些变量在给定相邻变量时是条件独立的, 那么这些变量就可以被同
        - 摘要：model  中采样的马尔可夫链还存在其他备选方法。比如说，
          關鍵詞：比如说, 中采样的马尔可夫链还存在其他备选方法
        - 摘要：设计从p；Metropolis-Hastings算法在其他领域中广泛使用。不过在深度学习的无；向模型中，我们主要使用Gibbs采样，很少使用其他方法。改进采样技
          關鍵詞：设计从, 向模型中, 我们主要使用, 采样, 算法在其他领域中广泛使用
    17.5：不同的峰值之间的混合挑战
        - 摘要：17.5.1　不同峰值之间通过回火来混合
          關鍵詞：不同峰值之间通过回火来混合
        - 摘要：17.5.2　深度也许会有助于混合
          關鍵詞：深度也许会有助于混合
        - 摘要：使用MCMC方法的主要难点在于马尔可夫链的混合  （Mixing）通常不；理想。在理想情况下，从设计好的马尔可夫链中采出的连续样本之间是；完全独立的，而且在  x 空间中，马尔可夫链会按概率大小访问许多不同
          關鍵詞：完全独立的, 在理想情况下, 方法的主要难点在于马尔可夫链的混合, 通常不, 马尔可夫链会按概率大小访问许多不同
        - 摘要：然而，MCMC方法采出的样本可能会具有很强的相关性，尤其是在高维；的情况下，我们把这种现象称为慢混合甚至混合失败。具有缓慢混合的；MCMC方法可以被视为对能量函数无意地执行类似于带噪声的梯度下降
          關鍵詞：方法采出的样本可能会具有很强的相关性, 然而, 的情况下, 尤其是在高维, 具有缓慢混合的
        - 摘要：当我们考虑Gibbs采样算法（见第17.4节）时，这种现象格外明显。在这；种情况下，我们考虑在一定步数内从一个峰值移动到一个临近峰值的概；率。决定这个概率的是两个峰值之间的“能量障碍”的形状。隔着一个巨
          關鍵詞：这种现象格外明显, 在这, 的形状, 我们考虑在一定步数内从一个峰值移动到一个临近峰值的概, 当我们考虑
        - 摘要：图17.1　对于三种分布使用Gibbs采样所产生的路径，所有的分布马尔可夫链初始值都设为峰；值。（左）一个带有两个独立变量的多维正态分布。由于变量之间是相互独立的，Gibbs采样混；合得很好。（中）变量之间存在高度相关性的一个多维正态分布。变量之间的相关性使得马尔
          關鍵詞：由于变量之间是相互独立的, 采样所产生的路径, 一个带有两个独立变量的多维正态分布, 变量之间存在高度相关性的一个多维正态分布, 采样混
        - 摘要：举一个简单的例子，考虑两个变量a、b基于能量的模型，这两个变量都；是二值的，取值＋1或者−1。如果对某个较大的正数w，E(a,b)＝−wab，；那么这个模型传达了一个强烈的信息，a和b有相同的符号。当a＝1时用
          關鍵詞：是二值的, 取值, 如果对某个较大的正数, 这两个变量都, 那么这个模型传达了一个强烈的信息
        - 摘要：在更实际的问题中，这种挑战更加艰巨。因为在实际问题中我们不能仅；仅关注在两个峰值之间的转移，更要关注在多个峰值之间的转移。如果；由于峰值之间混合困难，而导致某几个这样的转移难以完成，那么得到
          關鍵詞：仅关注在两个峰值之间的转移, 如果, 那么得到, 这种挑战更加艰巨, 由于峰值之间混合困难
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；通过寻找一些高度依赖变量的组以及分块同时更新块（组）中的变量，；这个问题有时候是可以被解决的。然而不幸的是，当依赖关系很复杂
          關鍵詞：这个问题有时候是可以被解决的, 当依赖关系很复杂, 通过寻找一些高度依赖变量的组以及分块同时更新块, 中的变量, 然而不幸的是
        - 摘要：在定义了一个联合分布p  model ( x ，  h  )的潜变量模型中，我们经常通过；交替地从p model ( x | h )和p model ( h | x )中采样来达到抽 x 的目的。从快；速混合的角度上说，我们更希望p model ( h | x )有很大的熵。然而，从学
          關鍵詞：在定义了一个联合分布, 的目的, 中采样来达到抽, 速混合的角度上说, 然而
        - 摘要：图17.2　深度概率模型中一个混合缓慢问题的例证。每张图都是按照从左到右从上到下的顺序；的。（左）Gibbs采样从MNIST数据集训练成的深度玻尔兹曼机中采出的连续样本。这些连续的；样本之间非常相似。由于Gibbs采样作用于一个深度图模型，相似度更多地是基于语义而非原始
          關鍵詞：这些连续的, 采样从, 采样作用于一个深度图模型, 样本之间非常相似, 深度概率模型中一个混合缓慢问题的例证
        - 摘要：当感兴趣的分布对于每个类具有单独的流形结构时，所有这些问题都使；MCMC方法变得不那么有用：分布集中在许多峰值周围，并且这些峰值；由大量高能量区域分割。我们在许多分类问题中遇到的是这种类型的分
          關鍵詞：并且这些峰值, 当感兴趣的分布对于每个类具有单独的流形结构时, 分布集中在许多峰值周围, 方法变得不那么有用, 我们在许多分类问题中遇到的是这种类型的分
        - 摘要：布，由于峰值之间混合缓慢，它将使得MCMC方法非常缓慢地收敛。
          關鍵詞：由于峰值之间混合缓慢, 方法非常缓慢地收敛, 它将使得
        - 摘要：17.5.1　不同峰值之间通过回火来混合
          關鍵詞：不同峰值之间通过回火来混合
        - 摘要：当一个分布有一些陡峭的峰并且被低概率区域包围时，很难在分布的不；同峰值之间混合。一些加速混合的方法是基于构造一个概率分布替代目；标分布，这个概率分布的峰值没有那么高，峰值周围的低谷也没有那么
          關鍵詞：标分布, 这个概率分布的峰值没有那么高, 很难在分布的不, 当一个分布有一些陡峭的峰并且被低概率区域包围时, 同峰值之间混合
        - 摘要：基于能量的模型可以通过添加一个额外的控制峰值尖锐程度的参数β来；加强：
          關鍵詞：加强, 基于能量的模型可以通过添加一个额外的控制峰值尖锐程度的参数
        - 摘要：β参数可以被理解为温度  （temperature）的倒数，反映了基于能量的模；型的统计物理学起源。当温度趋近于0时，β趋近于无穷大，此时的基于；能量的模型是确定性的。当温度趋近于无穷大时，β趋近于0，基于能量
          關鍵詞：趋近于, 当温度趋近于, 反映了基于能量的模, 参数可以被理解为温度, 基于能量
        - 摘要：通常情况下，在β＝1时训练一个模型。但我们也可以利用其他温度，尤；其是β＜1的情况。回火  （tempering）作为一种通用的策略，它通过从β；＜1模型中采样来实现在p 1 的不同峰值之间快速混合。
          關鍵詞：通常情况下, 作为一种通用的策略, 的不同峰值之间快速混合, 回火, 的情况
        - 摘要：基于回火转移  （tempered  transition）（Neal，1994）的马尔可夫链临时；从高温度的分布中采样使其在不同峰值之间混合，然后继续从单位温度；的分布中采样。这些技巧被应用在一些模型比如RBM中
          關鍵詞：这些技巧被应用在一些模型比如, 基于回火转移, 的马尔可夫链临时, 然后继续从单位温度, 的分布中采样
        - 摘要：（parallel
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；界温度  （critical  temperatures）时温度转移算子必须设置得非常慢（因；为温度需要逐渐下降）来确保回火的有效性。
          關鍵詞：为温度需要逐渐下降, 时温度转移算子必须设置得非常慢, 来确保回火的有效性, 界温度
        - 摘要：17.5.2　深度也许会有助于混合
          關鍵詞：深度也许会有助于混合
        - 摘要：当我们从潜变量模型p( h , x )中采样时，我们可以发现如果p( h | x )将 x；编码得非常好，那么从p(  x  | h  )中采样时，并不会太大地改变  x  ，那么；混合结果会很糟糕。解决这个问题的一种方法是使得 h 成为一种将 x 编
          關鍵詞：解决这个问题的一种方法是使得, 编码得非常好, 成为一种将, 那么从, 并不会太大地改变
        - 摘要：尽管存在混合的难点，蒙特卡罗技术仍然是一个有用的工具，通常也是；最好的可用工具。事实上，在遇到难以处理的无向模型中的配分函数；时，蒙特卡罗方法仍然是最主要的工具，这将在下一章详细阐述。
          關鍵詞：事实上, 最好的可用工具, 通常也是, 蒙特卡罗技术仍然是一个有用的工具, 这将在下一章详细阐述
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)  通常我们会倾向于计算方差的无偏估计，它由偏差的平方和除以n−1而非n得到。
          關鍵詞：它由偏差的平方和除以, 通常我们会倾向于计算方差的无偏估计, 得到, 而非
        - 摘要：17.5.1　不同峰值之间通；过回火来混合
          關鍵詞：不同峰值之间通, 过回火来混合
        - 摘要：17.5.2　深度也许会有助；于混合
          關鍵詞：于混合, 深度也许会有助
第18章：直面配分函数
    17.5：不同的峰值之间的混合挑战
        - 摘要：在第16.2.2节中，我们看到许多概率模型（通常是无向图模型）由一个；未归一化的概率分布；来归一化  ，以获得一个有效的概率分布：
          關鍵詞：我们看到许多概率模型, 来归一化, 由一个, 节中, 未归一化的概率分布
        - 摘要：定义。我们必须通过除以配分函数Z(  θ  )
          關鍵詞：定义, 我们必须通过除以配分函数
        - 摘要：配分函数是未归一化概率所有状态的积分（对于连续变量）或求和（对；于离散变量）：
          關鍵詞：配分函数是未归一化概率所有状态的积分, 或求和, 对于连续变量, 于离散变量
        - 摘要：或者
          關鍵詞：或者
        - 摘要：对于很多有趣的模型而言，以上积分或求和难以计算。
          關鍵詞：以上积分或求和难以计算, 对于很多有趣的模型而言
        - 摘要：正如我们将在第20章看到的，有些深度学习模型被设计成具有一个易于；处理的归一化常数，或被设计成能够在不涉及计算p（x  ）的情况下使；用。然而，其他一些模型会直接面对难以计算的配分函数的挑战。在本
          關鍵詞：或被设计成能够在不涉及计算, 的情况下使, 然而, 其他一些模型会直接面对难以计算的配分函数的挑战, 处理的归一化常数
    18.1：对数似然梯度
        - 摘要：通过最大似然学习无向模型特别困难的原因在于配分函数依赖于参数。；对数似然相对于参数的梯度具有一项对应于配分函数的梯度：
          關鍵詞：通过最大似然学习无向模型特别困难的原因在于配分函数依赖于参数, 对数似然相对于参数的梯度具有一项对应于配分函数的梯度
        - 摘要：这是机器学习中非常著名的正相  （positive  phase）和负相  （negative；phase）的分解。
          關鍵詞：和负相, 这是机器学习中非常著名的正相, 的分解
        - 摘要：对于大多数感兴趣的无向模型而言，负相是困难的。没有潜变量或潜变；量之间很少相互作用的模型通常会有一个易于计算的正相。RBM的隐；藏单元在给定可见单元的情况下彼此条件独立，是一个典型的具有简单
          關鍵詞：量之间很少相互作用的模型通常会有一个易于计算的正相, 对于大多数感兴趣的无向模型而言, 的隐, 负相是困难的, 藏单元在给定可见单元的情况下彼此条件独立
        - 摘要：让我们进一步分析log Z的梯度：
          關鍵詞：让我们进一步分析, 的梯度
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；对于保证所有的x
          關鍵詞：对于保证所有的
        - 摘要：代替
          關鍵詞：代替
        - 摘要：都有p（x；：
          關鍵詞：都有
        - 摘要：）＞0的模型，我们可以用
          關鍵詞：我们可以用, 的模型
        - 摘要：上述推导对离散的 x 进行求和，对连续的 x 进行积分也可以得到类似结；果。在连续版本的推导中，使用在积分符号内取微分的莱布尼兹法则可；以得到等式
          關鍵詞：进行积分也可以得到类似结, 在连续版本的推导中, 使用在积分符号内取微分的莱布尼兹法则可, 进行求和, 以得到等式
        - 摘要：该等式只适用于   和；上的一些特定规范条件。在测度论术语；中，这些条件是：（1）对每一个 θ  而言，未归一化分布  必须是 x  的
          關鍵詞：该等式只适用于, 对每一个, 而言, 在测度论术语, 未归一化分布
        - 摘要：R( x )使得
          關鍵詞：使得
        - 摘要：。幸运的是，
          關鍵詞：幸运的是
        - 摘要：大多数感兴趣的机器学习模型都具有这些性质。
          關鍵詞：大多数感兴趣的机器学习模型都具有这些性质
        - 摘要：等式
          關鍵詞：等式
        - 摘要：是使用各种蒙特卡罗方法近似最大化（具有难计算配分函数模型的）似；然的基础。
          關鍵詞：是使用各种蒙特卡罗方法近似最大化, 具有难计算配分函数模型的, 然的基础
        - 摘要：蒙特卡罗方法为学习无向模型提供了直观的框架，我们能够在其中考虑；正相和负相。在正相中，我们增大从数据中采样得到的；。在
          關鍵詞：正相和负相, 蒙特卡罗方法为学习无向模型提供了直观的框架, 在正相中, 我们增大从数据中采样得到的, 我们能够在其中考虑
        - 摘要：在深度学习文献中，经常会看到用能量函数（式（16.7））来参数化
          關鍵詞：来参数化, 经常会看到用能量函数, 在深度学习文献中
        - 摘要：。在这种情况下，正相可以解释为压低训练样本的能量，负相
          關鍵詞：正相可以解释为压低训练样本的能量, 负相, 在这种情况下
        - 摘要：可以解释为提高模型抽出的样本的能量，如图18.1所示。
          關鍵詞：如图, 所示, 可以解释为提高模型抽出的样本的能量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；18.2　随机最大似然和对比散度
          關鍵詞：随机最大似然和对比散度
    18.2：随机最大似然和对比散度
        - 摘要：实现式（18.15）的一个朴素方法是，每次需要计算梯度时，磨合随机；初始化的一组马尔可夫链。当使用随机梯度下降进行学习时，这意味着；马尔可夫链必须在每次梯度步骤中磨合。这种方法引导下的训练过程如
          關鍵詞：马尔可夫链必须在每次梯度步骤中磨合, 每次需要计算梯度时, 实现式, 这种方法引导下的训练过程如, 的一个朴素方法是
        - 摘要：我们可以将最大化似然的MCMC方法视为在两种力之间平衡，一种力拉；高数据出现时的模型分布，一种拉低模型采样出现时的模型分布。图；18.1展示了这个过程。这两种力分别对应最大化
          關鍵詞：我们可以将最大化似然的, 一种力拉, 展示了这个过程, 方法视为在两种力之间平衡, 高数据出现时的模型分布
        - 摘要：和最小化log
          關鍵詞：和最小化
        - 摘要：算法18.1 一种朴素的MCMC算法，使用梯度上升最大化具有难以计算配
          關鍵詞：一种朴素的, 使用梯度上升最大化具有难以计算配, 算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；分函数的对数似然。
          關鍵詞：分函数的对数似然
        - 摘要：设步长  为一个小正数。
          關鍵詞：设步长, 为一个小正数
        - 摘要：设吉布斯步数k大到足以允许磨合。在小图像集上训练一个RBM大致；设为100。
          關鍵詞：设吉布斯步数, 大到足以允许磨合, 设为, 大致, 在小图像集上训练一个
        - 摘要：while 不收敛do
          關鍵詞：不收敛
        - 摘要：从训练集中采包含m个样本{x (1) ，…，x (m) }的小批量。
          關鍵詞：个样本, 的小批量, 从训练集中采包含
        - 摘要：初始化m个样本；态分布中采，或大致与模型边缘分布匹配的分布）。
          關鍵詞：初始化, 个样本, 或大致与模型边缘分布匹配的分布, 态分布中采
        - 摘要：为随机值（例如，从均匀或正
          關鍵詞：从均匀或正, 为随机值, 例如
        - 摘要：for i＝1 to k do
          關鍵詞：
        - 摘要：for j＝1 to m do
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：end while
          關鍵詞：
        - 摘要：图18.1　算法18.1角度的“正相”和“负相”。（左）在正相中，我们从数据分布中采样，然后推高；它们未归一化的概率。这意味着概率越高的数据点，未归一化的概率被推高得越多。（右）在；负相中，我们从模型分布中采样，然后压低它们未归一化的概率。这与正相的倾向相反，给未
          關鍵詞：负相, 我们从数据分布中采样, 算法, 它们未归一化的概率, 给未
        - 摘要：因为负相涉及从模型分布中抽样，所以我们可以认为它在找模型信任度；很高的点。因为负相减少了这些点的概率，它们一般被认为代表了模型；不正确的信念。在文献中，它们经常被称为“幻觉”或“幻想粒子”。事实
          關鍵詞：所以我们可以认为它在找模型信任度, 它们经常被称为, 因为负相涉及从模型分布中抽样, 在文献中, 因为负相减少了这些点的概率
        - 摘要：的梯度，在睡觉时会遵循
          關鍵詞：的梯度, 在睡觉时会遵循
        - 摘要：这样理解学习正相和负相的作用之后，我们设计了一个比算法18.1计算；代价更低的替代算法。简单的MCMC算法的计算成本主要来自每一步的；随机初始化磨合马尔可夫链。一个自然的解决方法是初始化马尔可夫链
          關鍵詞：简单的, 一个自然的解决方法是初始化马尔可夫链, 这样理解学习正相和负相的作用之后, 算法的计算成本主要来自每一步的, 我们设计了一个比算法
        - 摘要：算法18.2 对比散度算法，使用梯度上升作为优化过程。
          關鍵詞：对比散度算法, 使用梯度上升作为优化过程, 算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；设步长  为一个小正数。
          關鍵詞：设步长, 为一个小正数
        - 摘要：设吉布斯步数k大到足以让从p data 初始化并从p（x ； θ ）采样的马尔；可夫链混合。在小图像集上训练一个RBM大致设为1∼20。
          關鍵詞：初始化并从, 设吉布斯步数, 在小图像集上训练一个, 采样的马尔, 大到足以让从
        - 摘要：while 不收敛do
          關鍵詞：不收敛
        - 摘要：从训练集中采包含m个样本{x (1) ，…，x (m) }的小批量。
          關鍵詞：个样本, 的小批量, 从训练集中采包含
        - 摘要：for i＝1 to m do
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：for i＝1 to k do
          關鍵詞：
        - 摘要：for j＝1 to m do
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：end while
          關鍵詞：
        - 摘要：对比散度  （CD，或者是具有k个Gibbs步骤的CD-k）算法在每个步骤中
          關鍵詞：或者是具有, 步骤的, 算法在每个步骤中, 对比散度
        - 摘要：初始化马尔可夫链为采样自数据分布中的样本（Hinton，2000，；2010），如算法18.2所示。从数据分布中获取样本是计算代价最小的，；因为它们已经在数据集中了。初始时，数据分布并不接近模型分布，因
          關鍵詞：数据分布并不接近模型分布, 初始时, 从数据分布中获取样本是计算代价最小的, 因为它们已经在数据集中了, 如算法
        - 摘要：当然，CD仍然是真实负相的一个近似。CD未能定性地实现真实负相的；主要原因是，它不能抑制远离真实训练样本的高概率区域。这些区域在；模型上具有高概率，但是在数据生成区域上具有低概率，被称为虚假模
          關鍵詞：仍然是真实负相的一个近似, 但是在数据生成区域上具有低概率, 主要原因是, 未能定性地实现真实负相的, 这些区域在
        - 摘要：Carreira-Perpiñan  and  Hinton（2005）实验上证明CD估计偏向于RBM和；完全可见的玻尔兹曼机，因为它会收敛到与最大似然估计不同的点。他；们认为，由于偏差较小，CD可以作为一种计算代价低的方式来初始化
          關鍵詞：们认为, 实验上证明, 因为它会收敛到与最大似然估计不同的点, 估计偏向于, 可以作为一种计算代价低的方式来初始化
        - 摘要：and
          關鍵詞：
        - 摘要：在训练诸如RBM的浅层网络时CD是很有用的。反过来，这些可以堆叠；起来初始化更深的模型，如DBN或DBM。但是CD并不直接有助于训练；更深的模型。这是因为在给定可见单元样本的情况下，很难获得隐藏单
          關鍵詞：并不直接有助于训练, 的浅层网络时, 在训练诸如, 但是, 是很有用的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图18.2　一个虚假模态。说明对比散度（算法18.2）的负相为何无法抑制虚假模态的例子。一个；虚假模态指的是一个在模型分布中出现数据分布中却不存在的模式。由于对比散度从数据点中
          關鍵詞：一个虚假模态, 由于对比散度从数据点中, 算法, 的负相为何无法抑制虚假模态的例子, 虚假模态指的是一个在模型分布中出现数据分布中却不存在的模式
        - 摘要：CD算法可以被理解为惩罚某类模型，这类模型的马尔可夫链会快速改；变来自数据的输入。这意味着使用CD训练从某种程度上说类似于训练；自编码器。即使CD估计比一些其他训练方法具有更大偏差，但是它有
          關鍵詞：估计比一些其他训练方法具有更大偏差, 训练从某种程度上说类似于训练, 即使, 自编码器, 这意味着使用
        - 摘要：Sutskever and Tieleman（2010）表明，CD的更新方向不是任何函数的梯；度。这使得CD可能存在永久循环的情况，但在实践中这并不是一个严；重的问题。
          關鍵詞：这使得, 重的问题, 但在实践中这并不是一个严, 表明, 的更新方向不是任何函数的梯
        - 摘要：另一个解决CD中许多问题的不同策略是，在每个梯度步骤中初始化马；尔可夫链为先前梯度步骤的状态值。这个方法首先被应用数学和统计学；社群发现，命名为随机最大似然  （SML）（Younes，1998），后来又
          關鍵詞：后来又, 社群发现, 在每个梯度步骤中初始化马, 命名为随机最大似然, 这个方法首先被应用数学和统计学
        - 摘要：体可以参考算法18.3。这种方法的基本思想是，只要随机梯度算法得到；的步长很小，那么前一步骤的模型将类似于当前步骤的模型。因此，来；自先前模型分布的样本将非常接近来自当前模型分布的客观样本，用这
          關鍵詞：体可以参考算法, 这种方法的基本思想是, 那么前一步骤的模型将类似于当前步骤的模型, 因此, 只要随机梯度算法得到
        - 摘要：因为每个马尔可夫链在整个学习过程中不断更新，而不是在每个梯度步；骤中重新开始，马尔可夫链可以自由探索很远，以找到模型的所有峰；值。因此，SML比CD更不容易形成具有虚假模态的模型。此外，因为
          關鍵詞：因为, 马尔可夫链可以自由探索很远, 骤中重新开始, 因此, 此外
        - 摘要：算法18.3  随机最大似然/持续性对比散度算法，使用梯度上升作为优化；过程。
          關鍵詞：随机最大似然, 算法, 使用梯度上升作为优化, 持续性对比散度算法, 过程
        - 摘要：设步长  为一个小正数。
          關鍵詞：设步长, 为一个小正数
        - 摘要：设吉布斯步数k大到足以让从；采样的马尔可夫链磨；合（从采自p(x  ;  θ  )的样本开始）。在小图像集上训练一个RBM大致设
          關鍵詞：设吉布斯步数, 从采自, 大致设, 采样的马尔可夫链磨, 大到足以让从
        - 摘要：初始化m个样本；分布中采，或大致与模型边缘分布匹配的分布）。
          關鍵詞：初始化, 分布中采, 个样本, 或大致与模型边缘分布匹配的分布
        - 摘要：为随机值（例如，从均匀或正态
          關鍵詞：从均匀或正态, 为随机值, 例如
        - 摘要：while 不收敛do
          關鍵詞：不收敛
        - 摘要：从训练集中采包含m个样本{x (1) ，…，x (m) }的小批量。
          關鍵詞：个样本, 的小批量, 从训练集中采包含
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；for i＝1 to k do
          關鍵詞：
        - 摘要：for j＝1 to m do
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：end while
          關鍵詞：
        - 摘要：在k太小或太大时，随机梯度算法移动模型的速率比马尔可夫链在迭代；步中混合更快，此时SML容易变得不准确。不幸的是，这些值的容许范；围高度依赖于具体问题。现在还没有方法能够正式地测试马尔可夫链是
          關鍵詞：容易变得不准确, 步中混合更快, 随机梯度算法移动模型的速率比马尔可夫链在迭代, 围高度依赖于具体问题, 这些值的容许范
        - 摘要：从使用SML训练的模型中评估采样必须非常小心。在模型训练完之后，；有必要从一个随机起点初始化的新马尔可夫链抽取样本。用于训练的连；续负相链中的样本受到了模型最近几个版本的影响，会使模型看起来具
          關鍵詞：在模型训练完之后, 有必要从一个随机起点初始化的新马尔可夫链抽取样本, 用于训练的连, 会使模型看起来具, 从使用
        - 摘要：Berglund  and  Raiko（2013）进行了实验来检验由CD和SML进行梯度估；计带来的偏差和方差。结果证明CD比基于精确采样的估计具有更低的；方差。而SML有更高的方差。CD方差低的原因是，其在正相和负相中
          關鍵詞：比基于精确采样的估计具有更低的, 有更高的方差, 进行梯度估, 计带来的偏差和方差, 进行了实验来检验由
        - 摘要：所有基于MCMC从模型中抽取样本的方法在原则上几乎可以与MCMC；的任何变体一起使用。这意味着诸如SML这样的技术可以使用第17章中；描述的任何增强MCMC的技术（例如并行回火）来加以改进
          關鍵詞：章中, 所有基于, 从模型中抽取样本的方法在原则上几乎可以与, 例如并行回火, 来加以改进
        - 摘要：一种在学习期间加速混合的方法是，不改变蒙特卡罗采样技术，而是改；变模型的参数化和代价函数。快速持续性对比散度  （fast；persistent
          關鍵詞：而是改, 一种在学习期间加速混合的方法是, 不改变蒙特卡罗采样技术, 变模型的参数化和代价函数, 快速持续性对比散度
        - 摘要：现在的参数是以前的两倍多，将其逐个相加以定义原始模型的参数。快；速复制参数可以使用更大的学习率来训练，从而使其快速响应学习的负；相，并促使马尔可夫链探索新的区域。这能够使马尔可夫链快速混合，
          關鍵詞：将其逐个相加以定义原始模型的参数, 这能够使马尔可夫链快速混合, 从而使其快速响应学习的负, 速复制参数可以使用更大的学习率来训练, 并促使马尔可夫链探索新的区域
        - 摘要：本节介绍的基于MCMC的方法，一个关键优点是它们提供了log  Z梯度；和log  Z两块。然；的估计，因此我们可以从本质上将问题分解为
          關鍵詞：梯度, 的估计, 因此我们可以从本质上将问题分解为, 本节介绍的基于, 两块
        - 摘要：下限的方法。然而，本章介绍处理log  Z的大多数其他方法都和基于
          關鍵詞：本章介绍处理, 下限的方法, 然而, 的大多数其他方法都和基于
        - 摘要：边界的正相方法是不兼容的。
          關鍵詞：边界的正相方法是不兼容的
    18.3：伪似然
        - 摘要：蒙特卡罗近似配分函数及其梯度需要直接处理配分函数。有些其他方法；通过训练不需要计算配分函数的模型来绕开这个问题。这些方法大多数；都基于以下观察：无向概率模型中很容易计算概率的比率。这是因为配
          關鍵詞：这些方法大多数, 这是因为配, 有些其他方法, 通过训练不需要计算配分函数的模型来绕开这个问题, 无向概率模型中很容易计算概率的比率
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；伪似然正是基于条件概率可以采用这种基于比率的形式，因此可以在没；有配分函数的情况下进行计算。假设我们将x  分为a、b  和c  ，其中a  包
          關鍵詞：假设我们将, 其中, 有配分函数的情况下进行计算, 伪似然正是基于条件概率可以采用这种基于比率的形式, 因此可以在没
        - 摘要：以上计算需要边缘化a ，假设a 和c 包含的变量并不多，那么这将是非常；高效的操作。在极端情况下，a  可以是单个变量，c  可以为空，那么该；计算仅需要估计与单个随机变量值一样多的  。
          關鍵詞：可以是单个变量, 以上计算需要边缘化, 包含的变量并不多, 高效的操作, 可以为空
        - 摘要：不幸的是，为了计算对数似然，我们需要边缘化很多变量。如果总共有；n个变量，那么我们必须边缘化n−1个变量。根据概率的链式法则，我们；有
          關鍵詞：根据概率的链式法则, 个变量, 如果总共有, 那么我们必须边缘化, 我们
        - 摘要：在这种情况下，我们已经使a  尽可能小，但是c  可以大到x  2:n  。如果我；们简单地将c  移到b  中以减少计算代价，那么会发生什么呢？这便产生；了伪似然  （pseudolikelihood）（Besag，1975）目标函数，给定所有其
          關鍵詞：移到, 但是, 如果我, 了伪似然, 在这种情况下
        - 摘要：如果每个随机变量有k个不同的值，那么计算   需要k×n次估计，而计；算配分函数需要k n 次估计。
          關鍵詞：那么计算, 如果每个随机变量有, 个不同的值, 而计, 次估计
        - 摘要：这看起来似乎是一个没有道理的策略，但可以证明最大化伪似然的估计；是渐近一致的（Mase，1995）。当然，在数据集不趋近于大采样极限的；情况下，伪似然可能表现出与最大似然估计不同的结果。
          關鍵詞：但可以证明最大化伪似然的估计, 情况下, 这看起来似乎是一个没有道理的策略, 伪似然可能表现出与最大似然估计不同的结果, 是渐近一致的
        - 摘要：我们可以使用广义伪似然估计 （generalized pseudolikelihood estimator）；来权衡计算复杂度和最大似然表现的偏差（Huang and Ogata，2002）。；，i＝1，…，m作为变量的指
          關鍵詞：作为变量的指, 我们可以使用广义伪似然估计, 来权衡计算复杂度和最大似然表现的偏差
        - 摘要：基于伪似然的方法的性能在很大程度上取决于模型是如何使用的。对于；完全联合分布p(x  )模型的任务（例如密度估计和采样），伪似然通常效；果不好。对于在训练期间只需要使用条件分布的任务而言，它的效果比
          關鍵詞：例如密度估计和采样, 完全联合分布, 它的效果比, 基于伪似然的方法的性能在很大程度上取决于模型是如何使用的, 伪似然通常效
        - 摘要：伪似然估计的一个弱点是它不能与仅在；上提供下界的其他近似；一起使用，例如第19章中介绍的变分推断。这是因为   出现在了分母
          關鍵詞：上提供下界的其他近似, 这是因为, 一起使用, 伪似然估计的一个弱点是它不能与仅在, 章中介绍的变分推断
        - 摘要：伪似然比SML在每个梯度步骤中的计算代价要大得多，这是由于其对所；有条件进行显式计算。但是，如果每个样本只计算一个随机选择的条；件，那么广义伪似然和类似标准仍然可以很好地运行，从而使计算代价
          關鍵詞：在每个梯度步骤中的计算代价要大得多, 但是, 如果每个样本只计算一个随机选择的条, 那么广义伪似然和类似标准仍然可以很好地运行, 伪似然比
        - 摘要：虽然伪似然估计没有显式地最小化log Z，但是我们仍然认为它具有类似；负相的效果。每个条件分布的分母会使得学习算法降低所有仅具有一个
          關鍵詞：虽然伪似然估计没有显式地最小化, 每个条件分布的分母会使得学习算法降低所有仅具有一个, 但是我们仍然认为它具有类似, 负相的效果
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；变量不同于训练样本的状态的概率。
          關鍵詞：变量不同于训练样本的状态的概率
        - 摘要：读者可以参考Marlin  and  de  Freitas（2011）了解伪似然渐近效率的理论；分析。
          關鍵詞：了解伪似然渐近效率的理论, 分析, 读者可以参考
    18.4：得分匹配和比率匹配
        - 摘要：得分匹配（Hyvärinen，2005b）提供了另一种训练模型而不需要估计Z；，被；或其导数的一致性方法。对数密度关于参数的导数
          關鍵詞：得分匹配, 或其导数的一致性方法, 提供了另一种训练模型而不需要估计, 对数密度关于参数的导数
        - 摘要：该目标函数避免了微分配分函数Z带来的难题，因为Z不是  x  的函数，；。最初，得分匹配似乎有一个新的困难：计算数据；所以
          關鍵詞：因为, 计算数据, 该目标函数避免了微分配分函数, 的函数, 所以
        - 摘要：其中n是 x 的维度。
          關鍵詞：的维度, 其中
        - 摘要：因为得分匹配需要关于x  的导数，所以它不适用于具有离散数据的模；型，但是模型中的潜变量可以是离散的。
          關鍵詞：但是模型中的潜变量可以是离散的, 因为得分匹配需要关于, 所以它不适用于具有离散数据的模, 的导数
        - 摘要：类似于伪似然，得分匹配只有在我们能够直接估计；及其导数；的时候才有效。它与对
          關鍵詞：得分匹配只有在我们能够直接估计, 及其导数, 它与对, 的时候才有效, 类似于伪似然
        - 摘要：的模型估计，例如稀疏编码模型或深度玻尔兹曼机。虽然得分匹配可以；用于预训练较大模型的第一个隐藏层，但是它没有被用于预训练较大模；型的较深层网络。这可能是因为这些模型的隐藏层通常包含一些离散变
          關鍵詞：虽然得分匹配可以, 型的较深层网络, 但是它没有被用于预训练较大模, 的模型估计, 例如稀疏编码模型或深度玻尔兹曼机
        - 摘要：虽然得分匹配没有明确显示具有负相信息，但是它可以被视为使用特定；类型马尔可夫链的对比散度的变种（Hyvärinen，2007a）。在这种情况；下，马尔可夫链并没有采用Gibbs采样，而是采用一种由梯度引导局部
          關鍵詞：在这种情况, 虽然得分匹配没有明确显示具有负相信息, 但是它可以被视为使用特定, 马尔可夫链并没有采用, 采样
        - 摘要：Lyu（2009）将得分匹配推广到离散的情况（但是推导有误，后由；Marlin et  al. （2010）修正）。Marlin et al.  （2010）发现，广义得分匹；配  （generalized score matching，GSM）在许多样本观测概率为0的高维
          關鍵詞：修正, 将得分匹配推广到离散的情况, 后由, 的高维, 但是推导有误
        - 摘要：一种更成功地将得分匹配的基本想法扩展到离散数据的方法是比率匹配；（ratio  matching）（Hyvärinen，2007b）。比率匹配特别适用于二值数；据。比率匹配最小化以下目标函数在样本上的均值：
          關鍵詞：比率匹配最小化以下目标函数在样本上的均值, 一种更成功地将得分匹配的基本想法扩展到离散数据的方法是比率匹配, 比率匹配特别适用于二值数
        - 摘要：其中f(  x  ,j)返回j处位值取反的x  。比率匹配使用了与伪似然估计相同的；策略来绕开配分函数：配分函数会在两个概率的比率中抵消掉。Marlin；et  al.  （2010）发现，训练模型给测试集图像去噪时，比率匹配的效果
          關鍵詞：处位值取反的, 比率匹配的效果, 其中, 训练模型给测试集图像去噪时, 配分函数会在两个概率的比率中抵消掉
        - 摘要：类似于伪似然估计，比率匹配对每个数据点都需要n个   的估计，因此；每次更新的计算代价大约比SML的计算代价高出n倍。
          關鍵詞：的估计, 比率匹配对每个数据点都需要, 类似于伪似然估计, 因此, 每次更新的计算代价大约比
        - 摘要：与伪似然估计一样，我们可以认为比率匹配减小了所有只有一个变量不；同于训练样本的状态的概率。由于比率匹配特别适用于二值数据，这意；味着在与数据的汉明距离为1内的所有状态上，比率匹配都是有效的。
          關鍵詞：与伪似然估计一样, 这意, 同于训练样本的状态的概率, 我们可以认为比率匹配减小了所有只有一个变量不, 由于比率匹配特别适用于二值数据
        - 摘要：比率匹配还可以作为处理高维稀疏数据（例如词计数向量）的基础。这
          關鍵詞：比率匹配还可以作为处理高维稀疏数据, 例如词计数向量, 的基础
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；类稀疏数据对基于MCMC的方法提出了挑战，因为以密集格式表示数据；是非常消耗计算资源的，而只有在模型学会表示数据分布的稀疏性之
          關鍵詞：类稀疏数据对基于, 因为以密集格式表示数据, 是非常消耗计算资源的, 而只有在模型学会表示数据分布的稀疏性之, 的方法提出了挑战
        - 摘要：读者可以参考Marlin  and  de  Freitas（2011）了解比率匹配渐近效率的理；论分析。
          關鍵詞：了解比率匹配渐近效率的理, 论分析, 读者可以参考
    18.5：去噪得分匹配
        - 摘要：某些情况下，我们希望拟合以下分布来正则化得分匹配
          關鍵詞：我们希望拟合以下分布来正则化得分匹配, 某些情况下
        - 摘要：而不是拟合真实分布p  data  。分布q(  x  ｜  y  )是一个损坏过程，通常在形；成 x 的过程中会向 y 中添加少量噪声。
          關鍵詞：的过程中会向, 而不是拟合真实分布, 通常在形, 分布, 中添加少量噪声
        - 摘要：去噪得分匹配非常有用，因为在实践中，通常我们不能获取真实的p  data；，而只能得到其样本确定的经验分布。给定足够容量，任何一致估计都；会使p  model 成为一组以训练点为中心的Dirac分布。考虑在第5.4.5节介绍
          關鍵詞：给定足够容量, 考虑在第, 节介绍, 成为一组以训练点为中心的, 任何一致估计都
        - 摘要：回顾第14.5.1节，有一些自编码器训练算法等价于得分匹配或去噪得分；匹配。因此，这些自编码器训练算法也是解决配分函数问题的一种方；式。
          關鍵詞：有一些自编码器训练算法等价于得分匹配或去噪得分, 因此, 这些自编码器训练算法也是解决配分函数问题的一种方, 回顾第, 匹配
    18.6：噪声对比估计
        - 摘要：具有难求解的配分函数的大多数模型估计都没有估计配分函数。SML和；CD只估计对数配分函数的梯度，而不是估计配分函数本身。得分匹配；和伪似然避免了和配分函数相关的计算。
          關鍵詞：而不是估计配分函数本身, 只估计对数配分函数的梯度, 得分匹配, 和伪似然避免了和配分函数相关的计算, 具有难求解的配分函数的大多数模型估计都没有估计配分函数
        - 摘要：噪声对比估计  （noise-contrastive  estimation，NCE）（Gutmann  and；Hyvarinen，2010）采取了一种不同的策略。在这种方法中，模型估计；的概率分布被明确表示为
          關鍵詞：的概率分布被明确表示为, 噪声对比估计, 采取了一种不同的策略, 模型估计, 在这种方法中
        - 摘要：其中c是−log  Z(  θ  )的近似。噪声对比估计过程将c视为另一参数，使用；相同的算法同时估计  θ 和c，而不是仅仅估计  θ  。因此，所得到的log  p；model (x  )可能并不完全对应有效的概率分布，但随着c估计的改进，它将
          關鍵詞：相同的算法同时估计, 它将, 因此, 的近似, 其中
        - 摘要：这种方法不可能使用最大似然作为估计的标准。最大似然标准可以设置；c为任意大的值，而不是设置c以创建一个有效的概率分布。
          關鍵詞：最大似然标准可以设置, 这种方法不可能使用最大似然作为估计的标准, 为任意大的值, 而不是设置, 以创建一个有效的概率分布
        - 摘要：NCE将估计p(x  )的无监督学习问题转化为学习一个概率二元分类器，其；中一个类别对应模型生成的数据。该监督学习问题中的最大似然估计定；义了原始问题的渐近一致估计。
          關鍵詞：将估计, 义了原始问题的渐近一致估计, 中一个类别对应模型生成的数据, 该监督学习问题中的最大似然估计定, 的无监督学习问题转化为学习一个概率二元分类器
        - 摘要：具体地说，我们引入第二个分布，噪声分布  （noise  distribution）p  noise；(x  )。噪声分布应该易于估计和从中采样。我们现在可以构造一个联合x；和新二值变量y的模型。在新的联合模型中，我们指定
          關鍵詞：我们引入第二个分布, 我们指定, 噪声分布, 的模型, 和新二值变量
        - 摘要：和
          關鍵詞：
        - 摘要：换言之，y是一个决定我们从模型还是从噪声分布中生成x  的开关变；量。
          關鍵詞：的开关变, 换言之, 是一个决定我们从模型还是从噪声分布中生成
        - 摘要：我们可以在训练数据上构造一个类似的联合模型。在这种情况下，开关；。正式地，；变量决定是从数据
          關鍵詞：开关, 正式地, 在这种情况下, 我们可以在训练数据上构造一个类似的联合模型, 变量决定是从数据
        - 摘要：还是从噪声分布中抽取x
          關鍵詞：还是从噪声分布中抽取
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；，p  train  (x  ｜y＝1)＝p  data  (x
          關鍵詞：
        - 摘要：)，和p train (x ｜y＝0)＝p noise (x )。
          關鍵詞：
        - 摘要：现在我们可以应用标准的最大似然学习拟合p joint 到p  train 的监督 学习问；题：
          關鍵詞：现在我们可以应用标准的最大似然学习拟合, 学习问, 的监督
        - 摘要：joint  本质上是将逻辑回归模型应用于模型和噪声分布之间的对数
          關鍵詞：本质上是将逻辑回归模型应用于模型和噪声分布之间的对数
        - 摘要：分布p；概率之差：
          關鍵詞：概率之差, 分布
        - 摘要：因此，只要；估计（以便评估p；使用。
          關鍵詞：估计, 因此, 以便评估, 只要, 使用
        - 摘要：易于反向传播，并且如上所述，p noise 应易于；joint  ）和采样（以生成训练数据），那么NCE就易于
          關鍵詞：和采样, 并且如上所述, 就易于, 易于反向传播, 应易于
        - 摘要：NCE能够非常成功地应用于随机变量较少的问题，但即使随机变量有很；多可以取的值时，它也很有效。例如，它已经成功地应用于给定单词上；下文建模单词的条件分布（Mnih  and  Kavukcuoglu，2013）。虽然单词
          關鍵詞：下文建模单词的条件分布, 虽然单词, 它已经成功地应用于给定单词上, 但即使随机变量有很, 多可以取的值时
        - 摘要：当NCE应用于具有许多随机变量的问题时，其效率会变得较低。当逻辑；回归分类器发现某个变量的取值不大可能时，它会拒绝这个噪声样本。；这意味着在p  model  学习了基本的边缘统计之后，学习进程会大大减慢。
          關鍵詞：回归分类器发现某个变量的取值不大可能时, 其效率会变得较低, 它会拒绝这个噪声样本, 应用于具有许多随机变量的问题时, 当逻辑
        - 摘要：想象一个使用非结构化高斯噪声作为p  noise  来学习面部图像的模型。如；果p  model  学会了眼睛，就算没有学习任何其他面部特征，比如嘴，它也；会拒绝几乎所有的非结构化噪声样本。
          關鍵詞：来学习面部图像的模型, 想象一个使用非结构化高斯噪声作为, 比如嘴, 学会了眼睛, 它也
        - 摘要：噪声分布p  noise  必须是易于估计和采样的约束可能是过于严格的限制。；当p  noise  比较简单时，大多数采样可能与数据有着明显不同，而不会迫；使p model 进行显著改进。
          關鍵詞：比较简单时, 必须是易于估计和采样的约束可能是过于严格的限制, 大多数采样可能与数据有着明显不同, 进行显著改进, 而不会迫
        - 摘要：类似于得分匹配和伪似然，如果   只有下界，那么NCE不会有效。这；样的下界能够用于构建p  joint  (y＝1｜x  )的下界，但是它只能用于构建p；joint  (y＝0｜x  )（出现在一半的NCE对象中）的上界。同样地，p  noise  的
          關鍵詞：只有下界, 如果, 对象中, 类似于得分匹配和伪似然, 不会有效
        - 摘要：在每个梯度步骤之前，模型分布被复制来定义新的噪声分布时，NCE定；义了一个被称为自对比估计 （self-contrastive  estimation）的过程，其梯；度期望等价于最大似然的梯度期望（Goodfellow，2014）。特殊情况的
          關鍵詞：的过程, 特殊情况的, 度期望等价于最大似然的梯度期望, 在每个梯度步骤之前, 模型分布被复制来定义新的噪声分布时
        - 摘要：在训练样本和生成样本（使用模型能量函数定义分类器）之间进行分类；以得到模型的梯度的方法，已经在更早的时候以各种形式提出来；（Welling et al. ，2003b；Bengio，2009）。
          關鍵詞：之间进行分类, 以得到模型的梯度的方法, 使用模型能量函数定义分类器, 在训练样本和生成样本, 已经在更早的时候以各种形式提出来
        - 摘要：噪声对比估计是基于良好生成模型应该能够区分数据和噪声的想法。一；个密切相关的想法是，良好的生成模型能够生成分类器无法将其与数据；区分的样本。这个想法诞生了生成式对抗网络（第20.10.4节）。
          關鍵詞：区分的样本, 个密切相关的想法是, 这个想法诞生了生成式对抗网络, 噪声对比估计是基于良好生成模型应该能够区分数据和噪声的想法, 良好的生成模型能够生成分类器无法将其与数据
    18.7：估计配分函数
        - 摘要：18.7.1　退火重要采样
          關鍵詞：退火重要采样
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；18.7.2　桥式采样
          關鍵詞：桥式采样
        - 摘要：尽管本章中的大部分内容都在避免计算与无向图模型相关的难以计算的；配分函数Z（  θ  ），但在本节中我们将会讨论几种直接估计配分函数的；方法。
          關鍵詞：但在本节中我们将会讨论几种直接估计配分函数的, 配分函数, 尽管本章中的大部分内容都在避免计算与无向图模型相关的难以计算的, 方法
        - 摘要：估计配分函数可能会很重要，当希望计算数据的归一化似然时，我们会
          關鍵詞：我们会, 估计配分函数可能会很重要, 当希望计算数据的归一化似然时
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；需要它。在评估模型、监控训练性能和比较模型时，这通常是很重要；的。
          關鍵詞：需要它, 这通常是很重要, 在评估模型, 监控训练性能和比较模型时
        - 摘要：例如，假设我们有两个模型：概率分布为
          關鍵詞：假设我们有两个模型, 概率分布为, 例如
        - 摘要：的模型
          關鍵詞：的模型
        - 摘要：和概率分布为
          關鍵詞：和概率分布为
        - 摘要：。比较模型的常用方法是评估和比较两个模型分配给独；的模型；立同分布测试数据集的似然。假设测试集含m个样本{  x  (1)  ，…，  x  (m)
          關鍵詞：个样本, 的模型, 立同分布测试数据集的似然, 假设测试集含, 比较模型的常用方法是评估和比较两个模型分配给独
        - 摘要：，或等价地，如果
          關鍵詞：如果, 或等价地
        - 摘要：是一个比
          關鍵詞：是一个比
        - 摘要：那么我们说；更好的模型（或者，至少可以说，；它在测试集上是一个更好的模型），这是指它有一个更好的测试对数似
          關鍵詞：更好的模型, 这是指它有一个更好的测试对数似, 那么我们说, 它在测试集上是一个更好的模型, 或者
        - 摘要：因此，我们可以在不知道任一模型的配分函数，而只知道它们比率的情；况下，判断模型；更优。正如我们将很快看到
          關鍵詞：况下, 因此, 我们可以在不知道任一模型的配分函数, 而只知道它们比率的情, 正如我们将很快看到
        - 摘要：是否比模型
          關鍵詞：是否比模型
        - 摘要：然而，如果我们想要计算测试数据在；上的真实概率，；我们需要计算配分函数的真实值。如果我们知道两个配分函数的比率，
          關鍵詞：我们需要计算配分函数的真实值, 如果我们知道两个配分函数的比率, 然而, 上的真实概率, 如果我们想要计算测试数据在
        - 摘要：或
          關鍵詞：
        - 摘要：，并且知道两者中一个的实际值，比如说Z( θ  A )，那
          關鍵詞：并且知道两者中一个的实际值, 比如说
        - 摘要：么我们可以计算另一个的值：
          關鍵詞：么我们可以计算另一个的值
        - 摘要：一种估计配分函数的简单方法是使用蒙特卡罗方法，例如简单重要采；样。以下用连续变量积分来表示该方法，也可以替换积分为求和，很容；易将其应用到离散变量的情况。我们使用提议分布
          關鍵詞：也可以替换积分为求和, 很容, 我们使用提议分布, 一种估计配分函数的简单方法是使用蒙特卡罗方法, 例如简单重要采
        - 摘要：，其在配分函数Z 0 和未归一化分布
          關鍵詞：和未归一化分布, 其在配分函数
        - 摘要：上易于采样和估计。
          關鍵詞：上易于采样和估计
        - 摘要：在最后一行，我们使用蒙特卡罗估计，使用从p  0  (x  )中抽取的采样计算；积分   ，然后用未归一化的   和提议分布p  0  的比率对每个采样加；权。
          關鍵詞：和提议分布, 我们使用蒙特卡罗估计, 使用从, 积分, 然后用未归一化的
        - 摘要：这种方法使得我们可以估计配分函数之间的比率：
          關鍵詞：这种方法使得我们可以估计配分函数之间的比率
        - 摘要：然后该值可以直接比较式（18.39）中的两个模型。
          關鍵詞：然后该值可以直接比较式, 中的两个模型
        - 摘要：0  接近p
          關鍵詞：接近
        - 摘要：如果分布p；1  ，那么式（18.44）能够有效地估计配分函数；（Minka，2005）。不幸的是，大多数时候p  1  都很复杂（通常是多峰值
          關鍵詞：那么式, 通常是多峰值, 能够有效地估计配分函数, 大多数时候, 不幸的是
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；和中产生（相对的）可忽略的贡献。
          關鍵詞：和中产生, 相对的, 可忽略的贡献
        - 摘要：如果求和中只有少数几个具有显著权重的样本，那么将会由于高方差而；导致估计的效果很差。这可以通过估计  的方差来定量地理解：
          關鍵詞：那么将会由于高方差而, 如果求和中只有少数几个具有显著权重的样本, 导致估计的效果很差, 这可以通过估计, 的方差来定量地理解
        - 摘要：当重要性权重
          關鍵詞：当重要性权重
        - 摘要：存在显著偏差时，上式的值是最大的。
          關鍵詞：存在显著偏差时, 上式的值是最大的
        - 摘要：我们现在关注两个解决高维空间复杂分布上估计配分函数的方法：退火；重要采样和桥式采样。两者都始于上面介绍的简单重要采样方法，并且；都试图通过引入缩小p 0 和p 1 之间差距的中间分布，来解决p 0 远离p 1 的
          關鍵詞：远离, 之间差距的中间分布, 重要采样和桥式采样, 来解决, 退火
        - 摘要：18.7.1　退火重要采样
          關鍵詞：退火重要采样
        - 摘要：在D KL (p 0 ǁp 1 )很大的情况下（即p 0 和p 1 之间几乎没有重叠），一种称；为退火重要采样 （annealed importance sampling，AIS）的方法试图通过；引入中间分布来缩小这种差距（Jarzynski，1997；Neal，2001）。考虑
          關鍵詞：引入中间分布来缩小这种差距, 很大的情况下, 为退火重要采样, 之间几乎没有重叠, 一种称
        - 摘要：的第一个和最后一个分别是p 0 和p 1 。
          關鍵詞：的第一个和最后一个分别是
        - 摘要：这种方法使我们能够估计定义在高维空间多峰分布（例如训练RBM时；定义的分布）上的配分函数。我们从一个已知配分函数的简单模型（例；如，权重为零的RBM）开始，估计两个模型配分函数之间的比率。该
          關鍵詞：估计两个模型配分函数之间的比率, 例如训练, 定义的分布, 开始, 上的配分函数
        - 摘要：现在我们可以将比率
          關鍵詞：现在我们可以将比率
        - 摘要：写作
          關鍵詞：写作
        - 摘要：如果对于所有的
          關鍵詞：如果对于所有的
        - 摘要：，分布
          關鍵詞：分布
        - 摘要：和
          關鍵詞：
        - 摘要：足够接
          關鍵詞：足够接
        - 摘要：近，那么我们能够使用简单的重要采样来估计每个因子
          關鍵詞：那么我们能够使用简单的重要采样来估计每个因子
        - 摘要：，然后
          關鍵詞：然后
        - 摘要：使用这些得到  的估计。
          關鍵詞：的估计, 使用这些得到
        - 摘要：这些中间分布是从哪里来的呢？正如最先的提议分布p  0  是一种设计选；择，分布序列；也是如此。也就是说，它们可以被
          關鍵詞：这些中间分布是从哪里来的呢, 正如最先的提议分布, 是一种设计选, 也是如此, 分布序列
        - 摘要：为了从这些中间分布中采样，我们定义了一组马尔可夫链转移函数T  ηj  (；x ＇｜ x )，定义了给定 x 转移到 x ＇的条件概率分布。转移算子T ηj ( x；＇｜ x )定义如下，保持p ηj ( x )不变：
          關鍵詞：定义如下, 不变, 转移到, 为了从这些中间分布中采样, 转移算子
        - 摘要：这些转移可以被构造为任何马尔可夫链蒙特卡罗方法（例如，；Metropolis-Hastings，Gibbs），包括涉及多次遍历所有随机变量或其他；迭代的方法。
          關鍵詞：这些转移可以被构造为任何马尔可夫链蒙特卡罗方法, 迭代的方法, 例如, 包括涉及多次遍历所有随机变量或其他
        - 摘要：然后，AIS采样方法从p  0  开始生成样本，并使用转移算子从中间分布顺；序地生成采样，直到我们得到目标分布p 1 的采样。
          關鍵詞：直到我们得到目标分布, 并使用转移算子从中间分布顺, 序地生成采样, 采样方法从, 的采样
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；对于采样k，通过连接式（18.49）给出的中间分布之间的重要性权重，；我们可以导出目标重要性权重：
          關鍵詞：通过连接式, 给出的中间分布之间的重要性权重, 对于采样, 我们可以导出目标重要性权重
        - 摘要：为了避免诸如上溢的数值问题，最佳方法可能是通过加法或减法计算；log w (k) ，而不是通过概率乘法和除法计算w (k) 。
          關鍵詞：最佳方法可能是通过加法或减法计算, 为了避免诸如上溢的数值问题, 而不是通过概率乘法和除法计算
        - 摘要：利用由此定义的采样过程和式（18.52）中给出的重要性权重，配分函；数的比率估计如下所示：
          關鍵詞：数的比率估计如下所示, 利用由此定义的采样过程和式, 配分函, 中给出的重要性权重
        - 摘要：为了验证该过程定义的重要采样方案是否有效，我们可以展示（Neal，；2001）AIS过程对应着扩展状态空间上的简单重要采样，其中数据点采；样自乘积空间
          關鍵詞：为了验证该过程定义的重要采样方案是否有效, 过程对应着扩展状态空间上的简单重要采样, 样自乘积空间, 我们可以展示, 其中数据点采
        - 摘要：。为此，我们将扩展空间上的
          關鍵詞：为此, 我们将扩展空间上的
        - 摘要：其中  是由T a 定义的转移算子的逆（应用贝叶斯规则）：
          關鍵詞：定义的转移算子的逆, 是由, 其中, 应用贝叶斯规则
        - 摘要：将以上代入到式（18.55）给出的扩展状态空间上的联合分布中，我们；得到
          關鍵詞：给出的扩展状态空间上的联合分布中, 得到, 我们, 将以上代入到式
        - 摘要：通过上面给定的采样方案，现在我们可以从扩展样本上的联合提议分布；q上生成采样，联合分布如下：
          關鍵詞：通过上面给定的采样方案, 上生成采样, 联合分布如下, 现在我们可以从扩展样本上的联合提议分布
        - 摘要：式（18.59）给出了扩展空间上的联合分布。将
          關鍵詞：给出了扩展空间上的联合分布
        - 摘要：作为扩展状态空间上的提议分布（我们
          關鍵詞：作为扩展状态空间上的提议分布, 我们
        - 摘要：会从中抽样），重要性权重如下：
          關鍵詞：会从中抽样, 重要性权重如下
        - 摘要：这些权重和AIS上的权重相同。因此，我们可以将AIS解释为应用于扩；展状态上的简单重要采样，其有效性直接来源于重要采样的有效性。
          關鍵詞：上的权重相同, 我们可以将, 因此, 解释为应用于扩, 展状态上的简单重要采样
        - 摘要：退火重要采样首先由Jarzynski（1997）发现，然后由Neal（2001）再次；独立发现。目前它是估计无向概率模型的配分函数的最常用方法。其原；因可能与一篇有影响力的论文（Salakhutdinov  and  Murray，2008）有
          關鍵詞：其原, 独立发现, 因可能与一篇有影响力的论文, 再次, 然后由
        - 摘要：关于AIS估计性质（例如，方差和效率）的讨论，请参看；Neal（2001）。
          關鍵詞：请参看, 估计性质, 例如, 的讨论, 方差和效率
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；18.7.2　桥式采样
          關鍵詞：桥式采样
        - 摘要：类似于AIS，桥式采样（Bennett，1976）是另一种处理重要采样缺点的；方法。并非将一系列中间分布连接在一起，桥式采样依赖于单个分布p  ∗；（被称为桥），在已知配分函数的分布p  0  和分布p  1  （我们试图估计其
          關鍵詞：在已知配分函数的分布, 桥式采样, 和分布, 被称为桥, 我们试图估计其
        - 摘要：桥式采样估计比率Z  1  /Z  0  ：；之间重要性权重的比率，
          關鍵詞：之间重要性权重的比率, 桥式采样估计比率
        - 摘要：和   之间重要性权重期望与   和
          關鍵詞：之间重要性权重期望与
        - 摘要：如果仔细选择桥式采样p ∗ ，使其与p 0 和p  1  都有很大重合的话，那么桥；式采样能够允许两个分布（或更正式地，D  KL (p  0  ǁp  1  )）之间有较大差；距（相对标准重要采样而言）。
          關鍵詞：或更正式地, 那么桥, 都有很大重合的话, 式采样能够允许两个分布, 使其与
        - 摘要：可以表明，最优的桥式采样是
          關鍵詞：可以表明, 最优的桥式采样是
        - 摘要：，其中r＝Z  1  /Z  0  。这似乎是一个不可行的解决方案，因为它似乎需要；我们估计数值Z  1  /Z  0  。然而，可以从粗糙的r开始估计，然后使用得到；的桥式采样逐步迭代以改进估计（Neal，2005）。也就是说，我们会迭
          關鍵詞：我们估计数值, 然后使用得到, 我们会迭, 其中, 然而
        - 摘要：链接重要采样  　AIS和桥式采样各有优点。如果D  KL  (p  0  ǁp  1  )不太大；（由于p 0 和p 1 足够接近）的话，那么桥式采样能比AIS更高效地估计配；分函数比率。然而，如果对于单个分布p  ∗  而言，两个分布相距太远难
          關鍵詞：如果, 链接重要采样, 和桥式采样各有优点, 而言, 然而
        - 摘要：在训练期间估计配分函数 　虽然AIS已经被认为是用于估计许多无向模；型配分函数的标准方法，但是它在计算上代价很高，以致其在训练期间
          關鍵詞：在训练期间估计配分函数, 以致其在训练期间, 虽然, 型配分函数的标准方法, 但是它在计算上代价很高
        - 摘要：仍然不很实用。研究者探索了一些在训练过程中估计配分函数的替代方；法。
          關鍵詞：研究者探索了一些在训练过程中估计配分函数的替代方, 仍然不很实用
        - 摘要：使用桥式采样、短链AIS和并行回火的组合，Desjardins  et  al.  （2011）；设计了一种在训练过程中追踪RBM配分函数的方法。该策略的基础；是，在并行回火方法操作的每个温度下，RBM配分函数的独立估计会
          關鍵詞：设计了一种在训练过程中追踪, 短链, 配分函数的独立估计会, 使用桥式采样, 配分函数的方法
        - 摘要：本章中描述的工具提供了许多不同的方法，以解决难处理的配分函数问；题，但是在训练和使用生成模型时，可能会存在一些其他问题，其中最；重要的是我们接下来会遇到的难以推断的问题。
          關鍵詞：以解决难处理的配分函数问, 本章中描述的工具提供了许多不同的方法, 可能会存在一些其他问题, 重要的是我们接下来会遇到的难以推断的问题, 但是在训练和使用生成模型时
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)    NCE也适用于具有易于处理的、不需要引入额外参数c的配分函数问题。它已经是最令人感；兴趣的、估计具有复杂配分函数模型的方法。
          關鍵詞：它已经是最令人感, 估计具有复杂配分函数模型的方法, 也适用于具有易于处理的, 兴趣的, 不需要引入额外参数
        - 摘要：18.7.1　退火重要采样
          關鍵詞：退火重要采样
        - 摘要：18.7.2　桥式采样
          關鍵詞：桥式采样
第19章：近似推断
    18.7：估计配分函数
        - 摘要：许多概率模型很难训练的原因是很难进行推断。在深度学习中，通常我；们有一系列可见变量 ν 和一系列潜变量 h 。推断困难通常是指难以计算；p( h ｜ ν )或其期望。而这样的操作在一些诸如最大似然学习的任务中往
          關鍵詞：而这样的操作在一些诸如最大似然学习的任务中往, 们有一系列可见变量, 通常我, 在深度学习中, 和一系列潜变量
        - 摘要：许多仅含一个隐藏层的简单图模型会定义成易于计算p( h ｜ ν )或其期望；的形式，例如受限玻尔兹曼机和概率PCA。不幸的是，大多数具有多层；隐藏变量的图模型的后验分布都很难处理。对于这些模型而言，精确推
          關鍵詞：例如受限玻尔兹曼机和概率, 对于这些模型而言, 隐藏变量的图模型的后验分布都很难处理, 精确推, 不幸的是
        - 摘要：在本章中，我们将会介绍几个用来解决这些难以处理的推断问题的技；巧。稍后，在第20章中，我们还将描述如何将这些技巧应用到训练其他；方法难以奏效的概率模型中，如深度信念网络、深度玻尔兹曼机。
          關鍵詞：我们还将描述如何将这些技巧应用到训练其他, 章中, 我们将会介绍几个用来解决这些难以处理的推断问题的技, 深度玻尔兹曼机, 如深度信念网络
        - 摘要：在深度学习中难以处理的推断问题通常源于结构化图模型中潜变量之间
          關鍵詞：在深度学习中难以处理的推断问题通常源于结构化图模型中潜变量之间
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；的相互作用。读者可以参考图19.1的几个例子。这些相互作用既可能是；无向模型的直接相互作用，也可能是有向模型中同一个可见变量的共同
          關鍵詞：的相互作用, 无向模型的直接相互作用, 读者可以参考图, 也可能是有向模型中同一个可见变量的共同, 这些相互作用既可能是
        - 摘要：图19.1　深度学习中难以处理的推断问题通常是由于结构化图模型中潜变量的相互作用。这些；相互作用产生于一个潜变量与另一个潜变量或者当V-结构的子节点可观察时与更长的激活路径；相连。（左）一个隐藏单元存在连接的 半受限波尔兹曼机 （semi-restricted Boltzmann
          關鍵詞：深度学习中难以处理的推断问题通常是由于结构化图模型中潜变量的相互作用, 这些, 相互作用产生于一个潜变量与另一个潜变量或者当, 相连, 半受限波尔兹曼机
    19.1：把推断视作优化问题
        - 摘要：精确推断问题可以描述为一个优化问题，有许多方法正是由此解决了推；断的困难。通过近似这样一个潜在的优化问题，我们往往可以推导出近；似推断算法。
          關鍵詞：有许多方法正是由此解决了推, 通过近似这样一个潜在的优化问题, 似推断算法, 我们往往可以推导出近, 断的困难
        - 摘要：为了构造这样一个优化问题，假设有一个包含可见变量 ν 和潜变量 h 的；概率模型。我们希望计算观察数据的对数概率log p( ν ； θ )。有时候如；果边缘化消去 h 的操作很费时，会难以计算log p( ν ； θ )。作为替代，
          關鍵詞：果边缘化消去, 的操作很费时, 和潜变量, 会难以计算, 作为替代
        - 摘要：其中q是关于 h 的一个任意概率分布。
          關鍵詞：其中, 是关于, 的一个任意概率分布
        - 摘要：因为log  p(  ν  )和；之间的距离是由KL散度来衡量的，且KL；散度总是非负的，我们可以发现   总是小于等于所求的对数概率。当
          關鍵詞：因为, 散度来衡量的, 总是小于等于所求的对数概率, 散度总是非负的, 之间的距离是由
        - 摘要：令人吃惊的是，对于某些分布q，计算  可以变得相当简单。通过简单；的代数运算我们可以把  重写成一个更加简单的形式：
          關鍵詞：令人吃惊的是, 通过简单, 可以变得相当简单, 对于某些分布, 的代数运算我们可以把
        - 摘要：这也给出了证据下界的标准定义：
          關鍵詞：这也给出了证据下界的标准定义
        - 摘要：对于一个选择的合适分布q来说，   是容易计算的。对任意分布q的选；择来说，  提供了似然函数的一个下界。越好地近似p( h ｜ ν )的分布；q( h ｜ ν )，得到的下界就越紧，换言之，就是与log p( ν )更加接近。当
          關鍵詞：就是与, 更加接近, 是容易计算的, 提供了似然函数的一个下界, 得到的下界就越紧
        - 摘要：)＝p(  h  ｜  ν；。
          關鍵詞：
        - 摘要：因此我们可以将推断问题看作找一个分布q使得  最大的过程。精确推；断能够在包含分布p(  h  ｜  ν  )的函数族中搜索一个函数，完美地最大化；。在本章中，我们将会讲到如何通过近似优化寻找分布q的方法来推
          關鍵詞：的方法来推, 我们将会讲到如何通过近似优化寻找分布, 因此我们可以将推断问题看作找一个分布, 最大的过程, 完美地最大化
        - 摘要：导出不同形式的近似推断。我们可以通过限定分布q的形式或者使用并；不彻底的优化方法来使得优化的过程更加高效（却更粗略），但是优化；的结果是不完美的，不求彻底地最大化  ，而只要显著地提升  。
          關鍵詞：却更粗略, 但是优化, 不彻底的优化方法来使得优化的过程更加高效, 的结果是不完美的, 我们可以通过限定分布
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；无论我们选择什么样的分布q，  始终是一个下界。我们可以通过选择；一个更简单或更复杂的计算过程来得到对应的更松或更紧的下界。通过
          關鍵詞：我们可以通过选择, 无论我们选择什么样的分布, 始终是一个下界, 一个更简单或更复杂的计算过程来得到对应的更松或更紧的下界, 通过
    19.2：期望最大化
        - 摘要：我们介绍的第一个最大化下界   的算法是期望最大化  （expectation；maximization，EM）算法。在潜变量模型中，这是一个非常常见的训练；算法。在这里我们描述Neal and Hinton（1999）所提出的EM算法。与大
          關鍵詞：这是一个非常常见的训练, 在这里我们描述, 算法, 我们介绍的第一个最大化下界, 的算法是期望最大化
        - 摘要：EM算法由交替迭代，直到收敛的两步运算组成。
          關鍵詞：直到收敛的两步运算组成, 算法由交替迭代
        - 摘要：E步  （expectation  step）：令  θ  (0)  表示在这一步开始时的参数值。；对任何我们想要训练的（对所有的或者小批量数据均成立）索引为；i的训练样本 ν (i) ，令q( h (i) ｜ ν )＝p( h (i) ｜ ν (i) ； θ (0) )。通过这个
          關鍵詞：通过这个, 的训练样本, 索引为, 对所有的或者小批量数据均成立, 对任何我们想要训练的
        - 摘要：这可以被看作通过坐标上升算法来最大化   。在第一步中，我们更新；分布q来最大化  ，而在另一步中，我们更新 θ 来最大化  。
          關鍵詞：而在另一步中, 来最大化, 我们更新, 分布, 在第一步中
        - 摘要：基于潜变量模型的随机梯度上升可以被看作一个EM算法的特例，其中；M步包括了单次梯度操作。EM算法的其他变种可以实现多次梯度操；作。对一些模型族来说，M步甚至可以直接推出解析解，不同于其他方
          關鍵詞：算法的特例, 其中, 步包括了单次梯度操作, 不同于其他方, 对一些模型族来说
        - 摘要：法，在给定当前q的情况下直接求出最优解。
          關鍵詞：在给定当前, 的情况下直接求出最优解
        - 摘要：尽管E步采用的是精确推断，我们仍然可以将EM算法视作是某种程度上；的近似推断。具体地说，M步假设一个分布q可以被所有的  θ  值分享。；当M步越来越远离E步中的 θ (0) 时，这将会导致  和真实的log p( ν  )之
          關鍵詞：步中的, 步采用的是精确推断, 值分享, 我们仍然可以将, 算法视作是某种程度上
        - 摘要：EM算法还包含一些不同的见解。首先，它包含了学习过程的一个基本；框架，就是我们通过更新模型参数来提高整个数据集的似然，其中缺失；变量的值是通过后验分布来估计的。这种特定的性质并非EM算法独有
          關鍵詞：其中缺失, 它包含了学习过程的一个基本, 算法还包含一些不同的见解, 框架, 变量的值是通过后验分布来估计的
    19.3：最大后验推断和稀疏编码
        - 摘要：我们通常使用推断  （inference）这个术语来指代给定一些其他变量的情；况下计算某些变量概率分布的过程。当训练带有潜变量的概率模型时，；我们通常关注于计算p( h ｜ ν )。另一种可选的推断形式是计算一个缺失
          關鍵詞：另一种可选的推断形式是计算一个缺失, 这个术语来指代给定一些其他变量的情, 我们通常使用推断, 当训练带有潜变量的概率模型时, 我们通常关注于计算
        - 摘要：这被称作最大后验 （Maximum A Posteriori）推断，简称MAP推断。
          關鍵詞：简称, 这被称作最大后验, 推断
        - 摘要：MAP推断并不被视作一种近似推断，它只是精确地计算了最有可能的一；个 h ∗ 。然而，如果我们希望设计一个最大化  ( ν , h ,q)的学习过程，；那么把MAP推断视作是输出一个q值的学习过程是很有帮助的。在这种
          關鍵詞：推断并不被视作一种近似推断, 然而, 那么把, 推断视作是输出一个, 它只是精确地计算了最有可能的一
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；最优的q。
          關鍵詞：最优的
        - 摘要：我们回过头来看看第19.1节中所描述的精确推断，它指的是关于一个在；无限制的概率分布族中的分布q使用精确的优化算法来最大化
          關鍵詞：使用精确的优化算法来最大化, 它指的是关于一个在, 无限制的概率分布族中的分布, 节中所描述的精确推断, 我们回过头来看看第
        - 摘要：我们通过限定分布q属于某个分布族，能够使得MAP推断成为一种形式；的近似推断。具体地说，我们令分布q满足一个Dirac分布：
          關鍵詞：能够使得, 我们令分布, 推断成为一种形式, 分布, 满足一个
        - 摘要：这也意味着现在我们可以通过  µ  来完全控制分布q。将   中不随  µ  变；化的项丢弃，我们只需解决一个优化问题：
          關鍵詞：这也意味着现在我们可以通过, 中不随, 化的项丢弃, 来完全控制分布, 我们只需解决一个优化问题
        - 摘要：这等价于MAP推断问题
          關鍵詞：推断问题, 这等价于
        - 摘要：因此我们能够证明一种类似于EM算法的学习算法，其中我们轮流迭代；两步，一步是用MAP推断估计出 h ∗  ，另一步是更新 θ 来增大log p(  h  ∗；，  ν  )。从EM算法角度来看，这也是对   的一种形式的坐标上升，交
          關鍵詞：来增大, 算法的学习算法, 一步是用, 两步, 推断估计出
        - 摘要：MAP推断作为特征提取器以及一种学习机制被广泛地应用在了深度学习；中。它主要用于稀疏编码模型中。
          關鍵詞：推断作为特征提取器以及一种学习机制被广泛地应用在了深度学习, 它主要用于稀疏编码模型中
        - 摘要：我们回过头来看第13.4节中的稀疏编码。稀疏编码是一种在隐藏单元上；加上了诱导稀疏性的先验知识的线性因子的模型。一个常用的选择是可；分解的Laplace先验，表示为
          關鍵詞：一个常用的选择是可, 加上了诱导稀疏性的先验知识的线性因子的模型, 分解的, 表示为, 稀疏编码是一种在隐藏单元上
        - 摘要：可见的节点是由一个线性变化加上噪声生成的：
          關鍵詞：可见的节点是由一个线性变化加上噪声生成的
        - 摘要：分布p( h ｜ ν )难以计算，甚至难以表达。每一对h  i  ，h  j  变量都是 ν 的；母节点。这也意味着当 ν 可被观察时，图模型包含了一条连接h i 和h j 的；活跃路径。因此p(  h  ｜  ν  )中所有的隐藏单元都包含在了一个巨大的团
          關鍵詞：变量都是, 这也意味着当, 可被观察时, 每一对, 母节点
        - 摘要：分布p(  x  ｜  h  )的难处理性导致了对数似然及其梯度也很难得到。因此；我们不能使用精确的最大似然估计来进行学习。取而代之的是，我们通；过MAP推断以及最大化由以 h 为中心的Dirac分布所定义而成的ELBO来
          關鍵詞：我们通, 取而代之的是, 为中心的, 因此, 分布所定义而成的
        - 摘要：如果我们将训练集中所有的向量  h  拼成矩阵  H  ，并将所有的向量  ν  拼；起来组成矩阵 V ，那么稀疏编码问题意味着最小化
          關鍵詞：起来组成矩阵, 并将所有的向量, 如果我们将训练集中所有的向量, 那么稀疏编码问题意味着最小化, 拼成矩阵
        - 摘要：为了避免如极端小的  H  和极端大的  W  这样的病态的解，大多数稀疏编；码的应用包含了权重衰减或者对 H 列范数的限制。
          關鍵詞：为了避免如极端小的, 和极端大的, 码的应用包含了权重衰减或者对, 这样的病态的解, 大多数稀疏编
        - 摘要：我们可以通过交替迭代，分别关于  H  和  W  最小化J的方式来最小化J。；且两个子问题都是凸的。事实上，关于  W  的最小化问题就是一个线性；回归问题。然而关于这两个变量同时最小化J的问题通常并不是凸的。
          關鍵詞：回归问题, 然而关于这两个变量同时最小化, 事实上, 我们可以通过交替迭代, 的方式来最小化
        - 摘要：关于  H  的最小化问题需要某些特别设计的算法，例如特征符号搜索方；法（Lee et al. ，2007）。
          關鍵詞：例如特征符号搜索方, 关于, 的最小化问题需要某些特别设计的算法
    19.4：变分推断和变分学习
        - 摘要：19.4.1　离散型潜变量
          關鍵詞：离散型潜变量
        - 摘要：19.4.2　变分法
          關鍵詞：变分法
        - 摘要：19.4.3　连续型潜变量
          關鍵詞：连续型潜变量
        - 摘要：19.4.4　学习和推断之间的相互作用
          關鍵詞：学习和推断之间的相互作用
        - 摘要：我们已经说明过了为什么证据下界  ( ν , θ ,q)是log p( ν ； θ )的一个下
          關鍵詞：的一个下, 我们已经说明过了为什么证据下界
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；界，如何将推断看作关于分布q最大化  的过程，以及如何将学习看作；关于参数  θ  最大化   的过程。我们也讲到了EM算法在给定了分布q的
          關鍵詞：的过程, 如何将推断看作关于分布, 以及如何将学习看作, 关于参数, 我们也讲到了
        - 摘要：变分学习的核心思想就是在一个关于q的有约束的分布族上最大化  。；的难易度。一个典；选择这个分布族时应该考虑到计算
          關鍵詞：选择这个分布族时应该考虑到计算, 一个典, 变分学习的核心思想就是在一个关于, 的有约束的分布族上最大化, 的难易度
        - 摘要：一种常用的变分学习的方法是加入一些限制使得q是一个因子分布：
          關鍵詞：是一个因子分布, 一种常用的变分学习的方法是加入一些限制使得
        - 摘要：这被称为均值场  （mean-field）方法。更一般地说，我们可以通过选择；分布q的形式来选择任何图模型的结构，通过选择变量之间相互作用的；多少来灵活地决定近似程度的大小。这种完全通用的图模型方法被称为
          關鍵詞：我们可以通过选择, 通过选择变量之间相互作用的, 这种完全通用的图模型方法被称为, 的形式来选择任何图模型的结构, 这被称为均值场
        - 摘要：变分方法的优点是，我们不需要为分布q设定一个特定的参数化形式。；我们设定它如何分解，之后通过解决优化问题来找出在这些分解限制下；最优的概率分布。对离散型潜变量来说，这意味着我们使用传统的优化
          關鍵詞：变分方法的优点是, 我们设定它如何分解, 设定一个特定的参数化形式, 对离散型潜变量来说, 我们不需要为分布
        - 摘要：因为  ( ν , θ ,q)被定义成log p( ν ; θ )−D KL (q( h ｜ ν )ǁp( h ｜ ν ; θ ))，；我们可以认为关于q最大化  的问题等价于（关于q）最小化D  KL  (q(  h；｜ ν )ǁp( h ｜ ν ))。在这种情况下，我们要用q来拟合p。然而，与以前的
          關鍵詞：因为, 与以前的, 我们可以认为关于, 然而, 在这种情况下
        - 摘要：方法不同，我们使用KL散度的相反方向来拟合一个近似。当我们使用；最大似然估计来用模型拟合数据时，我们最小化D KL (p data ǁp model )。如；图3.6所示，这意味着最大似然鼓励模型在每一个数据达到高概率的地
          關鍵詞：这意味着最大似然鼓励模型在每一个数据达到高概率的地, 方法不同, 我们最小化, 我们使用, 散度的相反方向来拟合一个近似
        - 摘要：19.4.1　离散型潜变量
          關鍵詞：离散型潜变量
        - 摘要：关于离散型潜变量的变分推断相对来说比较直接。我们定义一个分布；q，通常分布q的每个因子都由一些离散状态的可查询表格定义。在最简；单的情况中，  h  是二值的并且我们做了均值场假定，分布q可以根据每
          關鍵詞：单的情况中, 我们定义一个分布, 通常分布, 关于离散型潜变量的变分推断相对来说比较直接, 分布
        - 摘要：的每一个元素都代表一个概率，即
          關鍵詞：的每一个元素都代表一个概率
        - 摘要：。
          關鍵詞：
        - 摘要：在确定了如何表示分布q以后，我们只需要优化它的参数。在离散型潜；变量模型中，这是一个标准的优化问题。基本上分布q的选择可以通过；任何优化算法解决，比如梯度下降算法。
          關鍵詞：以后, 这是一个标准的优化问题, 我们只需要优化它的参数, 比如梯度下降算法, 变量模型中
        - 摘要：因为它在许多学习算法的内循环中出现，所以这个优化问题必须可以很；快求解。为了追求速度，我们通常使用特殊设计的优化算法。这些算法；通常能够在极少的循环内解决一些小而简单的问题。一个常见的选择是
          關鍵詞：为了追求速度, 一个常见的选择是, 因为它在许多学习算法的内循环中出现, 通常能够在极少的循环内解决一些小而简单的问题, 所以这个优化问题必须可以很
        - 摘要：我们反复地更新  不同的元素直到满足收敛准则。
          關鍵詞：我们反复地更新, 不同的元素直到满足收敛准则
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；为了具体化这些描述，我们接下来会讲如何将变分推断应用到二值稀疏；编码  （binary  sparse  coding）模型（这里我们所描述的模型是Henniges
          關鍵詞：这里我们所描述的模型是, 编码, 我们接下来会讲如何将变分推断应用到二值稀疏, 为了具体化这些描述, 模型
        - 摘要：，是由模型通过添加高斯噪；在二值稀疏编码模型中，输入；声到m个或有或无的不同成分的和而生成的。每一个成分可以是开或者
          關鍵詞：每一个成分可以是开或者, 声到, 在二值稀疏编码模型中, 是由模型通过添加高斯噪, 个或有或无的不同成分的和而生成的
        - 摘要：其中 b 是一个可以学习的偏置集合， W 是一个可以学习的权值矩阵，β；是一个可以学习的对角精度矩阵。
          關鍵詞：是一个可以学习的偏置集合, 是一个可以学习的权值矩阵, 其中, 是一个可以学习的对角精度矩阵
        - 摘要：使用最大似然来训练这样一个模型需要对参数进行求导。我们考虑对其；中一个偏置进行求导的过程：
          關鍵詞：我们考虑对其, 中一个偏置进行求导的过程, 使用最大似然来训练这样一个模型需要对参数进行求导
        - 摘要：这需要计算p(  h ｜ ν )下的期望。不幸的是，p( h ｜ ν  )是一个很复杂的；分布。关于p( h ， ν )和p( h ｜ ν )的图结构可以参考图19.2。隐藏单元的；后验分布对应的是关于隐藏单元的完全图，所以相对于暴力算法，变量
          關鍵詞：下的期望, 所以相对于暴力算法, 后验分布对应的是关于隐藏单元的完全图, 的图结构可以参考图, 变量
        - 摘要：图19.2　包含4个隐藏单元的二值稀疏编码的图结构。（左）p( h , ν )的图结构。要注意边是有向；的，每两个隐藏单元都是每个可见单元的共父。（右）p( h , ν )的图结构。为了解释共父之间的；活跃路径，后验分布所有隐藏单元之间都有边
          關鍵詞：每两个隐藏单元都是每个可见单元的共父, 包含, 个隐藏单元的二值稀疏编码的图结构, 要注意边是有向, 后验分布所有隐藏单元之间都有边
        - 摘要：取而代之的是，我们可以应用变分推断和变分学习来解决这个难点。
          關鍵詞：取而代之的是, 我们可以应用变分推断和变分学习来解决这个难点
        - 摘要：我们可以做一个均值场近似：
          關鍵詞：我们可以做一个均值场近似
        - 摘要：二值稀疏编码中的潜变量是二值的，所以为了表示可分解的q我们假设；对m个Bernoulli分布q(h  i  ｜  ν  )建模。表示Bernoulli分布的一种很自然的；。为了避免计算中
          關鍵詞：表示, 为了避免计算中, 建模, 分布的一种很自然的, 我们假设
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；或者1。
          關鍵詞：或者
        - 摘要：我们将会看到变分推断方程理论上永远不会赋予   为0或者1。然而在；软件实现过程中，机器的舍入误差会导致0或者1的值。在二值稀疏编码；的软件实现中，我们希望使用一个没有限制的变分参数向量z以及通过
          關鍵詞：的软件实现中, 在二值稀疏编码, 机器的舍入误差会导致, 以及通过, 软件实现过程中
        - 摘要：。
          關鍵詞：
        - 摘要：在开始二值稀疏编码模型中变分学习的推导时，我们首先说明了均值场；近似的使用可以使得学习过程更加简单。
          關鍵詞：在开始二值稀疏编码模型中变分学习的推导时, 近似的使用可以使得学习过程更加简单, 我们首先说明了均值场
        - 摘要：证据下界可以表示为
          關鍵詞：证据下界可以表示为
        - 摘要：尽管这些方程从美学观点来看有些不尽如人意。它们展示了   可以被；表示为少量简单的代数运算。因此，证据下界   是易于处理的。我们；可以把  看作难以处理的对数似然函数的一个替代。
          關鍵詞：表示为少量简单的代数运算, 它们展示了, 是易于处理的, 因此, 尽管这些方程从美学观点来看有些不尽如人意
        - 摘要：原则上说，我们可以使用关于 ν 和 h 的梯度上升。这会成为一个推断和；学习算法的完美组合。但是，由于两个原因，我们往往不这么做。第一；点，对每一个 ν 我们需要存储  。我们通常更加偏向于那些不需要为每
          關鍵詞：我们可以使用关于, 我们通常更加偏向于那些不需要为每, 第一, 学习算法的完美组合, 但是
        - 摘要：态更新的向量，使得算法很难处理几十亿的样本。第二个原因就是为了；能够识别 ν 的内容，我们希望能够有能力快速提取特征  。在实际应用；场景中，我们需要在有限时间内计算出  。
          關鍵詞：的内容, 在实际应用, 能够识别, 场景中, 使得算法很难处理几十亿的样本
        - 摘要：由于以上两个原因，我们通常不会采用梯度下降来计算均值场参数  。；取而代之的是，我们使用不动点方程来快速估计。
          關鍵詞：我们使用不动点方程来快速估计, 我们通常不会采用梯度下降来计算均值场参数, 取而代之的是, 由于以上两个原因
        - 摘要：不动点方程的核心思想是，我们寻找一个关于  h  的局部极大点，满足；。我们无法同时高效地计算所有  的元素。然而，
          關鍵詞：不动点方程的核心思想是, 的元素, 满足, 然而, 我们无法同时高效地计算所有
        - 摘要：我们可以解决单个变量的问题：
          關鍵詞：我们可以解决单个变量的问题
        - 摘要：我们可以迭代地将这个解应用到i＝1，…，m，然后重复这个循环直到；我们满足了收敛准则。常见的收敛准则包含了当整个循环所改进的L不；超过预设的容差量时停止，或者是循环中改变的   不超过某个值时停
          關鍵詞：我们满足了收敛准则, 然后重复这个循环直到, 超过预设的容差量时停止, 或者是循环中改变的, 我们可以迭代地将这个解应用到
        - 摘要：在很多不同的模型中，迭代的均值场不动点方程是一种能够提供快速变；分推断的通用算法。为了使它更加具体，我们详细地讲一下如何推导出；二值稀疏编码模型的更新过程。
          關鍵詞：为了使它更加具体, 迭代的均值场不动点方程是一种能够提供快速变, 我们详细地讲一下如何推导出, 在很多不同的模型中, 分推断的通用算法
        - 摘要：首先，我们给出了对   的导数表达式。为了得到这个表达式，我们将；式（19.36）代入到式（19.37）的左边：
          關鍵詞：我们给出了对, 的导数表达式, 我们将, 为了得到这个表达式, 的左边
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；为了应用固定点更新的推断规则，我们通过令式（19.43）等于0来解；：
          關鍵詞：我们通过令式, 等于, 来解, 为了应用固定点更新的推断规则
        - 摘要：此时，我们可以发现图模型中的推断和循环神经网络之间存在着紧密的；联系。具体地说，均值场不动点方程定义了一个循环神经网络。这个神；经网络的任务就是完成推断。我们已经从模型描述的角度介绍了如何推
          關鍵詞：这个神, 经网络的任务就是完成推断, 我们可以发现图模型中的推断和循环神经网络之间存在着紧密的, 均值场不动点方程定义了一个循环神经网络, 我们已经从模型描述的角度介绍了如何推
        - 摘要：在二值稀疏编码模型中，我们可以发现式（19.44）中描述的循环网络；连接包含了根据相邻隐藏单元变化值来反复更新当前隐藏单元的操作。；，然而隐藏单元
          關鍵詞：然而隐藏单元, 我们可以发现式, 连接包含了根据相邻隐藏单元变化值来反复更新当前隐藏单元的操作, 在二值稀疏编码模型中, 中描述的循环网络
        - 摘要：我们将式（19.44）重写成等价的形式来揭示一些深层的含义：
          關鍵詞：我们将式, 重写成等价的形式来揭示一些深层的含义
        - 摘要：看作输入，而不是  ν；在这种新的形式中，我们可以将；。因此，我们可以把第i个单元视作给定其他单元编码时给  ν  中的剩余
          關鍵詞：在这种新的形式中, 我们可以将, 看作输入, 因此, 个单元视作给定其他单元编码时给
        - 摘要：在这个例子中，我们已经推导出了每一次更新单个结点的更新规则。如；果能够同时更新更多的结点，那会更令人满意。某些图模型，比如深度；玻尔兹曼机，我们可以同时解出  中的许多元素。不幸的是，二值稀疏
          關鍵詞：果能够同时更新更多的结点, 我们可以同时解出, 比如深度, 二值稀疏, 某些图模型
        - 摘要：19.4.2　变分法
          關鍵詞：变分法
        - 摘要：在继续介绍变分学习之前，我们有必要简单地介绍一种变分学习中重要；的数学工具：变分法 （calculus of variations）。
          關鍵詞：变分法, 在继续介绍变分学习之前, 我们有必要简单地介绍一种变分学习中重要, 的数学工具
        - 摘要：许多机器学习的技巧是基于寻找一个输入向量；来最小化函；数J( θ )，使得它取到最小值。这个步骤可以利用多元微积分以及线性代
          關鍵詞：这个步骤可以利用多元微积分以及线性代, 来最小化函, 许多机器学习的技巧是基于寻找一个输入向量, 使得它取到最小值
        - 摘要：函数f的函数被称为泛函  （functional）J［f］。正如许多情况下对一个；函数求关于以向量的元素为变量的偏导数一样，我们可以使用泛函导数；（functional derivative），即在任意特定的 x 值，对一个泛函J［f］求关
          關鍵詞：我们可以使用泛函导数, 对一个泛函, 函数, 的函数被称为泛函, 求关
        - 摘要：函J的关于函数f在点 x 处的泛函导数被记作
          關鍵詞：在点, 的关于函数, 处的泛函导数被记作
        - 摘要：。
          關鍵詞：
        - 摘要：完整正式的泛函导数的推导不在本书的范围之内。对于我们的目标而；言，了解可微分函数f( x )以及带有连续导数的可微分函数g(y, x )就足够；了：
          關鍵詞：完整正式的泛函导数的推导不在本书的范围之内, 就足够, 对于我们的目标而, 以及带有连续导数的可微分函数, 了解可微分函数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；为了使上述等式更加直观，我们可以把f( x )看作一个有着无穷不可数多；元素的向量，由一个实数向量  x  表示。在这里（看作一个不完全的介
          關鍵詞：表示, 在这里, 我们可以把, 看作一个有着无穷不可数多, 为了使上述等式更加直观
        - 摘要：在其他机器学习文献中的许多结果则使用了更为通用的欧拉—拉格朗日；方程  （Euler-Lagrange  Equation），它能够使得g不仅依赖于f的导数，；而且也依赖于f的值。但是在本书中我们不需要这个通用版本。
          關鍵詞：它能够使得, 但是在本书中我们不需要这个通用版本, 而且也依赖于, 方程, 在其他机器学习文献中的许多结果则使用了更为通用的欧拉
        - 摘要：为了关于一个向量优化某个函数，我们求出了这个函数关于这个向量的；梯度，然后找这个梯度中每一个元素都为0的点。类似地，我们可以通；过寻找一个函数使得泛函导数的每个点都等于0，从而来优化一个泛
          關鍵詞：梯度, 我们可以通, 我们求出了这个函数关于这个向量的, 过寻找一个函数使得泛函导数的每个点都等于, 从而来优化一个泛
        - 摘要：下面介绍一个该过程如何运行的例子，我们考虑寻找一个定义在
          關鍵詞：下面介绍一个该过程如何运行的例子, 我们考虑寻找一个定义在
        - 摘要：上的有最大微分熵的概率密度函数。我们回过头来看一下一
          關鍵詞：上的有最大微分熵的概率密度函数, 我们回过头来看一下一
        - 摘要：个概率分布P(x)的熵，定义如下：
          關鍵詞：的熵, 个概率分布, 定义如下
        - 摘要：对于连续的值，这个期望可以被看作一个积分：
          關鍵詞：这个期望可以被看作一个积分, 对于连续的值
        - 摘要：我们不能简单地仅仅关于函数P(x)最大化H［p］，因为那样的话结果可；能不是一个概率分布。为了解决这个问题，我们需要使用一个拉格朗日；乘子来添加一个分布P(x)积分值为1的约束。同样地，当方差增大时，
          關鍵詞：能不是一个概率分布, 积分值为, 乘子来添加一个分布, 同样地, 当方差增大时
        - 摘要：为了关于p最小化拉格朗日乘子，我们令泛函导数等于0：
          關鍵詞：最小化拉格朗日乘子, 为了关于, 我们令泛函导数等于
        - 摘要：这个条件告诉我们P(x)的泛函形式。通过代数运算重组上述方程，我们；可以得到
          關鍵詞：可以得到, 这个条件告诉我们, 我们, 通过代数运算重组上述方程, 的泛函形式
        - 摘要：我们并没有直接假设P(x)取这种形式，而是通过最小化泛函从理论上得；到了这个P(x)的表达式。为了解决这个最小化问题，我们需要选择λ的；值来确保所有的约束都能够满足。我们有很大的自由去选择λ。因为只
          關鍵詞：为了解决这个最小化问题, 而是通过最小化泛函从理论上得, 因为只, 的表达式, 我们需要选择
        - 摘要：，从而得到
          關鍵詞：从而得到
        - 摘要：这也是当我们不知道真实的分布时，总是使用正态分布的一个原因。因；为正态分布拥有最大的熵，我们通过这个假定来保证了最小可能量的结；构。
          關鍵詞：这也是当我们不知道真实的分布时, 我们通过这个假定来保证了最小可能量的结, 总是使用正态分布的一个原因, 为正态分布拥有最大的熵
        - 摘要：当寻找熵的拉格朗日泛函的临界点并且给定一个固定的方差时，我们只；能找到一个对应最大熵的临界点。那最小化熵的概率密度函数是什么样；的呢？为什么我们无法发现对应着极小点的第二个临界点呢？原因是没
          關鍵詞：原因是没, 我们只, 那最小化熵的概率密度函数是什么样, 当寻找熵的拉格朗日泛函的临界点并且给定一个固定的方差时, 的呢
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在一个收敛的概率分布的序列，收敛到权重都在两个点上。这种情况能；够退化为混合Dirac分布。因为Dirac分布并不是一个单独的概率密度函
          關鍵詞：因为, 够退化为混合, 分布并不是一个单独的概率密度函, 收敛到权重都在两个点上, 这种情况能
        - 摘要：19.4.3　连续型潜变量
          關鍵詞：连续型潜变量
        - 摘要：当我们的图模型包含连续型潜变量时，仍然可以通过最大化   进行变；分推断和变分学习。然而，我们需要使用变分法来实现关于q( h ｜ ν )最；大化  。
          關鍵詞：仍然可以通过最大化, 我们需要使用变分法来实现关于, 然而, 当我们的图模型包含连续型潜变量时, 大化
        - 摘要：在大多数情况下，研究者并不需要解决任何变分法的问题。取而代之的；是，均值场固定点迭代更新有一个通用的方程。如果我们做了均值场近；似：
          關鍵詞：在大多数情况下, 均值场固定点迭代更新有一个通用的方程, 取而代之的, 如果我们做了均值场近, 研究者并不需要解决任何变分法的问题
        - 摘要：并且对任何的j≠i固定q(h  j  ｜ ν )，那么只需要满足分布p中任何联合分布；变量的概率值不为0，我们就可以通过归一化下面这个未归一的分布
          關鍵詞：我们就可以通过归一化下面这个未归一的分布, 中任何联合分布, 并且对任何的, 固定, 那么只需要满足分布
        - 摘要：来得到最优的q(h i ｜ ν )。在这个方程中计算期望就能得到正确的q(h i ｜；ν  )的表达式。我们只有在希望提出一种新形式的变分学习算法时才需要；使用变分法来直接推导q的函数形式。式（19.56）给出了适用于任何概
          關鍵詞：的函数形式, 我们只有在希望提出一种新形式的变分学习算法时才需要, 的表达式, 给出了适用于任何概, 使用变分法来直接推导
        - 摘要：式（19.56）是一个不动点方程，对每一个i它都被迭代地反复使用直到；收敛。然而，它还包含着更多的信息。它还包含了最优解取到的泛函形；式，无论我们是否能够通过不动点方程来解出它。这意味着我们可以利
          關鍵詞：对每一个, 它还包含了最优解取到的泛函形, 然而, 它都被迭代地反复使用直到, 它还包含着更多的信息
        - 摘要：我们拿一个简单的概率模型作为例子，其中潜变量满足；见变量只有一个ν。假设
          關鍵詞：见变量只有一个, 我们拿一个简单的概率模型作为例子, 其中潜变量满足, 假设
        - 摘要：，可；以及；，我们可以积掉 h  来简化这个模型，结
          關鍵詞：来简化这个模型, 以及, 我们可以积掉
        - 摘要：果是关于ν的高斯分布。这个模型本身并不有趣。只是为了说明变分法；如何应用在概率建模之中，我们才构造了这个模型。
          關鍵詞：只是为了说明变分法, 这个模型本身并不有趣, 我们才构造了这个模型, 的高斯分布, 果是关于
        - 摘要：忽略归一化常数时，真实的后验分布如下：
          關鍵詞：真实的后验分布如下, 忽略归一化常数时
        - 摘要：在上式中，我们发现由于带有h  1  、h  2  乘积项的存在，真实的后验并不；能关于h 1 、h 2 分解。
          關鍵詞：能关于, 分解, 真实的后验并不, 乘积项的存在, 我们发现由于带有
        - 摘要：应用式（19.56），我们可以得到
          關鍵詞：应用式, 我们可以得到
        - 摘要：从这里，我们可以发现其中我们只需要从q(h  2  ｜  ν  )中获得两个有效；值：；，我们可以得到：
          關鍵詞：从这里, 中获得两个有效, 我们可以得到, 我们可以发现其中我们只需要从
        - 摘要：。把这两项记作
          關鍵詞：把这两项记作
        - 摘要：和
          關鍵詞：
        - 摘要：和
          關鍵詞：
        - 摘要：从这里，我们可以发现   的泛函形式满足高斯分布。因此，我们可以得；，其中 µ 和对角的β是变分参数，我们可；到q( h ｜ ν )＝
          關鍵詞：是变分参数, 我们可, 因此, 其中, 和对角的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；以使用任何方法来优化它。有必要再强调一下，我们并没有假设q是一；个高斯分布，这个高斯的形式是使用变分法来关于分布q最大化  而推
          關鍵詞：个高斯分布, 是一, 这个高斯的形式是使用变分法来关于分布, 有必要再强调一下, 而推
        - 摘要：当然，上述模型只是为了说明情况的一个简单例子。深度学习中关于变；分学习中连续型变量的实际应用可以参考Goodfellow et al. （2013f）。
          關鍵詞：当然, 分学习中连续型变量的实际应用可以参考, 深度学习中关于变, 上述模型只是为了说明情况的一个简单例子
        - 摘要：19.4.4　学习和推断之间的相互作用
          關鍵詞：学习和推断之间的相互作用
        - 摘要：在学习算法中使用近似推断会影响学习的过程，反过来学习的过程也会；影响推断算法的准确性。
          關鍵詞：在学习算法中使用近似推断会影响学习的过程, 反过来学习的过程也会, 影响推断算法的准确性
        - 摘要：具体来说，训练算法倾向于朝使得近似推断算法中的近似假设变得更加；真实的方向来适应模型。当训练参数时，变分学习增加
          關鍵詞：变分学习增加, 具体来说, 真实的方向来适应模型, 当训练参数时, 训练算法倾向于朝使得近似推断算法中的近似假设变得更加
        - 摘要：对于一个特定的 ν ，对于q( h ｜ ν )中概率很大的 h ，它增加了p( h ｜ ν；)；对于q( h ｜ ν )中概率很小的 h ，它减小了p( h ｜ ν )。
          關鍵詞：它减小了, 它增加了, 对于一个特定的, 中概率很大的, 中概率很小的
        - 摘要：这种行为使得我们做的近似假设变得合理。如果我们用单峰值近似后验；来训练模型，那么所得具有真实后验的模型会比我们使用精确推断训练；模型获得的模型更接近单峰值。
          關鍵詞：如果我们用单峰值近似后验, 那么所得具有真实后验的模型会比我们使用精确推断训练, 模型获得的模型更接近单峰值, 来训练模型, 这种行为使得我们做的近似假设变得合理
        - 摘要：因此，估计变分近似对模型的破坏程度是很困难的。存在几种估计log；p( ν )的方式。通常我们在训练模型之后估计log p( ν ； θ )，然后发现它；和   (  ν  ,  θ  ,q)的差距是很小的。从这里我们可以得出结论，对于特定
          關鍵詞：存在几种估计, 因此, 然后发现它, 对于特定, 的方式
        - 摘要：。
          關鍵詞：
        - 摘要：≈
          關鍵詞：
        - 摘要：难发现的，因为只有在我们有一个能够找到  θ  ∗  的较好的学习算法时，；才能确定进行上述的比较。
          關鍵詞：难发现的, 才能确定进行上述的比较, 的较好的学习算法时, 因为只有在我们有一个能够找到
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；19.5　学成近似推断
          關鍵詞：学成近似推断
        - 摘要：我们已经看到了推断可以被视作一个增加函数   值的优化过程。显式；地通过迭代方法（比如不动点方程或者基于梯度的优化算法）来进行优；化的过程通常是代价很高且耗时巨大的。通过学习一个近似推断，许多
          關鍵詞：地通过迭代方法, 来进行优, 许多, 值的优化过程, 化的过程通常是代价很高且耗时巨大的
        - 摘要：的神经网络来近似它。
          關鍵詞：的神经网络来近似它
        - 摘要：19.5.1　醒眠算法
          關鍵詞：醒眠算法
        - 摘要：训练一个可以用 ν 来推断 h 的模型的一个主要难点在于我们没有一个监；督训练集来训练模型。给定一个 ν ，我们无法获知一个合适的 h 。从  ν；到 h 的映射依赖于模型族的选择，并且在学习过程中随着 θ 的改变而变
          關鍵詞：的模型的一个主要难点在于我们没有一个监, 来推断, 的改变而变, 训练一个可以用, 的映射依赖于模型族的选择
        - 摘要：在第18.2节中，我们看到睡眠做梦在人类和动物中作用的一个可能解释；是，做梦可以提供蒙特卡罗训练算法用于近似无向模型中对数配分函数；负梯度的负相样本。生物做梦的另一个可能解释是它提供来自p( h ， ν )
          關鍵詞：生物做梦的另一个可能解释是它提供来自, 我们看到睡眠做梦在人类和动物中作用的一个可能解释, 节中, 在第, 做梦可以提供蒙特卡罗训练算法用于近似无向模型中对数配分函数
        - 摘要：如何能够保持清醒几个小时（它们清醒的时间越长，   和log  p(  ν  )之；间的差距越大，但是   仍然是下限），并且睡眠几个小时（生成模型；本身在睡眠期间不被修改），而不损害它们的内部模型。当然，这些想
          關鍵詞：仍然是下限, 如何能够保持清醒几个小时, 生成模型, 而不损害它们的内部模型, 但是
        - 摘要：19.5.2　学成推断的其他形式
          關鍵詞：学成推断的其他形式
        - 摘要：这种学成近似推断策略已经被应用到了其他模型中。Salakhutdinov  and；Larochelle（2010）证明了在学成推断网络中的单遍传递相比于在深度；玻尔兹曼机中的迭代均值场不动点方程能够得到更快的推断。其训练过
          關鍵詞：证明了在学成推断网络中的单遍传递相比于在深度, 玻尔兹曼机中的迭代均值场不动点方程能够得到更快的推断, 其训练过, 这种学成近似推断策略已经被应用到了其他模型中
        - 摘要：我们已经在第14.8节中看到，预测性的稀疏分解模型训练一个浅层编码；器网络，从而预测输入的稀疏编码。这可以被看作自编码器和稀疏编码；之间的混合。为模型设计概率语义是可能的，其中编码器可以被视为执
          關鍵詞：预测性的稀疏分解模型训练一个浅层编码, 其中编码器可以被视为执, 为模型设计概率语义是可能的, 节中看到, 这可以被看作自编码器和稀疏编码
        - 摘要：近来学成近似推断已经成为变分自编码器形式的生成模型中的主要方法；之一（Kingma，2013；Rezende et al. ，2014）。在这种优美的方法中，；不需要为推断网络构造显式的目标。反之，推断网络仅仅被用来定义
          關鍵詞：推断网络仅仅被用来定义, 近来学成近似推断已经成为变分自编码器形式的生成模型中的主要方法, 在这种优美的方法中, 不需要为推断网络构造显式的目标, 之一
        - 摘要：我们可以使用近似推断来训练和使用很多不同的模型。其中许多模型将；在下一章中描述。
          關鍵詞：在下一章中描述, 其中许多模型将, 我们可以使用近似推断来训练和使用很多不同的模型
        - 摘要：19.4.1　离散型潜变量
          關鍵詞：离散型潜变量
        - 摘要：19.4.2　变分法
          關鍵詞：变分法
        - 摘要：19.4.3　连续型潜变量
          關鍵詞：连续型潜变量
        - 摘要：19.4.4　学习和推断之间；的相互作用
          關鍵詞：的相互作用, 学习和推断之间
    19.5：学成近似推断
        - 摘要：19.5.1　醒眠算法
          關鍵詞：醒眠算法
        - 摘要：19.5.2　学成推断的其他形式
          關鍵詞：学成推断的其他形式
        - 摘要：19.5.1　醒眠算法
          關鍵詞：醒眠算法
        - 摘要：19.5.2　学成推断的其他；形式
          關鍵詞：形式, 学成推断的其他
第20章：深度生成模型
    19.4：变分推断和变分学习
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在本章中，我们介绍几种具体的生成模型，这些模型可以使用第16章至；第19章中出现的技术构建和训练。所有这些模型在某种程度上都代表了
          關鍵詞：章至, 这些模型可以使用第, 章中出现的技术构建和训练, 所有这些模型在某种程度上都代表了, 我们介绍几种具体的生成模型
    20.1：玻尔兹曼机
        - 摘要：玻尔兹曼机最初作为一种广义的“联结主义”引入，用来学习二值向量上；的任意概率分布（Fahlman et al. ，1983；Ackley et al. ，1985；Hinton et；al.  ，1984b；Hinton  and  Sejnowski，1986）。玻尔兹曼机的变体（包含
          關鍵詞：联结主义, 的任意概率分布, 包含, 引入, 玻尔兹曼机的变体
        - 摘要：我们在d维二值随机向量  x  ∈{0，1}  d  上定义玻尔兹曼机。玻尔兹曼机；是一种基于能量的模型（第16.2.4节），意味着我们可以使用能量函数；定义联合概率分布：
          關鍵詞：是一种基于能量的模型, 意味着我们可以使用能量函数, 定义联合概率分布, 维二值随机向量, 我们在
        - 摘要：其中E(  x  )是能量函数，Z是确保；曼机的能量函数如下给出：
          關鍵詞：曼机的能量函数如下给出, 其中, 是能量函数, 是确保
        - 摘要：的配分函数。玻尔兹
          關鍵詞：玻尔兹, 的配分函数
        - 摘要：其中 U 是模型参数的“权重”矩阵， b 是偏置向量。
          關鍵詞：是偏置向量, 矩阵, 其中, 权重, 是模型参数的
        - 摘要：在一般设定下，给定一组训练样本，每个样本都是n维的。式（20.1）；描述了观察到的变量的联合概率分布。虽然这种情况显然可行，但它限；制了观察到的变量和权重矩阵描述的变量之间相互作用的类型。具体来
          關鍵詞：具体来, 每个样本都是, 但它限, 给定一组训练样本, 制了观察到的变量和权重矩阵描述的变量之间相互作用的类型
        - 摘要：当不是所有变量都能被观察到时，玻尔兹曼机变得更强大。在这种情况；下，潜变量类似于多层感知机中的隐藏单元，并模拟可见单元之间的高；阶交互。正如添加隐藏单元将逻辑回归转换为MLP，导致MLP成为函数
          關鍵詞：在这种情况, 并模拟可见单元之间的高, 玻尔兹曼机变得更强大, 导致, 正如添加隐藏单元将逻辑回归转换为
        - 摘要：正式地，我们将单元 x 分解为两个子集：可见单元 ν 和潜在（或隐藏）；单元 h 。能量函数变为
          關鍵詞：能量函数变为, 分解为两个子集, 正式地, 和潜在, 可见单元
        - 摘要：玻尔兹曼机的学习  　玻尔兹曼机的学习算法通常基于最大似然。所有；玻尔兹曼机都具有难以处理的配分函数，因此最大似然梯度必须使用第；18章中的技术来近似。
          關鍵詞：章中的技术来近似, 玻尔兹曼机的学习, 玻尔兹曼机的学习算法通常基于最大似然, 所有, 玻尔兹曼机都具有难以处理的配分函数
        - 摘要：玻尔兹曼机有一个有趣的性质，当基于最大似然的学习规则训练时，连；接两个单元的特定权重的更新仅取决于这两个单元在不同分布下收集的；统计信息：P model ( ν )和
          關鍵詞：接两个单元的特定权重的更新仅取决于这两个单元在不同分布下收集的, 当基于最大似然的学习规则训练时, 统计信息, 玻尔兹曼机有一个有趣的性质
        - 摘要：不仅仅使用局部统计信息的其他学习算法似乎需要假设更多的学习机；制。例如，对于大脑在多层感知机中实现的反向传播，似乎需要维持一；个辅助通信的网络，并借此向后传输梯度信息。已经有学者（Hinton，
          關鍵詞：并借此向后传输梯度信息, 个辅助通信的网络, 例如, 不仅仅使用局部统计信息的其他学习算法似乎需要假设更多的学习机, 对于大脑在多层感知机中实现的反向传播
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；从生物学的角度看，玻尔兹曼机学习中的负相阶段有点难以解释。正如；第18.2节所主张的，人类在睡眠时做梦可能是一种形式的负相采样。尽
          關鍵詞：玻尔兹曼机学习中的负相阶段有点难以解释, 人类在睡眠时做梦可能是一种形式的负相采样, 节所主张的, 正如, 从生物学的角度看
    20.2：受限玻尔兹曼机
        - 摘要：20.2.1　条件分布
          關鍵詞：条件分布
        - 摘要：20.2.2　训练受限玻尔兹曼机
          關鍵詞：训练受限玻尔兹曼机
        - 摘要：受限玻尔兹曼机以簧风琴  （harmonium）之名（Smolensky，1986）面；世之后，成为了深度概率模型中最常见的组件之一。我们之前在第；16.7.1节简要介绍了RBM。在这里我们回顾以前的内容并探讨更多的细
          關鍵詞：在这里我们回顾以前的内容并探讨更多的细, 成为了深度概率模型中最常见的组件之一, 之名, 世之后, 节简要介绍了
        - 摘要：我们从二值版本的受限玻尔兹曼机开始，但如我们之后所见，这还可以；扩展为其他类型的可见和隐藏单元。
          關鍵詞：这还可以, 扩展为其他类型的可见和隐藏单元, 但如我们之后所见, 我们从二值版本的受限玻尔兹曼机开始
        - 摘要：更正式地说，令观察层由一组n  ν  个二值随机变量组成，我们统称为向；量v 。我们将n h 个二值随机变量的潜在或隐藏层记为 h 。
          關鍵詞：我们将, 更正式地说, 个二值随机变量的潜在或隐藏层记为, 个二值随机变量组成, 我们统称为向
        - 摘要：就像普通的玻尔兹曼机，受限玻尔兹曼机也是基于能量的模型，其联合；概率分布由能量函数指定：
          關鍵詞：概率分布由能量函数指定, 受限玻尔兹曼机也是基于能量的模型, 其联合, 就像普通的玻尔兹曼机
        - 摘要：RBM的能量函数由下给出
          關鍵詞：的能量函数由下给出
        - 摘要：其中Z是被称为配分函数的归一化常数：
          關鍵詞：其中, 是被称为配分函数的归一化常数
        - 摘要：从配分函数Z的定义显而易见，计算Z的朴素方法（对所有状态进行穷；举求和）计算上可能是难以处理的，除非有巧妙设计的算法可以利用概
          關鍵詞：的朴素方法, 的定义显而易见, 计算上可能是难以处理的, 对所有状态进行穷, 从配分函数
        - 摘要：率分布中的规则来更快地计算Z。在受限玻尔兹曼机的情况下，Long；and  Servedio（2010）正式证明配分函数Z是难解的。难解的配分函数Z；意味着归一化联合概率分布P (ν)也难以评估。
          關鍵詞：正式证明配分函数, 率分布中的规则来更快地计算, 难解的配分函数, 意味着归一化联合概率分布, 也难以评估
        - 摘要：图20.1　可以用受限玻尔兹曼机构建的模型示例。（a）受限玻尔兹曼机本身是基于二分图的无；向图模型，图的一部分具有可见单元，另一部分具有隐藏单元。可见单元之间没有连接，隐藏；单元之间也没有任何连接。通常每个可见单元连接到每个隐藏单元，但也可以构造稀疏连接的
          關鍵詞：向图模型, 可见单元之间没有连接, 通常每个可见单元连接到每个隐藏单元, 受限玻尔兹曼机本身是基于二分图的无, 单元之间也没有任何连接
        - 摘要：20.2.1　条件分布
          關鍵詞：条件分布
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；虽然P；布P (h | v )和P (v | h )是因子的，并且计算和采样是相对简单的。
          關鍵詞：是因子的, 并且计算和采样是相对简单的, 虽然
        - 摘要：(ν)难解，但RBM的二分图结构具有非常特殊的性质，其条件分
          關鍵詞：其条件分, 难解, 的二分图结构具有非常特殊的性质
        - 摘要：从联合分布中导出条件分布是直观的：
          關鍵詞：从联合分布中导出条件分布是直观的
        - 摘要：由于我们相对可见单元v  计算条件概率，相对于分布P  (h  |  v  )我们可以；将它们视为常数。条件分布P  (h  |  v  )因子相乘的本质，我们可以将向量；h 上的联合概率写成单独元素h  j  上（未归一化）分布的乘积。现在原问
          關鍵詞：我们可以, 我们可以将向量, 现在原问, 条件分布, 由于我们相对可见单元
        - 摘要：现在我们可以将关于隐藏层的完全条件分布表达为因子形式：
          關鍵詞：现在我们可以将关于隐藏层的完全条件分布表达为因子形式
        - 摘要：类似的推导将显示我们感兴趣的另一个条件分布，P  (ν  |  h  )也是因子形；式的分布：
          關鍵詞：也是因子形, 类似的推导将显示我们感兴趣的另一个条件分布, 式的分布
        - 摘要：20.2.2　训练受限玻尔兹曼机
          關鍵詞：训练受限玻尔兹曼机
        - 摘要：的估计和微分，并且还允许高效地（以块；因为RBM允许高效计算；吉布斯采样的形式）进行MCMC采样，所以我们很容易使用第18章中训
          關鍵詞：因为, 所以我们很容易使用第, 以块, 吉布斯采样的形式, 采样
        - 摘要：20.2.1　条件分布
          關鍵詞：条件分布
        - 摘要：20.2.2　训练受限玻尔兹；曼机
          關鍵詞：曼机, 训练受限玻尔兹
    20.3：深度信念网络
        - 摘要：深度信念网络  （deep  belief  network，DBN）是第一批成功应用深度架；构训练的非卷积模型之一（Hinton  et  al.  ，2006a；Hinton，2007b）。；2006年深度信念网络的引入开始了当前深度学习的复兴。在引入深度信
          關鍵詞：年深度信念网络的引入开始了当前深度学习的复兴, 是第一批成功应用深度架, 深度信念网络, 构训练的非卷积模型之一, 在引入深度信
        - 摘要：深度信念网络是具有若干潜变量层的生成模型。潜变量通常是二值的，；而可见单元可以是二值或实数。尽管构造连接比较稀疏的DBN是可能；的，但在一般的模型中，每层的每个单元连接到每个相邻层中的每个单
          關鍵詞：潜变量通常是二值的, 每层的每个单元连接到每个相邻层中的每个单, 但在一般的模型中, 而可见单元可以是二值或实数, 尽管构造连接比较稀疏的
        - 摘要：具有l个隐藏层的DBN包含l个权重矩阵：；含l＋1个偏置向量：；表示的概率分布由下式给出：
          關鍵詞：包含, 个偏置向量, 个权重矩阵, 个隐藏层的, 具有
        - 摘要：，同时也包；，其中b  (0)  是可见层的偏置。DBN
          關鍵詞：是可见层的偏置, 同时也包, 其中
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；在实值可见单元的情况下，替换
          關鍵詞：在实值可见单元的情况下, 替换
        - 摘要：为便于处理，β  为对角形式。至少在理论上，推广到其他指数族的可见；单元是直观的。只有一个隐藏层的DBN只是一个RBM。
          關鍵詞：为对角形式, 只有一个隐藏层的, 至少在理论上, 只是一个, 推广到其他指数族的可见
        - 摘要：为了从DBN中生成样本，我们先在顶部的两个隐藏层上运行几个Gibbs；采样步骤。这个阶段主要从RBM（由顶部两个隐藏层定义）中采一个；样本。然后，我们可以对模型的其余部分使用单次原始采样，以从可见
          關鍵詞：由顶部两个隐藏层定义, 这个阶段主要从, 样本, 我们先在顶部的两个隐藏层上运行几个, 我们可以对模型的其余部分使用单次原始采样
        - 摘要：深度信念网络引发许多与有向模型和无向模型同时相关的问题。
          關鍵詞：深度信念网络引发许多与有向模型和无向模型同时相关的问题
        - 摘要：由于每个有向层内的相消解释效应，并且由于无向连接的两个隐藏层之；间的相互作用，深度信念网络中的推断是难解的。评估或最大化对数似；然的标准证据下界也是难以处理的，因为证据下界基于大小等于网络宽
          關鍵詞：由于每个有向层内的相消解释效应, 然的标准证据下界也是难以处理的, 因为证据下界基于大小等于网络宽, 评估或最大化对数似, 间的相互作用
        - 摘要：评估或最大化对数似然，不仅需要面对边缘化潜变量时难以处理的推断；问题，而且还需要处理顶部两层无向模型内难处理的配分函数问题。
          關鍵詞：问题, 评估或最大化对数似然, 而且还需要处理顶部两层无向模型内难处理的配分函数问题, 不仅需要面对边缘化潜变量时难以处理的推断
        - 摘要：为训练深度信念网络，我们可以先使用对比散度或随机最大似然方法训；练RBM以最大化；。RBM的参数定义了DBN第一
          關鍵詞：以最大化, 第一, 我们可以先使用对比散度或随机最大似然方法训, 为训练深度信念网络, 的参数定义了
        - 摘要：其中p (1) 是第一个RBM表示的概率分布，p (2) 是第二个RBM表示的概率；分布。换句话说，第二个RBM被训练为模拟由第一个RBM的隐藏单元；采样定义的分布，而第一个RBM由数据驱动。这个过程能无限重复，
          關鍵詞：表示的概率, 表示的概率分布, 被训练为模拟由第一个, 的隐藏单元, 采样定义的分布
        - 摘要：DBN下似然概率的变分下界（Hinton et al. ，2006a）。
          關鍵詞：下似然概率的变分下界
        - 摘要：在大多数应用中，对DBN进行贪心逐层训练后，不需要再花工夫对其进；行联合训练。然而，使用醒眠算法对其进行生成精调是可能的。
          關鍵詞：使用醒眠算法对其进行生成精调是可能的, 在大多数应用中, 行联合训练, 然而, 进行贪心逐层训练后
        - 摘要：训练好的DBN可以直接用作生成模型，但是DBN的大多数兴趣来自它；们改进分类模型的能力。我们可以从DBN获取权重，并使用它们定义；MLP：
          關鍵詞：并使用它们定义, 但是, 的大多数兴趣来自它, 我们可以从, 们改进分类模型的能力
        - 摘要：利用DBN的生成训练后获得的权重和偏置初始化该MLP之后，我们可以；训练该MLP来执行分类任务。这种MLP的额外训练是判别性精调的示；例。
          關鍵詞：之后, 我们可以, 来执行分类任务, 训练该, 这种
        - 摘要：与第19章中从基本原理导出的许多推断方程相比，这种特定选择的MLP；有些随意。这个MLP是一个启发式选择，似乎在实践中效果不错，并在；文献中一贯使用。许多近似推断技术是由它们在一些约束下，并在对数
          關鍵詞：似乎在实践中效果不错, 这种特定选择的, 文献中一贯使用, 有些随意, 与第
        - 摘要：虽然DBN的对数似然是难处理的，但它可以使用AIS近似；（Salakhutdinov  and  Murray，2008）。通过近似，可以评估其作为生成；模型的质量。
          關鍵詞：的对数似然是难处理的, 虽然, 但它可以使用, 模型的质量, 通过近似
        - 摘要：术语“深度信念网络”通常不正确地用于指代任意种类的深度神经网络，；甚至没有潜变量意义的网络。这个术语应特指最深层中具有无向连接，；而在所有其他连续层之间存在向下有向连接的模型。
          關鍵詞：甚至没有潜变量意义的网络, 而在所有其他连续层之间存在向下有向连接的模型, 术语, 深度信念网络, 这个术语应特指最深层中具有无向连接
        - 摘要：这个术语也可能导致一些混乱，因为术语“信念网络”有时指纯粹的有向；模型，而深度信念网络包含一个无向层。深度信念网络也与动态贝叶斯
          關鍵詞：因为术语, 信念网络, 而深度信念网络包含一个无向层, 模型, 这个术语也可能导致一些混乱
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；网络（dynamic  Bayesian  networks）（Dean  and  Kanazawa，1989）共享；首字母缩写DBN，动态贝叶斯网络表示马尔可夫链的贝叶斯网络。
          關鍵詞：网络, 动态贝叶斯网络表示马尔可夫链的贝叶斯网络, 共享, 首字母缩写
    20.4：深度玻尔兹曼机
        - 摘要：20.4.1　有趣的性质
          關鍵詞：有趣的性质
        - 摘要：20.4.2　DBM均匀场推断
          關鍵詞：均匀场推断
        - 摘要：20.4.3　DBM的参数学习
          關鍵詞：的参数学习
        - 摘要：20.4.4　逐层预训练
          關鍵詞：逐层预训练
        - 摘要：20.4.5　联合训练深度玻尔兹曼机
          關鍵詞：联合训练深度玻尔兹曼机
        - 摘要：深度玻尔兹曼机  （Deep  Boltzmann  Machine，DBM）（Salakhutdinov；and  Hinton，2009a）是另一种深度生成模型。与深度信念网络（DBN）；不同的是，它是一个完全无向的模型。与RBM不同的是，DBM有几层
          關鍵詞：有几层, 不同的是, 深度玻尔兹曼机, 它是一个完全无向的模型, 是另一种深度生成模型
        - 摘要：图20.2　具有一个可见层（底部）和两个隐藏层的深度玻尔兹曼机的图模型。仅在相邻层的单；元之间存在连接，没有层内连接
          關鍵詞：仅在相邻层的单, 底部, 元之间存在连接, 和两个隐藏层的深度玻尔兹曼机的图模型, 具有一个可见层
        - 摘要：与RBM和DBN一样，DBM通常仅包含二值单元（正如我们为简化模型
          關鍵詞：一样, 通常仅包含二值单元, 正如我们为简化模型
        - 摘要：的演示而假设的），但很容易就能扩展到实值可见单元。
          關鍵詞：的演示而假设的, 但很容易就能扩展到实值可见单元
        - 摘要：DBM是基于能量的模型，这意味着模型变量的联合概率分布由能量函；数E参数化。在一个深度玻尔兹曼机包含一个可见层
          關鍵詞：参数化, 是基于能量的模型, 这意味着模型变量的联合概率分布由能量函, 在一个深度玻尔兹曼机包含一个可见层
        - 摘要：ν  和3个隐藏层
          關鍵詞：个隐藏层
        - 摘要：和
          關鍵詞：
        - 摘要：的情况下，联合概率由下式给出：
          關鍵詞：联合概率由下式给出, 的情况下
        - 摘要：为简化表示，式（20.25）省略了偏置参数。DBM能量函数定义如下：
          關鍵詞：为简化表示, 能量函数定义如下, 省略了偏置参数
        - 摘要：与RBM的能量函数（式（20.5））相比，DBM能量函数以权重矩阵（W；(2) 和W (4) 的形式表示隐藏单元（潜变量）之间的连接。正如我们将看到；的，这些连接对模型行为以及我们如何在模型中进行推断都有重要的影
          關鍵詞：这些连接对模型行为以及我们如何在模型中进行推断都有重要的影, 的形式表示隐藏单元, 潜变量, 的能量函数, 能量函数以权重矩阵
        - 摘要：与全连接的玻尔兹曼机（每个单元连接到其他每个单元）相比，DBM；提供了类似于RBM的一些优点。
          關鍵詞：与全连接的玻尔兹曼机, 的一些优点, 提供了类似于, 相比, 每个单元连接到其他每个单元
        - 摘要：具体来说，如图20.3所示，DBM的层可以组织成一个二分图，其中奇数；层在一侧，偶数层在另一侧。容易发现，当我们条件于偶数层中的变量；时，奇数层中的变量变得条件独立。当然，当我们条件于奇数层中的变
          關鍵詞：当我们条件于奇数层中的变, 当我们条件于偶数层中的变量, 所示, 层在一侧, 其中奇数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图20.3　深度玻尔兹曼机，重新排列后显示为二分图结构
          關鍵詞：深度玻尔兹曼机, 重新排列后显示为二分图结构
        - 摘要：DBM的二分图结构意味着，我们可以应用之前用于RBM条件分布的相；同式子来确定DBM中的条件分布。在给定相邻层值的情况下，层内的；单元彼此条件独立，因此二值变量的分布可以由Bernoulli参数（描述每
          關鍵詞：参数, 因此二值变量的分布可以由, 条件分布的相, 的二分图结构意味着, 同式子来确定
        - 摘要：和
          關鍵詞：
        - 摘要：二分图结构使Gibbs采样能在深度玻尔兹曼机中高效采样。Gibbs采样的；方法是一次只更新一个变量。RBM允许所有可见单元以一个块的方式；更新，而所有隐藏单元在另一个块上更新。我们可以简单地假设具有l
          關鍵詞：更新, 采样的, 而所有隐藏单元在另一个块上更新, 我们可以简单地假设具有, 方法是一次只更新一个变量
        - 摘要：训练尤其重要。
          關鍵詞：训练尤其重要
        - 摘要：20.4.1　有趣的性质
          關鍵詞：有趣的性质
        - 摘要：深度玻尔兹曼机具有许多有趣的性质。
          關鍵詞：深度玻尔兹曼机具有许多有趣的性质
        - 摘要：DBM在DBN之后开发。与DBN相比，DBM的后验分布P  (  h  |  ν  )  更简；单。有点违反直觉的是，这种后验分布的简单性允许更加丰富的后验近；似。在DBN的情况下，我们使用启发式的近似推断过程进行分类，其中
          關鍵詞：更简, 这种后验分布的简单性允许更加丰富的后验近, 我们使用启发式的近似推断过程进行分类, 有点违反直觉的是, 的后验分布
        - 摘要：使用适当的均匀场允许DBM的近似推断过程捕获自顶向下反馈相互作；用的影响。这从神经科学的角度来看是有趣的，因为根据已知，人脑使；用许多自上而下的反馈连接。由于这个性质，DBM已被用作真实神经
          關鍵詞：用的影响, 的近似推断过程捕获自顶向下反馈相互作, 由于这个性质, 因为根据已知, 这从神经科学的角度来看是有趣的
        - 摘要：DBM一个不理想的特性是从中采样是相对困难的。DBN只需要在其顶；部的一对层中使用MCMC采样。其他层仅在采样过程末尾涉及，并且只；需在一个高效的原始采样过程。要从DBM生成样本，必须在所有层中
          關鍵詞：只需要在其顶, 必须在所有层中, 要从, 采样, 生成样本
        - 摘要：20.4.2　DBM均匀场推断
          關鍵詞：均匀场推断
        - 摘要：给定相邻层，一个DBM层上的条件分布是因子的。在有两个隐藏层的；DBM的示例中，这些分布是P (ν| h (1) )、P ( h (1) ) |ν, h (2) )和P ( h (2) | h (1)；)。因为层之间的相互作用，所有隐藏层上的分布通常不是因子的。在
          關鍵詞：的示例中, 这些分布是, 给定相邻层, 因为层之间的相互作用, 层上的条件分布是因子的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；有两个隐藏层的示例中，由于 h (1) 和 h  (2)  之间的交互权重 W  (2)  使得这；些变量相互依赖，P ( h (1) |ν, h (2) )不是因子的。
          關鍵詞：有两个隐藏层的示例中, 些变量相互依赖, 之间的交互权重, 由于, 不是因子的
        - 摘要：与DBN的情况一样，我们还是要找出近似DBM后验分布的方法。然；而，与DBN不同，DBM在其隐藏单元上的后验分布（复杂的）很容易；用变分近似来近似（如第19.4节所讨论），具体是一个均匀场近似。均
          關鍵詞：的情况一样, 很容易, 节所讨论, 具体是一个均匀场近似, 我们还是要找出近似
        - 摘要：在推断的变分近似中，我们通过一些相当简单的分布族近似特定目标分；布——在这里指给定可见单元时隐藏单元的后验分布。在均匀场近似的；情况下，近似族是隐藏单元条件独立的分布集合。
          關鍵詞：情况下, 在均匀场近似的, 我们通过一些相当简单的分布族近似特定目标分, 在这里指给定可见单元时隐藏单元的后验分布, 在推断的变分近似中
        - 摘要：我们现在为具有两个隐藏层的示例推导均匀场方法。令Q ( h (1) , h  (2)  |ν)；为P ( h (1) , h (2) |ν)的近似。均匀场假设意味着
          關鍵詞：我们现在为具有两个隐藏层的示例推导均匀场方法, 的近似, 均匀场假设意味着
        - 摘要：均匀场近似试图找到这个分布族中最适合真实后验P ( h  (1)  , h  (2)  |ν)的成；员。重要的是，每次我们使用 ν  的新值时，必须再次运行推断过程以找；到不同的分布Q。
          關鍵詞：每次我们使用, 均匀场近似试图找到这个分布族中最适合真实后验, 到不同的分布, 的新值时, 重要的是
        - 摘要：我们可以设想很多方法来衡量Q (h |ν)与P (h |ν)的拟合程度。均匀场方法；是最小化
          關鍵詞：是最小化, 均匀场方法, 我们可以设想很多方法来衡量, 的拟合程度
        - 摘要：一般来说，除了要保证独立性假设，我们不必提供参数形式的近似分；布。变分近似过程通常能够恢复近似分布的函数形式。然而，在二值隐；藏单元（我们在这里推导的情况）的均匀场假设的情况下，不会由于预
          關鍵詞：不会由于预, 我们不必提供参数形式的近似分, 然而, 的均匀场假设的情况下, 除了要保证独立性假设
        - 摘要：我们将Q作为Bernoulli分布的乘积进行参数化，即我们将  h  (1)  每个元素
          關鍵詞：作为, 我们将, 每个元素, 即我们将, 分布的乘积进行参数化
        - 摘要：的概率与一个参数相关联。具体来说，对于每个j，；，其中
          關鍵詞：具体来说, 对于每个, 的概率与一个参数相关联, 其中
        - 摘要：。另外，对于每个k，；。因此，我们有以下近似后验：
          關鍵詞：因此, 对于每个, 另外, 我们有以下近似后验
        - 摘要：，其中
          關鍵詞：其中
        - 摘要：当然，对于具有更多层的DBM，近似后验的参数化可以通过明显的方；式扩展，即利用图的二分结构，遵循Gibbs采样相同的调度，同时更新；所有偶数层，然后同时更新所有奇数层。
          關鍵詞：即利用图的二分结构, 式扩展, 近似后验的参数化可以通过明显的方, 同时更新, 然后同时更新所有奇数层
        - 摘要：现在我们已经指定了近似分布Q的函数族，但仍然需要指定用于选择该；函数族中最适合P的成员的过程。最直接的方法是使用式（19.56）指定；的均匀场方程。这些方程是通过求解变分下界导数为零的位置而导出，
          關鍵詞：的函数族, 这些方程是通过求解变分下界导数为零的位置而导出, 现在我们已经指定了近似分布, 函数族中最适合, 的成员的过程
        - 摘要：应用这些一般的方程，我们得到以下更新规则（再次忽略偏置项）：
          關鍵詞：再次忽略偏置项, 应用这些一般的方程, 我们得到以下更新规则
        - 摘要：在该方程组的不动点处，我们具有变分下界；的局部最大值。因；此，这些不动点更新方程定义了迭代算法，其中我们交替更新   （使
          關鍵詞：其中我们交替更新, 在该方程组的不动点处, 我们具有变分下界, 这些不动点更新方程定义了迭代算法, 的局部最大值
        - 摘要：20.4.3　DBM的参数学习
          關鍵詞：的参数学习
        - 摘要：DBM中的学习必须面对难解配分函数的挑战（使用第18章中的技；术），以及难解后验分布的挑战（使用第19章中的技术）。
          關鍵詞：中的学习必须面对难解配分函数的挑战, 章中的技术, 使用第, 以及难解后验分布的挑战, 章中的技
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；如第20.4.2节中所描述的，变分推断允许构建近似难处理的P  (  h  |ν)的分；布Q ( h  |ν)。然后通过最大化
          關鍵詞：的分, 然后通过最大化, 变分推断允许构建近似难处理的, 如第, 节中所描述的
        - 摘要：）学习。
          關鍵詞：学习
        - 摘要：对于具有两个隐藏层的深度玻尔兹曼机，  由下式给出
          關鍵詞：对于具有两个隐藏层的深度玻尔兹曼机, 由下式给出
        - 摘要：。由于深度玻尔兹曼机包含；该表达式仍然包含对数配分函数；受限玻尔兹曼机作为组件，用于计算受限玻尔兹曼机的配分函数和采样
          關鍵詞：受限玻尔兹曼机作为组件, 由于深度玻尔兹曼机包含, 该表达式仍然包含对数配分函数, 用于计算受限玻尔兹曼机的配分函数和采样
        - 摘要：非变分版本的随机最大似然算法已经在第18.2节讨论过。算法20.1给出；了应用于DBM的变分随机最大似然算法。回想一下，我们描述的是；DBM的简化变体（缺少偏置参数），很容易推广到包含偏置参数的情
          關鍵詞：节讨论过, 很容易推广到包含偏置参数的情, 算法, 非变分版本的随机最大似然算法已经在第, 的简化变体
        - 摘要：20.4.4　逐层预训练
          關鍵詞：逐层预训练
        - 摘要：不幸的是，随机初始化后使用随机最大似然训练（如上所述）的DBM；通常导致失败。在一些情况下，模型不能学习如何充分地表示分布。在；其他情况下，DBM可以很好地表示分布，但是没有比仅使用RBM获得
          關鍵詞：模型不能学习如何充分地表示分布, 其他情况下, 获得, 如上所述, 但是没有比仅使用
        - 摘要：如第20.4.5节所述，目前已经开发了允许联合训练的各种技术。然而，；克服DBM的联合训练问题最初和最流行的方法是贪心逐层预训练。在；该方法中，DBM的每一层被单独视为RBM进行训练。第一层被训练为
          關鍵詞：目前已经开发了允许联合训练的各种技术, 该方法中, 的每一层被单独视为, 然而, 克服
        - 摘要：对输入数据进行建模。每个后续RBM被训练为对来自前一RBM后验分；布的样本进行建模。在以这种方式训练了所有RBM之后，它们可以被；组合成DBM。然后可以用PCD训练DBM。通常，PCD训练将仅使模型
          關鍵詞：之后, 训练, 通常, 组合成, 每个后续
        - 摘要：图20.4　用于分类MNIST数据集的深度玻尔兹曼机训练过程（Salakhutdinov and Hinton，；2009a；Srivastava et al. ，2014）。（a）使用CD近似最大化log P( ν )来训练RBM。（b）训练第；二个RBM，使用CD-k近似最大化log P( h (1) ,y)来建模 h (1) 和目标类y，其中 h (1) 采自第一个
          關鍵詞：来训练, 用于分类, 数据集的深度玻尔兹曼机训练过程, 来建模, 近似最大化
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；算法20.1  　用于训练具有两个隐藏层的DBM的变分随机最大似然算；法。
          關鍵詞：用于训练具有两个隐藏层的, 的变分随机最大似然算, 算法
        - 摘要：设步长  一个小正数
          關鍵詞：一个小正数, 设步长
        - 摘要：设定吉布斯步数k，大到足以让；夫链能磨合（从来自
          關鍵詞：从来自, 大到足以让, 设定吉布斯步数, 夫链能磨合
        - 摘要：的样本开始）。
          關鍵詞：的样本开始
        - 摘要：的马尔可
          關鍵詞：的马尔可
        - 摘要：初始化3个矩阵，；如，来自Bernoulli分布，边缘分布大致与模型匹配）。
          關鍵詞：边缘分布大致与模型匹配, 个矩阵, 分布, 来自, 初始化
        - 摘要：和
          關鍵詞：
        - 摘要：每个都将m行设为随机值（例
          關鍵詞：每个都将, 行设为随机值
        - 摘要：while 没有收敛（学习循环）do
          關鍵詞：没有收敛, 学习循环
        - 摘要：从训练数据采包含m个样本的小批量，并将它们排列为设计矩阵  V；的行。
          關鍵詞：从训练数据采包含, 并将它们排列为设计矩阵, 的行, 个样本的小批量
        - 摘要：初始化矩阵
          關鍵詞：初始化矩阵
        - 摘要：和
          關鍵詞：
        - 摘要：，使其大致符合模型的边缘分布。
          關鍵詞：使其大致符合模型的边缘分布
        - 摘要：while 没有收敛（均匀场推断循环）do
          關鍵詞：没有收敛, 均匀场推断循环
        - 摘要：end while
          關鍵詞：
        - 摘要：for l＝1 to k（Gibbs采样）do
          關鍵詞：采样
        - 摘要：Gibbs block 1:
          關鍵詞：
        - 摘要：Gibbs block 2:
          關鍵詞：
        - 摘要：end for
          關鍵詞：
        - 摘要：效，如具有衰减学习率的动量）
          關鍵詞：如具有衰减学习率的动量
        - 摘要：（这是大概的描述，实践中使用的算法更高
          關鍵詞：这是大概的描述, 实践中使用的算法更高
        - 摘要：end while
          關鍵詞：
        - 摘要：这种贪心逐层训练过程不仅仅是坐标上升，因为我们在每个步骤优化参；数的一个子集，它与坐标上升具有一些传递相似性。这两种方法是不同；的，因为贪心逐层训练过程中，我们在每个步骤都使用了不同的目标函
          關鍵詞：这两种方法是不同, 这种贪心逐层训练过程不仅仅是坐标上升, 因为我们在每个步骤优化参, 我们在每个步骤都使用了不同的目标函, 数的一个子集
        - 摘要：DBM的贪心逐层预训练与DBN的贪心逐层预训练不同。每个单独的；RBM的参数可以直接复制到相应的DBN。在DBM的情况下，RBM的参；数在包含到DBM中之前必须修改。RBM栈的中间层仅使用自底向上的
          關鍵詞：的贪心逐层预训练与, 的参数可以直接复制到相应的, 中之前必须修改, 数在包含到, 栈的中间层仅使用自底向上的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；输入进行训练，但在栈组合形成DBM后，该层将同时具有自底向上和；自顶向下的输入。为了解释这种效应，Salakhutdinov
          關鍵詞：该层将同时具有自底向上和, 但在栈组合形成, 输入进行训练, 自顶向下的输入, 为了解释这种效应
        - 摘要：为了使用深度玻尔兹曼机获得最好结果，我们需要修改标准的SML算；法，即在联合PCD训练步骤的负相期间使用少量的均匀场；（Salakhutdinov  and  Hinton，2009a）。具体来说，应当相对于其中所有
          關鍵詞：我们需要修改标准的, 训练步骤的负相期间使用少量的均匀场, 具体来说, 应当相对于其中所有, 为了使用深度玻尔兹曼机获得最好结果
        - 摘要：et  al
          關鍵詞：
        - 摘要：20.4.5　联合训练深度玻尔兹曼机
          關鍵詞：联合训练深度玻尔兹曼机
        - 摘要：经典DBM需要贪心无监督预训练，并且为了更好的分类，需要在它们；提取的隐藏特征之上，使用独立的基于MLP的分类器。这种方法有一些；不理想的性质，因为我们不能在训练第一个RBM时评估完整DBM的属
          關鍵詞：需要在它们, 使用独立的基于, 并且为了更好的分类, 不理想的性质, 经典
        - 摘要：主要有两种方法可以处理深度玻尔兹曼机的联合训练问题。第一个是中；心化深度玻尔兹曼机  （centered  deep  Boltzmann  machine）（Montavon；and  Muller，2012），通过重参数化模型使其在开始学习过程时代价函
          關鍵詞：心化深度玻尔兹曼机, 通过重参数化模型使其在开始学习过程时代价函, 第一个是中, 主要有两种方法可以处理深度玻尔兹曼机的联合训练问题
        - 摘要：MCMC估计梯度的问题。不幸的是，新的准则不会导致良好的似然性或；样本，但是相比MCMC方法，它确实会导致更好的分类性能和良好的推；断缺失输入的能力。
          關鍵詞：样本, 方法, 断缺失输入的能力, 估计梯度的问题, 它确实会导致更好的分类性能和良好的推
        - 摘要：如果我们回到玻尔兹曼机的一般观点，即包括一组权重矩阵 U 和偏置 b；的单元 x ，玻尔兹曼机中心化技巧是最容易描述的。回顾式（20.2） ，；能量函数由下式给出
          關鍵詞：回顾式, 的单元, 玻尔兹曼机中心化技巧是最容易描述的, 如果我们回到玻尔兹曼机的一般观点, 能量函数由下式给出
        - 摘要：在权重矩阵  U  中使用不同的稀疏模式，我们可以实现不同架构的玻尔；兹曼机，如RBM或具有不同层数的DBM。将  x  分割成可见和隐藏单；元，并将  U  中不相互作用的单元归零可以实现这些架构。中心化玻尔
          關鍵詞：中心化玻尔, 我们可以实现不同架构的玻尔, 中使用不同的稀疏模式, 在权重矩阵, 兹曼机
        - 摘要：通常 μ 在开始训练时固定为一个超参数。当模型初始化时，通常选择为；x  −  μ  ≈0。这种重参数化不改变模型可表示的概率分布的集合，但它确；实改变了应用于似然的随机梯度下降的动态。具体来说，在许多情况
          關鍵詞：在开始训练时固定为一个超参数, 通常, 实改变了应用于似然的随机梯度下降的动态, 在许多情况, 这种重参数化不改变模型可表示的概率分布的集合
        - 摘要：et
          關鍵詞：
        - 摘要：联合训练深度玻尔兹曼机的另一种方法是多预测深度玻尔兹曼机（MP-；DBM），它将均匀场方程视为定义一系列用于近似求解每个可能推断；问题的循环网络（Goodfellow et  al.  ，2013d）。模型被训练为使每个循
          關鍵詞：模型被训练为使每个循, 它将均匀场方程视为定义一系列用于近似求解每个可能推断, 联合训练深度玻尔兹曼机的另一种方法是多预测深度玻尔兹曼机, 问题的循环网络
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图20.5　深度玻尔兹曼机多预测训练过程的示意图。每一行指示相同训练步骤内小批量中的不；同样本。每列表示均匀场推断过程中的时间步。对于每个样本，我们对数据变量的子集进行采
          關鍵詞：对于每个样本, 每一行指示相同训练步骤内小批量中的不, 我们对数据变量的子集进行采, 每列表示均匀场推断过程中的时间步, 深度玻尔兹曼机多预测训练过程的示意图
        - 摘要：这种用于近似推断，通过计算图进行反向传播的一般原理已经应用于其
          關鍵詞：通过计算图进行反向传播的一般原理已经应用于其, 这种用于近似推断
        - 摘要：他模型（Stoyanov et  al. ，2011；Brakel  et  al.  ，2013）。在这些模型和；MP-DBM中，最终损失不是似然的下界。相反，最终损失通常基于近似；推断网络对缺失值施加的近似条件分布。这意味着这些模型的训练有些
          關鍵詞：最终损失通常基于近似, 这意味着这些模型的训练有些, 推断网络对缺失值施加的近似条件分布, 在这些模型和, 他模型
        - 摘要：通过推断图的反向传播有两个主要优点。首先，它以模型真正使用的方；式训练模型——使用近似推断。这意味着在MP-DBM中，进行如填充缺；失的输入或执行分类（尽管存在缺失的输入）的近似推断比在原始
          關鍵詞：进行如填充缺, 尽管存在缺失的输入, 式训练模型, 使用近似推断, 通过推断图的反向传播有两个主要优点
        - 摘要：MP-DBM启发了对NADE框架的扩展NADE-k（Raiko  et  al.  ，2014）  ，；我们将在第20.10.10节中描述。
          關鍵詞：框架的扩展, 启发了对, 我们将在第, 节中描述
        - 摘要：MP-DBM与Dropout有一定联系。Dropout在许多不同的计算图之间共享；相同的参数，每个图之间的差异是包括还是排除每个单元。MP-DBM还；在许多计算图之间共享参数。在MP-DBM的情况下，图之间的差异是每
          關鍵詞：相同的参数, 在许多不同的计算图之间共享, 每个图之间的差异是包括还是排除每个单元, 在许多计算图之间共享参数, 的情况下
        - 摘要：20.4.1　有趣的性质
          關鍵詞：有趣的性质
        - 摘要：20.4.2　DBM均匀场推断
          關鍵詞：均匀场推断
        - 摘要：20.4.3　DBM的参数学习
          關鍵詞：的参数学习
        - 摘要：20.4.4　逐层预训练
          關鍵詞：逐层预训练
        - 摘要：20.4.5　联合训练深度玻；尔兹曼机
          關鍵詞：尔兹曼机, 联合训练深度玻
    20.5：实值数据上的玻尔兹曼机
        - 摘要：20.5.1　Gaussian-Bernoulli RBM
          關鍵詞：
        - 摘要：20.5.2　条件协方差的无向模型
          關鍵詞：条件协方差的无向模型
        - 摘要：虽然玻尔兹曼机最初是为二值数据而开发的，但是许多应用，例如图像；和音频建模似乎需要表示实值上概率分布的能力。在一些情况下，我们；可以将区间［0，1］中的实值数据视为表示二值变量的期望。例如，
          關鍵詞：可以将区间, 例如图像, 我们, 在一些情况下, 例如
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；率值。每个像素定义二值变量为1的概率，并且二值像素的采样都彼此；独立。这是评估灰度图像数据集上二值模型的常见过程。然而，这种方
          關鍵詞：并且二值像素的采样都彼此, 每个像素定义二值变量为, 然而, 这是评估灰度图像数据集上二值模型的常见过程, 这种方
        - 摘要：20.5.1　Gaussian-Bernoulli RBM
          關鍵詞：
        - 摘要：受限玻尔兹曼机可以用于许多指数族的条件分布（Welling  et  al.  ，；2005）。其中，最常见的是具有二值隐藏单元和实值可见单元的；RBM，其中可见单元上的条件分布是高斯分布（均值为隐藏单元的函
          關鍵詞：最常见的是具有二值隐藏单元和实值可见单元的, 其中, 均值为隐藏单元的函, 受限玻尔兹曼机可以用于许多指数族的条件分布, 其中可见单元上的条件分布是高斯分布
        - 摘要：有很多方法可以参数化Gaussian-Bernoulli  RBM。首先，我们可以选择；协方差矩阵或精度矩阵来参数化高斯分布。这里，我们介绍选择精度矩；阵的情况。我们可以通过简单的修改获得协方差的形式。我们希望条件
          關鍵詞：我们介绍选择精度矩, 阵的情况, 我们可以选择, 协方差矩阵或精度矩阵来参数化高斯分布, 我们可以通过简单的修改获得协方差的形式
        - 摘要：通过扩展未归一化的对数条件分布可以找到需要添加到能量函数中的；项：
          關鍵詞：通过扩展未归一化的对数条件分布可以找到需要添加到能量函数中的
        - 摘要：此处f封装所有的参数，但不包括模型中的随机变量。因为f的唯一作用；是归一化分布，并且我们选择的任何可作为配分函数的能量函数都能起；到这个作用，所以我们可以忽略f。
          關鍵詞：因为, 封装所有的参数, 的唯一作用, 到这个作用, 但不包括模型中的随机变量
        - 摘要：如果我们在能量函数中包含式（20.39）中涉及 ν 的所有项（其符号被翻；转），并且不添加任何其他涉及 ν  的项，那么我们的能量函数就能表示；想要的条件分布p (ν| h )。
          關鍵詞：如果我们在能量函数中包含式, 想要的条件分布, 的所有项, 那么我们的能量函数就能表示, 中涉及
        - 摘要：其他条件分布比较自由，如p ( h |ν)。注意式（20.39）包含一项
          關鍵詞：包含一项, 其他条件分布比较自由, 注意式
        - 摘要：因为该项包含h i h j 项，它不能被全部包括在内。这些对应于隐藏单元之；间的边。如果我们包括这些项，将得到一个线性因子模型，而不是受限；玻尔兹曼机。当设计我们的玻尔兹曼机时，简单地省略这些h  i  h  j  交叉
          關鍵詞：交叉, 简单地省略这些, 因为该项包含, 这些对应于隐藏单元之, 间的边
        - 摘要：的事实（因为h  i  ∈{0，1}）。如果我们；在上面，我们使用了；在能量函数中包含此项（符号被翻转），则当该单元的权重较大且以高
          關鍵詞：因为, 的事实, 符号被翻转, 如果我们, 在上面
        - 摘要：因此，在Gaussian-Bernoulli RBM上定义能量函数的一种方式：
          關鍵詞：因此, 上定义能量函数的一种方式
        - 摘要：但我们还可以添加额外的项或者通过方差而不是精度参数化能量。
          關鍵詞：但我们还可以添加额外的项或者通过方差而不是精度参数化能量
        - 摘要：在这个推导中，我们没有在可见单元上添加偏置项，但添加这样的偏置；是容易的。Gaussian-Bernoulli  RBM参数化一个最终变化的来源是如何；处理精度矩阵的选择。它可以被固定为常数（可能基于数据的边缘精度
          關鍵詞：它可以被固定为常数, 在这个推导中, 参数化一个最终变化的来源是如何, 可能基于数据的边缘精度, 但添加这样的偏置
        - 摘要：20.5.2　条件协方差的无向模型
          關鍵詞：条件协方差的无向模型
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；et
          關鍵詞：
        - 摘要：虽然高斯RBM已成为实值数据的标准能量模型，Ranzato；al.；（2010a）认为高斯RBM感应偏置不能很好地适合某些类型的实值数据
          關鍵詞：感应偏置不能很好地适合某些类型的实值数据, 虽然高斯, 已成为实值数据的标准能量模型, 认为高斯
        - 摘要：均值和协方差RBM 　mcRBM使用隐藏单元独立地编码所有可观察单元；的条件均值和协方差。mcRBM的隐藏层分为两组单元：均值单元和协；方差单元。建模条件均值的那组单元是简单的高斯RBM。另一半是协
          關鍵詞：均值和协方差, 的隐藏层分为两组单元, 使用隐藏单元独立地编码所有可观察单元, 方差单元, 均值单元和协
        - 摘要：具体来说，在二值均值的单元  h  (m)  和二值协方差单元  h  (c)  的情况下，；mcRBM模型被定义为两个能量函数的组合：
          關鍵詞：模型被定义为两个能量函数的组合, 的情况下, 具体来说, 在二值均值的单元, 和二值协方差单元
        - 摘要：其中E m 为标准的Gaussian-Bernoulli RBM能量函数 (2) ，
          關鍵詞：为标准的, 其中, 能量函数
        - 摘要：E c 是cRBM建模条件协方差信息的能量函数：
          關鍵詞：建模条件协方差信息的能量函数
        - 摘要：参数 r  (j) 与  关联的协方差权重向量对应，  b  (c)  是一个协方差偏置向；量。组合后的能量函数定义联合分布，
          關鍵詞：参数, 组合后的能量函数定义联合分布, 是一个协方差偏置向, 关联的协方差权重向量对应
        - 摘要：以及给定  h  (m)  和  h  (c)  后，关于观察数据相应的条件分布（为一个多元；高斯分布）：
          關鍵詞：为一个多元, 关于观察数据相应的条件分布, 高斯分布, 以及给定
        - 摘要：注意协方差矩阵
          關鍵詞：注意协方差矩阵
        - 摘要：是非
          關鍵詞：是非
        - 摘要：对角的，且  W  是与建模条件均值的高斯RBM相关联的权重矩阵。由于；非对角的条件协方差结构，难以通过对比散度或持续性对比散度来训练；mcRBM。CD和PCD需要从 x 、 h (m) 、 h (c) 的联合分布中采样，这在标
          關鍵詞：需要从, 非对角的条件协方差结构, 相关联的权重矩阵, 这在标, 是与建模条件均值的高斯
        - 摘要：)采样，避免了直接从条件
          關鍵詞：避免了直接从条件, 采样
        - 摘要：x
          關鍵詞：
        - 摘要：抽样。
          關鍵詞：抽样
        - 摘要：学生t分布均值乘积 　学生t分布均值乘积（mPoT）模型（Ranzato et al.；，2010b）以类似mcRBM扩展cRBM的方式扩展PoT模型（Welling  et  al.；，2003a），通过添加类似高斯RBM中隐藏单元的非零高斯均值来实
          關鍵詞：分布均值乘积, 中隐藏单元的非零高斯均值来实, 学生, 的方式扩展, 扩展
        - 摘要：是关于正实数且
          關鍵詞：是关于正实数且
        - 摘要：mPoT的能量函数为
          關鍵詞：的能量函数为
        - 摘要：其中  r  (j)  是与单元   相关联的协方差权重向量，；（20.44）所定义。
          關鍵詞：所定义, 其中, 是与单元, 相关联的协方差权重向量
        - 摘要：如式
          關鍵詞：如式
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；正如mcRBM一样，mPoT模型能量函数指定一个多元高斯分布，其中关；x  的条件分布具有非对角的协方差。mPoT模型中的学习（也像
          關鍵詞：正如, 其中关, 一样, 模型中的学习, 也像
        - 摘要：尖峰和平板RBM  　尖峰和平板RBM（spike  and  slab  RBM，ssRBM）；（Courville et  al.  ，2011b）提供对实值数据的协方差结构建模的另一种；方法。与mcRBM相比，ssRBM具有既不需要矩阵求逆也不需要哈密尔
          關鍵詞：提供对实值数据的协方差结构建模的另一种, 相比, 尖峰和平板, 具有既不需要矩阵求逆也不需要哈密尔, 方法
        - 摘要：尖峰和平板RBM有两类隐藏单元：二值尖峰  （spike）单元h  和实值平；板  （slab）单元s  。条件于隐藏单元的可见单元均值由；给
          關鍵詞：二值尖峰, 条件于隐藏单元的可见单元均值由, 和实值平, 尖峰和平板, 单元
        - 摘要：运的是，使用Gibbs采样的对比散度和持续性对比散度仍然适用。此处；无须对任何矩阵求逆。
          關鍵詞：运的是, 无须对任何矩阵求逆, 使用, 采样的对比散度和持续性对比散度仍然适用, 此处
        - 摘要：形式上，ssRBM模型通过其能量函数定义：
          關鍵詞：形式上, 模型通过其能量函数定义
        - 摘要：其中b i 是尖峰h i 的偏置，Λ 是观测值 x 上的对角精度矩阵。参数α i ＞0；是实值平板变量s i 的标量精度参数。参数Φ  i  是定义 x 上的 h 调制二次；惩罚的非负对角矩阵。每个μ i 是平板变量s i 的均值参数。
          關鍵詞：是平板变量, 参数, 是定义, 是观测值, 调制二次
        - 摘要：利用能量函数定义的联合分布，能相对容易地导出ssRBM条件分布。例；如，通过边缘化平板变量 s ，给定二值尖峰变量 h ，关于观察量的条件；分布由下式给出
          關鍵詞：关于观察量的条件, 利用能量函数定义的联合分布, 通过边缘化平板变量, 分布由下式给出, 条件分布
        - 摘要：其中；有在协方差矩阵
          關鍵詞：其中, 有在协方差矩阵
        - 摘要：正定时成立。
          關鍵詞：正定时成立
        - 摘要：。最后的等式只
          關鍵詞：最后的等式只
        - 摘要：尖峰变量选通意味着h  ⊙s  上的真实边缘分布是稀疏的。这不同于稀疏；编码，其中来自模型的样本在编码中“几乎从不”（在测度理论意义上）；包含零，并且需要MAP推断来强加稀疏性。
          關鍵詞：上的真实边缘分布是稀疏的, 并且需要, 编码, 这不同于稀疏, 尖峰变量选通意味着
        - 摘要：相比mcRBM和mPoT模型，ssRBM以明显不同的方式参数化观察量的条；件协方差。mcRBM和mPoT都通过；建模观察量的
          關鍵詞：都通过, 建模观察量的, 模型, 件协方差, 相比
        - 摘要：尖峰和平板RBM的主要缺点是，参数的一些设置会对应于非正定的协；方差矩阵。这种协方差矩阵会在离均值更远的值上放置更大的未归一化；概率，导致所有可能结果上的积分发散。通常这个问题可以通过简单的
          關鍵詞：通常这个问题可以通过简单的, 的主要缺点是, 参数的一些设置会对应于非正定的协, 这种协方差矩阵会在离均值更远的值上放置更大的未归一化, 概率
        - 摘要：定性地，ssRBM的卷积变体能产生自然图像的优秀样本。图16.1中展示；了一些样例。
          關鍵詞：了一些样例, 的卷积变体能产生自然图像的优秀样本, 定性地, 中展示
        - 摘要：ssRBM允许几个扩展，包括平板变量的高阶交互和平均池化（Courville；et  al.  ，2014）使得模型能够在标注数据稀缺时为分类器学习到出色的
          關鍵詞：使得模型能够在标注数据稀缺时为分类器学习到出色的, 允许几个扩展, 包括平板变量的高阶交互和平均池化
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；特征。向能量函数添加一项能防止配分函数在稀疏编码模型下变得不确；定，如尖峰和平板稀疏编码（Goodfellow  et  al.  ，2013g），也称为
          關鍵詞：向能量函数添加一项能防止配分函数在稀疏编码模型下变得不确, 如尖峰和平板稀疏编码, 特征, 也称为
        - 摘要：20.5.1　Gaussian-；Bernoulli RBM
          關鍵詞：
        - 摘要：20.5.2　条件协方差的无；向模型
          關鍵詞：向模型, 条件协方差的无
    20.6：卷积玻尔兹曼机
        - 摘要：如第9章所示，超高维度输入（如图像）会对机器学习模型的计算、内；存和统计要求造成很大的压力。通过使用小核的离散卷积来替换矩阵乘；法是解决具有空间平移不变性或时间结构的输入问题的标准方式。
          關鍵詞：通过使用小核的离散卷积来替换矩阵乘, 章所示, 如图像, 法是解决具有空间平移不变性或时间结构的输入问题的标准方式, 如第
        - 摘要：深度卷积网络通常需要池化操作，使得每个连续层的空间大小减小。前；馈卷积网络通常使用池化函数，例如池化元素的最大值。目前尚不清楚；如何将其推广到基于能量的模型的设定中。我们可以在n个二值检测器
          關鍵詞：使得每个连续层的空间大小减小, 深度卷积网络通常需要池化操作, 例如池化元素的最大值, 个二值检测器, 馈卷积网络通常使用池化函数
        - 摘要：et
          關鍵詞：
        - 摘要：al.  （2009）针对这个问题，开发了一个称为概率最大池化；Lee；（probabilistic  max  pooling）的解决方案（不要与“随机池化”混淆，“随
          關鍵詞：针对这个问题, 混淆, 的解决方案, 随机池化, 开发了一个称为概率最大池化
        - 摘要：虽然高效的概率最大池化确实能强迫检测器单元互斥，这在某些情景下；可能是有用的正则化约束，而在其他情景下是对模型容量有害的限制。；它也不支持重叠池化区域。从前馈卷积网络获得最佳性能通常需要重叠
          關鍵詞：可能是有用的正则化约束, 这在某些情景下, 它也不支持重叠池化区域, 从前馈卷积网络获得最佳性能通常需要重叠, 而在其他情景下是对模型容量有害的限制
        - 摘要：Lee et al. （2009）证明概率最大池化可以用于构建卷积深度玻尔兹曼机
          關鍵詞：证明概率最大池化可以用于构建卷积深度玻尔兹曼机
        - 摘要：(3) 。该模型能够执行诸如填补输入缺失部分的操作。虽然这种模型在理；论上有吸引力，让它在实践中工作是具有挑战性的，作为分类器通常不；如通过监督训练的传统卷积网络。
          關鍵詞：作为分类器通常不, 如通过监督训练的传统卷积网络, 虽然这种模型在理, 该模型能够执行诸如填补输入缺失部分的操作, 论上有吸引力
        - 摘要：许多卷积模型对于许多不同空间大小的输入同样有效。对于玻尔兹曼；机，由于各种原因很难改变输入尺寸。配分函数随着输入大小的改变而；改变。此外，许多卷积网络按与输入大小成比例地缩放池化区域来实现
          關鍵詞：由于各种原因很难改变输入尺寸, 此外, 改变, 配分函数随着输入大小的改变而, 许多卷积模型对于许多不同空间大小的输入同样有效
        - 摘要：图像边界处的像素也带来一些困难，由于玻尔兹曼机中的连接是对称的；事实而加剧。如果我们不隐式地补零输入，则将会导致比可见单元更少；的隐藏单元，并且图像边界处的可见单元将不能被良好地建模，因为它
          關鍵詞：并且图像边界处的可见单元将不能被良好地建模, 则将会导致比可见单元更少, 的隐藏单元, 由于玻尔兹曼机中的连接是对称的, 事实而加剧
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；20.7　用于结构化或序列输出的玻尔兹曼机
          關鍵詞：用于结构化或序列输出的玻尔兹曼机
    20.7：用于结构化或序列输出的玻尔兹曼
        - 摘要：在结构化输出场景中，我们希望训练可以从一些输入 x 映射到一些输出；y  的模型，  y  的不同条目彼此相关，并且必须遵守一些约束。例如，在
          關鍵詞：并且必须遵守一些约束, 在结构化输出场景中, 映射到一些输出, 我们希望训练可以从一些输入, 的模型
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；语音合成任务中， y 是波形，并且整个波形听起来必须像连贯的发音。
          關鍵詞：并且整个波形听起来必须像连贯的发音, 语音合成任务中, 是波形
        - 摘要：表示 y 中的条目之间关系的自然方式是使用概率分布p( y ｜ x )。扩展到；建模条件分布的玻尔兹曼机可以支持这种概率模型。
          關鍵詞：表示, 中的条目之间关系的自然方式是使用概率分布, 建模条件分布的玻尔兹曼机可以支持这种概率模型, 扩展到
        - 摘要：使用玻尔兹曼机条件建模的相同工具不仅可以用于结构化输出任务，还；可以用于序列建模。在后一种情况下，模型必须估计变量序列上的概率；，而不仅仅是将输入 x 映射到输出 y 。为完成这
          關鍵詞：可以用于序列建模, 模型必须估计变量序列上的概率, 在后一种情况下, 而不仅仅是将输入, 使用玻尔兹曼机条件建模的相同工具不仅可以用于结构化输出任务
        - 摘要：视频游戏和电影工业中一个重要序列建模任务是建模用于渲染3-D人物；骨架关节角度的序列。这些序列通常通过记录角色移动的运动捕获系统；收集。人物运动的概率模型允许生成新的（之前没见过的）但真实的动
          關鍵詞：这些序列通常通过记录角色移动的运动捕获系统, 人物, 人物运动的概率模型允许生成新的, 但真实的动, 之前没见过的
        - 摘要：。该模型是
          關鍵詞：该模型是
        - 摘要：另一个序列建模任务是对构成歌曲音符序列的分布进行建模。；Boulanger-Lewandowski et  al.  （2012）引入了RNN-RBM  序列模型并应；用于这个任务。RNN-RBM由RNN（产生用于每个时间步的RBM参数）
          關鍵詞：参数, 另一个序列建模任务是对构成歌曲音符序列的分布进行建模, 用于这个任务, 序列模型并应, 产生用于每个时间步的
    20.8：其他玻尔兹曼机
        - 摘要：玻尔兹曼机的许多其他变种是可能的。
          關鍵詞：玻尔兹曼机的许多其他变种是可能的
        - 摘要：玻尔兹曼机可以用不同的训练准则扩展。我们专注于训练为大致最大化；生成标准log p( ν )的玻尔兹曼机。相反，旨在最大化log p(y｜  ν )来训练；判别的RBM也是有可能的（Larochelle  and  Bengio，2008a）。当使用生
          關鍵詞：来训练, 也是有可能的, 当使用生, 旨在最大化, 判别的
        - 摘要：在实践中使用的大多数玻尔兹曼机在其能量函数中仅具有二阶相互作；用，意味着它们的能量函数是许多项的和，并且每个单独项仅包括两个；随机变量之间的乘积。这种项的一个例子是ν i W i,j h j 。我们还可以训练
          關鍵詞：在实践中使用的大多数玻尔兹曼机在其能量函数中仅具有二阶相互作, 这种项的一个例子是, 意味着它们的能量函数是许多项的和, 我们还可以训练, 随机变量之间的乘积
        - 摘要：更一般地说，玻尔兹曼机框架是一个丰富的模型空间，允许比迄今为止；已经探索的更多的模型结构。开发新形式的玻尔兹曼机相比于开发新的；神经网络层需要更多细心和创造力，因为它通常很难找到一个能保持玻
          關鍵詞：已经探索的更多的模型结构, 因为它通常很难找到一个能保持玻, 玻尔兹曼机框架是一个丰富的模型空间, 开发新形式的玻尔兹曼机相比于开发新的, 更一般地说
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；20.9　通过随机操作的反向传播
          關鍵詞：通过随机操作的反向传播
        - 摘要：传统的神经网络对一些输入变量  x  施加确定性变换。当开发生成模型；时，我们经常希望扩展神经网络以实现 x 的随机变换。这样做的一个直；接方法是使用额外输入  z  （从一些简单的概率分布采样得到，如均匀或
          關鍵詞：的随机变换, 施加确定性变换, 如均匀或, 这样做的一个直, 传统的神经网络对一些输入变量
        - 摘要：作为示例，让我们考虑从均值μ和方差σ 2 的高斯分布中采样y的操作：
          關鍵詞：让我们考虑从均值, 的高斯分布中采样, 和方差, 作为示例, 的操作
        - 摘要：因为y的单个样本不是由函数产生的，而是由一个采样过程产生，它的；输出会随我们的每次查询发生变化，所以取y相对于其分布的参数μ和σ 2；的导数似乎是违反直觉的。然而，我们可以将采样过程重写，对基本随
          關鍵詞：因为, 它的, 所以取, 而是由一个采样过程产生, 然而
        - 摘要：进行转换以从期望的分布获得样本：
          關鍵詞：进行转换以从期望的分布获得样本
        - 摘要：现在我们将其视为具有额外输入z的确定性操作，可以通过采样操作来；反向传播。至关重要的是，额外输入是一个随机变量，其分布不是任何；我们想对其计算导数的变量的函数。如果我们可以用相同的z值再次重
          關鍵詞：我们想对其计算导数的变量的函数, 现在我们将其视为具有额外输入, 的确定性操作, 其分布不是任何, 可以通过采样操作来
        - 摘要：能够通过该采样操作反向传播允许我们将其并入更大的图中。我们可以；在采样分布的输出之上构建图元素。例如，我们可以计算一些损失函数；J(y)的导数。我们还可以构建这样的图元素，其输出是采样操作的输入
          關鍵詞：我们可以, 我们还可以构建这样的图元素, 在采样分布的输出之上构建图元素, 例如, 能够通过该采样操作反向传播允许我们将其并入更大的图中
        - 摘要：构建更大
          關鍵詞：构建更大
        - 摘要：和
          關鍵詞：
        - 摘要：。
          關鍵詞：
        - 摘要：在该高斯采样示例中使用的原理能更广泛地应用。我们可以将任何形为；，其中   是同时包；p(y;  θ )或p(y｜ x  ; θ  )的概率分布表示为
          關鍵詞：在该高斯采样示例中使用的原理能更广泛地应用, 的概率分布表示为, 其中, 我们可以将任何形为, 是同时包
        - 摘要：含参数 θ 和输入 x  的变量（如果适用的话）。给定从分布；样的值y（其中  可以是其他变量的函数），我们可以将
          關鍵詞：含参数, 如果适用的话, 给定从分布, 我们可以将, 可以是其他变量的函数
        - 摘要：采
          關鍵詞：
        - 摘要：重写为
          關鍵詞：重写为
        - 摘要：其中  z  是随机性的来源。只要f是几乎处处连续可微的，我们就可以使；用传统工具（例如应用于f的反向传播算法）计算y相对于   的导数。；至关重要的是，  不能是 z 的函数，且 z 不能是  的函数。这种技术
          關鍵詞：用传统工具, 是随机性的来源, 的反向传播算法, 相对于, 其中
        - 摘要：要求f是连续可微的，当然需要  y  是连续的。如果我们希望通过产生离；散值样本的采样过程进行反向传播，则可以使用强化学习算法（如；REINFORCE算法（Williams，1992）的变体）来估计   上的梯度，这
          關鍵詞：要求, 算法, 的变体, 则可以使用强化学习算法, 当然需要
        - 摘要：在神经网络应用中，我们通常选择从一些简单的分布中采样  z  ，如单位；均匀分布或单位高斯分布，并通过网络的确定性部分重塑其输入来实现；更复杂的分布。
          關鍵詞：均匀分布或单位高斯分布, 在神经网络应用中, 更复杂的分布, 如单位, 并通过网络的确定性部分重塑其输入来实现
        - 摘要：通过随机操作扩展梯度或优化的想法可追溯到20世纪中叶（Price，；1958；Bonnet，1964），并且首先在强化学习（Williams，1992）的情；景下用于机器学习。最近，它已被应用于变分近似（Opper
          關鍵詞：世纪中叶, 的情, 通过随机操作扩展梯度或优化的想法可追溯到, 并且首先在强化学习, 最近
        - 摘要：20.9.1　通过离散随机操作的反向传播
          關鍵詞：通过离散随机操作的反向传播
        - 摘要：当模型发射离散变量  y  时，重参数化技巧不再适用。假设模型采用输入
          關鍵詞：假设模型采用输入, 重参数化技巧不再适用, 当模型发射离散变量
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；x 和参数 θ  ，两者都封装在向量   中，并且将它们与随机噪声  z  组合；以产生 y ：
          關鍵詞：以产生, 并且将它们与随机噪声, 和参数, 组合, 两者都封装在向量
        - 摘要：因为  y  是离散的，f必须是一个阶跃函数。阶跃函数的导数在任何点都；是没用的。在每个阶跃边界，导数是未定义的，但这是一个小问题。大；问题是导数在阶跃边界之间的区域几乎处处为零。因此，任何代价函数
          關鍵詞：因为, 但这是一个小问题, 必须是一个阶跃函数, 在每个阶跃边界, 因此
        - 摘要：REINFORCE算法（REward；Increment  ＝  nonnegative  Factor×Offset；Reinforcement×Characteristic  Eligibility）提供了定义一系列简单而强大
          關鍵詞：提供了定义一系列简单而强大, 算法
        - 摘要：是具有无用导数的阶跃函数，期望代价；通常是服从梯度下降的光滑函数。虽然当  y  是高维；（或者是许多离散随机决策组合的结果）时，该期望通常是难解的，但
          關鍵詞：是高维, 或者是许多离散随机决策组合的结果, 虽然当, 期望代价, 通常是服从梯度下降的光滑函数
        - 摘要：通过简单地微分期望成本，我们可以推导出REINFORCE最简单的版；本：
          關鍵詞：我们可以推导出, 最简单的版, 通过简单地微分期望成本
        - 摘要：式（20.60）依赖于J不直接引用   的假设。放松这个假设来扩展该方；法是简单的。式（20.61）利用对数的导数规则，
          關鍵詞：不直接引用, 依赖于, 放松这个假设来扩展该方, 利用对数的导数规则, 法是简单的
        - 摘要：。式（20.62）给出了该梯度的无偏蒙
          關鍵詞：给出了该梯度的无偏蒙
        - 摘要：特卡罗估计。
          關鍵詞：特卡罗估计
        - 摘要：在本节中我们写的
          關鍵詞：在本节中我们写的
        - 摘要：，可以等价地写成
          關鍵詞：可以等价地写成
        - 摘要：。这是因为
          關鍵詞：这是因为
        - 摘要：由
          關鍵詞：
        - 摘要：参数化，并且如果 x 存在，则
          關鍵詞：并且如果, 参数化, 存在
        - 摘要：包含 θ 和 x 两者。
          關鍵詞：两者, 包含
        - 摘要：简单REINFORCE估计的一个问题是其具有非常高的方差，需要采  y  的；许多样本才能获得对梯度的良好估计，或者等价地，如果仅绘制一个样；本，则SGD将收敛得非常缓慢并将需要较小的学习率。通过使用方差减
          關鍵詞：许多样本才能获得对梯度的良好估计, 如果仅绘制一个样, 简单, 需要采, 将收敛得非常缓慢并将需要较小的学习率
        - 摘要：这意味着
          關鍵詞：这意味着
        - 摘要：此外，我们可以通过计算
          關鍵詞：此外, 我们可以通过计算
        - 摘要：关于p( y )的方差，并关于；最佳基线
          關鍵詞：最佳基线, 并关于, 关于, 的方差
        - 摘要：对于向量
          關鍵詞：对于向量
        - 摘要：最小化获得最优；的每个元素ω i 是不同的：
          關鍵詞：最小化获得最优, 的每个元素, 是不同的
        - 摘要：。我们发现这个
          關鍵詞：我们发现这个
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；相对于ω i 的梯度估计则变为
          關鍵詞：的梯度估计则变为, 相对于
        - 摘要：其中；神经网络，并训练新输出对
          關鍵詞：神经网络, 并训练新输出对, 其中
        - 摘要：估计上述
          關鍵詞：估计上述
        - 摘要：。获得估计b通常需要将额外输出添加到；的每个元素估计
          關鍵詞：获得估计, 通常需要将额外输出添加到, 的每个元素估计
        - 摘要：和
          關鍵詞：
        - 摘要：。这
          關鍵詞：
        - 摘要：些额外的输出可以用均方误差目标训练，对于给定的
          關鍵詞：对于给定的, 些额外的输出可以用均方误差目标训练
        - 摘要：，从p(  y  )采
          關鍵詞：
        - 摘要：样  y  时，分别用
          關鍵詞：分别用
        - 摘要：和
          關鍵詞：
        - 摘要：作目标。然
          關鍵詞：作目标
        - 摘要：后可以将这些估计代入式（20.68）就能恢复估计b。Mnih；Gregor（2014）倾向于使用通过目标J(  y
          關鍵詞：倾向于使用通过目标, 后可以将这些估计代入式, 就能恢复估计
        - 摘要：and；)训练的单个共享输出（跨越
          關鍵詞：跨越, 训练的单个共享输出
        - 摘要：的所有元素i），并使用
          關鍵詞：并使用, 的所有元素
        - 摘要：作为基线。
          關鍵詞：作为基线
        - 摘要：在强化学习背景下引入的方差减小方法（Sutton  et  al.  ，2000；Weaver；and  Tao，2001），Dayan（1990）推广了二值奖励的前期工作。可以参；考Bengio  et  al.  （2013b）、Mnih  and  Gregor（2014）、Ba  et  al.
          關鍵詞：可以参, 推广了二值奖励的前期工作, 在强化学习背景下引入的方差减小方法
        - 摘要：，Mnih  and  Gregor（2014）发现可以在训练期间调整；的尺度（即除以训练期间的移动平均估计的标准；差），即作为一种适应性学习率，可以抵消训练过程中该量大小发生的
          關鍵詞：的尺度, 可以抵消训练过程中该量大小发生的, 发现可以在训练期间调整, 即除以训练期间的移动平均估计的标准, 即作为一种适应性学习率
        - 摘要：基于REINFORCE的估计器可以被理解为将  y  的选择与J(  y  )的对应值相；关联来估计梯度。如果在当前参数化下不太可能出现 y  的良好值，则可；能需要很长时间来偶然获得它，并且获得所需信号的配置应当被加强。
          關鍵詞：如果在当前参数化下不太可能出现, 则可, 的对应值相, 的估计器可以被理解为将, 能需要很长时间来偶然获得它
    20.9：通过随机操作的反向传播
        - 摘要：20.9.1　通过离散随机操作的反向传播
          關鍵詞：通过离散随机操作的反向传播
        - 摘要：20.9.1　通过离散随机操；作的反向传播
          關鍵詞：通过离散随机操, 作的反向传播
    20.10：有向生成网络
        - 摘要：20.10.1　sigmoid信念网络
          關鍵詞：信念网络
        - 摘要：20.10.2　可微生成器网络
          關鍵詞：可微生成器网络
        - 摘要：20.10.3　变分自编码器
          關鍵詞：变分自编码器
        - 摘要：20.10.4　生成式对抗网络
          關鍵詞：生成式对抗网络
        - 摘要：20.10.5　生成矩匹配网络
          關鍵詞：生成矩匹配网络
        - 摘要：20.10.6　卷积生成网络
          關鍵詞：卷积生成网络
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；20.10.7　自回归网络
          關鍵詞：自回归网络
        - 摘要：20.10.8　线性自回归网络
          關鍵詞：线性自回归网络
        - 摘要：20.10.9　神经自回归网络
          關鍵詞：神经自回归网络
        - 摘要：20.10.10　NADE
          關鍵詞：
        - 摘要：如第16章所讨论的，有向图模型构成了一类突出的图模型。虽然有向图
          關鍵詞：虽然有向图, 章所讨论的, 如第, 有向图模型构成了一类突出的图模型
        - 摘要：模型在更大的机器学习社群中非常流行，但在较小的深度学习社群中，；大约直到2013年它们都掩盖在无向模型（如RBM）的光彩之下。
          關鍵詞：大约直到, 年它们都掩盖在无向模型, 的光彩之下, 但在较小的深度学习社群中, 模型在更大的机器学习社群中非常流行
        - 摘要：在本节中，我们回顾一些传统上与深度学习社群相关的标准有向图模；型。
          關鍵詞：在本节中, 我们回顾一些传统上与深度学习社群相关的标准有向图模
        - 摘要：我们已经描述过部分有向的模型——深度信念网络。我们还描述过可以；被认为是浅度有向生成模型的稀疏编码模型。尽管在样本生成和密度估；计方面表现不佳，在深度学习的背景下它们通常被用作特征学习器。我
          關鍵詞：我们已经描述过部分有向的模型, 我们还描述过可以, 在深度学习的背景下它们通常被用作特征学习器, 尽管在样本生成和密度估, 深度信念网络
        - 摘要：20.10.1　sigmoid信念网络
          關鍵詞：信念网络
        - 摘要：sigmoid信念网络（Neal，1990）是一种具有特定条件概率分布的有向图；模型的简单形式。一般来说，我们可以将sigmoid信念网络视为具有二；值向量的状态s ，其中状态的每个元素都受其祖先影响：
          關鍵詞：我们可以将, 信念网络视为具有二, 信念网络, 模型的简单形式, 一般来说
        - 摘要：sigmoid信念网络最常见的结构是被分为许多层的结构，其中原始采样；通过一系列多个隐藏层进行，然后最终生成可见层。这种结构与深度信；念网络非常相似，但它们在采样过程开始时的单元彼此独立，而不是从
          關鍵詞：然后最终生成可见层, 念网络非常相似, 其中原始采样, 但它们在采样过程开始时的单元彼此独立, 而不是从
        - 摘要：虽然生成可见单元的样本在sigmoid信念网络中是非常高效的，但是其；他大多数操作不是很高效。给定可见单元，对隐藏单元的推断是难解；的。因为变分下界涉及对包含整个层的团求期望，均匀场推断也是难以
          關鍵詞：给定可见单元, 虽然生成可见单元的样本在, 他大多数操作不是很高效, 均匀场推断也是难以, 但是其
        - 摘要：在sigmoid信念网络中执行推断的一种方法是构造专用于sigmoid信念网；络的不同下界（Saul  et  al.  ，1996）。这种方法只适用于非常小的网；络。另一种方法是使用学成推断机制，如第19.5节中描述的。Helmholtz
          關鍵詞：节中描述的, 另一种方法是使用学成推断机制, 如第, 络的不同下界, 信念网
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；机（Dayan et  al. ，1995；Dayan and Hinton，1996）结合了一个sigmoid；信念网络与一个预测隐藏单元上均匀场分布参数的推断网络。sigmoid
          關鍵詞：结合了一个, 信念网络与一个预测隐藏单元上均匀场分布参数的推断网络
        - 摘要：sigmoid信念网络的一种特殊情况是没有潜变量的情况。在这种情况下；学习是高效的，因为没有必要将潜变量边缘化到似然之外。一系列称为；自回归网络的模型将这个完全可见的信念网络泛化到其他类型的变量
          關鍵詞：自回归网络的模型将这个完全可见的信念网络泛化到其他类型的变量, 因为没有必要将潜变量边缘化到似然之外, 在这种情况下, 一系列称为, 学习是高效的
        - 摘要：20.10.2　可微生成器网络
          關鍵詞：可微生成器网络
        - 摘要：许多生成模型基于使用可微生成器网络  （generator  network）的想法。；将潜变量z  的样本变换为样本x  或样；这种模型使用可微函数
          關鍵詞：的想法, 的样本变换为样本, 许多生成模型基于使用可微生成器网络, 这种模型使用可微函数, 将潜变量
        - 摘要：生成器网络本质上仅是用于生成样本的参数化计算过程，其中的体系结；构提供了从中采样的可能分布族以及选择这些族内分布的参数。
          關鍵詞：构提供了从中采样的可能分布族以及选择这些族内分布的参数, 其中的体系结, 生成器网络本质上仅是用于生成样本的参数化计算过程
        - 摘要：作为示例，从具有均值 μ 和协方差Σ 的正态分布绘制样本的标准过程是；将来自零均值和单位协方差的正态分布的样本  z  馈送到非常简单的生成；器网络中。这个生成器网络只包含一个仿射层：
          關鍵詞：和协方差, 的正态分布绘制样本的标准过程是, 作为示例, 馈送到非常简单的生成, 这个生成器网络只包含一个仿射层
        - 摘要：其中 L 由Σ 的Cholesky分解给出。
          關鍵詞：分解给出, 其中
        - 摘要：伪随机数发生器也可以使用简单分布的非线性变换。例如，逆变换采样；（inverse transform sampling）（Devroye，2013）从U(0,1)中采一个标量
          關鍵詞：伪随机数发生器也可以使用简单分布的非线性变换, 中采一个标量, 例如, 逆变换采样
        - 摘要：z，并且对标量x应用非线性变换。在这种情况下，g(z)由累积分布函数
          關鍵詞：并且对标量, 由累积分布函数, 在这种情况下, 应用非线性变换
        - 摘要：的反函数给出。如果我们能够指定P(x)，在x上积；分，并取所得函数的反函数，我们不用通过机器学习就能从P(x)进行采；样。
          關鍵詞：的反函数给出, 进行采, 我们不用通过机器学习就能从, 如果我们能够指定, 上积
        - 摘要：为了从更复杂的分布（难以直接指定、难以积分或难以求所得积分的反；函数）中生成样本，我们使用前馈网络来表示非线性函数g的参数族，；并使用训练数据来推断参数以选择所期望的函数。
          關鍵詞：并使用训练数据来推断参数以选择所期望的函数, 为了从更复杂的分布, 函数, 的参数族, 我们使用前馈网络来表示非线性函数
        - 摘要：我们可以认为g提供了变量的非线性变化，将z 上的分布变换成x  上想要；的分布。
          關鍵詞：上想要, 提供了变量的非线性变化, 上的分布变换成, 的分布, 我们可以认为
        - 摘要：回顾式（3.47），对于可求反函数的、可微的、连续的g，
          關鍵詞：回顾式, 可微的, 连续的, 对于可求反函数的
        - 摘要：这隐含地对x 施加概率分布：
          關鍵詞：施加概率分布, 这隐含地对
        - 摘要：当然，取决于g的选择，这个公式可能难以评估，因此我们经常需要使；用间接学习g的方法，而不是直接尝试最大化log p( x )。
          關鍵詞：取决于, 而不是直接尝试最大化, 的选择, 因此我们经常需要使, 的方法
        - 摘要：在某些情况下，我们使用g来定义 x 上的条件分布，而不是使用g直接提；x  的样本。例如，我们可以使用一个生成器网络，其最后一层由；供
          關鍵詞：而不是使用, 我们可以使用一个生成器网络, 我们使用, 直接提, 例如
        - 摘要：在这种情况下，我们使用g来定义p( x ｜z  )时，通过边缘化  z  来对  x  施；加分布：
          關鍵詞：通过边缘化, 我们使用, 加分布, 来对, 在这种情况下
        - 摘要：两种方法都定义了一个分布p  g  (  x  )，并允许我们使用第20.9节中的重参；数化技巧来训练p g 的各种评估准则。
          關鍵詞：节中的重参, 的各种评估准则, 两种方法都定义了一个分布, 并允许我们使用第, 数化技巧来训练
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；表示生成器网络的两种不同方法（发出条件分布的参数相对直接发射样；品）具有互补的优缺点。当生成器网络在 x 上定义条件分布时，它不但
          關鍵詞：表示生成器网络的两种不同方法, 上定义条件分布时, 当生成器网络在, 发出条件分布的参数相对直接发射样, 它不但
        - 摘要：基于可微生成器网络的方法是由分类可微前馈网络中梯度下降的成功应；用而推动的。在监督学习的背景中，基于梯度训练学习的深度前馈网络；在给定足够的隐藏单元和足够的训练数据的情况下，在实践中似乎能保
          關鍵詞：基于梯度训练学习的深度前馈网络, 在监督学习的背景中, 基于可微生成器网络的方法是由分类可微前馈网络中梯度下降的成功应, 在实践中似乎能保, 用而推动的
        - 摘要：生成式建模似乎比分类或回归更困难，因为学习过程需要优化难以处理；的准则。在可微生成器网络的情况中，准则是难以处理的，因为数据不；指定生成器网络的输入 z 和输出 x 。在监督学习的情况下，输入 x 和输
          關鍵詞：在监督学习的情况下, 生成式建模似乎比分类或回归更困难, 输入, 的准则, 和输
        - 摘要：Dosovitskiy et  al.  （2015）研究了一个简化问题，其中  z  和  x  之间的对；应关系已经给出。具体来说，训练数据是计算机渲染的椅子图。潜变量；z  是渲染引擎的参数，描述了椅子模型的选择、椅子的位置以及影响图
          關鍵詞：是渲染引擎的参数, 应关系已经给出, 潜变量, 描述了椅子模型的选择, 其中
        - 摘要：在接下来的章节中，我们讨论仅给出 x 的训练样本，训练可微生成器网；络的几种方法。
          關鍵詞：络的几种方法, 的训练样本, 我们讨论仅给出, 在接下来的章节中, 训练可微生成器网
        - 摘要：20.10.3　变分自编码器
          關鍵詞：变分自编码器
        - 摘要：auto-encoder，VAE）（Kingma，2013；；变分自编码器  （variational；Rezende et  al. ，2014）是一个使用学好的近似推断的有向模型，可以纯
          關鍵詞：是一个使用学好的近似推断的有向模型, 变分自编码器, 可以纯
        - 摘要：粹地使用基于梯度的方法进行训练。
          關鍵詞：粹地使用基于梯度的方法进行训练
        - 摘要：为了从模型生成样本，VAE首先从编码分布；后使样本通过可微生成器网络g(；z
          關鍵詞：为了从模型生成样本, 首先从编码分布, 后使样本通过可微生成器网络
        - 摘要：间，近似推断网络（或编码器）
          關鍵詞：近似推断网络, 或编码器
        - 摘要：则被视为解码器网络。
          關鍵詞：则被视为解码器网络
        - 摘要：中采样 z 。然；)。最后，从分布；中采样  x  。然而在训练期
          關鍵詞：从分布, 最后, 然而在训练期, 中采样
        - 摘要：用于获得
          關鍵詞：用于获得
        - 摘要：变分自编码器背后的关键思想是，它们可以通过最大化与数据点 x 相关；联的变分下界
          關鍵詞：联的变分下界, 相关, 它们可以通过最大化与数据点, 变分自编码器背后的关键思想是
        - 摘要：来训练：
          關鍵詞：来训练
        - 摘要：在式（20.76）中，我们将第一项视为潜变量的近似后验下可见和隐藏；变量的联合对数似然性（正如EM一样，不同的是我们使用近似而不是；精确后验）。第二项则可视为近似后验的熵。当q被选择为高斯分布，
          關鍵詞：精确后验, 我们将第一项视为潜变量的近似后验下可见和隐藏, 在式, 被选择为高斯分布, 正如
        - 摘要：彼此
          關鍵詞：彼此
        - 摘要：变分推断和学习的传统方法是通过优化算法推断q，通常是迭代不动点；方程（第19.4节）。这些方法是缓慢的，并且通常需要以闭解形式计算；。变分自编码器背后的主要思想是训练产
          關鍵詞：方程, 这些方法是缓慢的, 通常是迭代不动点, 变分自编码器背后的主要思想是训练产, 变分推断和学习的传统方法是通过优化算法推断
        - 摘要：变分自编码器方法是优雅的，理论上令人愉快的，并且易于实现。它也
          關鍵詞：并且易于实现, 变分自编码器方法是优雅的, 理论上令人愉快的, 它也
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；获得了出色的结果，是生成式建模中的最先进方法之一。它的主要缺点；是从在图像上训练的变分自编码器中采样的样本往往有些模糊。这种现
          關鍵詞：这种现, 它的主要缺点, 是生成式建模中的最先进方法之一, 获得了出色的结果, 是从在图像上训练的变分自编码器中采样的样本往往有些模糊
        - 摘要：et
          關鍵詞：
        - 摘要：VAE框架可以直接扩展到大范围的模型架构。相比玻尔兹曼机，这是关；键的优势，因为玻尔兹曼机需要非常仔细地设计模型来保持易解性。；VAE可以与广泛的可微算子族一起良好工作。一个特别复杂的VAE是深
          關鍵詞：相比玻尔兹曼机, 因为玻尔兹曼机需要非常仔细地设计模型来保持易解性, 是深, 框架可以直接扩展到大范围的模型架构, 一个特别复杂的
        - 摘要：VAE框架已不仅仅扩展到传统的变分下界，还有重要加权自编码器；（importance-weighted autoencoder）（Burda et al. ，2015）的目标：
          關鍵詞：框架已不仅仅扩展到传统的变分下界, 的目标, 还有重要加权自编码器
        - 摘要：这个新的目标在k＝1时等同于传统的下界   。然而，它也可以被解释；的重要采样而形成的真实；为基于提议分布
          關鍵詞：它也可以被解释, 这个新的目标在, 然而, 时等同于传统的下界, 的重要采样而形成的真实
        - 摘要：中
          關鍵詞：
        - 摘要：z
          關鍵詞：
        - 摘要：估计。重要加权自编码器目标也是
          關鍵詞：估计, 重要加权自编码器目标也是
        - 摘要：的下界，并且随着k增加而变得更紧。
          關鍵詞：并且随着, 增加而变得更紧, 的下界
        - 摘要：变分自编码器与MP-DBM和其他涉及通过近似推断图的反向传播方法有；一些有趣的联系（Goodfellow  et  al.  ，2013d；Stoyanov  et  al.  ，2011；；Brakel et al. ，2013）。这些以前的方法需要诸如均匀场不动点方程的推
          關鍵詞：和其他涉及通过近似推断图的反向传播方法有, 这些以前的方法需要诸如均匀场不动点方程的推, 一些有趣的联系, 变分自编码器与
        - 摘要：变分自编码器的一个非常好的特性是，同时训练参数编码器与生成器网；络的组合迫使模型学习一个编码器可以捕获的可预测的坐标系。这使得；它成为一个优秀的流形学习算法。图20.6展示了由变分自编码器学到的
          關鍵詞：这使得, 同时训练参数编码器与生成器网, 它成为一个优秀的流形学习算法, 展示了由变分自编码器学到的, 络的组合迫使模型学习一个编码器可以捕获的可预测的坐标系
        - 摘要：图20.6　由变分自编码器学习的高维流形在二维坐标系中的示例（Kingma and Welling，
          關鍵詞：由变分自编码器学习的高维流形在二维坐标系中的示例
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；2014a）。我们可以在纸上直接绘制两个可视化的维度，因此可以使用二维潜在编码训练模型来；了解模型的工作原理（即使我们认为数据流形的固有维度要高得多）。图中所示的图像不是来
          關鍵詞：即使我们认为数据流形的固有维度要高得多, 图中所示的图像不是来, 我们可以在纸上直接绘制两个可视化的维度, 了解模型的工作原理, 因此可以使用二维潜在编码训练模型来
        - 摘要：20.10.4　生成式对抗网络
          關鍵詞：生成式对抗网络
        - 摘要：生成式对抗网络  （generative  adversarial  network，GAN）（Goodfellow；et al. ，2014c）是基于可微生成器网络的另一种生成式建模方法。
          關鍵詞：是基于可微生成器网络的另一种生成式建模方法, 生成式对抗网络
        - 摘要：生成式对抗网络基于博弈论场景，其中生成器网络必须与对手竞争。生；成器网络直接产生样本；。其对手，判别器网络
          關鍵詞：生成式对抗网络基于博弈论场景, 成器网络直接产生样本, 其对手, 判别器网络, 其中生成器网络必须与对手竞争
        - 摘要：形式化表示生成式对抗网络中学习的最简单方式是零和游戏，其中函数；作为它自；己的收益。在学习期间，每个玩家尝试最大化自己的收益，因此收敛在
          關鍵詞：己的收益, 在学习期间, 其中函数, 形式化表示生成式对抗网络中学习的最简单方式是零和游戏, 每个玩家尝试最大化自己的收益
        - 摘要：确定判别器的收益。生成器接收
          關鍵詞：确定判别器的收益, 生成器接收
        - 摘要：v的默认选择是
          關鍵詞：的默认选择是
        - 摘要：这驱使判别器试图学习将样品正确地分类为真的或伪造的。同时，生成；器试图欺骗分类器以让其相信样本是真实的。在收敛时，生成器的样本
          關鍵詞：同时, 器试图欺骗分类器以让其相信样本是真实的, 在收敛时, 这驱使判别器试图学习将样品正确地分类为真的或伪造的, 生成
        - 摘要：与实际数据不可区分，并且判别器处处都输出   。然后就可以丢弃判
          關鍵詞：并且判别器处处都输出, 与实际数据不可区分, 然后就可以丢弃判
        - 摘要：别器。
          關鍵詞：别器
        - 摘要：设计GAN的主要动机是学习过程既不需要近似推断，也不需要配分函数；梯度的近似。当max  d  ν(g ,d )在 θ  (g)  中是凸的（例如，在概率密度函数；的空间中直接执行优化的情况）时，该过程保证收敛并且是渐近一致
          關鍵詞：该过程保证收敛并且是渐近一致, 在概率密度函数, 例如, 梯度的近似, 设计
        - 摘要：的。
          關鍵詞：
        - 摘要：不幸的是，在实践中由神经网络表示的g和d以及max  d  ν(g  ,d)  不凸时，；GAN中的学习可能是困难的。Goodfellow（2014）认为不收敛可能会引；起GAN的欠拟合问题。一般来说，同时对两个玩家的成本梯度下降不能
          關鍵詞：同时对两个玩家的成本梯度下降不能, 的欠拟合问题, 认为不收敛可能会引, 在实践中由神经网络表示的, 不幸的是
        - 摘要：Goodfellow（2014）确定了另一种替代的形式化收益公式，其中博弈不；再是零和，每当判别器最优时，具有与最大似然学习相同的预期梯度。；因为最大似然训练收敛，这种GAN博弈的重述在给定足够的样本时也应
          關鍵詞：确定了另一种替代的形式化收益公式, 具有与最大似然学习相同的预期梯度, 这种, 因为最大似然训练收敛, 博弈的重述在给定足够的样本时也应
        - 摘要：在真实实验中，GAN博弈的最佳表现形式既不是零和，也不等价于最大；似然，而是Good-fellow et al. （2014c）引入的带有启发式动机的不同形；式化。在这种最佳性能的形式中，生成器旨在增加判别器发生错误的对
          關鍵詞：而是, 式化, 在这种最佳性能的形式中, 引入的带有启发式动机的不同形, 似然
        - 摘要：稳定GAN学习仍然是一个开放的问题。幸运的是，当仔细选择模型架构；和超参数时，GAN学习效果很好。Radford et al. （2015）设计了一个深；度卷积GAN（DCGAN），在图像合成的任务上表现非常好，并表明其
          關鍵詞：在图像合成的任务上表现非常好, 学习效果很好, 学习仍然是一个开放的问题, 稳定, 和超参数时
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图20.7 在LSUN数据集上训练后，由GAN生成的图像。（左）由DCGAN模型生成的卧室图像，；经Radford et al. （2015）许可转载。（右）由LAPGAN模型生成的教堂图像，经Denton et al.
          關鍵詞：许可转载, 模型生成的卧室图像, 模型生成的教堂图像, 生成的图像, 数据集上训练后
        - 摘要：GAN学习问题也可以通过将生成过程分成许多级别的细节来简化。我们；可以训练有条件的GAN（Mirza and Osindero，2014），并学习从分布p(；x  ｜  y  )中采样，而不是简单地从边缘分布p(  x  )中采样。Denton  et  al.
          關鍵詞：可以训练有条件的, 学习问题也可以通过将生成过程分成许多级别的细节来简化, 我们, 并学习从分布, 而不是简单地从边缘分布
        - 摘要：GAN训练过程中一个不寻常的能力是它可以拟合向训练点分配零概率的；概率分布。生成器网络学习跟踪特定点在某种程度上类似于训练点的流；形，而不是最大化该点的对数概率。有点矛盾的是，这意味着模型可以
          關鍵詞：生成器网络学习跟踪特定点在某种程度上类似于训练点的流, 概率分布, 训练过程中一个不寻常的能力是它可以拟合向训练点分配零概率的, 有点矛盾的是, 而不是最大化该点的对数概率
        - 摘要：Dropout似乎在判别器网络中很重要。特别地，在计算生成器网络的梯；度时，单元应当被随机地丢弃。使用权重除以二的确定性版本的判别器
          關鍵詞：使用权重除以二的确定性版本的判别器, 特别地, 似乎在判别器网络中很重要, 度时, 在计算生成器网络的梯
        - 摘要：其梯度似乎不是那么有效。同样，从不使用Dropout似乎会产生不良的；结果。
          關鍵詞：从不使用, 似乎会产生不良的, 结果, 其梯度似乎不是那么有效, 同样
        - 摘要：虽然GAN框架被设计为用于可微生成器网络，但是类似的原理可以用于；训练其他类型的模型。例如，自监督提升 （self-supervised boosting）可；al.  ，
          關鍵詞：虽然, 训练其他类型的模型, 自监督提升, 例如, 框架被设计为用于可微生成器网络
        - 摘要：et
          關鍵詞：
        - 摘要：20.10.5　生成矩匹配网络
          關鍵詞：生成矩匹配网络
        - 摘要：生成矩匹配网络  （generative  moment  matching  network）（Li  et  al.  ，；2015；Dziugaite  et  al.  ，2015）是另一种基于可微生成器网络的生成模；型。与VAE和GAN不同，它们不需要将生成器网络与任何其他网络配
          關鍵詞：不同, 生成矩匹配网络, 它们不需要将生成器网络与任何其他网络配, 是另一种基于可微生成器网络的生成模
        - 摘要：生成矩匹配网络使用称为矩匹配  （moment  matching）的技术训练。矩；匹配背后的基本思想是以如下的方式训练生成器——令模型生成的样本；的许多统计量尽可能与训练集中的样本相似。在此情景下，矩
          關鍵詞：匹配背后的基本思想是以如下的方式训练生成器, 生成矩匹配网络使用称为矩匹配, 的许多统计量尽可能与训练集中的样本相似, 的技术训练, 在此情景下
        - 摘要：其中
          關鍵詞：其中
        - 摘要：是一个非负整数的向量。
          關鍵詞：是一个非负整数的向量
        - 摘要：在第一次检查时，这种方法似乎在计算上是不可行的。例如，如果我们；想匹配形式为x i x j 的所有矩，那么我们需要最小化在 x 的维度上是二次；的多个值之间的差。此外，甚至匹配所有第一和第二矩将仅足以拟合多
          關鍵詞：这种方法似乎在计算上是不可行的, 的所有矩, 如果我们, 的维度上是二次, 此外
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；相反，我们可以通过最小化一个被称为最大平均偏差  （maximum  mean；discrepancy，MMD）（Schölkopf  and  Smola，2002；Gretton  et  al.  ，
          關鍵詞：我们可以通过最小化一个被称为最大平均偏差, 相反
        - 摘要：从可视化方面看，来自生成矩匹配网络的样本有点令人失望。幸运的；是，它们可以通过将生成器网络与自编码器组合来改进。首先，训练自；编码器以重构训练集。接下来，自编码器的编码器用于将整个训练集转
          關鍵詞：来自生成矩匹配网络的样本有点令人失望, 接下来, 自编码器的编码器用于将整个训练集转, 从可视化方面看, 它们可以通过将生成器网络与自编码器组合来改进
        - 摘要：与GAN不同，代价函数仅关于一批同时来自训练集和生成器网络的实例；定义。我们不可能将训练更新作为一个训练样本或仅来自生成器网络的；一个样本的函数，这是因为必须将矩计算为许多样本的经验平均值。当
          關鍵詞：定义, 一个样本的函数, 这是因为必须将矩计算为许多样本的经验平均值, 我们不可能将训练更新作为一个训练样本或仅来自生成器网络的, 不同
        - 摘要：与GAN一样，即使生成器网络为训练点分配零概率，也可以使用MMD；训练生成器网络。
          關鍵詞：一样, 也可以使用, 即使生成器网络为训练点分配零概率, 训练生成器网络
        - 摘要：20.10.6　卷积生成网络
          關鍵詞：卷积生成网络
        - 摘要：当生成图像时，将卷积结构引入生成器网络通常是有用的（见；Goodfellow  et  al.  （2014c）或Dosovitskiy  et  al.  （2015）的例子）。为；此，我们使用卷积算子的“转置”，如第9.5节所述。这种方法通常能产生
          關鍵詞：将卷积结构引入生成器网络通常是有用的, 当生成图像时, 我们使用卷积算子的, 的例子, 如第
        - 摘要：用于识别任务的卷积网络具有从图像到网络顶部的某些概括层（通常是；类标签）的信息流。当该图像通过网络向上流动时，随着图像的表示变；得对于有害变换保持不变，信息也被丢弃。在生成器网络中，情况恰恰
          關鍵詞：当该图像通过网络向上流动时, 在生成器网络中, 类标签, 的信息流, 得对于有害变换保持不变
        - 摘要：本身（具有对象位置、姿势、纹理以及明暗）。在卷积识别网络中丢弃；信息的主要机制是池化层，而生成器网络似乎需要添加信息。由于大多；数池化函数不可逆，我们不能将池化层求逆后放入生成器网络。更简单
          關鍵詞：本身, 姿势, 由于大多, 具有对象位置, 信息的主要机制是池化层
        - 摘要：20.10.7　自回归网络
          關鍵詞：自回归网络
        - 摘要：自回归网络是没有潜在随机变量的有向概率模型。这些模型中的条件概；率分布由神经网络表示（有时是极简单的神经网络，例如逻辑回归）。；这些模型的图结构是完全图。它们可以通过概率的链式法则分解观察变
          關鍵詞：例如逻辑回归, 率分布由神经网络表示, 自回归网络是没有潜在随机变量的有向概率模型, 它们可以通过概率的链式法则分解观察变, 有时是极简单的神经网络
        - 摘要：20.10.8　线性自回归网络
          關鍵詞：线性自回归网络
        - 摘要：自回归网络的最简单形式是没有隐藏单元、没有参数或特征共享的形；式。每个；据的线性回归，对于二值数据的逻辑回归，对于离散数据的softmax回
          關鍵詞：没有参数或特征共享的形, 对于离散数据的, 自回归网络的最简单形式是没有隐藏单元, 据的线性回归, 每个
        - 摘要：被参数化为线性模型（对于实值数
          關鍵詞：被参数化为线性模型, 对于实值数
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；有
          關鍵詞：
        - 摘要：个参数，如图20.8所示。
          關鍵詞：如图, 所示, 个参数
        - 摘要：图20.8　完全可见的信念网络从前i−1个变量预测第i个变量。（上）FVBN的有向图模型。；（下）对数FVBN相应的计算图，其中每个预测由线性预测器作出
          關鍵詞：相应的计算图, 的有向图模型, 个变量, 其中每个预测由线性预测器作出, 个变量预测第
        - 摘要：如果变量是连续的，线性自回归网络只是表示多元高斯分布的另一种方；式，只能捕获观察变量之间线性的成对相互作用。
          關鍵詞：如果变量是连续的, 线性自回归网络只是表示多元高斯分布的另一种方, 只能捕获观察变量之间线性的成对相互作用
        - 摘要：线性自回归网络本质上是线性分类方法在生成式建模上的推广。因此，；它们具有与线性分类器相同的优缺点。像线性分类器一样，它们可以用；凸损失函数训练，并且有时允许闭解形式（如在高斯情况下）。像线性
          關鍵詞：它们可以用, 像线性分类器一样, 它们具有与线性分类器相同的优缺点, 并且有时允许闭解形式, 因此
        - 摘要：20.10.9　神经自回归网络
          關鍵詞：神经自回归网络
        - 摘要：神经自回归网络（Bengio  and  Bengio，2000a，b）具有与逻辑自回归网；络相同的从左到右的图模型（见图20.8），但在该图模型结构内采用不
          關鍵詞：络相同的从左到右的图模型, 但在该图模型结构内采用不, 神经自回归网络, 具有与逻辑自回归网, 见图
        - 摘要：同的条件分布参数。新的参数化更强大，它可以根据需要随意增加容；量，并允许近似任意联合分布。新的参数化还可以引入深度学习中常见；的参数共享和特征共享原理来改进泛化能力。设计这些模型的动机是避
          關鍵詞：并允许近似任意联合分布, 同的条件分布参数, 新的参数化还可以引入深度学习中常见, 它可以根据需要随意增加容, 设计这些模型的动机是避
        - 摘要：（1）通过具有(i−1)×k个输入和k个输出的神经网络（如果变量是离散的；并有k个值，使用one-hot编码）参数化每个；，让我们不需要指数量级参数（和样本）的情况下就能估计条件概率，
          關鍵詞：个输入和, 通过具有, 如果变量是离散的, 让我们不需要指数量级参数, 个输出的神经网络
        - 摘要：（2）不需要对预测每个x  i  使用不同的神经网络，如图20.9所示的从左；到右连接，允许将所有神经网络合并成一个。等价地，它意味着为预测；x  i  所计算的隐藏层特征可以重新用于预测x  i＋k  （k＞0）。因此隐藏单
          關鍵詞：不需要对预测每个, 因此隐藏单, 等价地, 到右连接, 所示的从左
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；图20.9　神经自回归网络从前i−1个变量预测第i个变量x i ，但经参数化后，作为x 1 ，…，x i 函；数的特征（表示为h i 的隐藏单元的组）可以在预测所有后续变量x i＋1 ，x i＋2 ，…，x d 时重
          關鍵詞：作为, 可以在预测所有后续变量, 神经自回归网络从前, 个变量, 表示为
        - 摘要：如在第6.2.2.1节中讨论的，使神经网络的输出预测x  i  条件分布的参数，；就可以表示一个条件分布。虽然原始神；每个
          關鍵詞：条件分布的参数, 虽然原始神, 使神经网络的输出预测, 如在第, 节中讨论的
        - 摘要：20.10.10　NADE
          關鍵詞：
        - 摘要：神经自回归密度估计器  （neural  auto-regressive  density  estimator，；NADE）是最近非常成功的神经自回归网络的一种形式（Larochelle  and；Murray，2011）。与Bengio  and  Bengio（2000b）的原始神经自回归网
          關鍵詞：的原始神经自回归网, 是最近非常成功的神经自回归网络的一种形式, 神经自回归密度估计器
        - 摘要：图20.10　神经自回归密度估计器（NADE）的示意图。隐藏单元被组织在组 h (j) 中，使得只有；输入x 1 ，…，x i 参与计算 h (i) 和预测；用特定的权重共享模式区别于早期的神经自回归网络：
          關鍵詞：和预测, 神经自回归密度估计器, 使得只有, 的示意图, 用特定的权重共享模式区别于早期的神经自回归网络
        - 摘要：（对于j＞i）。NADE使；被共享于所有从x i 到任
          關鍵詞：对于, 到任, 被共享于所有从
        - 摘要：何j≥i组中第k个单元的权重（在图中使用相同的线型表示复制权重的每个实例）。注意向量
          關鍵詞：个单元的权重, 组中第, 在图中使用相同的线型表示复制权重的每个实例, 注意向量
        - 摘要：记为
          關鍵詞：记为
        - 摘要：从第i个输入x i 到第j组隐藏单元的第k个元素；组内共享的：
          關鍵詞：组隐藏单元的第, 到第, 个输入, 从第, 个元素
        - 摘要：的权重
          關鍵詞：的权重
        - 摘要：是
          關鍵詞：
        - 摘要：其余j＜i的权重为0。
          關鍵詞：其余, 的权重为
        - 摘要：Larochelle and  Murray（2011）选择了这种共享方案，使得NADE模型中；的正向传播与在均匀场推断中执行的计算大致相似，以填充RBM中缺；失的输入。这个均匀场推断对应于运行具有共享权重的循环网络，并且
          關鍵詞：模型中, 的正向传播与在均匀场推断中执行的计算大致相似, 中缺, 这个均匀场推断对应于运行具有共享权重的循环网络, 并且
        - 摘要：如前所述，自回归网络可以被扩展成处理连续数据。用于参数化连续密；i  （组i的系数或先验概；度的特别强大和通用的方法是混合权重为α
          關鍵詞：如前所述, 的系数或先验概, 自回归网络可以被扩展成处理连续数据, 用于参数化连续密, 度的特别强大和通用的方法是混合权重为
        - 摘要：另一个非常有趣的神经自回归架构的扩展摆脱了为观察到的变量选择任；意顺序的需要（Murray  and  Larochelle，2014）。在自回归网络中，该；想法是训练网络能够通过随机采样顺序来处理任何顺序，并将信息提供
          關鍵詞：在自回归网络中, 并将信息提供, 想法是训练网络能够通过随机采样顺序来处理任何顺序, 意顺序的需要, 另一个非常有趣的神经自回归架构的扩展摆脱了为观察到的变量选择任
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；最后，由于变量的许多顺序是可能的（对于n个变量是n!），并且变量；的每个顺序o产生不同的p(x ｜o)，我们可以组成许多o值模型的集成：
          關鍵詞：由于变量的许多顺序是可能的, 我们可以组成许多, 值模型的集成, 的每个顺序, 对于
        - 摘要：这个集成模型通常能更好地泛化，并且为测试集分配比单个排序定义的；单个模型更高的概率。
          關鍵詞：单个模型更高的概率, 并且为测试集分配比单个排序定义的, 这个集成模型通常能更好地泛化
        - 摘要：and
          關鍵詞：
        - 摘要：在同一篇文章中，作者提出了深度版本的架构，但不幸的是，这立即使；计算成本像原始神经自回归网络一样高（Bengio；Bengio，
          關鍵詞：在同一篇文章中, 计算成本像原始神经自回归网络一样高, 作者提出了深度版本的架构, 这立即使, 但不幸的是
        - 摘要：。；（假设在每个层存在n组h
          關鍵詞：假设在每个层存在
        - 摘要：20.10.1　sigmoid信念网；络
          關鍵詞：信念网
        - 摘要：20.10.2　可微生成器网络
          關鍵詞：可微生成器网络
        - 摘要：20.10.3　变分自编码器
          關鍵詞：变分自编码器
        - 摘要：20.10.4　生成式对抗网络
          關鍵詞：生成式对抗网络
        - 摘要：20.10.5　生成矩匹配网络
          關鍵詞：生成矩匹配网络
        - 摘要：20.10.6　卷积生成网络
          關鍵詞：卷积生成网络
        - 摘要：20.10.7　自回归网络
          關鍵詞：自回归网络
        - 摘要：20.10.8　线性自回归网络
          關鍵詞：线性自回归网络
        - 摘要：20.10.9　神经自回归网络
          關鍵詞：神经自回归网络
        - 摘要：20.10.10　NADE
          關鍵詞：
    20.11：从自编码器采样
        - 摘要：20.11.1　与任意去噪自编码器相关的马尔可夫链
          關鍵詞：与任意去噪自编码器相关的马尔可夫链
        - 摘要：20.11.2　夹合与条件采样
          關鍵詞：夹合与条件采样
        - 摘要：20.11.3　回退训练过程
          關鍵詞：回退训练过程
        - 摘要：在第14章中，我们看到许多种学习数据分布的自编码器。得分匹配、去；噪自编码器和收缩自编码器之间有着密切的联系。这些联系表明某些类；型的自编码器以某些方式学习数据分布。我们还没有讨论如何从这样的
          關鍵詞：章中, 我们还没有讨论如何从这样的, 噪自编码器和收缩自编码器之间有着密切的联系, 得分匹配, 我们看到许多种学习数据分布的自编码器
        - 摘要：某些类型的自编码器，例如变分自编码器，明确地表示概率分布并且允；许直接的原始采样。而大多数其他类型的自编码器则需要MCMC采样。
          關鍵詞：例如变分自编码器, 某些类型的自编码器, 采样, 而大多数其他类型的自编码器则需要, 明确地表示概率分布并且允
        - 摘要：收缩自编码器被设计为恢复数据流形切面的估计。这意味着使用注入噪；声的重复编码和解码将引起沿着流形表面的随机游走（Rifai  et  al.  ，；2012；Mesnil  et  al.  ，2012）。这种流形扩散技术是马尔可夫链的一
          關鍵詞：这种流形扩散技术是马尔可夫链的一, 收缩自编码器被设计为恢复数据流形切面的估计, 声的重复编码和解码将引起沿着流形表面的随机游走, 这意味着使用注入噪
        - 摘要：更一般的马尔可夫链还可以从任何去噪自编码器中采样。
          關鍵詞：更一般的马尔可夫链还可以从任何去噪自编码器中采样
        - 摘要：20.11.1　与任意去噪自编码器相关的马尔可夫链
          關鍵詞：与任意去噪自编码器相关的马尔可夫链
        - 摘要：上述讨论留下了一个开放问题——注入什么噪声和从哪获得马尔可夫链；（可以根据自编码器估计的分布生成样本）。Bengio et al.  （2013c）展；示了如何构建这种用于广义去噪自编码器  （generalized
          關鍵詞：可以根据自编码器估计的分布生成样本, 注入什么噪声和从哪获得马尔可夫链, 上述讨论留下了一个开放问题, 示了如何构建这种用于广义去噪自编码器
        - 摘要：根据估计分布生成的马尔可夫链的每个步骤由以下子步骤组成，如图；20.11所示。
          關鍵詞：如图, 根据估计分布生成的马尔可夫链的每个步骤由以下子步骤组成, 所示
        - 摘要：图20.11　马尔可夫链的每个步骤与训练好的去噪自编码器相关联，根据由去噪对数似然准则隐；式训练的概率模型生成样本。每个步骤包括：（a）通过损坏过程C向状态 x 注入噪声产生；（b）用函数f对其编码，产生
          關鍵詞：通过损坏过程, 马尔可夫链的每个步骤与训练好的去噪自编码器相关联, 注入噪声产生, 根据由去噪对数似然准则隐, 每个步骤包括
        - 摘要：；；；（c）用函数g解码结果，产生用于重构分布的参
          關鍵詞：用函数, 解码结果, 产生用于重构分布的参
        - 摘要：，损坏包括添加高斯噪声，并且从
          關鍵詞：损坏包括添加高斯噪声, 并且从
        - 摘要：，从重构分布
          關鍵詞：从重构分布
        - 摘要：采样新状态。在典型的平
          關鍵詞：在典型的平, 采样新状态
        - 摘要：，并估计
          關鍵詞：并估计
        - 摘要：的采样包括第二次向重构
          關鍵詞：的采样包括第二次向重构
        - 摘要：添加高斯噪声。后者的噪声水平应对应于重构的均方误；差，而注入的噪声是控制混合速度以及估计器平滑经验分布程度的超参数（Vincent，2011）。；在所示的例子中，只有C和p条件是随机步骤（f和g是确定性计算），我们也可以在自编码器内
          關鍵詞：是确定性计算, 在所示的例子中, 我们也可以在自编码器内, 而注入的噪声是控制混合速度以及估计器平滑经验分布程度的超参数, 后者的噪声水平应对应于重构的均方误
        - 摘要：（1）从先前状态 x 开始，注入损坏噪声，从
          關鍵詞：从先前状态, 开始, 注入损坏噪声
        - 摘要：中采样  。
          關鍵詞：中采样
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；（2）将  编码为
          關鍵詞：编码为
        - 摘要：。
          關鍵詞：
        - 摘要：（3）解码h以获得
          關鍵詞：解码, 以获得
        - 摘要：。
          關鍵詞：
        - 摘要：（4）从
          關鍵詞：
        - 摘要：的参数
          關鍵詞：的参数
        - 摘要：采样下一状态 x 。
          關鍵詞：采样下一状态
        - 摘要：Bengio  et  al.  （2014）表明，如果自编码器；件分布的一致估计量，则上述马尔可夫链的平稳分布形成数据生成分；布x 的一致估计量（虽然是隐式的）。
          關鍵詞：件分布的一致估计量, 虽然是隐式的, 的一致估计量, 如果自编码器, 表明
        - 摘要：形成对应真实条
          關鍵詞：形成对应真实条
        - 摘要：20.11.2　夹合与条件采样
          關鍵詞：夹合与条件采样
        - 摘要：与玻尔兹曼机类似，去噪自编码器及其推广（例如下面描述的GSN）可；用于从条件分布；中采样，只需夹合观察单元x  f  并在给定x
          關鍵詞：与玻尔兹曼机类似, 只需夹合观察单元, 例如下面描述的, 用于从条件分布, 去噪自编码器及其推广
        - 摘要：在图20.12中展示了夹合一半像素（图像的右部分）并在另一半上运行；马尔可夫链的实验。
          關鍵詞：在图, 马尔可夫链的实验, 中展示了夹合一半像素, 图像的右部分, 并在另一半上运行
        - 摘要：图20.12　在每步仅重采样左半部分，夹合图像的右半部分并运行马尔可夫链的示意图。这些样；本来自重构MNIST数字的GSN（每个时间步使用回退过程）
          關鍵詞：本来自重构, 夹合图像的右半部分并运行马尔可夫链的示意图, 这些样, 每个时间步使用回退过程, 数字的
        - 摘要：20.11.3　回退训练过程
          關鍵詞：回退训练过程
        - 摘要：回退训练过程由Bengio et  al.  （2013c）等人提出，作为一种加速去噪自；编码器生成训练收敛的方法。不像执行一步编码-解码重建，该过程有；代替的多个随机编码-解码步骤组成（如在生成马尔可夫链中），以训
          關鍵詞：编码器生成训练收敛的方法, 如在生成马尔可夫链中, 代替的多个随机编码, 作为一种加速去噪自, 该过程有
        - 摘要：训练k个步骤与训练一个步骤是等价的（在实现相同稳态分布的意义；上），但是实际上可以更有效地去除来自数据的伪模式。
          關鍵詞：训练, 但是实际上可以更有效地去除来自数据的伪模式, 个步骤与训练一个步骤是等价的, 在实现相同稳态分布的意义
        - 摘要：20.11.1　与任意去噪自编；码器相关的马尔可夫链
          關鍵詞：与任意去噪自编, 码器相关的马尔可夫链
        - 摘要：20.11.2　夹合与条件采样
          關鍵詞：夹合与条件采样
        - 摘要：20.11.3　回退训练过程
          關鍵詞：回退训练过程
    20.12：生成随机网络
        - 摘要：20.12.1　判别性GSN
          關鍵詞：判别性
        - 摘要：生成随机网络 （generative stochastic network，GSN）（Bengio et al. ，；2014）是去噪自编码器的推广，除可见变量（通常表示为x  ）之外，在；生成马尔可夫链中还包括潜变量h 。
          關鍵詞：生成随机网络, 生成马尔可夫链中还包括潜变量, 是去噪自编码器的推广, 通常表示为, 除可见变量
        - 摘要：GSN由两个条件概率分布参数化，指定马尔可夫链的一步。
          關鍵詞：由两个条件概率分布参数化, 指定马尔可夫链的一步
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；（1）；变量。这种“重建分布”也可以在去噪自编码器、RBM、DBN和DBM中
          關鍵詞：这种, 也可以在去噪自编码器, 重建分布, 变量
        - 摘要：指示在给定当前潜在状态下如何产生下一个可见
          關鍵詞：指示在给定当前潜在状态下如何产生下一个可见
        - 摘要：（2）；量下如何更新潜在状态变量。
          關鍵詞：量下如何更新潜在状态变量
        - 摘要：指示在给定先前的潜在状态和可见变
          關鍵詞：指示在给定先前的潜在状态和可见变
        - 摘要：去噪自编码器和GSN不同于经典的概率模型（有向或无向），它们自己；参数化生成过程，而不是通过可见和潜变量的联合分布的数学形式。相；反，后者如果存在则隐式地定义为生成马尔可夫链的稳态分布。存在稳
          關鍵詞：有向或无向, 参数化生成过程, 后者如果存在则隐式地定义为生成马尔可夫链的稳态分布, 它们自己, 而不是通过可见和潜变量的联合分布的数学形式
        - 摘要：我们可以想象GSN不同的训练准则。由Bengio  et  al.  （2014）提出和评；估的只对可见单元上对数概率的重建，如应用于去噪自编码器。通过；将x (0) ＝ x 夹合到观察到的样本并且在一些后续时间步处使生成  x 的概
          關鍵詞：夹合到观察到的样本并且在一些后续时间步处使生成, 提出和评, 我们可以想象, 不同的训练准则, 估的只对可见单元上对数概率的重建
        - 摘要：从链中采样。为了估计相对于模型其他部分的；的梯度，Bengio  et  al.  （2014）使用了在第
          關鍵詞：为了估计相对于模型其他部分的, 从链中采样, 使用了在第, 的梯度
        - 摘要：，其中给定
          關鍵詞：其中给定
        - 摘要：(k)
          關鍵詞：
        - 摘要：20.9节中介绍的重参数化技巧。
          關鍵詞：节中介绍的重参数化技巧
        - 摘要：回退训练过程（在第20.11.3节中描述）可以用来改善训练GSN的收敛性；（Bengio et al. ，2014）。
          關鍵詞：可以用来改善训练, 的收敛性, 在第, 回退训练过程, 节中描述
        - 摘要：20.12.1　判别性GSN
          關鍵詞：判别性
        - 摘要：GSN的原始公式（Bengio et  al. ，2014）用于无监督学习和对观察数据x；的p(x )的隐式建模，但是我们可以修改框架来优化p(y ｜ x )。
          關鍵詞：的原始公式, 但是我们可以修改框架来优化, 用于无监督学习和对观察数据, 的隐式建模
        - 摘要：例如，Zhou  and  Troyanskaya（2014）以如下方式推广GSN：只反向传；播输出变量上的重建对数概率，并保持输入变量固定。他们将这种方式；成功应用于建模序列（蛋白质二级结构），并在马尔可夫链的转换算子
          關鍵詞：播输出变量上的重建对数概率, 蛋白质二级结构, 成功应用于建模序列, 他们将这种方式, 以如下方式推广
        - 摘要：其他层的值（例如下面一个和上面一个）的输入。
          關鍵詞：其他层的值, 的输入, 例如下面一个和上面一个
        - 摘要：因此，马尔可夫链确实不只是输出变量（与更高层的隐藏层相关联），；并且输入序列仅用于条件化该链，其中反向传播使得它能够学习输入序；列如何条件化由马尔可夫链隐含表示的输出分布。因此这是在结构化输
          關鍵詞：其中反向传播使得它能够学习输入序, 与更高层的隐藏层相关联, 因此, 马尔可夫链确实不只是输出变量, 因此这是在结构化输
        - 摘要：and  Pernkopf（2014）引入了一个混合模型，通过简单地添加；Zöhrer；（使用不同的权重）监督和非监督成本即y  和x  的重建对数概率，组合
          關鍵詞：通过简单地添加, 引入了一个混合模型, 监督和非监督成本即, 的重建对数概率, 组合
        - 摘要：20.12.1　判别性GSN
          關鍵詞：判别性
    20.13：其他生成方案
        - 摘要：目前为止我们已经描述的方法，使用MCMC采样、原始采样或两者的一；些混合来生成样本。虽然这些是生成式建模中最流行的方法，但它们绝；不是唯一的方法。
          關鍵詞：原始采样或两者的一, 虽然这些是生成式建模中最流行的方法, 目前为止我们已经描述的方法, 些混合来生成样本, 不是唯一的方法
        - 摘要：Sohl-Dickstein  et  al.  （2015）开发了一种基于非平衡热力学学习生成模；型的扩散反演  （diffusion  inversion）训练方案。该方法基于我们希望从；中采样的概率分布具有结构的想法。这种结构会被递增地使概率分布具
          關鍵詞：该方法基于我们希望从, 开发了一种基于非平衡热力学学习生成模, 型的扩散反演, 训练方案, 中采样的概率分布具有结构的想法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；处出现的假性模式。
          關鍵詞：处出现的假性模式
        - 摘要：样本生成的另一种方法是近似贝叶斯计算  （approximate；Bayesian；computation，ABC）框架（Rubin et al. ，1984）。在这种方法中，样本
          關鍵詞：样本生成的另一种方法是近似贝叶斯计算, 样本, 框架, 在这种方法中
        - 摘要：我们期待更多等待发现的其他生成式建模方法。
          關鍵詞：我们期待更多等待发现的其他生成式建模方法
    20.14：评估生成模型
        - 摘要：研究生成模型的研究者通常需要将一个生成模型与另一个生成模型比；较，通常是为了证明新发明的生成模型比之前存在的模型更能捕获一些；分布。
          關鍵詞：通常是为了证明新发明的生成模型比之前存在的模型更能捕获一些, 分布, 研究生成模型的研究者通常需要将一个生成模型与另一个生成模型比
        - 摘要：这可能是一个困难且微妙的任务。通常，我们不能实际评估模型下数据；的对数概率，但仅可以评估一个近似。在这些情况下，重要的是思考和；沟通清楚正在测量什么。例如，假设我们可以评估模型A对数似然的随
          關鍵詞：沟通清楚正在测量什么, 通常, 这可能是一个困难且微妙的任务, 但仅可以评估一个近似, 的对数概率
        - 摘要：评估生成模型的另一个微妙之处是，评估指标往往是自身困难的研究问；题。可能很难确定模型是否被公平比较。例如，假设我们使用AIS来估；计log  Z，以便为我们刚刚发明的新模型计算
          關鍵詞：来估, 评估生成模型的另一个微妙之处是, 可能很难确定模型是否被公平比较, 以便为我们刚刚发明的新模型计算, 例如
        - 摘要：。因此可能难以判断高似然估计是否是良好模
          關鍵詞：因此可能难以判断高似然估计是否是良好模
        - 摘要：。
          關鍵詞：
        - 摘要：机器学习的其他领域通常允许在数据预处理中有一些变化。例如，当比
          關鍵詞：机器学习的其他领域通常允许在数据预处理中有一些变化, 当比, 例如
        - 摘要：较对象识别算法的准确性时，通常可接受的是对每种算法略微不同地预；处理输入图像（基于每种算法具有何种输入要求）。而因为预处理的变；化，会导致生成式建模的不同，甚至非常小和微妙的变化也是完全不可
          關鍵詞：通常可接受的是对每种算法略微不同地预, 会导致生成式建模的不同, 而因为预处理的变, 基于每种算法具有何种输入要求, 甚至非常小和微妙的变化也是完全不可
        - 摘要：预处理的问题通常在基于MNIST数据集上的生成模型产生，MNIST数；据集是非常受欢迎的生成式建模基准之一。MNIST由灰度图像组成。一；些模型将MNIST图像视为实向量空间中的点，而其他模型将其视为二
          關鍵詞：而其他模型将其视为二, 由灰度图像组成, 预处理的问题通常在基于, 数据集上的生成模型产生, 图像视为实向量空间中的点
        - 摘要：因为从数据分布生成真实样本是生成模型的目标之一，所以实践者通常；通过视觉检查样本来评估生成模型。在最好的情况下，这不是由研究人；员本身，而是由不知道样品来源的实验受试者完成（Denton  et  al.  ，
          關鍵詞：所以实践者通常, 因为从数据分布生成真实样本是生成模型的目标之一, 而是由不知道样品来源的实验受试者完成, 这不是由研究人, 员本身
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；的。在更实际的设定中，在具有数万个模式的数据上训练后的生成模型；可以忽略少数模式，并且人类观察者不能容易地检查或记住足够的图像
          關鍵詞：在更实际的设定中, 并且人类观察者不能容易地检查或记住足够的图像, 可以忽略少数模式, 在具有数万个模式的数据上训练后的生成模型
        - 摘要：由于样本的视觉质量不是可靠的标准，所以当计算可行时，我们通常还；评估模型分配给测试数据的对数似然。不幸的是，在某些情况下，似然；性似乎不可能测量我们真正关心的模型的任何属性。例如，MNIST的实
          關鍵詞：由于样本的视觉质量不是可靠的标准, 评估模型分配给测试数据的对数似然, 我们通常还, 似然, 例如
        - 摘要：Theis  et  al.  （2015）回顾了评估生成模型所涉及的许多问题，包括上述；的许多想法。他们强调了生成模型有许多不同的用途，并且指标的选择；必须与模型的预期用途相匹配。例如，一些生成模型更好地为大多数真
          關鍵詞：包括上述, 回顾了评估生成模型所涉及的许多问题, 他们强调了生成模型有许多不同的用途, 并且指标的选择, 的许多想法
        - 摘要：还是
          關鍵詞：还是
        - 摘要：，如图3.6所；示。不幸的是，即使我们将每个指标的使用限制在最适合的任务上，目；前使用的所有指标仍存在严重的缺陷。因此，生成式建模中最重要的研
          關鍵詞：前使用的所有指标仍存在严重的缺陷, 即使我们将每个指标的使用限制在最适合的任务上, 因此, 不幸的是, 如图
    20.15：结论
        - 摘要：参考文献
          關鍵詞：参考文献
        - 摘要：索引
          關鍵詞：索引
        - 摘要：为了让模型理解基于给定训练数据表示的大千世界，训练具有隐藏单元；和表示；的生成模型是一种有力方法。通过学习模型
          關鍵詞：为了让模型理解基于给定训练数据表示的大千世界, 和表示, 通过学习模型, 训练具有隐藏单元, 的生成模型是一种有力方法
        - 摘要：————————————————————
          關鍵詞：
        - 摘要：(1)  术语“mcRBM”根据字母M-C-R-B-M发音；“mc”不是“McDonald's”中的“Mc”的发音。
          關鍵詞：的发音, 发音, 根据字母, 术语, 不是
        - 摘要：(2)    这个版本的Gaussian-Bernoulli  RBM能量函数假定图像数据的每个像素具有零均值。考虑非；零像素均值时，可以简单地将像素偏移添加到模型中。
          關鍵詞：可以简单地将像素偏移添加到模型中, 零像素均值时, 这个版本的, 考虑非, 能量函数假定图像数据的每个像素具有零均值
        - 摘要：(3)    该论文将模型描述为“深度信念网络”，但因为它可以被描述为纯无向模型（具有易处理逐；层均匀场不动点更新），所以它最适合深度玻尔兹曼机的定义。
          關鍵詞：所以它最适合深度玻尔兹曼机的定义, 该论文将模型描述为, 层均匀场不动点更新, 深度信念网络, 但因为它可以被描述为纯无向模型
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；参考文献
          關鍵詞：参考文献
        - 摘要：S.，Davis，A.，Dean，J.，Devin，M.，
          關鍵詞：
        - 摘要：Abadi，M.，Agarwal，A.，Barham，P.，Brevdo，E.，Chen，Z.，；Citro，C.，Corrado，G.；Ghemawat，S.，Goodfellow，I.，Harp，A.，Irving，G.，Isard，M.，
          關鍵詞：
        - 摘要：Ackley，D.  H.，Hinton，G.  E.，and  Sejnowski，T.  J.（1985）.  A；learning  algorithm  for  Boltzmann  machines.  Cognitive Science  ，9  ，147–；169.
          關鍵詞：
        - 摘要：Alain，G.  and  Bengio，Y.（2013）.  What  regularized  auto-encoders  learn；from the data generating distribution. In ICLR'2013，arXiv:1211.4246 .
          關鍵詞：
        - 摘要：Alain，G.，Bengio，Y.，Yao，L.，Éric  Thibodeau-Laufer，Yosinski，；J.，and  Vincent，P.（2015）.  GSNs:  Generative  stochastic  networks.；arXiv:1503.05571.
          關鍵詞：
        - 摘要：Anderson，E.（1935）.  The  Irises  of  the  Gaspé  Peninsula.  Bulletin  of  the；American Iris Society ，59 ，2–5.
          關鍵詞：
        - 摘要：Ba，J.，Mnih，V.，and  Kavukcuoglu，K.（2014）.  Multiple  object；recognition with visual attention. arXiv:1412.7755 .
          關鍵詞：
        - 摘要：Bachman，P.  and  Precup，D.（2015）.  Variational  generative  stochastic；networks  with  collaborative  shaping.  In  Proceedings  of；the  32nd
          關鍵詞：
        - 摘要：Precup，D.（2015）.；Bacon，P.-L.，Bengio，E.，Pineau，J.，and；Conditional  computation  in  neural  networks  using  a  decision-theoretic
          關鍵詞：
        - 摘要：Bagnell，J. A. and Bradley，D. M.（2009）. Differentiable sparse coding.；In NIPS'2009 ，pages 113–120.
          關鍵詞：
        - 摘要：Bahdanau，D.，Cho，K.，and  Bengio，Y.（2015）.  Neural  machine；translation  by  jointly  learning  to  align  and  translate.  In  ICLR'2015，；arXiv:1409.0473 .
          關鍵詞：
        - 摘要：Bahl，L. R.，Brown，P.，de Souza，P. V.，and Mercer，R. L.（1987）.；Speech  recognition  with  continuous-parameter  hidden  Markov  models.；Computer，Speech and Language ，2 ，219–234.
          關鍵詞：
        - 摘要：Baldi，P.  and  Hornik，K.（1989）.  Neural  networks  and  principal；component  analysis:Learning  from  examples  without  local  minima.  Neural；Networks ，2 ，53–58.
          關鍵詞：
        - 摘要：Pollastri，G.；Baldi，P.，Brunak，S.，Frasconi，P.，Soda，G.，and；（1999）.  Exploiting  the  past  and  the  future  in  protein  secondary  structure
          關鍵詞：
        - 摘要：Baldi，P.，Sadowski，P.，and  Whiteson，D.（2014）.  Searching；exotic  particles；communications ，5 .
          關鍵詞：
        - 摘要：in  high-energy  physics  with  deep
          關鍵詞：
        - 摘要：for；learning.  Nature
          關鍵詞：
        - 摘要：Ballard，D.  H.，Hinton，G.  E.，and  Sejnowski，T.  J.（1983）.  Parallel；vision computation. Nature .
          關鍵詞：
        - 摘要：Barlow，H.  B.（1989）.  Unsupervised  learning.  Neural  Computation  ，1；，295–311.
          關鍵詞：
        - 摘要：Barron，A. E.（1993）. Universal approximation bounds for superpositions；of  a  sigmoidal  function.  IEEE  Trans.  on  Information  Theory  ，39  ，930–；945.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Bartholomew，D. J.（1987）. Latent variable models and factor analysis  .；Oxford University Press.
          關鍵詞：
        - 摘要：Basilevsky，A.（1994）.  Statistical  Factor  Analysis  and  Related；Methods:Theory and Applications . Wiley.
          關鍵詞：
        - 摘要：Bastien，F.，Lamblin，P.，Pascanu，R.，Bergstra，J.，Goodfellow，；I.，Bergeron，A.，Bouchard，N.，Warde-Farley，D.，and  Bengio，Y.；（2012a）.  Theano:new  features  and  speed  improvements.  Submited  to  the
          關鍵詞：
        - 摘要：Bastien，F.，Lamblin，P.，Pascanu，R.，Bergstra，J.，Goodfellow，I.；J.，Bergeron，A.，Bouchard，N.，and；Theano:new  features  and  speed
          關鍵詞：
        - 摘要：Bengio，Y.（2012b）.；improvements.  Deep  Learning  and
          關鍵詞：
        - 摘要：Basu，S.  and  Christensen，J.（2013）.  Teaching  classification  boundaries；to humans. In AAAI'2013 .
          關鍵詞：
        - 摘要：International
          關鍵詞：
        - 摘要：Baxter，J.（1995）. Learning internal representations. In Proceedings of the；8th；Learning
          關鍵詞：
        - 摘要：Computational
          關鍵詞：
        - 摘要：Conference
          關鍵詞：
        - 摘要：on
          關鍵詞：
        - 摘要：Bayer，J.  and  Osendorfer，C.（2014）.  Learning  stochastic  recurrent；networks. ArXiv e-prints .
          關鍵詞：
        - 摘要：Becker，S. and Hinton，G.（1992）. A self-organizing neural network that；discovers surfaces in random-dot stereograms. Nature ，355 ，161–163.
          關鍵詞：
        - 摘要：Behnke，S.（2001）.  Learning  iterative  image  reconstruction  in  the  neural；abstraction pyramid. Int. J. Computational Intelligence and Applications ，1；（4），427–438.
          關鍵詞：
        - 摘要：Beiu，V.，Quintana，J.  M.，and  Avedillo，M.；threshold；implementations  of
          關鍵詞：
        - 摘要：J.（2003）.  VLSI；logic-a  comprehensive  survey.  Neural
          關鍵詞：
        - 摘要：Networks，IEEE Transactions on ，14 （5），1217–1243.
          關鍵詞：
        - 摘要：Belkin，M.  and  Niyogi，P.（2002）.  Laplacian  eigenmaps  and  spectral；techniques  for  embedding  and  clustering.  In  T.  Dietterich，S.  Becker，and；Z.  Ghahramani，editors，Advances
          關鍵詞：
        - 摘要：and  Niyogi，P.（2003a）.  Laplacian
          關鍵詞：
        - 摘要：Belkin，M.；for；dimensionality reduction and  data representation. Neural  Computation ，15
          關鍵詞：
        - 摘要：eigenmaps
          關鍵詞：
        - 摘要：Belkin，M.  and  Niyogi，P.（2003b）.  Using  manifold  structure  for；In  S.  Becker，S.  Thrun，and  K.；partially
          關鍵詞：
        - 摘要：labeled  classification.
          關鍵詞：
        - 摘要：Bengio，E.，Bacon，P.-L.，Pineau，J.，and；Conditional；arXiv:1511.06297.
          關鍵詞：
        - 摘要：computation
          關鍵詞：
        - 摘要：in  neural  networks
          關鍵詞：
        - 摘要：Precup，D.（2015a）.；faster  models.
          關鍵詞：
        - 摘要：for
          關鍵詞：
        - 摘要：Bengio，S.  and  Bengio，Y.（2000a）.  Taking  on；the  curse  of；dimensionality
          關鍵詞：
        - 摘要：joint  distributions  using  neural  networks.
          關鍵詞：
        - 摘要：in
          關鍵詞：
        - 摘要：Bengio，S.，Vinyals，O.，Jaitly，N.，and；Shazeer，N.（2015b）.；Scheduled sampling for sequence prediction with recurrent neural networks.
          關鍵詞：
        - 摘要：Bengio，Y.（1991）.  Artificial  Neural  Networks  and  their  Application  to；Sequence  Recognition.  Ph.D.  thesis，McGill  University  ，（Computer；Science），Montreal，Canada.
          關鍵詞：
        - 摘要：Bengio，Y.（2000）.  Gradient-based  optimization  of  hyperparameters.；Neural Computation ，12 （8），1889–1900.
          關鍵詞：
        - 摘要：Bengio，Y.（2002）.  New  distributed  probabilistic
          關鍵詞：
        - 摘要：language  models.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Technical Report 1215，Dept. IRO，Université de Montréal.
          關鍵詞：
        - 摘要：Bengio，Y.（2009）. Learning deep architectures for AI . Now Publishers.
          關鍵詞：
        - 摘要：Bengio，Y.（2013）. Deep learning of representations: looking forward. In；Statistical Language and Speech Processing ，volume 7978 of Lecture Notes；in  Computer  Science  ，pages  1–37.  Springer，also
          關鍵詞：
        - 摘要：Bengio，Y.（2015）. Early inference in energy-based models approximates；back-propagation.  Technical  Report  arXiv:1510.02777，Universite  de；Montreal.
          關鍵詞：
        - 摘要：Bengio，Y. and Bengio，S.（2000b）. Modeling high-dimensional discrete；data  with  multi-layer  neural  networks.  In  NIPS  12  ，pages  400–406.  MIT；Press.
          關鍵詞：
        - 摘要：Bengio，Y.  and  Delalleau，O.（2009）.  Justifying  and  generalizing；contrastive divergence. Neural Computation ，21 （6），1601–1621.
          關鍵詞：
        - 摘要：Bengio，Y.  and  Grandvalet，Y.（2004）.  No  unbiased  estimator  of  the；variance of k-fold cross-validation. In JML（1），pages 1089–1105.
          關鍵詞：
        - 摘要：Bengio，Y. and LeCun，Y.（2007a）. Scaling learning algorithms towards；AI. In Large Scale Kernel Machines .
          關鍵詞：
        - 摘要：Bengio，Y. and LeCun，Y.（2007b）. Scaling learning algorithms towards；J.  Weston，；AI.
          關鍵詞：
        - 摘要：In  L.  Bottou，O.  Chapelle，D.  DeCoste，and
          關鍵詞：
        - 摘要：Bengio，Y.  and  Monperrus，M.（2005）.  Non-local  manifold  tangent；learning.  In  L.  Saul，Y.  Weiss，and  L.  Bottou，editors，Advances  in；Neural  Information  Processing  Systems  17（NIPS'04）  ，pages  129–136.
          關鍵詞：
        - 摘要：Bengio，Y.  and  Sénécal，J.-S.（2003）.  Quick  training  of  probabilistic；neural nets by importance sampling. In Proceedings of AISTATS 2003 .
          關鍵詞：
        - 摘要：Bengio，Y.  and  Sénécal，J.-S.（2008）.  Adaptive  importance  sampling  to；accelerate  training  of  a  neural  probabilistic  language  model.  IEEE  Trans.；Neural Networks ，19 （4），713–722.
          關鍵詞：
        - 摘要：Bengio，Y.，De  Mori，R.，Flammia，G.，and  Kompe，R.（1991）.；speech；Phonetically  motivated  acoustic  parameters
          關鍵詞：
        - 摘要：for  continuous
          關鍵詞：
        - 摘要：Bengio，Y.，De  Mori，R.，Flammia，G.，and  Kompe，R.（1992）.；Neural  network-Gaussian  mix-ture  hybrid  for  speech  recognition  or  density；estimation. In NIPS 4 ，pages 175–182. Morgan Kaufmann.
          關鍵詞：
        - 摘要：Bengio，Y.，Frasconi，P.，and  Simard，P.（1993）.  The  problem  of；learning long-term dependencies in recurrent networks. In IEEE International；Conference on Neural Networks，pages 1183–1195 ，San Francisco. IEEE
          關鍵詞：
        - 摘要：Bengio，Y.，Simard，P.，and  Frasconi，P.（1994a）.  Learning；term dependencies with gradient descent is difficult. IEEE Tr. Neural Nets .
          關鍵詞：
        - 摘要：long-
          關鍵詞：
        - 摘要：Bengio，Y.，Simard，P.，and  Frasconi，P.（1994b）.  Learning；long-；term  dependencies  with  gradient  descent  is  difficult.  IEEE  Transactions  on
          關鍵詞：
        - 摘要：Bengio，Y.，Simard，P.，and  Frasconi，P.（1994c）.  Learning；long-；term  dependencies  with  gradient  descent  is  difficult.  IEEE  Transactions  on
          關鍵詞：
        - 摘要：Bengio，Y.，Latendresse，S.，and  Dugas，C.（1999）.  Gradient-based；learning of hyper-parameters. In Learning Conference .
          關鍵詞：
        - 摘要：Bengio，Y.，Ducharme，R.，and  Vincent，P.（2001a）.  A；neural；probabilistic  language  model.  In  T.  Leen，T.  Dietterich，and  V.  Tresp，
          關鍵詞：
        - 摘要：in  Neural
          關鍵詞：
        - 摘要：Information
          關鍵詞：
        - 摘要：Processing
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Bengio，Y.，Ducharme，R.，and  Vincent，P.（2001b）.  A；neural
          關鍵詞：
        - 摘要：Bengio，Y.，Ducharme，R.，Vincent，P.，and  Jauvin，C.（2003）.  A；neural probabilistic language model. JMLR ，3 ，1137–1155.
          關鍵詞：
        - 摘要：Bengio，Y.，Delalleau，O.，and  Le  Roux，N.（2006a）.  The  curse  of；highly variable functions for local kernel machines. In NIPS'2005 .
          關鍵詞：
        - 摘要：Bengio，Y.，Larochelle，H.，and  Vincent，P.（2006b）.  Non-local；manifold Parzen windows. In NIPS'2005 . MIT Press.
          關鍵詞：
        - 摘要：Bengio，Y.，Lamblin，P.，Popovici，D.，and；（2007a）. Greedy layer-wise training of deep networks. In NIPS'2006 .
          關鍵詞：
        - 摘要：Larochelle，H.
          關鍵詞：
        - 摘要：Bengio，Y.，Lamblin，P.，Popovici，D.，and；Larochelle，H.；（2007b）. Greedy layer-wise training of deep networks. In B. Schölkopf，
          關鍵詞：
        - 摘要：Bengio，Y.，Lamblin，P.，Popovici，D.，and；Larochelle，H.；（2007c）. Greedy layer-wise training of deep networks. In Adv. Neural Inf.
          關鍵詞：
        - 摘要：Bengio，Y.，Lamblin，P.，Popovici，D.，and；Larochelle，H.；（2007d）.  Greedy  layer-wise  training  of  deep  networks.  In  NIPS  19  ，
          關鍵詞：
        - 摘要：Bengio，Y.，Louradour，J.，Collobert，R.，and  Weston，J.（2009）.；Curriculum learning. In ICML'09 . ACM.
          關鍵詞：
        - 摘要：Bengio，Y.，Mesnil，G.，Dauphin，Y.，and  Rifai，S.（2013a）.  Better；mixing via deep representa-tions. In ICML'2013 .
          關鍵詞：
        - 摘要：Bengio，Y.，Léonard，N.，and  Courville，A.（2013b）.  Estimating  or；conditional；propagating  gradients
          關鍵詞：
        - 摘要：stochastic  neurons
          關鍵詞：
        - 摘要：through
          關鍵詞：
        - 摘要：for
          關鍵詞：
        - 摘要：computation. arXiv:1308.3432.
          關鍵詞：
        - 摘要：Bengio，Y.，Yao，L.，Alain，G.，and；Generalized denoising auto-encoders as generative models. In NIPS'2013 .
          關鍵詞：
        - 摘要：Vincent，P.（2013c）.
          關鍵詞：
        - 摘要：Bengio，Y.，Courville，A.，and  Vincent，P.（2013d）.  Representation；learning:  A  review  and  new  perspectives.  Pattern  Analysis  and  Machine；Intelligence，IEEE Transactions on ，35 （8），1798–1828.
          關鍵詞：
        - 摘要：Bengio，Y.，Thibodeau-Laufer，E.，Alain，G.，and；Yosinski，J.；（2014）.  Deep  generative  stochastic  networks  trainable  by  backprop.  In
          關鍵詞：
        - 摘要：Bennett，C.（1976）.  Efficient  estimation  of  free  energy  differences  from；Monte Carlo data. Journal of Computational Physics ，22 （2），245–268.
          關鍵詞：
        - 摘要：Bennett，J. and Lanning，S.（2007）. The Netflix prize.
          關鍵詞：
        - 摘要：Berger，A.  L.，Della  Pietra，V.  J.，and  Della  Pietra，S.  A.（1996）.  A；maximum  entropy  approach  to  natural  language  processing.  Computational；Linguistics ，22 ，39–71.
          關鍵詞：
        - 摘要：Berglund，M.  and  Raiko，T.（2013）.  Stochastic  gradient  estimate；variance  in  contrastive  diver-gence  and  persistent  contrastive  divergence.；CoRR ，abs/1312.6002 .
          關鍵詞：
        - 摘要：Bergstra，J.（2011）.  Incorporating  Complex  Cells  into  Neural  Networks；for Pattern Classification . Ph.D. thesis，Université de Montréal.
          關鍵詞：
        - 摘要：Bergstra，J.  and  Bengio，Y.（2009）.  Slow，decorrelated  features  for；pretraining  complex  cell-like  networks.  In  NIPS  22  ，pages  99–107.  MIT；Press.
          關鍵詞：
        - 摘要：Bergstra，J. and Bengio，Y.（2011）. Random search for hyper-parameter；optimization. The Learning Workshop ，Fort Lauderdale，Florida.
          關鍵詞：
        - 摘要：Bergstra，J. and Bengio，Y.（2012）. Random search for hyper-parameter
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；optimization. J. Machine Learning Res. ，13 ，281–305.
          關鍵詞：
        - 摘要：Bergstra，J.，Breuleux，O.，Bastien，F.，Lamblin，P.，Pascanu，R.，；Bengio，Y.；Desjardins，G.，Turian，J.，Warde-Farley，D.，and
          關鍵詞：
        - 摘要：Bergstra，J.，Breuleux，O.，Bastien，F.，Lamblin，P.，Pascanu，R.，；Desjardins，G.，Turian，J.，Warde-Farley，D.，and；Bengio，Y.
          關鍵詞：
        - 摘要：Bergstra，J.，Breuleux，O.，Bastien，F.，Lamblin，P.，Pascanu，R.，；Bengio，Y.；Desjardins，G.，Turian，J.，Warde-Farley，D.，and
          關鍵詞：
        - 摘要：Bergstra，J.，Bardenet，R.，Bengio，Y.，and；Algorithms for hyper-parameter optimization. In NIPS'2011 .
          關鍵詞：
        - 摘要：Kégl，B.（2011）.
          關鍵詞：
        - 摘要：Berkes，P.  and  Wiskott，L.（2005）.  Slow  feature  analysis  yields  a  rich；repertoire  of  complex  cell  properties.  Journal  of  Vision  ，5  （6），579–；602.
          關鍵詞：
        - 摘要：Bertsekas，D. P. and Tsitsiklis，J.（1996）. Neuro-Dynamic Programming；. Athena Scientific.
          關鍵詞：
        - 摘要：Besag，J.（1975）.  Statistical  analysis  of  non-lattice  data.  The Statistician；，24 （3），179–195.
          關鍵詞：
        - 摘要：Bishop，C. M.（1994）. Mixture density networks.
          關鍵詞：
        - 摘要：Bishop，C.  M.（1995a）.  Regularization  and  complexity  control  in  feed-；forward  networks.  In  Proceedings  International  Conference  on  Artificial；Neural Networks ICANN'95 ，volume 1，page 141–148.
          關鍵詞：
        - 摘要：Bishop，C.  M.（1995b）.  Training  with  noise  is  equivalent  to  Tikhonov；regularization. Neural Computation ，7 （1），108–116.
          關鍵詞：
        - 摘要：Bishop，C.  M.（2006）.  Pattern  Recognition  and  Machine  Learning  .；Springer.
          關鍵詞：
        - 摘要：Blum，A. L. and Rivest，R. L.（1992）. Training a 3-node neural network；is NP-complete.
          關鍵詞：
        - 摘要：Blumer，A.，Ehrenfeucht，A.，Haussler，D.，and  Warmuth，M.  K.；（1989）. Learnability and the Vapnik–Chervonenkis dimension. Journal of；the ACM ，36 （4），865–929.
          關鍵詞：
        - 摘要：Bonnet，G.（1964）.  Transformations  des  signaux  aléatoires  à  travers  les；systèmes non linéaires sans mémoire. Annales des Télécommunications ，19；（9–10），203–220.
          關鍵詞：
        - 摘要：Bordes，A.，Weston，J.，Collobert，R.，and；Learning structured embeddings of knowledge bases. In AAAI 2011 .
          關鍵詞：
        - 摘要：Bengio，Y.（2011）.
          關鍵詞：
        - 摘要：Bordes，A.，Glorot，X.，Weston，J.，and  Bengio，Y.（2012）.  Joint；learning  of  words  and  meaning  representations  for  open-text  semantic；parsing. AISTATS'2012 .
          關鍵詞：
        - 摘要：Bordes，A.，Glorot，X.，Weston，J.，and  Bengio，Y.（2013a）.  A；semantic  matching  energy  func-tion  for  learning  with  multi-relational  data.；Machine Learning: Special Issue on Learning Semantics .
          關鍵詞：
        - 摘要：Bordes，A.，Usunier，N.，Garcia-Duran，A.，Weston，J.，and；Yakhnenko，O.（2013b）.  Translating  embeddings  for  modeling  multi-；relational  data.  In  C.  Burges，L.  Bottou，M.  Welling，Z.  Ghahramani，
          關鍵詞：
        - 摘要：Bornschein，J.  and  Bengio，Y.（2015）.  Reweighted  wake-sleep.  In；ICLR'2015，arXiv:1406.2751 .
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Bornschein，J.，Shabanian，S.，Fischer，A.，and  Bengio，Y.（2015）.；report，
          關鍵詞：
        - 摘要：Boser，B.  E.，Guyon，I.  M.，and  Vapnik，V.  N.（1992）.  A  training；algorithm for optimal margin classifiers. In COLT '92: Proceedings of thefifth；annual workshop on Computational learning theory ，pages 144–152，New
          關鍵詞：
        - 摘要：Bottou，L.（1998）.  Online  algorithms  and  stochastic  approximations.  In；D.  Saad，editor，Online  Learning  in  Neural  Networks；.  Cambridge
          關鍵詞：
        - 摘要：Bottou，L.（2011）.  From  machine；Technical report，arXiv.1102.1808.
          關鍵詞：
        - 摘要：learning
          關鍵詞：
        - 摘要：to  machine  reasoning.
          關鍵詞：
        - 摘要：Bottou，L.（2015）.  Multilayer  neural  networks.  Deep  Learning  Summer；School.
          關鍵詞：
        - 摘要：Bottou，L.  and  Bousquet，O.（2008a）.  The  tradeoffs  of  large  scale；learning.  In  J.  Platt，D.  Koller，Y.  Singer，and  S.  Roweis，editors，；Advances  in  Neural  Information  Processing  Systems  20（NIPS'07）  ，
          關鍵詞：
        - 摘要：Bottou，L.  and  Bousquet，O.（2008b）.  The  tradeoffs  of  large  scale；learning. In NIPS'2008 .
          關鍵詞：
        - 摘要：Boulanger-Lewandowski，N.，Bengio，Y.，and  Vincent，P.（2012）.；Modeling；sequences:
          關鍵詞：
        - 摘要：temporal  dependen-cies
          關鍵詞：
        - 摘要：in  high-dimensional
          關鍵詞：
        - 摘要：Boureau，Y.，Ponce，J.，and LeCun，Y.（2010）. A theoretical analysis；of feature pooling in vision algorithms. In Proc. International Conference on；Machine learning（ICML'10） .
          關鍵詞：
        - 摘要：Boureau，Y.，Le  Roux，N.，Bach，F.，Ponce，J.，and  LeCun，Y.；（2011）. Ask the locals: multi-way local pooling for image recognition. In
          關鍵詞：
        - 摘要：Proc. International Conference on Computer Vision（ICCV'11） . IEEE.
          關鍵詞：
        - 摘要：Bourlard，H.  and  Kamp，Y.（1988）.  Auto-association  by  multilayer；perceptrons  and  singular  value  decomposition.  Biological Cybernetics  ，59；，291–294.
          關鍵詞：
        - 摘要：Bourlard，H.  and  Wellekens，C.（1989）.  Speech  pattern  discrimination；and multi-layered percep-trons. Computer Speech and Language ，3 ，1–19.
          關鍵詞：
        - 摘要：Boyd，S.  and  Vandenberghe，L.（2004）.  Convex  Optimization；Cambridge University Press，New York，NY，USA.
          關鍵詞：
        - 摘要：.
          關鍵詞：
        - 摘要：L.，Raghavan，R.，and
          關鍵詞：
        - 摘要：Brady，M.；Back-；propagation fails to separate where perceptrons succeed. IEEE Transactions
          關鍵詞：
        - 摘要：Slawny，J.（1989）.
          關鍵詞：
        - 摘要：Brakel，P.，Stroobandt，D.，and；Schrauwen，B.（2013）.  Training；energy-based  models  for  time-series  imputation.  Journal  of  Machine
          關鍵詞：
        - 摘要：Brand，M.（2003a）.  Charting  a  manifold.  In  S.  Becker，S.  Thrun，and；K.  Obermayer，editors，Advances；in  Neural  Information  Processing
          關鍵詞：
        - 摘要：Brand，M.（2003b）. Charting a manifold. In NIPS'2002 ，pages 961–968.；MIT Press.
          關鍵詞：
        - 摘要：Breiman，L.（1994）.  Bagging  predictors.  Machine  Learning  ，24；（2），123–140.
          關鍵詞：
        - 摘要：Breiman，L.，Friedman，J.  H.，Olshen，R.  A.，and  Stone，C.；J.；（1984）.  Classification  and  Regression  Trees  .  Wadsworth  International
          關鍵詞：
        - 摘要：Bridle，J. S.（1990）.  Alphanets: a recurrent ‘neural’ network architecture；with  a  hidden  Markov  model  interpretation.  Speech  Communication  ，9；（1），83–92.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；N.，and；Briggman，K.，Denk，W.，Seung，S.，Helmstaedter，M.
          關鍵詞：
        - 摘要：Brown，P.  F.，Cocke，J.，Pietra，S.  A.  D.，Pietra，V.  J.  D.，Jelinek，；F.，Lafferty，J.  D.，Mercer，R.  L.，and  Roossin，P.  S.（1990）.  A；statistical  approach  to  machine  translation.  Computational  linguistics  ，16
          關鍵詞：
        - 摘要：Brown，P.  F.，Pietra，V.  J.  D.，DeSouza，P.  V.，Lai，J.  C.，and；Mercer，R.  L.（1992）.  Class-based  n  -gram  models  of  natural  language.；Computational Linguistics ，18 ，467–479.
          關鍵詞：
        - 摘要：Bryson，A. and Ho，Y.（1969）. Applied optimal control: optimization，；estimation，and control . Blaisdell Pub. Co.
          關鍵詞：
        - 摘要：Bryson，Jr.，A.  E.  and  Denham，W.  F.（1961）.  A  steepest-ascent；method for solving optimum programming problems. Technical Report BR-；1303，Raytheon Company，Missle and Space Division.
          關鍵詞：
        - 摘要：Buciluǎ，C.，Caruana，R.，and  Niculescu-Mizil，A.（2006）.  Model；compression.  In  Proceedings  of  the  12th  ACM  SIGKDD  international；conference  on  Knowledge  discovery  and  data  mining  ，pages  535–541.
          關鍵詞：
        - 摘要：Burda，Y.，Grosse，R.，and  Salakhutdinov，R.（2015）.；weighted autoencoders. arXiv preprint arXiv:1509.00519 .
          關鍵詞：
        - 摘要：Importance
          關鍵詞：
        - 摘要：speech
          關鍵詞：
        - 摘要：Cai，M.，Shi，Y.，and  Liu，J.（2013）.  Deep  maxout  neural  networks；In  Automatic  Speech  Recognition  and；for
          關鍵詞：
        - 摘要：recognition.
          關鍵詞：
        - 摘要：Carreira-Perpiñan，M.  A.  and  Hinton，G.  E.（2005）.  On  contrastive；divergence learning. In AISTATS'2005 ，pages 33–40.
          關鍵詞：
        - 摘要：Caruana，R.（1993）.  Multitask  connectionist  learning.  In  Proceedings  of
          關鍵詞：
        - 摘要：the 1993 Connectionist Models Summer School ，pages 372–379.
          關鍵詞：
        - 摘要：Cauchy，A.（1847）.  Méthode  générale  pour  la  résolution  de  systèmes；d'équations  simultanées.  In  Compte  rendu  des  séances  de  l'académie  des；sciences ，pages 536–538.
          關鍵詞：
        - 摘要：Cayton，L.（2005）.  Algorithms  for  manifold  learning.  Technical  Report；CS2008-0923，UCSD.
          關鍵詞：
        - 摘要：Chandola，V.，Banerjee，A.，and；detection: A survey. ACM computing surveys（CSUR） ，41 （3），15.
          關鍵詞：
        - 摘要：Kumar，V.（2009）.
          關鍵詞：
        - 摘要：Anomaly
          關鍵詞：
        - 摘要：Chapelle，O.，Weston，J.，and  Schölkopf，B.（2003）.  Cluster  kernels；for semi-supervised learning. In S. Becker，S. Thrun，and K. Obermayer，；editors，Advances
          關鍵詞：
        - 摘要：in  Neural
          關鍵詞：
        - 摘要：Information
          關鍵詞：
        - 摘要：Chapelle，O.，Schölkopf，B.，and  Zien，A.，editors（2006）.  Semi-；Supervised Learning . MIT Press，Cambridge，MA.
          關鍵詞：
        - 摘要：Chellapilla，K.，Puri，S.，and  Simard，P.（2006）.  High  Performance；Convolutional Neural Net-works for Document Processing. In Guy Lorette，；editor，Tenth
          關鍵詞：
        - 摘要：Chen，B.，Ting，J.-A.，Marlin，B.  M.，and  de  Freitas，N.（2010）.；Deep  learning  of  invariant  spatio-temporal  features  from  video.  NIPS*2010；Deep Learning and Unsupervised Feature Learning Workshop.
          關鍵詞：
        - 摘要：Chen，S.  F.  and  Goodman，J.  T.（1999）.  An  empirical  study  of；smoothing  techniques  for  language  modeling.  Computer，Speech  and；Language ，13 （4），359–393.
          關鍵詞：
        - 摘要：Chen，T.，Du，Z.，Sun，N.，Wang，J.，Wu，C.，Chen，Y.，and；Temam，O.（2014a）.  DianNao:  A；small-footprint  high-throughput
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；international  conference  on  Architectural  support；languages and operating systems ，pages 269–284. ACM.
          關鍵詞：
        - 摘要：for  programming
          關鍵詞：
        - 摘要：Chen，T.，Li，M.，Li，Y.，Lin，M.，Wang，N.，Wang，M.，；Xiao，T.，Xu，B.，Zhang，C.，and  Zhang，Z.（2015）.  MXNet:  A；flexible and  efficient machine learning library for heterogeneous distributed
          關鍵詞：
        - 摘要：Chen，Y.，Luo，T.，Liu，S.，Zhang，S.，He，L.，Wang，J.，Li，；L.，Chen，T.，Xu，Z.，Sun，N.，et  al.  （2014b）.  DaDianNao:  A；machine-learning  supercomputer.  In  Microarchitecture（MICRO），2014
          關鍵詞：
        - 摘要：Kalyanaraman，K.；Chilimbi，T.，Suzue，Y.，Apacible，J.，and；（2014）.  Project  Adam:  Building  an  efficient  and  scalable  deep  learning
          關鍵詞：
        - 摘要：Cho，K.，Raiko，T.，and；is；efficient  for  learning  restricted  Boltzmann  machines.  In  Proceedings  of  the
          關鍵詞：
        - 摘要：Ilin，A.（2010a）.  Parallel
          關鍵詞：
        - 摘要：tempering
          關鍵詞：
        - 摘要：Cho，K.，Raiko，T.，and；efficient for learning restricted Boltzmann machines. In IJCNN'2010.
          關鍵詞：
        - 摘要：Ilin，A.（2010b）.  Parallel
          關鍵詞：
        - 摘要：tempering
          關鍵詞：
        - 摘要：is
          關鍵詞：
        - 摘要：Cho，K.，Raiko，T.，and  Ilin，A.（2011）.  Enhanced  gradient  and；adaptive  learning  rate  for  training  restricted  Boltzmann  machines.  In；ICML'2011 ，pages 105–112.
          關鍵詞：
        - 摘要：Merriënboer，B.，Gülçehre，Ç.，Bahdanau，D.，；Cho，K.，Van；Bougares，F.，Schwenk，H.，and Bengio，Y.（2014a）. Learning phrase
          關鍵詞：
        - 摘要：Merriënboer，B.，Gulcehre，C.，Bougares，F.，；Cho，K.，van；Schwenk，H.，and Bengio，Y.（2014b）. Learning phrase representations
          關鍵詞：
        - 摘要：the  Empiricial  Methods
          關鍵詞：
        - 摘要：for  statistical  machine
          關鍵詞：
        - 摘要：translation.
          關鍵詞：
        - 摘要：Cho，K.，Van  Merriënboer，B.，Bahdanau，D.，and；Bengio，Y.；（2014c）. On the properties of neural machine translation: Encoder-decoder
          關鍵詞：
        - 摘要：Choromanska，A.，Henaff，M.，Mathieu，M.，Arous，G.；LeCun，Y.（2014）. The loss surface of multilayer networks.
          關鍵詞：
        - 摘要：B.，and
          關鍵詞：
        - 摘要：Chorowski，J.，Bahdanau，D.，Cho，K.，and；Bengio，Y.（2014）.；End-to-end  continuous  speech  recognition  using  attention-based  recurrent
          關鍵詞：
        - 摘要：Christianson，B.（1992）.  Automatic  Hessians  by  reverse  accumulation.；IMA Journal of Numerical Analysis ，12 （2），135–150.
          關鍵詞：
        - 摘要：Chrupala，G.，Kadar，A.，and Alishahi，A.（2015）. Learning language；through pictures. arXiv 1506.03694.
          關鍵詞：
        - 摘要：Bengio，Y.（2014）.；Chung，J.，Gulcehre，C.，Cho，K.，and；Empirical  evaluation  of  gated  recurrent  neural  networks  on  sequence
          關鍵詞：
        - 摘要：Chung，J.，Gülçehre，Ç.，Cho，K.，and  Bengio，Y.（2015a）.  Gated；feedback recurrent neural networks. In ICML'15 .
          關鍵詞：
        - 摘要：Chung，J.，Kastner，K.，Dinh，L.，Goel，K.，Courville，A.，and；Bengio，Y.（2015b）. A recurrent latent variable model for sequential data.；In NIPS'2015 .
          關鍵詞：
        - 摘要：Schmidhuber，J.（2012）.；Ciresan，D.，Meier，U.，Masci，J.，and；Multi-column  deep  neural  network  for  traffic  sign  classification.  Neural
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Ciresan，D.  C.，Meier，U.，Gambardella，L.  M.，and  Schmidhuber，J.；（2010）.  Deep  big  simple  neural  nets  for  handwritten  digit  recognition.
          關鍵詞：
        - 摘要：Coates，A.  and  Ng，A.  Y.（2011）.  The  importance  of  encoding  versus；training with sparse coding and vector quantization. In ICML'2011 .
          關鍵詞：
        - 摘要：Coates，A.，Lee，H.，and  Ng，A.  Y.（2011）.  An  analysis  of  single-；layer  networks  in  unsuper-vised  feature  learning.  In  Proceedings  of  the；Thirteenth
          關鍵詞：
        - 摘要：International  Conference  on  Artificial
          關鍵詞：
        - 摘要：Coates，A.，Huval，B.，Wang，T.，Wu，D.，Catanzaro，B.，and；Andrew，N.（2013）.  Deep  learning  with  COTS  HPC  systems.  In  S.；and  D.  McAllester，editors，Proceedings  of
          關鍵詞：
        - 摘要：Cohen，N.，Sharir，O.，and  Shashua，A.（2015）.  On  the  expressive；power of deep learning: A tensor analysis. arXiv:1509.05009.
          關鍵詞：
        - 摘要：Collobert，R.（2004）.  Large  Scale  Machine  Learning  .  Ph.D.  thesis，；Université de Paris VI，LIP6.
          關鍵詞：
        - 摘要：Collobert，R.（2011）.  Deep  learning  for  efficient  discriminative  parsing.；In AISTATS'2011 .
          關鍵詞：
        - 摘要：Collobert，R.  and Weston，J.（2008a）. A unified architecture for natural；language  processing:  Deep  neural  networks  with  multitask  learning.  In；ICML'2008 .
          關鍵詞：
        - 摘要：Collobert，R. and Weston，J.（2008b）. A unified architecture for natural；language  processing:  Deep  neural  networks  with  multitask  learning.  In；ICML'2008 .
          關鍵詞：
        - 摘要：Collobert，R.，Bengio，S.，and Bengio，Y.（2001）. A parallel mixture；of SVMs for very large scale problems. Technical Report 12，IDIAP.
          關鍵詞：
        - 摘要：Collobert，R.，Bengio，S.，and Bengio，Y.（2002）. Parallel mixture of；SVMs for very large scale problem. Neural Computation .
          關鍵詞：
        - 摘要：Collobert，R.，Weston，J.，Bottou，L.，Karlen，M.，Kavukcuoglu，；K.，and  Kuksa，P.（2011a）.  Natural  language  processing（almost）；from  scratch.  The  Journal  of  Machine  Learning  Research  ，12  ，2493–
          關鍵詞：
        - 摘要：Collobert，R.，Kavukcuoglu，K.，and  Farabet，C.（2011b）.  Torch7:  A；In  BigLearn，NIPS；Matlab-like  environment
          關鍵詞：
        - 摘要：for  machine
          關鍵詞：
        - 摘要：learning.
          關鍵詞：
        - 摘要：Comon，P.（1994）.；concept？Signal Processing ，36 ，287–314.
          關鍵詞：
        - 摘要：Independent
          關鍵詞：
        - 摘要：component
          關鍵詞：
        - 摘要：analysis-a
          關鍵詞：
        - 摘要：new
          關鍵詞：
        - 摘要：Cortes，C.  and  Vapnik，V.（1995）.  Support  vector  networks.  Machine；Learning ，20 ，273–297.
          關鍵詞：
        - 摘要：Couprie，C.，Farabet，C.，Najman，L.，and；LeCun，Y.（2013）.；Indoor  semantic  segmentation  using  depth  information.  In  International
          關鍵詞：
        - 摘要：Courbariaux，M.，Bengio，Y.，and David，J.-P.（2015）. Low precision；arithmetic for deep learning. In Arxiv:1412.7024，ICLR'2015 Workshop .
          關鍵詞：
        - 摘要：Courville，A.，Bergstra，J.，and  Bengio，Y.（2011a）.  Unsupervised；models of images by spike-and-slab RBMs. In ICML'2011 .
          關鍵詞：
        - 摘要：Courville，A.，Bergstra，J.，and  Bengio，Y.（2011b）.  Unsupervised；models of images by spike-and-slab RBMs. In ICM（1b）.
          關鍵詞：
        - 摘要：Courville，A.，Desjardins，G.，Bergstra，J.，and  Bengio，Y.（2014）.；The  spike-and-slab  RBM  and  extensions  to  discrete  and  sparse  data；Intelligence，IEEE
          關鍵詞：
        - 摘要：and  Machine
          關鍵詞：
        - 摘要：Cover，T.  M.  and  Thomas，J.  A.（2006）.  Elements  of  Information
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Theory，2nd Edition . Wiley-Interscience.
          關鍵詞：
        - 摘要：Cox，D.  and  Pinto，N.（2011）.  Beyond  simple  features:  A  large-scale；feature search approach to unconstrained face recognition. In Automatic Face；&  Gesture  Recognition  and  Workshops（FG  2011），2011
          關鍵詞：
        - 摘要：Cramér，H.（1946）.  Mathematical  methods  of  statistics；University Press.
          關鍵詞：
        - 摘要：.  Princeton
          關鍵詞：
        - 摘要：Crick，F. H. C. and Mitchison，G.（1983）. The function of dream sleep.；Nature ，304 ，111–114.
          關鍵詞：
        - 摘要：Cybenko，G.（1989）.  Approximation  by  superpositions  of  a  sigmoidal；function. Mathematics of Control，Signals，and Systems ，2 ，303–314.
          關鍵詞：
        - 摘要：E.，Ranzato，M.，Mohamed，A.，and  Hinton，G.
          關鍵詞：
        - 摘要：Dahl，G.；E.；（2010）. Phone recognition with the mean-covariance restricted Boltzmann
          關鍵詞：
        - 摘要：Dahl，G.  E.，Yu，D.，Deng，L.，and  Acero，A.（2012）.  Context-；dependent  pre-trained  deep  neural  networks  for  large  vocabulary  speech；IEEE  Transactions  on  Audio，Speech，and  Language
          關鍵詞：
        - 摘要：Dahl，G.  E.，Sainath，T.  N.，and  Hinton，G.  E.（2013）.  Improving；deep neural networks for LVCSR using rectified linear units and dropout. In；ICASSP'2013 .
          關鍵詞：
        - 摘要：Dahl，G.  E.，Jaitly，N.，and  Salakhutdinov，R.（2014）.  Multi-task；neural networks for QSAR predictions. arXiv:1406.1231.
          關鍵詞：
        - 摘要：Dauphin，Y. and Bengio，Y.（2013）. Stochastic ratio matching of RBMs；for sparse high-dimensional inputs. In NIP（1）.
          關鍵詞：
        - 摘要：Dauphin，Y.，Glorot，X.，and；learning of embeddings with recon-struction sampling. In ICML'2011 .
          關鍵詞：
        - 摘要：Bengio，Y.（2011）.
          關鍵詞：
        - 摘要：Large-scale
          關鍵詞：
        - 摘要：Dauphin，Y.，Pascanu，R.，Gulcehre，C.，Cho，K.，Ganguli，S.，；and  Bengio，Y.（2014）.  Identifying  and  attacking  the  saddle  point；problem in high-dimensional non-convex optimization. In NIPS'2014 .
          關鍵詞：
        - 摘要：Davis，A.，Rubinstein，M.，Wadhwa，N.，Mysore，G.，Durand，F.，；and Freeman，W. T.（2014）. The visual microphone: Passive recovery of；sound  from  video.  ACM  Transactions  on  Graphics（Proc.  SIGGRAPH）
          關鍵詞：
        - 摘要：Dayan，P.（1990）.  Reinforcement  comparison.  In  Connectionist  Models:；Proceedings of the 1990 Connectionist Summer School ，San Mateo，CA.
          關鍵詞：
        - 摘要：Dayan，P.  and  Hinton，G.  E.（1996）.  Varieties  of  Helmholtz  machine.；Neural Networks ，9 （8），1385–1403.
          關鍵詞：
        - 摘要：Dayan，P.，Hinton，G.  E.，Neal，R.  M.，and  Zemel，R.  S.（1995）.；The Helmholtz machine. Neural computation ，7 （5），889–904.
          關鍵詞：
        - 摘要：Dean，J.，Corrado，G.，Monga，R.，Chen，K.，Devin，M.，Le，；Q.，Mao，M.，Ranzato，M.，Senior，A.，Tucker，P.，Yang，K.，and；Ng，A. Y.（2012）. Large scale distributed deep networks. In NIPS'2012 .
          關鍵詞：
        - 摘要：Dean，T.  and  Kanazawa，K.（1989）.  A  model  for  reasoning  about；persistence and causation. Computational Intelligence ，5 （3），142–150.
          關鍵詞：
        - 摘要：Deerwester，S.，Dumais，S.  T.，Furnas，G.  W.，Landauer，T.  K.，and；Harshman，R.（1990）. Indexing by latent semantic analysis. Journal of the；American Society for Information Science ，41 （6），391–407.
          關鍵詞：
        - 摘要：Delalleau，O.  and  Bengio，Y.（2011）.  Shallow  vs.  deep  sum-product；networks. In NIPS .
          關鍵詞：
        - 摘要：Deng，J.，Dong，W.，Socher，R.，Li，L.-J.，Li，K.，and  Fei-Fei，L.；（2009）.  ImageNet:  A  Large-Scale  Hierarchical  Image  Database.  In；CVPR09 .
          關鍵詞：
        - 摘要：Deng，J.，Berg，A.  C.，Li，K.，and  Fei-Fei，L.（2010a）.  What  does
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；classifying  more  than  10，000  image  categories  tell  us?  In  Proceedings  of；the  11th  European  Conference  on  Computer  Vision:  Part  V  ，ECCV'10，
          關鍵詞：
        - 摘要：Deng，L.  and  Yu，D.（2014）.  Deep  learning–methods  and  applications.；Foundations and Trends in Signal Processing .
          關鍵詞：
        - 摘要：Deng，L.，Seltzer，M.，Yu，D.，Acero，A.，Mohamed，A.，and；Hinton，G.（2010b）.  Binary  coding  of  speech  spectrograms  using  a  deep；auto-encoder. In Interspeech 2010 ，Makuhari，Chiba，Japan.
          關鍵詞：
        - 摘要：Denil，M.，Bazzani，L.，Larochelle，H.，and  de  Freitas，N.（2012）.；Learning where to attend with deep architectures for image tracking. Neural；Computation ，24 （8），2151–2184.
          關鍵詞：
        - 摘要：Denton，E.，Chintala，S.，Szlam，A.，and  Fergus，R.（2015）.  Deep；generative image models using a Laplacian pyramid of adversarial networks.；NIPS .
          關鍵詞：
        - 摘要：Desjardins，G.  and  Bengio，Y.（2008）.  Empirical  evaluation  of；convolutional  RBMs  for  vision.  Technical  Report  1327，Département；d'Informatique et de Recherche Opérationnelle，Université de Montréal.
          關鍵詞：
        - 摘要：C.，Bengio，Y.，Vincent，P.，and；Desjardins，G.，Courville，A.；Delalleau，O.（2010）.  Tempered  Markov  chain  Monte  Carlo  for  training
          關鍵詞：
        - 摘要：Desjardins，G.，Courville，A.，and  Bengio，Y.（2011）.  On  tracking；the partition function. In NIPS'2011 .
          關鍵詞：
        - 摘要：Devlin，J.，Zbib，R.，Huang，Z.，Lamar，T.，Schwartz，R.，and；Makhoul，J.（2014）.  Fast  and  robust  neural  network  joint  models  for；statistical machine translation. In Proc. ACL'2014 .
          關鍵詞：
        - 摘要：Devroye，L.（2013）.  Non-Uniform  Random  Variate  Generation；SpringerLink: Bücher. Springer New York.
          關鍵詞：
        - 摘要：.
          關鍵詞：
        - 摘要：J.（2013）.  Mechanisms；DiCarlo，J.；recognition:Humans vs. neurons vs. machines. NIPS Tutorial.
          關鍵詞：
        - 摘要：underlying
          關鍵詞：
        - 摘要：visual
          關鍵詞：
        - 摘要：object
          關鍵詞：
        - 摘要：Dinh，L.，Krueger，D.，and  Bengio，Y.（2014）.  NICE:  Non-linear；independent components esti-mation. arXiv:1410.8516.
          關鍵詞：
        - 摘要：A.，Guadarrama，S.，Rohrbach，M.，；Donahue，J.，Hendricks，L.；Venugopalan，S.，Saenko，K.，and  Darrell，T.（2014）.  Long-term
          關鍵詞：
        - 摘要：Donoho，D. L.  and Grimes，C.（2003）. Hessian eigenmaps: new locally；linear  embedding  tech-niques  for  high-dimensional  data.  Technical  Report；2003-08，Dept. Statistics，Stanford University.
          關鍵詞：
        - 摘要：Dosovitskiy，A.，Springenberg，J.  T.，and  Brox，T.（2015）.  Learning；to generate chairs with convolutional neural networks. In Proceedings of the；IEEE  Conference  on  Computer  Vision  and  Pattern  Recognition  ，pages
          關鍵詞：
        - 摘要：Doya，K.（1993）.  Bifurcations  of  recurrent  neural  networks  in  gradient；descent learning. IEEE Transactions on Neural Networks ，1 ，75–80.
          關鍵詞：
        - 摘要：Dreyfus，S.  E.（1962）.  The  numerical  solution  of  variational  problems.；Journal of Mathematical Analysis and Applications ，5 （1），30–45.
          關鍵詞：
        - 摘要：Dreyfus，S.  E.（1973）.  The  computational  solution  of  optimal  control；problems  with  time  lag.  IEEE  Transactions  on  Automatic  Control  ，18；（4），383–385.
          關鍵詞：
        - 摘要：and  LeCun，Y.（1992）.
          關鍵詞：
        - 摘要：Drucker，H.；generalisation；performance  using  double  back-propagation.  IEEE  Transactions  on  Neural
          關鍵詞：
        - 摘要：Improving
          關鍵詞：
        - 摘要：Duchi，J.，Hazan，E.，and  Singer，Y.（2011）.  Adaptive  subgradient；methods for online learning and stochastic optimization. Journal of Machine；Learning Research .
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Dudik，M.，Langford，J.，and  Li，L.（2011）.  Doubly  robust  policy；evaluation and learning. In Proceedings of the 28th International Conference
          關鍵詞：
        - 摘要：Dugas，C.，Bengio，Y.，Bélisle，F.，and；Nadeau，C.（2001）.；Incorporating second-order functional knowledge for better option pricing. In
          關鍵詞：
        - 摘要：Dziugaite，G.  K.，Roy，D.  M.，and  Ghahramani，Z.（2015）.  Training；generative  neural  networks  via  maximum  mean  discrepancy  optimization.；arXiv preprint arXiv:1505.03906 .
          關鍵詞：
        - 摘要：El  Hihi，S.  and  Bengio，Y.（1996）.  Hierarchical  recurrent  neural；networks for long-term depen-dencies. In NIPS 8 . MIT Press.
          關鍵詞：
        - 摘要：Elkahky，A.  M.，Song，Y.，and  He，X.（2015）.  A  multi-view  deep；learning  approach  for  cross  domain  user  modeling  in  recommendation；systems. In Proceedings of the 24th International Conference on World Wide
          關鍵詞：
        - 摘要：Elman，J.  L.（1993）.  Learning  and  development  in  neural  networks:  The；importance of starting small. Cognition ，48 ，781–799.
          關鍵詞：
        - 摘要：Erhan，D.，Manzagol，P.-A.，Bengio，Y.，Bengio，S.，and  Vincent，；P.（2009）.  The  difficulty  of  training  deep  architectures  and  the  effect  of；unsupervised pre-training. In AISTATS'2009 ，pages 153–160.
          關鍵詞：
        - 摘要：Erhan，D.，Bengio，Y.，Courville，A.，Manzagol，P.，Vincent，P.，；and  Bengio，S.（2010）.  Why  does  unsupervised  pre-training  help  deep；learning? J. Machine Learning Res .
          關鍵詞：
        - 摘要：Fahlman，S.  E.，Hinton，G.  E.，and  Sejnowski，T.；J.（1983）.；Massively  parallel  architectures  for  AI:  NETL，thistle，and  Boltzmann
          關鍵詞：
        - 摘要：Fang，H.，Gupta，S.，Iandola，F.，Srivastava，R.，Deng，L.，；Dollár，P.，Gao，J.，He，X.，Mitchell，M.，Platt，J.  C.，Zitnick，C.；L.，and  Zweig，G.（2015）.  From  captions  to  visual  concepts  and  back.
          關鍵詞：
        - 摘要：Farabet，C.，LeCun，Y.，Kavukcuoglu，K.，Culurciello，E.，；Martini，B.，Akselrod，P.，and  Talay，S.（2011）.  Large-scale  FPGA-；based  convolutional  networks.  In  R.  Bekkerman，M.  Bilenko，and  J.
          關鍵詞：
        - 摘要：LeCun，Y.（2013）.；Farabet，C.，Couprie，C.，Najman，L.，and；Learning  hierarchical  features  for  scene  labeling.  IEEE  Transactions  on
          關鍵詞：
        - 摘要：Fei-Fei，L.，Fergus，R.，and  Perona，P.（2006）.  One-shot  learning  of；object  categories.  IEEE  Transactions  on  Pattern  Analysis  and  Machine；Intelligence，28 （4），594–611.
          關鍵詞：
        - 摘要：Finn，C.，Tan，X.；Abbeel，P.（2015）.  Learning  visual；manipulation  with
          關鍵詞：
        - 摘要：Y.，Duan，Y.，Darrell，T.，Levine，S.，and；robotic；preprint
          關鍵詞：
        - 摘要：feature；autoencoders.
          關鍵詞：
        - 摘要：spaces
          關鍵詞：
        - 摘要：spatial
          關鍵詞：
        - 摘要：arXiv
          關鍵詞：
        - 摘要：deep
          關鍵詞：
        - 摘要：for
          關鍵詞：
        - 摘要：Fisher，R.  A.（1936）.  The  use  of  multiple  measurements  in  taxonomic；problems. Annals of Eugenics ，7 ，179–188.
          關鍵詞：
        - 摘要：Földiák，P.（1989）.  Adaptive  network；feature；extraction. In International Joint Conference on Neural Networks（IJCNN）
          關鍵詞：
        - 摘要：for  optimal
          關鍵詞：
        - 摘要：linear
          關鍵詞：
        - 摘要：Franzius，M.，Sprekeler，H.，and  Wiskott，L.（2007）.  Slowness  and；sparseness lead to place，head-direction，and spatial-view cells.
          關鍵詞：
        - 摘要：Franzius，M.，Wilbert，N.，and  Wiskott，L.（2008）.  Invariant  object；recognition  with  slow  feature  analysis.  In  Proceedings  of  the  18th；international  conference  on  Artificial  Neural  Networks，Part  I  ，ICANN
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；'08，pages 961–970，Berlin，Heidelberg. Springer-Verlag.
          關鍵詞：
        - 摘要：Frasconi，P.，Gori，M.，and  Sperduti，A.（1997）.  On  the  efficient；classification of data structures by neural networks. In Proc. Int. Joint Conf.；on Artificial Intelligence .
          關鍵詞：
        - 摘要：Frasconi，P.，Gori，M.，and；general；framework for adaptive processing of data structures. IEEE Transactions on
          關鍵詞：
        - 摘要：Sperduti，A.（1998）.
          關鍵詞：
        - 摘要：A
          關鍵詞：
        - 摘要：Freund，Y.  and  Schapire，R.  E.（1996a）.  Experiments  with  a  new；boosting  algorithm.  In  Machine  Learning:  Proceedings  of  Thirteenth；International Conference ，pages 148–156，USA. ACM.
          關鍵詞：
        - 摘要：theory，on-line；Freund，Y.  and  Schapire，R.  E.（1996b）.  Game；prediction and boosting. In Proceedings of the Ninth Annual Conference  on
          關鍵詞：
        - 摘要：Frey，B.  J.（1998）.  Graphical  models  for  machine  learning  and  digital；communication . MIT Press.
          關鍵詞：
        - 摘要：Frey，B.  J.，Hinton，G.  E.，and  Dayan，P.（1996）.  Does  the  wake-；sleep  algorithm  learn  good  density  estimators?  In  D.  Touretzky，M.；Mozer，and  M.  Hasselmo，editors，Advances
          關鍵詞：
        - 摘要：Frobenius，G.（1908）.  Über  matrizen  aus  positiven  elementen，s.  B.；Preuss. Akad. Wiss. Berlin，Germany .
          關鍵詞：
        - 摘要：Fukushima，K.（1975）.  Cognitron:  A  self-organizing  multilayered  neural；network. Biological Cybernetics ，20 ，121–136.
          關鍵詞：
        - 摘要：Fukushima，K.（1980）.  Neocognitron:  A  self-organizing  neural  network；model for a mechanism of pattern recognition unaffected by shift in position.；Biological Cybernetics ，36 ，193–202.
          關鍵詞：
        - 摘要：Gal，Y.  and  Ghahramani，Z.（2015）.  Bayesian  convolutional  neural；networks  with  Bernoulli  approximate  variational  inference.  arXiv  preprint；arXiv:1506.02158 .
          關鍵詞：
        - 摘要：Gallinari，P.，LeCun，Y.，Thiria，S.，and；（1987）.  Memoires；COGNITIVA 87 ，Paris，La Villette.
          關鍵詞：
        - 摘要：associatives  distribuees.
          關鍵詞：
        - 摘要：Fogelman-Soulie，F.；In  Proceedings  of
          關鍵詞：
        - 摘要：Grandvalet，Y.；Garcia-Duran，A.，Bordes，A.，Usunier，N.，and；（2015）.  Combining  two  and  three-way  embeddings  models  for  link
          關鍵詞：
        - 摘要：Garofolo，J.  S.，Lamel，L.  F.，Fisher，W.  M.，Fiscus，J.  G.，and；Pallett，D.  S.（1993）.  Darpa  timit  acoustic-phonetic  continous  speech；corpus cd-rom. nist speech disc 1-1.1. NASA STI/Recon Technical Report N
          關鍵詞：
        - 摘要：Garson，J.（1900）.  The  metric  system  of  identification  of  criminals，as；used  in  Great  Britain  and  Ireland.  The  Journal  of  the  Anthropological；Institute of Great Britain and Ireland ，（2），177–227.
          關鍵詞：
        - 摘要：Gers，F.  A.，Schmidhuber，J.，and  Cummins，F.（2000）.  Learning  to；forget: Continual prediction with LSTM. Neural computation ，12 （10），；2451–2471.
          關鍵詞：
        - 摘要：Ghahramani，Z.  and  Hinton，G.  E.（1996）.  The  EM  algorithm  for；mixtures  of  factor  analyzers.  Technical  Report  CRG-TR-96-1，Dpt.  of；Comp. Sci.，Univ. of Toronto.
          關鍵詞：
        - 摘要：Gillick，D.，Brunk，C.，Vinyals，O.，and  Subramanya，A.（2015）.；Multilingual；preprint
          關鍵詞：
        - 摘要：processing
          關鍵詞：
        - 摘要：language
          關鍵詞：
        - 摘要：bytes.
          關鍵詞：
        - 摘要：arXiv
          關鍵詞：
        - 摘要：from
          關鍵詞：
        - 摘要：Girshick，R.，Donahue，J.，Darrell，T.，and；Malik，J.（2015）.；Region-based  convolutional  networks  for  accurate  object  detection  and
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Giudice，M. D.，Manera，V.，and Keysers，C.（2009）. Programmed to；learn? The ontogeny of mirror neurons. Dev. Sci. ，12 （2），350–363.
          關鍵詞：
        - 摘要：Glorot，X.  and  Bengio，Y.（2010）.  Understanding  the  difficulty  of；training deep feedforward neural networks. In AISTATS'2010 .
          關鍵詞：
        - 摘要：Glorot，X.，Bordes，A.，and Bengio，Y.（2011a）. Deep sparse rectifier；neural networks. In AISTATS'2011 .
          關鍵詞：
        - 摘要：Glorot，X.，Bordes，A.，and  Bengio，Y.（2011b）.  Domain  adaptation；for  large-scale  sentiment  classification:  A  deep  learning  approach.  In；ICML'2011 .
          關鍵詞：
        - 摘要：Glorot，X.，Bordes，A.，and  Bengio，Y.（2011c）.  Domain  adaptation；for  large-scale  sentiment  classification:  A  deep  learning  approach.  In；ICM（1b），pages 97–110.
          關鍵詞：
        - 摘要：Goldberger，J.，Roweis，S.，Hinton，G.  E.，and  Salakhutdinov，R.；（2005）. Neighbourhood components analysis. In L. Saul，Y. Weiss，and；L.  Bottou，editors，Advances  in  Neural  Information  Processing  Systems
          關鍵詞：
        - 摘要：Gong，S.，McKenna，S.，and  Psarrou，A.（2000）.  Dynamic  Vision:；From Images to Face Recognition . Imperial College Press.
          關鍵詞：
        - 摘要：Goodfellow，I.，Le，Q.，Saxe，A.，and  Ng，A.（2009）.  Measuring；In  Y.  Bengio，D.  Schuurmans，C.；invariances
          關鍵詞：
        - 摘要：in  deep  networks.
          關鍵詞：
        - 摘要：Goodfellow，I.，Koenig，N.，Muja，M.，Pantofaru，C.，Sorokin，；A.，and Takayama，L.（2010）. Help me help you: Interfaces for personal；robots.  In  Proc.  of  Human  Robot  Interaction（HRI）  ，Osaka，Japan.
          關鍵詞：
        - 摘要：Goodfellow，I.，Mirza，M.，Xiao，D.，Courville，A.，and Bengio，Y.；（2014a）. An empirical inves-tigation of catastrophic forgetting in gradient-
          關鍵詞：
        - 摘要：based neural networks. In ICLR'14 .
          關鍵詞：
        - 摘要：Goodfellow，I.；report:Multidimensional，；downsampled convolution for autoencoders. Technical report，Université de
          關鍵詞：
        - 摘要：J.（2010）.
          關鍵詞：
        - 摘要：Technical
          關鍵詞：
        - 摘要：Goodfellow，I.  J.（2014）.  On  distinguishability  criteria  for  estimating；generative  models.；Learning
          關鍵詞：
        - 摘要：International  Conference
          關鍵詞：
        - 摘要：on
          關鍵詞：
        - 摘要：In
          關鍵詞：
        - 摘要：Goodfellow，I.  J.，Courville，A.，and  Bengio，Y.（2011）.  Spike-and-；slab sparse coding for unsu-pervised feature discovery. In NIPS Workshop on；Challenges in Learning Hierarchical Models .
          關鍵詞：
        - 摘要：Goodfellow，I.  J.，Warde-Farley，D.，Mirza，M.，Courville，A.，and；Bengio，Y.（2013a）. Maxout networks. In ICML'2013 .
          關鍵詞：
        - 摘要：Goodfellow，I.  J.，Warde-Farley，D.，Mirza，M.，Courville，A.，and；Bengio，Y.（2013b）.  Maxout  networks.  In  ICM（1c），pages  1319–；1327.
          關鍵詞：
        - 摘要：Goodfellow，I.  J.，Warde-Farley，D.，Mirza，M.，Courville，A.，and；Bengio，Y.（2013c）.  Maxout；Report
          關鍵詞：
        - 摘要：Technical
          關鍵詞：
        - 摘要：networks.
          關鍵詞：
        - 摘要：Goodfellow，I.；（2013d）. Multi-prediction deep Boltzmann machines. In NIP（1）.
          關鍵詞：
        - 摘要：J.，Mirza，M.，Courville，A.，and
          關鍵詞：
        - 摘要：Bengio，Y.
          關鍵詞：
        - 摘要：Goodfellow，I.；J.，Warde-Farley，D.，Lamblin，P.，Dumoulin，V.，；Mirza，M.，Pascanu，R.，Bergstra，J.，Bastien，F.，and  Bengio，Y.
          關鍵詞：
        - 摘要：Goodfellow，I.  J.，Courville，A.，and  Bengio，Y.（2013f）.  Scaling  up；spike-and-slab  models  for  unsupervised  feature  learning.  IEEE  T.  PAMI ，；pages 1902–1914.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Goodfellow，I. J.，Courville，A.，and Bengio，Y.（2013g）. Scaling  up；spike-and-slab models for un-supervised feature learning. IEEE Transactions
          關鍵詞：
        - 摘要：Goodfellow，I.  J.，Shlens，J.，and  Szegedy，C.（2014b）.  Explaining；and harnessing adversarial examples. CoRR ，abs/1412.6572 .
          關鍵詞：
        - 摘要：Goodfellow，I.；Farley，D.，Ozair，S.，Courville，A.，and；Generative adversarial networks. In NIPS'2014 .
          關鍵詞：
        - 摘要：J.，Pouget-Abadie，J.，Mirza，M.，Xu，B.，Warde-；Bengio，Y.（2014c）.
          關鍵詞：
        - 摘要：Goodfellow，I.  J.，Bulatov，Y.，Ibarz，J.，Arnoud，S.，and  Shet，V.；（2014d）. Multi-digit number recognition from Street View imagery using；deep convolutional neural networks. In International Conference on Learning
          關鍵詞：
        - 摘要：Goodfellow，I. J.，Vinyals，O.，and Saxe，A. M.（2015）. Qualitatively；characterizing  neural  network  optimization  problems.  In  International；Conference on Learning Representations .
          關鍵詞：
        - 摘要：Goodman，J.（2001）.  Classes  for  fast  maximum  entropy  training.  In；International；Signal
          關鍵詞：
        - 摘要：Acoustics，Speech
          關鍵詞：
        - 摘要：Conference
          關鍵詞：
        - 摘要：and
          關鍵詞：
        - 摘要：on
          關鍵詞：
        - 摘要：Gori，M.  and  Tesi，A.（1992）.  On  the  problem  of  local  minima  in；backpropagation.  IEEE  Transactions  on  Pattern  Analysis  and  Machine；Intelligence ，PAMI-14 （1），76–86.
          關鍵詞：
        - 摘要：Gosset，W.  S.（1908）.  The  probable  error  of  a  mean.  Biometrika  ，6；（1），1–25. Originally published under the pseudonym“Student”.
          關鍵詞：
        - 摘要：Gouws，S.，Bengio，Y.，and  Corrado，G.（2014）.  BilBOWA:  Fast；bilingual  distributed  representations  without  word  alignments.  Technical；report，arXiv:1410.2455.
          關鍵詞：
        - 摘要：Graf，H.  P.  and  Jackel，L.  D.（1989）.  Analog  electronic  neural  network；circuits. Circuits and Devices Magazine，IEEE ，5 （4），44–49.
          關鍵詞：
        - 摘要：Graves，A.（2011）. Practical variational inference for neural networks. In；NIPS'2011 .
          關鍵詞：
        - 摘要：Graves，A.（2012）. Supervised Sequence Labelling with Recurrent Neural；Networks . Studies in Computational Intelligence. Springer.
          關鍵詞：
        - 摘要：Graves，A.（2013）. Generating sequences with recurrent neural networks.；Technical report，arXiv:1308.0850.
          關鍵詞：
        - 摘要：Graves，A.；recognition with recurrent neural networks. In ICML'2014 .
          關鍵詞：
        - 摘要：Jaitly，N.（2014）.  Towards
          關鍵詞：
        - 摘要：and
          關鍵詞：
        - 摘要：end-to-end
          關鍵詞：
        - 摘要：speech
          關鍵詞：
        - 摘要：and
          關鍵詞：
        - 摘要：Graves，A.；phoneme；classification  with  bidirectional  LSTM  and  other  neural  network
          關鍵詞：
        - 摘要：Schmidhuber，J.（2005）.
          關鍵詞：
        - 摘要：Framewise
          關鍵詞：
        - 摘要：Graves，A. and Schmidhuber，J.（2009）. Offine handwriting recognition；with  multidimensional  recurrent  neural  networks.  In  D.  Koller，D.；Schuurmans，Y.  Bengio，and  L.  Bottou，editors，NIPS'2008  ，pages
          關鍵詞：
        - 摘要：Graves，A.，Fernández，S.，Gomez，F.，and；Schmidhuber，J.；（2006）.  Connectionist  temporal  classification:  Labelling  unsegmented
          關鍵詞：
        - 摘要：Graves，A.，Liwicki，M.，Bunke，H.，Schmidhuber，J.，and；Fernández，S.（2008）.  Unconstrained  on-line  handwriting  recognition；with  recurrent  neural  networks.  In  J.  Platt，D.  Koller，Y.  Singer，and  S.
          關鍵詞：
        - 摘要：Graves，A.，Liwicki，M.，Fernández，S.，Bertolami，R.，Bunke，；H.，and  Schmidhuber，J.（2009）.  A  novel  connectionist  system  for；unconstrained  handwriting  recognition.  Pattern  Analysis  and  Machine
          關鍵詞：
        - 摘要：Graves，A.，Mohamed，A.，and
          關鍵詞：
        - 摘要：Hinton，G.（2013）.
          關鍵詞：
        - 摘要：Speech
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；recognition  with  deep  recurrent  neural  networks.  In  ICASSP'2013  ，pages；6645–6649.
          關鍵詞：
        - 摘要：Graves，A.，Wayne，G.，and  Danihelka，I.（2014）.  Neural  Turing；machines. arXiv:1410.5401.
          關鍵詞：
        - 摘要：Grefenstette，E.，Hermann，K.  M.，Suleyman，M.，and  Blunsom，P.；（2015）. Learning to transduce with unbounded memory. In NIPS'2015 .
          關鍵詞：
        - 摘要：Greff，K.，Srivastava，R.  K.，Koutník，J.，Steunebrink，B.  R.，and；Schmidhuber，J.（2015）.  LSTM:  a  search  space  odyssey.  arXiv  preprint；arXiv:1503.04069 .
          關鍵詞：
        - 摘要：Gregor，K. and LeCun，Y.（2010a）. Emergence of complex-like cells in；a  temporal  product  network  with  local  receptivefields.  Technical  report，；arXiv:1006.0448.
          關鍵詞：
        - 摘要：Gregor，K.  and  LeCun，Y.（2010b）.  Learning  fast  approximations  of；sparse  coding.  In  L.  Bottou  and  M.  Littman，editors，Proceedings  of  the；Twenty-seventh  International  Conference  on  Machine  Learning（ICML-
          關鍵詞：
        - 摘要：Gregor，K.，Danihelka，I.，Mnih，A.，Blundell，C.，and Wierstra，D.；（2014）.  Deep  autoregressive  networks.  In  International  Conference  on；Machine Learning（ICML'2014） .
          關鍵詞：
        - 摘要：Gregor，K.，Danihelka，I.，Graves，A.，and  Wierstra，D.（2015）.；DRAW:  A  recurrent  neural  network  for  image  generation.  arXiv  preprint；arXiv:1502.04623 .
          關鍵詞：
        - 摘要：Gretton，A.，Borgwardt，K.  M.，Rasch，M.  J.，Schölkopf，B.，and；Smola，A.（2012）.  A  kernel  two-sample  test.  The  Journal  of  Machine；Learning Research ，13 （1），723–773.
          關鍵詞：
        - 摘要：Guillaume  Desjardins，Karen  Simonyan，R.  P.  K.  K.（2015）.  Natural；neural networks. Technical report，arXiv:1507.00210.
          關鍵詞：
        - 摘要：Gulcehre，C. and Bengio，Y.（2013）. Knowledge matters: Importance of；prior  information  for  optimization.  Technical  Report  arXiv:1301.4083，；Universite de Montreal.
          關鍵詞：
        - 摘要：Guo，H.  and  Gelfand，S.  B.（1992）.  Classification  trees  with  neural；network  feature  extraction.  Neural  Networks，IEEE  Transactions  on  ，3；（6），923–933.
          關鍵詞：
        - 摘要：Gupta，S.，Agrawal，A.，Gopalakrishnan，K.，and；（2015）.  Deep；learning  with
          關鍵詞：
        - 摘要：Narayanan，P.；limited  numerical  precision.  CoRR
          關鍵詞：
        - 摘要：Gutmann，M.  and  Hyvarinen，A.（2010）.  Noise-contrastive  estimation:；A  new  estimation  princi-ple  for  unnormalized  statistical  models.  In；Proceedings  of  The  Thirteenth  International  Conference  on  Artificial
          關鍵詞：
        - 摘要：Hadsell，R.，Sermanet，P.，Ben，J.，Erkan，A.，Han，J.，Muller，；U.，and  LeCun，Y.（2007）.  Online  learning  for  offroad  robots:  Spatial；label  propagation  to  learn  long-range  traversability.  In  Proceedings  of
          關鍵詞：
        - 摘要：Hajnal，A.，Maass，W.，Pudlak，P.，Szegedy，M.，and；Turan，G.；（1993）. Threshold circuits of bounded depth. J. Comput. System. Sci. ，46
          關鍵詞：
        - 摘要：Håstad，J.（1986）. Almost optimal lower bounds for small depth circuits.；In Proceedings of the 18th annual ACM Symposium on Theory of Computing；，pages 6–20，Berkeley，California. ACM Press.
          關鍵詞：
        - 摘要：Håstad，J.  and  Goldmann，M.（1991）.  On  the  power  of  small-depth；threshold circuits. Computational Complexity ，1 ，113–129.
          關鍵詞：
        - 摘要：Hastie，T.，Tibshirani，R.，and  Friedman，J.（2001）.  The  elements  of；statistical learning: data mining，inference and prediction . Springer Series；in Statistics. Springer Verlag.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；He，K.，Zhang，X.，Ren，S.，and  Sun，J.（2015）.  Delving  deep  into；rectifiers:  Surpassing  human-level  performance  on  ImageNet  classification.
          關鍵詞：
        - 摘要：Hebb，D. O.（1949）. The Organization of Behavior . Wiley，New York.
          關鍵詞：
        - 摘要：Henaff，M.，Jarrett，K.，Kavukcuoglu，K.，and  LeCun，Y.（2011）.；Unsupervised learning of sparse features for scalable audio classification. In；ISMIR'11 .
          關鍵詞：
        - 摘要：Henderson，J.（2003）. Inducing history representations for broad coverage；statistical parsing. In HLT-NAACL ，pages 103–110.
          關鍵詞：
        - 摘要：Henderson，J.（2004）.  Discriminative；training  of  a  neural  network；statistical parser. In Proceedings of the 42nd Annual Meeting on Association
          關鍵詞：
        - 摘要：Henniges，M.，Puertas，G.，Bornschein，J.，Eggert，J.，and Lücke，J.；（2010）.  Binary  sparse  coding.  In  Latent  Variable  Analysis  and  Signal；Separation ，pages 450–457. Springer.
          關鍵詞：
        - 摘要：Herault，J.  and  Ans，B.（1984）.  Circuits  neuronaux  à  synapses；modifiables:  Décodage  de  messages  composites  par  apprentissage  non；supervisé. Comptes Rendus de l'Académie des Sciences ，299（III-13） ，
          關鍵詞：
        - 摘要：Hinton，G.，Deng，L.，Dahl，G.；Senior，A.，Vanhoucke，V.，Nguyen，P.，Sainath，T.，and；Kingsbury，B.（2012a）.  Deep  neural  networks  for  acoustic  modeling  in
          關鍵詞：
        - 摘要：E.，Mohamed，A.，Jaitly，N.，
          關鍵詞：
        - 摘要：Hinton，G.，Vinyals，O.，and  Dean，J.（2015）.  Distilling；knowledge in a neural network. arXiv preprint arXiv:1503.02531 .
          關鍵詞：
        - 摘要：the
          關鍵詞：
        - 摘要：Hinton，G.  E.（1989）.  Connectionist；Intelligence ，40 ，185–234.
          關鍵詞：
        - 摘要：learning  procedures.  Artificial
          關鍵詞：
        - 摘要：Hinton，G. E.（1990）.  Mapping part-whole hierarchies into connectionist；networks. Artificial Intelligence ，46 （1），47–75.
          關鍵詞：
        - 摘要：Hinton，G.  E.（1999）.  Products  of  experts.  In  Proceedings  of  the  Ninth；International  Conference  on  Artificial  Neural  Networks（ICANN）  ，；volume 1，pages 1–6，Edinburgh，Scotland. IEE.
          關鍵詞：
        - 摘要：Hinton，G.  E.（2000）.  Training  products  of  experts  by  minimizing；contrastive  divergence.  Technical  Report  GCNU  TR  2000-004，Gatsby；Unit，University College London.
          關鍵詞：
        - 摘要：Hinton，G.  E.（2006）.  To  recognize  shapes，first  learn  to  generate；images. Technical Report UTML TR 2006-003，University of Toronto.
          關鍵詞：
        - 摘要：Hinton，G. E.（2007a）. How to do backpropagation in a brain. Invited talk；at the NIPS'2007 Deep Learning Workshop.
          關鍵詞：
        - 摘要：Hinton，G. E.（2007b）. Learning multiple layers of representation. Trends；in cognitive sciences ，11 （10），428–434.
          關鍵詞：
        - 摘要：Hinton，G.  E.（2010）.  A  practical  guide  to  training  restricted  Boltzmann；machines.  Technical  Report  UTML  TR  2010-003，Comp.  Sc.，University；of Toronto.
          關鍵詞：
        - 摘要：Hinton，G. E.（2012）. Tutorial on deep learning. IPAM Graduate Summer；School: Deep Learning，Feature Learning.
          關鍵詞：
        - 摘要：Hinton，G.  E.  and  Ghahramani，Z.（1997）.  Generative  models  for；discovering sparse distributed representations. Philosophical Transactions of；the Royal Society of London .
          關鍵詞：
        - 摘要：Hinton，G. E. and McClelland，J. L.（1988）. Learning representations by；recirculation. In NIPS'1987 ，pages 358–366.
          關鍵詞：
        - 摘要：Hinton，G. E. and Roweis，S.（2003）. Stochastic neighbor embedding. In；NIPS'2002 .
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Hinton，G.  E.；the
          關鍵詞：
        - 摘要：Salakhutdinov，R.（2006）.  Reducing
          關鍵詞：
        - 摘要：and
          關鍵詞：
        - 摘要：Hinton，G.  E.  and  Sejnowski，T.  J.（1986）.  Learning  and  relearning  in；Boltzmann  machines.  In  D.  E.  Rumelhart  and  J.  L.  McClelland，；editors，Parallel  Distributed  Processing  ，volume  1，chapter  7，pages
          關鍵詞：
        - 摘要：Hinton，G.  E.  and  Sejnowski，T.  J.（1999）.  Unsupervised  learning:；foundations of neural computation . MIT press.
          關鍵詞：
        - 摘要：Hinton，G.  E.  and  Shallice，T.（1991）.  Lesioning  an  attractor  network:；investigations of acquired dyslexia. Psychological review ，98 （1），74.
          關鍵詞：
        - 摘要：Hinton，G.  E.  and  Zemel，R.  S.（1994）.  Autoencoders，minimum；description length，and Helmholtz free energy. In NIPS'1993 .
          關鍵詞：
        - 摘要：Hinton，G.  E.，Sejnowski，T.；J.，and  Ackley，D.  H.（1984a）.；Boltzmann  machines:  Constraint  satisfaction  networks  that  learn.  Technical
          關鍵詞：
        - 摘要：J.，and  Ackley，D.  H.（1984b）.；Hinton，G.  E.，Sejnowski，T.；Boltzmann  machines:  Constraint  satisfaction  networks  that  learn.  Technical
          關鍵詞：
        - 摘要：Hinton，G. E.，McClelland，J.，and Rumelhart，D.（1986）. Distributed；representations. In D. E. Rumelhart and J. L. McClelland，editors，Parallel；Distributed  Processing:  Explorations  in  the  Microstructure  of  Cognition ，
          關鍵詞：
        - 摘要：Hinton，G.  E.，Revow，M.，and  Dayan，P.（1995a）.  Recognizing；handwritten  digits  using  mixtures  of  linear  models.  In  G.  Tesauro，D.；Touretzky，and  T.  Leen，editors，Advances
          關鍵詞：
        - 摘要：in  Neural
          關鍵詞：
        - 摘要：Cambridge，MA.
          關鍵詞：
        - 摘要：Hinton，G.  E.，Dayan，P.，Frey，B.  J.，and  Neal，R.  M.（1995b）.；The  wake-sleep  algorithm  for  unsupervised  neural  networks.  Science ，268；，1558–1161.
          關鍵詞：
        - 摘要：Hinton，G.  E.，Dayan，P.，and  Revow，M.（1997）.  Modelling  the；manifolds  of  images  of  hand-written  digits.  IEEE  Transactions  on  Neural；Networks ，8 ，65–74.
          關鍵詞：
        - 摘要：Hinton，G.  E.，Welling，M.，Teh，Y.  W.，and  Osindero，S.（2001）.；A  new  view  of  ICA.  In  Proceedings  of  3rd  International  Conference  on；Independent Component Analysis and Blind Signal Separation（ICA'01） ，
          關鍵詞：
        - 摘要：Hinton，G.  E.，Osindero，S.，and  Teh，Y.（2006a）.  A  fast  learning；algorithm for deep belief nets. Neural Computation ，18 ，1527–1554.
          關鍵詞：
        - 摘要：Hinton，G. E.，Osindero，S.，and Teh，Y.-W.（2006b）. A fast learning；algorithm for deep belief nets. Neural Computation ，18 ，1527–1554.
          關鍵詞：
        - 摘要：Hinton，G.  E.，Deng，L.，Yu，D.，Dahl，G.  E.，Mohamed，A.，；Jaitly，N.，Senior，A.，Vanhoucke，V.，Nguyen，P.，Sainath，T.；N.，and  Kingsbury，B.（2012b）.  Deep  neural  networks  for  acoustic
          關鍵詞：
        - 摘要：Hinton，G.；E.，Srivastava，N.，Krizhevsky，A.，Sutskever，I.，and；Salakhutdinov，R.（2012c）. Improving neural networks by preventing co-
          關鍵詞：
        - 摘要：Hinton，G.；E.，Srivastava，N.，Krizhevsky，A.，Sutskever，I.，and；Salakhutdinov，R.（2012d）. Improving neural networks by preventing co-
          關鍵詞：
        - 摘要：Hinton，G.  E.，Vinyals，O.，and  Dean，J.（2014）.  Dark  knowledge.；Invited talk at the BayLearn Bay Area Machine Learning Symposium.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Hochreiter，S.（1991a）.  Untersuchungen  zu  dynamischen  neuronalen；Netzen. Diploma thesis，T.U. München.
          關鍵詞：
        - 摘要：Hochreiter，S.（1991b）.  Untersuchungen  zu  dynamischen  neuronalen；Netzen.  Diploma  thesis，Institut  für  Informatik，Lehrstuhl  Prof.  Brauer，；Technische Universität München.
          關鍵詞：
        - 摘要：Hochreiter，S.  and  Schmidhuber，J.（1995）.  Simplifying  neural  nets  by；discoveringflat  minima.  In  Advances  in  Neural  Information  Processing；Systems 7 ，pages 529–536. MIT Press.
          關鍵詞：
        - 摘要：Hochreiter，S.  and  Schmidhuber，J.（1997）.  Long  short-term  memory.；Neural Computation ，9 （8），1735–1780.
          關鍵詞：
        - 摘要：Hochreiter，S.，Bengio，Y.，and  Frasconi，P.（2001）.  Gradientflow  in；recurrent nets: the difficulty of learning long-term dependencies. In J. Kolen；and  S.  Kremer，editors，Field  Guide  to  Dynamical  Recurrent  Networks  .
          關鍵詞：
        - 摘要：Holi，J.  L.  and  Hwang，J.-N.（1993）.  Finite  precision  error  analysis  of；neural  network  hardware  implementations.  Computers，IEEE  Transactions；on ，42 （3），281–290.
          關鍵詞：
        - 摘要：Holt，J. L. and Baker，T. E.（1991）. Back propagation simulations using；limited  precision  calculations.  In  Neural  Networks，1991.，IJCNN-91-；Seattle  International  Joint  Conference  on  ，volume  2，pages  121–126.
          關鍵詞：
        - 摘要：Hornik，K.，Stinchcombe，M.，and  White，H.（1989）.  Multilayer；feedforward networks are universal approximators. Neural Networks ，2  ，；359–366.
          關鍵詞：
        - 摘要：Hornik，K.，Stinchcombe，M.，and  White，H.（1990）.  Universal；approximation  of  an  unknown  mapping  and  its  derivatives  using  multilayer；feedforward networks. Neural networks ，3 （5），551–560.
          關鍵詞：
        - 摘要：Hsu，F.-H.（2002）.  Behind  Deep  Blue:  Building  the  Computer  That
          關鍵詞：
        - 摘要：Defeated  the  World  Chess  Champion  .  Princeton  University  Press，；Princeton，NJ，USA.
          關鍵詞：
        - 摘要：and  Ogata，Y.（2002）.  Generalized  pseudo-likelihood；Huang，F.；estimates  for  Markov  random  fields  on  lattice.  Annals  of  the  Institute  of
          關鍵詞：
        - 摘要：Huang，P.-S.，He，X.，Gao，J.，Deng，L.，Acero，A.，and Heck，L.；（2013）.  Learning  deep  structured  semantic  models  for  web  search  using；clickthrough data. In Proceedings of the 22nd ACM international conference
          關鍵詞：
        - 摘要：Hubel，D.  and  Wiesel，T.（1968）.  Receptivefields  and；functional；architecture  of  monkey  striate  cortex.  Journal  of  Physiology（London）
          關鍵詞：
        - 摘要：Hubel，D.  H.  and  Wiesel，T.  N.（1959）.  Receptivefields  of  single；neurons in the cat's striate cortex. Journal of Physiology ，148 ，574–591.
          關鍵詞：
        - 摘要：Hubel，D.  H.  and  Wiesel，T.  N.（1962）.  Receptivefields，binocular；interaction，and functional architecture in the cat's visual cortex. Journal of；Physiology（London） ，160 ，106–154.
          關鍵詞：
        - 摘要：Huszar，F.（2015）. How（not） to train your generative model: schedule；sampling，likelihood，adversary? arXiv:1511.05101 .
          關鍵詞：
        - 摘要：Hutter，F.，Hoos，H.，and；Sequential；model-based  optimization  for  general  algorithm  configuration.  In  LION-5  .
          關鍵詞：
        - 摘要：Leyton-Brown，K.（2011）.
          關鍵詞：
        - 摘要：Hyotyniemi，H.（1996）.  Turing  machines  are  recurrent  neural  networks.；In STeP'96 ，pages 13–24.
          關鍵詞：
        - 摘要：Hyvärinen，A.（1999）.  Survey  on；Neural Computing Surveys ，2 ，94–128.
          關鍵詞：
        - 摘要：independent  component  analysis.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Hyvärinen，A.（2005a）.  Estimation  of  non-normalized  statistical  models；using  score  matching.  Journal  of  Machine  Learning  Research  ，6  ，695–
          關鍵詞：
        - 摘要：Hyvärinen，A.（2005b）.  Estimation  of  non-normalized  statistical  models；using score matching. J. Machine Learning Res. ，6 .
          關鍵詞：
        - 摘要：Hyvärinen，A.（2007a）.  Connections；contrastive  divergence，and  pseu-dolikelihood；variables. IEEE Transactions on Neural Networks ，18 ，1529–1531.
          關鍵詞：
        - 摘要：score  matching，；for  continuous-valued
          關鍵詞：
        - 摘要：between
          關鍵詞：
        - 摘要：Hyvärinen，A.（2007b）.；Computational Statistics and Data Analysis ，51 ，2499–2512.
          關鍵詞：
        - 摘要：extensions
          關鍵詞：
        - 摘要：Some
          關鍵詞：
        - 摘要：of
          關鍵詞：
        - 摘要：score  matching.
          關鍵詞：
        - 摘要：Hyvärinen，A.  and  Hoyer，P.  O.（1999）.  Emergence  of  topography  and；complex cell properties from natural images using extensions of ica. In NIPS；，pages 827–833.
          關鍵詞：
        - 摘要：Hyvärinen，A.；independent；component analysis: Existence and uniqueness results. Neural Networks ，12
          關鍵詞：
        - 摘要：and  Pajunen，P.（1999）.  Nonlinear
          關鍵詞：
        - 摘要：Hyvärinen，A.，Karhunen，J.，and  Oja，E.（2001a）.；Component Analysis . Wiley-Interscience.
          關鍵詞：
        - 摘要：Independent
          關鍵詞：
        - 摘要：Hyvärinen，A.，Hoyer，P.  O.，and  Inki，M.  O.（2001b）.  Topographic；independent  component  analysis.  Neural  Computation  ，13  （7），1527–；1558.
          關鍵詞：
        - 摘要：Hyvärinen，A.，Hurri，J.，and  Hoyer，P.  O.（2009）.  Natural  Image；Statistics: A probabilistic approach to early computational vision . Springer-；Verlag.
          關鍵詞：
        - 摘要：Iba，Y.（2001）. Extended ensemble Monte Carlo. International Journal of；Modern Physics ，C12 ，623–656.
          關鍵詞：
        - 摘要：Inayoshi，H.  and  Kurita，T.（2005）.  Improved  generalization  by  adding
          關鍵詞：
        - 摘要：both  auto-association  and  hidden-layer  noise  to  neural-network-based-；classifiers. IEEE  Workshop  on  Machine  Learning  for  Signal  Processing  ，；pages 141–146.
          關鍵詞：
        - 摘要：Ioffe，S.  and  Szegedy，C.（2015）.  Batch  normalization:  Accelerating；deep network training by reducing internal covariate shift.
          關鍵詞：
        - 摘要：Jacobs，R.  A.（1988）.  Increased  rates  of  convergence  through  learning；rate adaptation. Neural networks ，1 （4），295–307.
          關鍵詞：
        - 摘要：Jacobs，R.  A.，Jordan，M.  I.，Nowlan，S.  J.，and  Hinton，G.  E.；（1991）.  Adaptive  mixtures  of  local  experts.  Neural  Computation ，3  ，；79–87.
          關鍵詞：
        - 摘要：Jaeger，H.（2003）.  Adaptive  nonlinear  system  identification  with  echo；state networks. In Advances in Neural Information Processing Systems 15 .
          關鍵詞：
        - 摘要：Jaeger，H.（2007a）.  Discovering  multiscale  dynamical  features  with；hierarchical echo state networks. Technical report，Jacobs University.
          關鍵詞：
        - 摘要：Jaeger，H.（2007b）. Echo state network. Scholarpedia ，2 （9），2330.
          關鍵詞：
        - 摘要：Jaeger，H.（2012）.  Long  short-term  memory  in  echo  state  networks:；Details  of  a  simulation  study.  Technical  report，Technical  report，Jacobs；University Bremen.
          關鍵詞：
        - 摘要：Jaeger，H.  and  Haas，H.（2004）.  Harnessing  nonlinearity:  Predicting；chaotic systems and saving energy in wireless communication. Science ，304；（5667），78–80.
          關鍵詞：
        - 摘要：Jaeger，H.，Lukosevicius，M.，Popovici，D.，and；Siewert，U.；（2007）. Optimization and applications of echo state networks with leaky-
          關鍵詞：
        - 摘要：Jain，V.，Murray，J.；F.，Roth，F.，Turaga，S.，Zhigulin，V.，；Briggman，K.  L.，Helmstaedter，M.  N.，Denk，W.，and  Seung，H.  S.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；networks. In Computer  Vision，2007.  ICCV  2007.  IEEE  11th  International；Conference on ，pages 1–8. IEEE.
          關鍵詞：
        - 摘要：Jaitly，N.  and  Hinton，G.（2011）.  Learning  a  better  representation  of；speech  soundwaves  using  restricted  Boltzmann  machines.  In  Acoustics，；Speech  and  Signal  Processing（ICASSP），2011  IEEE  International
          關鍵詞：
        - 摘要：Jaitly，N.；tract；perturbation（VTLP） improves speech recognition. In ICML'2013 .
          關鍵詞：
        - 摘要：E.（2013）.  Vocal
          關鍵詞：
        - 摘要：and  Hinton，G.
          關鍵詞：
        - 摘要：length
          關鍵詞：
        - 摘要：LeCun，Y.；Jarrett，K.，Kavukcuoglu，K.，Ranzato，M.，and；（2009a）. What is the best multi-stage architecture for object recognition?
          關鍵詞：
        - 摘要：Jarrett，K.，Kavukcuoglu，K.，Ranzato，M.，and；LeCun，Y.；（2009b）. What is the best multi-stage architecture for object recognition?
          關鍵詞：
        - 摘要：Jarzynski，C.（1997）. Nonequilibrium equality for free energy differences.；Phys. Rev. Lett. ，78 ，2690–2693.
          關鍵詞：
        - 摘要：Jaynes，E.  T.（2003）.  Probability  Theory:  The  Logic  of  Science；Cambridge University Press.
          關鍵詞：
        - 摘要：.
          關鍵詞：
        - 摘要：Jean，S.，Cho，K.，Memisevic，R.，and  Bengio，Y.（2014）.  On；using  very；translation.
          關鍵詞：
        - 摘要：target  vocabulary  for  neural  machine
          關鍵詞：
        - 摘要：large
          關鍵詞：
        - 摘要：Jelinek，F. and Mercer，R. L.（1980）. Interpolated estimation of Markov；source  parameters  from  sparse  data.  In  E.  S.  Gelsema  and  L.  N.  Kanal，；editors，Pattern Recognition in Practice . North-Holland，Amsterdam.
          關鍵詞：
        - 摘要：Jia，Y.（2013）.  Caffe:An  open  source  convolutional  architecture  for  fast；feature embedding. http://caffe.berkeleyvision.org/.
          關鍵詞：
        - 摘要：Jia，Y.，Huang，C.，and  Darrell，T.（2012）. Beyond spatial pyramids:；Receptivefield  learning  for  pooled  image  features.  In  Computer  Vision  and；Pattern  Recognition（CVPR），2012  IEEE  Conference  on ，pages  3370–
          關鍵詞：
        - 摘要：Jim，K.-C.，Giles，C.  L.，and  Horne，B.  G.（1996）.  An  analysis  of；noise  in  recurrent  neural  networks:  convergence  and  generalization.  IEEE；Transactions on Neural Networks ，7 （6），1424–1438.
          關鍵詞：
        - 摘要：Jordan，M.  I.（1998）.  Learning；Dordrecht，Netherlands.
          關鍵詞：
        - 摘要：in  Graphical  Models
          關鍵詞：
        - 摘要：.  Kluwer，
          關鍵詞：
        - 摘要：Joulin，A.  and  Mikolov，T.（2015）.  Inferring  algorithmic  patterns  with；stack-augmented recurrent nets. arXiv preprint arXiv:1503.01007 .
          關鍵詞：
        - 摘要：Jozefowicz，R.，Zaremba，W.，and Sutskever，I.（2015）. An empirical；evaluation of recurrent network architectures. In ICML'2015 .
          關鍵詞：
        - 摘要：Judd，J.  S.（1989）.  Neural  Network  Design  and  the  Complexity  of；Learning . MIT press.
          關鍵詞：
        - 摘要：Jutten，C. and Herault，J.（1991）. Blind separation of sources，part I: an；adaptive  algorithm  based  on  neuromimetic  architecture.  Signal  Processing；，24 ，1–10.
          關鍵詞：
        - 摘要：E.，Pal，C.，Bouthillier，X.，Froumenty，P.，Gülçehre，
          關鍵詞：
        - 摘要：Kahou，S.；c.，Memisevic，R.，Vincent，P.，Courville，A.，Bengio，Y.，；Ferrari，R.  C.，Mirza，M.，Jean，S.，Carrier，P.  L.，Dauphin，Y.，
          關鍵詞：
        - 摘要：Kalchbrenner，N.  and  Blunsom，P.（2013）.  Recurrent  continuous；translation models. In EMNLP'2013 .
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Kalchbrenner，N.，Danihelka，I.，and  Graves，A.（2015）.  Grid  long；short-term memory. arXiv preprint arXiv:1507.01526 .
          關鍵詞：
        - 摘要：Kamyshanska，H. and Memisevic，R.（2015）. The potential energy of an；autoencoder.  IEEE  Transactions  on  Pattern  Analysis  and  Machine；Intelligence .
          關鍵詞：
        - 摘要：Karpathy，A. and Li，F.-F.（2015）. Deep visual-semantic alignments for；generating image de-scriptions. In CVPR'2015 . arXiv:1412.2306.
          關鍵詞：
        - 摘要：Karpathy，A.，Toderici，G.，Shetty，S.，Leung，T.，Sukthankar，；R.，and  Fei-Fei，L.（2014）.  Large-scale  video  classification  with；convolutional neural networks. In CVPR .
          關鍵詞：
        - 摘要：Karush，W.（1939）.  Minima  of  Functions  of  Several  Variables  with；Inequalities  as  Side  Constraints.  Master's  thesis ，Dept.  of  Mathematics，；Univ. of Chicago.
          關鍵詞：
        - 摘要：Katz，S.  M.（1987）.  Estimation  of  probabilities  from  sparse  data  for  the；language  model  compo-nent  of  a  speech  recognizer.  IEEE  Transactions  on；Acoustics，Speech，and Signal Processing ，ASSP-35 （3），400–401.
          關鍵詞：
        - 摘要：Kavukcuoglu，K.，Ranzato，M.，and；Fast；inference in sparse coding algorithms with applications to object recognition.
          關鍵詞：
        - 摘要：LeCun，Y.（2008）.
          關鍵詞：
        - 摘要：Kavukcuoglu，K.，Ranzato，M.-A.，Fergus，R.，and；LeCun，Y.；（2009）.  Learning  invariant  features  through  topographicfilter  maps.  In
          關鍵詞：
        - 摘要：Kavukcuoglu，K.，Sermanet，P.，Boureau，Y.-L.，Gregor，K.，；Mathieu，M.，and  LeCun，Y.（2010）.  Learning  convolutional  feature；hierarchies for visual recognition. In NIPS'2010 .
          關鍵詞：
        - 摘要：Kelley，H. J.（1960）. Gradient theory of optimalflight paths. ARS Journal；，30 （10），947–954.
          關鍵詞：
        - 摘要：Khan，F.，Zhu，X.，and Mutlu，B.（2011）. How do humans teach: On；curriculum  learning  and  teaching  dimension.  In  Advances  in  Neural；Information Processing Systems 24（NIPS'11） ，pages 1449–1457.
          關鍵詞：
        - 摘要：Kim，S.  K.，McAfee，L.  C.，McMahon，P.  L.，and  Olukotun，K.；（2009）.  A  highly  scalable  restricted  Boltzmann  machine  FPGA；implementation. In Field Programmable Logic and Applications，2009. FPL
          關鍵詞：
        - 摘要：Kindermann，R.（1980）.  Markov；Random；Applications（Contemporary  Mathe-matics；V.
          關鍵詞：
        - 摘要：Fields；1）
          關鍵詞：
        - 摘要：and
          關鍵詞：
        - 摘要：Their；.  American
          關鍵詞：
        - 摘要：Kingma，D.  and  Ba，J.（2014）.  Adam:  A  method  for  stochastic；optimization. arXiv preprint arXiv:1412.6980 .
          關鍵詞：
        - 摘要：Kingma，D.  and  LeCun，Y.（2010a）.  Regularized  estimation  of  image；statistics by score matching. In NIPS'2010 .
          關鍵詞：
        - 摘要：Kingma，D.  and  LeCun，Y.（2010b）.  Regularized  estimation  of  image；statistics  by  score  matching.  In  J.  Lafferty，C.  K.  I.  Williams，J.  Shawe-；Taylor，R.  Zemel，and  A.  Culotta，editors，Advances
          關鍵詞：
        - 摘要：Kingma，D.，Rezende，D.，Mohamed，S.，and  Welling，M.（2014）.；Semi-supervised learning with deep generative models. In NIPS'2014 .
          關鍵詞：
        - 摘要：Kingma，D.  P.（2013）.  Fast  gradient-based  inference  with  continuous；latent variable models in auxiliary form. Technical report，arxiv:1306.0733.
          關鍵詞：
        - 摘要：Kingma，D.  P.  and  Welling，M.（2014a）.  Auto-encoding  variational；bayes.  In  Proceedings  of  the  International  Conference  on  Learning；Representations（ICLR） .
          關鍵詞：
        - 摘要：Kingma，D.  P.  and  Welling，M.（2014b）.  Efficient  gradient-based；inference  through  transforma-tions  between  bayes  nets  and  neural  nets.；Technical report，arxiv:1402.0480.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Kirkpatrick，S.，Jr.，C.  D.  G.，and  Vecchi，M.；Optimization by simulated annealing. Science ，220 ，671–680.
          關鍵詞：
        - 摘要：P.（1983）.
          關鍵詞：
        - 摘要：Kiros，R.，Salakhutdinov，R.，and  Zemel，R.（2014a）.  Multimodal；neural language models. In ICML'2014 .
          關鍵詞：
        - 摘要：Zemel，R.（2014b）.  Unifying；Kiros，R.，Salakhutdinov，R.，and；visual-semantic embeddings with multimodal neural language models. arXiv
          關鍵詞：
        - 摘要：Klementiev，A.，Titov，I.，and；Inducing；crosslingual distributed representations of words. In Proceedings of COLING
          關鍵詞：
        - 摘要：Bhattarai，B.（2012）.
          關鍵詞：
        - 摘要：Knowles-Barley，S.，Jones，T.  R.，Morgan，J.，Lee，D.，Kasthuri，；N.，Lichtman，J.  W.，and  Pfister，H.（2014）.  Deep  learning  for  the；connectome. GPU Technology Conference .
          關鍵詞：
        - 摘要：Koller，D.  and  Friedman，N.（2009）.  Probabilistic  Graphical  Models:；Principles and Techniques . MIT Press.
          關鍵詞：
        - 摘要：Konig，Y.，Bourlard，H.，and Morgan，N.（1996）. REMAP:Recursive；estimation  and  maxi-mization  of  a  posteriori  probabilities–application  to；transition-based  connectionist  speech  recognition.  In  D.  Touretzky，M.
          關鍵詞：
        - 摘要：Koren，Y.（2009）. The BellKor solution to the Netflix grand prize.
          關鍵詞：
        - 摘要：Kotzias，D.，Denil，M.，de Freitas，N.，and Smyth，P.（2015）. From；group to individual labels using deep features. In ACM SIGKDD .
          關鍵詞：
        - 摘要：Koutnik，J.，Greff，K.，Gomez，F.，and  Schmidhuber，J.（2014）.  A；clockwork RNN. In ICML'2014 .
          關鍵詞：
        - 摘要：Kočiský，T.，Hermann，K.  M.，and  Blunsom，P.（2014）.  Learning；In；Bilingual  Word  Representations  by  Marginalizing  Alignments.
          關鍵詞：
        - 摘要：Proceedings of ACL .
          關鍵詞：
        - 摘要：Krause，O.，Fischer，A.，Glasmachers，T.，and；Igel，C.（2013）.；Approximation properties of DBNs with binary hidden units and real-valued
          關鍵詞：
        - 摘要：Krizhevsky，A.（2010）.  Convolutional  deep  belief  networks  on  CIFAR-；10.  Technical  report，Uni-versity  of  Toronto.  Unpublished  Manuscript:；http://www.cs.utoronto.ca/kriz/conv-cifar10-aug2010.pdf.
          關鍵詞：
        - 摘要：Krizhevsky，A.  and  Hinton，G.（2009）.  Learning  multiple  layers  of；features from tiny images. Technical report，University of Toronto.
          關鍵詞：
        - 摘要：Krizhevsky，A. and Hinton，G. E.（2011）. Using very deep autoencoders；for content-based image retrieval. In ESANN .
          關鍵詞：
        - 摘要：Krizhevsky，A.，Sutskever，I.，and  Hinton，G.（2012a）.；classification with deep convo-lutional neural networks. In NIPS'2012 .
          關鍵詞：
        - 摘要：ImageNet
          關鍵詞：
        - 摘要：Krizhevsky，A.，Sutskever，I.，and  Hinton，G.（2012b）.；ImageNet；classification  with  deep  convolutional  neural  networks.  In  Advances  in
          關鍵詞：
        - 摘要：Krueger，K. A. and Dayan，P.（2009）. Flexible shaping: how learning in；small steps helps. Cognition ，110 ，380–394.
          關鍵詞：
        - 摘要：Kuhn，H.  W.  and  Tucker，A.  W.（1951）.  Nonlinear  programming.  In；Proceedings of the Sec-ond Berkeley Symposium on Mathematical Statistics；and  Probability  ，pages  481–492，Berkeley，Calif.  University  of
          關鍵詞：
        - 摘要：Kumar，A.，Irsoy，O.，Ondruska，P.，Iyyer，M.，Bradbury，J.，；Gulrajani，I.，and  Socher，R.（2015a）.  Ask  me  anything:  Dynamic；memory  networks  for  natural  language  processing.  Technical  report，
          關鍵詞：
        - 摘要：Kumar，A.，Irsoy，O.，Su，J.，Bradbury，J.，English，R.，Pierce，
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Socher，R.；B.，Ondruska，P.，Iyyer，M.，Gulrajani，I.，and
          關鍵詞：
        - 摘要：Kumar，M. P.，Packer，B.，and Koller，D.（2010）. Self-paced learning；for  latent  variable  models.  In  J.  Lafferty，C.  K.  I.  Williams，J.  Shawe-；Taylor，R.  Zemel，and  A.  Culotta，editors，Advances
          關鍵詞：
        - 摘要：Lang，K.  J.  and  Hinton，G.  E.（1988）.  The  development  of  the  time-；delay  neural  network  archi-tecture  for  speech  recognition.  Technical  Report；CMU-CS-88-152，Carnegie-Mellon University.
          關鍵詞：
        - 摘要：Lang，K.  J.，Waibel，A.  H.，and  Hinton，G.  E.（1990）.  A  time-delay；neural  network  architecture  for  isolated  word  recognition.  Neural  networks；，3 （1），23–43.
          關鍵詞：
        - 摘要：Langford，J.  and  Zhang，T.（2008）.  The  epoch-greedy  algorithm  for；contextual multi-armed bandits. In NIPS'2008 ，pages 1096–1103.
          關鍵詞：
        - 摘要：Lappalainen，H.，Giannakopoulos，X.，Honkela，A.，and Karhunen，J.；（2000）.  Nonlinear  independent  component  analysis  using  ensemble；learning: Experiments and discussion. In Proc. ICA. Citeseer.
          關鍵詞：
        - 摘要：Larochelle，H.；discriminative restricted Boltzmann machines. In ICML'2008 .
          關鍵詞：
        - 摘要：and  Bengio，Y.（2008a）.  Classification
          關鍵詞：
        - 摘要：using
          關鍵詞：
        - 摘要：Larochelle，H.；using；discriminative  restricted  Boltzmann  machines.  In  ICM（1a），pages  536–
          關鍵詞：
        - 摘要：and  Bengio，Y.（2008b）.  Classification
          關鍵詞：
        - 摘要：Larochelle，H.  and  Hinton，G.  E.（2010）.  Learning  to  combine  foveal；glimpses  with  a  third-order  Boltzmann  machine.  In  Advances  in  Neural；Information Processing Systems 23 ，pages 1243–1251.
          關鍵詞：
        - 摘要：Larochelle，H.  and  Murray，I.（2011）.  The  Neural  Autoregressive；Distribution Estimator. In AISTATS'2011 .
          關鍵詞：
        - 摘要：Larochelle，H.，Erhan，D.，and Bengio，Y.（2008）. Zero-data learning；of new tasks. In AAAI Conference on Artificial Intelligence .
          關鍵詞：
        - 摘要：Larochelle，H.，Bengio，Y.，Louradour，J.，and Lamblin，P.（2009）.；Exploring strategies for training deep neural networks. In JML（1），pages；1–40.
          關鍵詞：
        - 摘要：Lasserre，J.  A.，Bishop，C.  M.，and  Minka，T.  P.（2006）.  Principled；hybrids  of  generative  and  discriminative  models.  In  Proceedings  of  the；Computer Vision and Pattern Recognition Conference（CVPR'06） ，pages
          關鍵詞：
        - 摘要：Le，Q.，Ngiam，J.，Chen，Z.，hao  Chia，D.  J.，Koh，P.  W.，and；Ng，A.（2010）. Tiled convolutional neural networks. In J. Lafferty，C. K.；I.  Williams，J.  Shawe-Taylor，R.  Zemel，and  A.  Culotta，editors，
          關鍵詞：
        - 摘要：Le，Q.，Ngiam，J.，Coates，A.，Lahiri，A.，Prochnow，B.，and；Ng，A.（2011）.  On  optimization  methods  for  deep  learning.  In  Proc.；ICML'2011 . ACM.
          關鍵詞：
        - 摘要：Le，Q.，Ranzato，M.，Monga，R.，Devin，M.，Corrado，G.，Chen，；K.，Dean，J.，and  Ng，A.（2012）.  Building  high-level  features  using；large scale unsupervised learning. In ICML'2012 .
          關鍵詞：
        - 摘要：Le  Roux，N.  and  Bengio，Y.（2008）.  Representational  power  of；restricted  Boltzmann  machines  and  deep  belief  networks.  Neural；Computation ，20 （6），1631–1649.
          關鍵詞：
        - 摘要：Le Roux，N. and Bengio，Y.（2010）. Deep belief networks are compact；universal approximators. Neural Computation ，22 （8），2192–2207.
          關鍵詞：
        - 摘要：LeCun，Y.（1985）.  Une  procédure  d'apprentissage  pour  Réseau  à  seuil；assymétrique. In Cognitiva 85: A la Frontière de l'Intelligence Artificielle，；des  Sciences  de  la  Connaissance  et  des  Neurosciences ，pages  599–604，
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；LeCun，Y.（1986）.  Learning  processes  in  an  asymmetric  threshold；network.  In  E.  Bienenstock，F.  Fogelman-Soulié，and  G.  Weisbuch，
          關鍵詞：
        - 摘要：LeCun，Y.（1987）.  Modèles  connexionistes  de  l'apprentissage  .  Ph.D.；thesis，Université de Paris VI.
          關鍵詞：
        - 摘要：LeCun，Y.（1989）.  Generalization  and  network  design  strategies.；Technical Report CRG-TR-89-4，University of Toronto.
          關鍵詞：
        - 摘要：LeCun，Y.，Jackel，L.  D.，Boser，B.，Denker，J.  S.，Graf，H.  P.，；Guyon，I.，Henderson，D.，Howard，R.；Hubbard，W.
          關鍵詞：
        - 摘要：E.，and
          關鍵詞：
        - 摘要：LeCun，Y.，Bottou，L.，Orr，G.  B.，and  Müller，K.-R.（1998a）.；Efficient  backprop.  In  Neural  Networks，Tricks  of  the  Trade  ，Lecture；Notes in Computer Science LNCS 1524. Springer Verlag.
          關鍵詞：
        - 摘要：LeCun，Y.，Bottou，L.，Orr，G.；Efficient backprop. In Neural Networks，Tricks of the Trade .
          關鍵詞：
        - 摘要：B.，and  Müller，K.（1998b）.
          關鍵詞：
        - 摘要：LeCun，Y.，Bottou，L.，Bengio，Y.，and；Gradient based learning applied to document recognition. Proc. IEEE .
          關鍵詞：
        - 摘要：Haffner，P.（1998c）.
          關鍵詞：
        - 摘要：LeCun，Y.，Kavukcuoglu，K.，and  Farabet，C.（2010）.  Convolutional；networks  and  applications  in  vision.  In  Circuits  and  Systems（ISCAS），；Proceedings  of  2010  IEEE  International  Symposium  on  ，pages  253–256.
          關鍵詞：
        - 摘要：L'Ecuyer，P.（1994）.  Efficiency  improvement  and  variance  reduction.  In；Proceedings of the 1994 Winter Simulation Conference ，pages 122–132.
          關鍵詞：
        - 摘要：Lee，C.-Y.，Xie，S.，Gallagher，P.，Zhang，Z.，and  Tu，Z.（2014）.；Deeply-supervised nets. arXiv preprint arXiv:1409.5185 .
          關鍵詞：
        - 摘要：Lee，H.，Battle，A.，Raina，R.，and  Ng，A.（2007）.  Efficient  sparse；coding  algorithms.  In  B.  Schölkopf，J.  Platt，and  T.  Hoffman，editors，；Advances in Neural Information Processing Systems 19（NIPS'06） ，pages
          關鍵詞：
        - 摘要：Lee，H.，Ekanadham，C.，and  Ng，A.（2008）.  Sparse  deep  belief  net；model for visual area V2. In NIPS'07 .
          關鍵詞：
        - 摘要：Y.（2009）.；Lee，H.，Grosse，R.，Ranganath，R.，and；Convolutional  deep  belief  net-works  for  scalable  unsupervised  learning  of
          關鍵詞：
        - 摘要：Ng，A.
          關鍵詞：
        - 摘要：Lee，Y.  J.  and  Grauman，K.（2011）.  Learning  the  easy  thingsfirst:  self-；paced visual category discovery. In CVPR'2011 .
          關鍵詞：
        - 摘要：Leibniz，G.  W.（1676）.  Memoir  using  the  chain  rule.（Cited  in  TMME；7:2&3 p 321-332，2010）.
          關鍵詞：
        - 摘要：Lenat，D.  B.  and  Guha，R.  V.（1989）.  Building  large  knowledge-based；systems；representation and inference in the Cyc project . Addison-Wesley；Longman Publishing Co.，Inc.
          關鍵詞：
        - 摘要：Leshno，M.，Lin，V.  Y.，Pinkus，A.，and  Schocken，S.（1993）.；Multilayer  feedforward  networks  with  a  nonpolynomial  activation  function；can approximate any function. Neural Networks ，6 ，861–867.
          關鍵詞：
        - 摘要：Levenberg，K.（1944）.  A  method  for  the  solution  of  certain  non-linear；problems  in  least  squares.  Quarterly  Journal  of  Applied  Mathematics  ，II；（2），164–168.
          關鍵詞：
        - 摘要：L'Hôpital，G.  F.  A.（1696）.  Analyse  des；l'intelligence des lignes courbes . Paris: L'Imprimerie Royale.
          關鍵詞：
        - 摘要：infiniment  petits，pour
          關鍵詞：
        - 摘要：Li，Y.，Swersky，K.，and  Zemel，R.  S.（2015）.  Generative  moment；matching networks. CoRR ，abs/1502.02761 .
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Lin，T.，Horne，B.  G.，Tino，P.，and  Giles，C.  L.（1996）.  Learning；long-term  dependencies  is  not  as  difficult  with  NARX  recurrent  neural
          關鍵詞：
        - 摘要：Lin，Y.，Liu，Z.，Sun，M.，Liu，Y.，and Zhu，X.（2015）. Learning；entity  and  relation  embeddings  for  knowledge  graph  completion.  In  Proc.；AAAI'15 .
          關鍵詞：
        - 摘要：Linde，N.（1992）.  The  machine  that  changed  the  world，episode  3.；Documentary miniseries.
          關鍵詞：
        - 摘要：Lindsey，C.  and  Lindblad，T.（1994）.  Review  of  hardware  neural；networks: a user's perspective. In Proc. Third Workshop on Neural Networks:；From  Biology  to  High  Energy  Physics  ，pages  195–202，Isola  d'Elba，
          關鍵詞：
        - 摘要：Linnainmaa，S.（1976）.  Taylor  expansion  of  the  accumulated  rounding；error. BIT Numerical Mathematics ，16 （2），146–160.
          關鍵詞：
        - 摘要：LISA（2008）.  Deep  learning  tutorials:Restricted  Boltzmann  machines.；Technical report，LISA Lab，Université de Montréal.
          關鍵詞：
        - 摘要：Long，P. M. and Servedio，R. A.（2010）. Restricted Boltzmann machines；are  hard  to  approximately  evaluate  or  simulate.  In  Proceedings  of  the  27th；International Conference on Machine Learning（ICML'10） .
          關鍵詞：
        - 摘要：Lotter，W.，Kreiman，G.，and Cox，D.（2015）. Unsupervised learning；of  visual  structure  using  predictive  generative  networks.  arXiv  preprint；arXiv:1511.06380 .
          關鍵詞：
        - 摘要：Lovelace，A.（1842）.  Notes  upon  L.  F.  Menabrea's“Sketch  of  the；Analytical Engine invented by Charles Babbage”.
          關鍵詞：
        - 摘要：Lu，L.，Zhang，X.，Cho，K.，and  Renals，S.（2015）.  A  study  of  the；recurrent  neural  network  encoder-decoder  for  large  vocabulary  speech；recognition. In Proc. Interspeech .
          關鍵詞：
        - 摘要：Lu，T.，Pál，D.，and Pál，M.（2010）. Contextual multi-armed bandits.；In International Conference  on Artificial Intelligence and Statistics ，pages；485–492.
          關鍵詞：
        - 摘要：Luenberger，D. G.（1984）. Linear and Nonlinear Programming . Addison；Wesley.
          關鍵詞：
        - 摘要：Lukoševičius，M.；computing；approaches  to  recurrent  neural  network  training.  Computer  Science  Review
          關鍵詞：
        - 摘要：Jaeger，H.（2009）.  Reservoir
          關鍵詞：
        - 摘要：and
          關鍵詞：
        - 摘要：Luo，H.，Shen，R.，Niu，C.，and Ullrich，C.（2011）. Learning class-；relevant features and class-irrelevant features via a hybrid third-order RBM.；In International  Conference on Artificial Intelligence and Statistics ，pages
          關鍵詞：
        - 摘要：Luo，H.，Carrier，P.  L.，Courville，A.，and  Bengio，Y.（2013）.；Texture  modeling  with  convolutional  spike-and-slab  RBMs  and  deep；extensions. In AISTATS'2013 .
          關鍵詞：
        - 摘要：Lyu，S.（2009）.  Interpretation  and  generalization  of  score  matching.  In；Proceedings  of  the  Twenty-fifth  Conference  in  Uncertainty  in  Artificial；Intelligence（UAI'09） .
          關鍵詞：
        - 摘要：Ma，J.，Sheridan，R.  P.，Liaw，A.，Dahl，G.  E.，and  Svetnik，V.；（2015）.  Deep  neural  nets  as  a  method  for  quantitative  structure–activity；relationships. J. Chemical information and modeling .
          關鍵詞：
        - 摘要：Maas，A.  L.，Hannun，A.  Y.，and  Ng，A.  Y.（2013）.  Rectifier；nonlinearities  improve  neural  network  acoustic  models.  In  ICML  Workshop；on Deep Learning for Audio，Speech，and Language Processing .
          關鍵詞：
        - 摘要：Maass，W.（1992）.  Bounds  for  the  computational  power  and  learning；complexity of analog neural nets（extended abstract）. In Proc. of the 25th；ACM Symp. Theory of Computing ，pages 335–344.
          關鍵詞：
        - 摘要：Maass，W.，Schnitger，G.，and  Sontag，E.  D.（1994）.  A  comparison
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；of  the  computational  power  of  sigmoid  and  Boolean  threshold  circuits.；Theoretical  Advances  in  Neural  Computation  and  Learning  ，pages  127–
          關鍵詞：
        - 摘要：Maass，W.，Natschlaeger，T.，and  Markram，H.（2002）.  Real-time；computing  without  stable  states:  A  new  framework  for  neural  computation；based on perturbations. Neural Computation ，14 （11），2531–2560.
          關鍵詞：
        - 摘要：MacKay，D.（2003）.；Algorithms . Cambridge University Press.
          關鍵詞：
        - 摘要：Information  Theory，Inference  and  Learning
          關鍵詞：
        - 摘要：Maclaurin，D.，Duvenaud，D.，and  Adams，R.  P.（2015）.  Gradient-；based  hyperparameter  optimization  through  reversible  learning.  arXiv；preprint arXiv:1502.03492 .
          關鍵詞：
        - 摘要：Mao，J.，Xu，W.，Yang，Y.，Wang，J.，and；Yuille，A.（2014）.；Deep  captioning  with  multimodal  recurrent  neural  networks（m-rnn）.
          關鍵詞：
        - 摘要：Marcotte，P.  and  Savard，G.（1992）.  Novel  approaches；the；discrimination problem. Zeitschrift für Operations Research（Theory） ，36
          關鍵詞：
        - 摘要：to
          關鍵詞：
        - 摘要：Marlin，B.  and  de  Freitas，N.（2011）.  Asymptotic  efficiency  of；deterministic  estimators  for  discrete  energy-based  models:  Ratio  matching；and pseudolikelihood. In UAI'2011 .
          關鍵詞：
        - 摘要：Marlin，B.，Swersky，K.，Chen，B.，and；Inductive  principles；AISTATS'2010 ，pages 509–516.
          關鍵詞：
        - 摘要：for
          關鍵詞：
        - 摘要：restricted  Boltzmann  machine
          關鍵詞：
        - 摘要：de
          關鍵詞：
        - 摘要：Freitas，N.（2010）.；In
          關鍵詞：
        - 摘要：learning.
          關鍵詞：
        - 摘要：Marquardt，D.  W.（1963）.  An  algorithm  for  least-squares  estimation  of；non-linear  parameters.  Journal  of  the  Society  of  Industrial  and  Applied；Mathematics ，11 （2），431–441.
          關鍵詞：
        - 摘要：Marr，D.  and  Poggio，T.（1976）.  Cooperative  computation  of  stereo；disparity. Science ，194 .
          關鍵詞：
        - 摘要：Martens，J.（2010）.  Deep  learning  via  Hessian-free  optimization.  In；ICML'2010 ，pages 735–742.
          關鍵詞：
        - 摘要：Martens，J. and Medabalimi，V.（2014）. On the expressive efficiency of；sum product networks. arXiv:1411.7717 .
          關鍵詞：
        - 摘要：Martens，J. and Sutskever，I.（2011）. Learning recurrent neural networks；with Hessian-free optimization. In Proc. ICML'2011 . ACM.
          關鍵詞：
        - 摘要：Mase，S.（1995）.  Consistency  of；the  maximum  pseudo-likelihood；estimator  of  continuous  state  space  Gibbsian  processes.  The  Annals  of
          關鍵詞：
        - 摘要：McClelland，J.，Rumelhart，D.，and Hinton，G.（1995）. The appeal of；parallel distributed processing. In Computation & intelligence ，pages  305–；341. American Association for Artificial Intelligence.
          關鍵詞：
        - 摘要：McCulloch，W.  S.  and  Pitts，W.（1943）.  A  logical  calculus  of  ideas；immanent  in  nervous  activity.  Bulletin  of  Mathematical  Biophysics  ，5  ，；115–133.
          關鍵詞：
        - 摘要：Mead，C. and Ismail，M.（2012）. Analog VLSI implementation of neural；systems ，volume 80. Springer Science & Business Media.
          關鍵詞：
        - 摘要：Melchior，J.，Fischer，A.，and  Wiskott，L.（2013）.  How  to  center；binary deep Boltzmann machines. arXiv preprint arXiv:1311.1354 .
          關鍵詞：
        - 摘要：Memisevic，R.  and  Hinton，G.  E.（2007）.  Unsupervised  learning  of；image  transformations.  In  Proceedings  of  the  Computer  Vision  and  Pattern；Recognition Conference（CVPR'07） .
          關鍵詞：
        - 摘要：Memisevic，R.  and  Hinton，G.  E.（2010）.  Learning  to  represent  spatial；transformations  with  factored  higher-order  Boltzmann  machines.  Neural；Computation ，22 （6），1473–1492.
          關鍵詞：
        - 摘要：Mesnil，G.，Dauphin，Y.，Glorot，X.，Rifai，S.，Bengio，Y.，；Goodfellow，I.，Lavoie，E.，Muller，X.，Desjardins，G.，Warde-
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Bergstra，J.（2011）.；Farley，D.，Vincent，P.，Courville，A.，and
          關鍵詞：
        - 摘要：Mesnil，G.，Rifai，S.，Dauphin，Y.，Bengio，Y.，and；（2012）. Surfing on the manifold. Learning Workshop，Snowbird.
          關鍵詞：
        - 摘要：Vincent，P.
          關鍵詞：
        - 摘要：Miikkulainen，R.  and  Dyer，M.  G.（1991）.  Natural  language  processing；with modular PDP networks and distributed lexicon. Cognitive Science ，15；，343–399.
          關鍵詞：
        - 摘要：Mikolov，T.（2012）.  Statistical  Language  Models  based  on  Neural；Networks . Ph.D. thesis，Brno University of Technology.
          關鍵詞：
        - 摘要：Mikolov，T.，Deoras，A.，Kombrink，S.，Burget，L.，and Cernocky，；J.（2011a）.  Empirical  evaluation  and  combination  of  advanced  language；modeling  techniques.  In  Proc.  12th  annual  conference  of  the  international
          關鍵詞：
        - 摘要：Mikolov，T.，Deoras，A.，Povey，D.，Burget，L.，and  Cernocky，J.；（2011b）.  Strategies  for  training  large  scale  neural  network  language；models. In Proc. ASRU'2011 .
          關鍵詞：
        - 摘要：Dean，J.（2013a）.；Mikolov，T.，Chen，K.，Corrado，G.，and；Efficient estimation of word representations in vector space. In International
          關鍵詞：
        - 摘要：Mikolov，T.，Le，Q.  V.，and  Sutskever，I.（2013b）.  Exploiting；similarities  among  languages  for  machine  translation.  Technical  report，；arXiv:1309.4168.
          關鍵詞：
        - 摘要：Minka，T.（2005）.  Divergence  measures  and  message  passing.  Microsoft；Research Cambridge UK Tech Rep MSRTR2005173 ，72 （TR-2005-173）.
          關鍵詞：
        - 摘要：Minsky，M.  L.  and  Papert，S.  A.（1969）.  Perceptrons  .  MIT  Press，；Cambridge.
          關鍵詞：
        - 摘要：Mirza，M.  and  Osindero，S.（2014）.  Conditional  generative  adversarial；nets. arXiv preprint arXiv:1411.1784 .
          關鍵詞：
        - 摘要：Mishkin，D.  and  Matas，J.（2015）.  All  you  need  is  a  good  init.  arXiv；preprint arXiv:1511.06422 .
          關鍵詞：
        - 摘要：Misra，J. and Saha，I.（2010）. Artificial neural networks in hardware: A；survey of two decades of progress. Neurocomputing ，74 （1），239–255.
          關鍵詞：
        - 摘要：Mitchell，T. M.（1997）. Machine Learning . McGraw-Hill，New York.
          關鍵詞：
        - 摘要：Ishii，S.；Miyato，T.，Maeda，S.，Koyama，M.，Nakae，K.，and；（2015）. Distributional smoothing with virtual adversarial training. In ICLR
          關鍵詞：
        - 摘要：Mnih，A.  and  Gregor，K.（2014）.  Neural  variational  inference  and；learning in belief networks. In ICML'2014 .
          關鍵詞：
        - 摘要：Mnih，A.  and  Hinton，G.  E.（2007）.  Three  new  graphical  models  for；statistical  language  mod-elling.  In  Z.  Ghahramani，editor，Proceedings  of；Machine
          關鍵詞：
        - 摘要：Twenty-fourth
          關鍵詞：
        - 摘要：International
          關鍵詞：
        - 摘要：Conference
          關鍵詞：
        - 摘要：on
          關鍵詞：
        - 摘要：Mnih，A.  and  Hinton，G.  E.（2009）.  A  scalable  hierarchical  distributed；language  model.  In  D.  Koller，D.  Schuurmans，Y.  Bengio，and  L.；Bottou，editors，Advances  in  Neural  Information  Processing  Systems
          關鍵詞：
        - 摘要：Mnih，A.  and  Kavukcuoglu，K.（2013）.  Learning  word  embeddings；efficiently  with  noise-contrastive  estimation.  In  C.  Burges，L.  Bottou，M.；Welling，Z.  Ghahramani，and  K.  Weinberger，editors，Advances
          關鍵詞：
        - 摘要：Mnih，A.  and  Teh，Y.  W.（2012）.  A  fast  and  simple  algorithm  for；training neural probabilistic language models. In ICML'2012 ，pages  1751–；1758.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Mnih，V.  and  Hinton，G.（2010）.  Learning  to  detect  roads  in  high-；resolution aerial images. In Proceedings of the 11th European Conference on
          關鍵詞：
        - 摘要：Mnih，V.，Larochelle，H.，and；Conditional；restricted Boltzmann machines for structure output prediction. In Proc. Conf.
          關鍵詞：
        - 摘要：Hinton，G.（2011）.
          關鍵詞：
        - 摘要：Mnih，V.，Kavukcuoglo，K.，Silver，D.，Graves，A.，Antonoglou，；I.，and  Wierstra，D.（2013）.  Playing  Atari  with  deep  reinforcement；learning. Technical report，arXiv:1312.5602.
          關鍵詞：
        - 摘要：Mnih，V.，Heess，N.，Graves，A.，and；Kavukcuoglu，K.（2014）.；Recurrent  models  of  visual  attention.  In  Z.  Ghahramani，M.  Welling，C.
          關鍵詞：
        - 摘要：Mnih，V.，Kavukcuoglo，K.，Silver，D.，Rusu，A.  A.，Veness，J.，；Bellemare，M.  G.，Graves，A.，Riedmiller，M.，Fidgeland，A.  K.，；Ostrovski，G.，Petersen，S.，Beattie，C.，Sadik，A.，Antonoglou，
          關鍵詞：
        - 摘要：Mobahi，H.  and  Fisher，III，J.  W.（2015）.  A  theoretical  analysis  of；optimization by Gaussian continuation. In AAAI'2015 .
          關鍵詞：
        - 摘要：Mobahi，H.，Collobert，R.，and  Weston，J.（2009）.  Deep；learning；from  temporal  coherence  in  video.  In  L.  Bottou  and  M.  Littman，
          關鍵詞：
        - 摘要：Mohamed，A.，Dahl，G.，and  Hinton，G.（2009）.  Deep；networks for phone recognition.
          關鍵詞：
        - 摘要：belief
          關鍵詞：
        - 摘要：Mohamed，A.，Sainath，T.；N.，Dahl，G.，Ramabhadran，B.，；Hinton，G. E.，and Picheny，M. A.（2011）. Deep belief networks using
          關鍵詞：
        - 摘要：discriminative  features  for  phone  recognition.  In  Acoustics，Speech  and；Signal Processing（ICASSP），2011 IEEE International Conference on ，；pages 5060–5063. IEEE.
          關鍵詞：
        - 摘要：Mohamed，A.，Dahl，G.，and Hinton，G.（2012a）. Acoustic modeling；using  deep  belief  networks.  IEEE  Trans.  on  Audio，Speech  and  Language；Processing ，20 （1），14–22.
          關鍵詞：
        - 摘要：Mohamed，A.，Hinton，G.，and  Penn，G.（2012b）.  Understanding；how deep belief networks perform acoustic modelling. In Acoustics，Speech；and  Signal  Processing（ICASSP），2012  IEEE  International  Conference
          關鍵詞：
        - 摘要：Moller，M.（1993）. Efficient Training of Feed-Forward Neural Networks；. Ph.D. thesis，Aarhus University，Aarhus，Denmark.
          關鍵詞：
        - 摘要：Montavon，G. and Muller，K.-R.（2012）. Deep Boltzmann machines and；the centering trick. In G. Montavon，G. Orr，and K.-R. Müller，editors，；Neural  Networks:  Tricks  of  the  Trade，volume  7700  of  Lecture  Notes  in
          關鍵詞：
        - 摘要：621–637.
          關鍵詞：
        - 摘要：Science
          關鍵詞：
        - 摘要：Montúfar，G.（2014）. Universal approximation depth and errors of narrow；belief networks with discrete units. Neural Computation ，26 .
          關鍵詞：
        - 摘要：Montúfar，G.；and  Ay，N.（2011）.  Refinements；universal
          關鍵詞：
        - 摘要：of
          關鍵詞：
        - 摘要：Montufar，G. F.，Pascanu，R.，Cho，K.，and Bengio，Y.（2014）. On；the number of linear regions of deep neural networks. In NIPS'2014 .
          關鍵詞：
        - 摘要：Mor-Yosef，S.，Samueloff，A.，Modan，B.，Navot，D.，and；Schenker，J.  G.（1990）.  Ranking  the  risk  factors  for  cesarean:  logistic；regression analysis of a nationwide study. Obstet Gynecol ，75 （6），944–
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Morin，F.  and  Bengio，Y.（2005）.  Hierarchical  probabilistic  neural；network language model. In AISTATS'2005 .
          關鍵詞：
        - 摘要：Mozer，M.  C.（1992）.  The  induction  of  multiscale  temporal  structure.  In；J.  M.  S.  Hanson  and  R.  Lippmann，editors，Advances；in  Neural
          關鍵詞：
        - 摘要：Murphy，K.  P.（2012）.  Machine  Learning:  a  Probabilistic  Perspective  .；MIT Press，Cambridge，MA，USA.
          關鍵詞：
        - 摘要：Murray，B. U. I. and Larochelle，H.（2014）. A deep and tractable density；estimator. In ICML'2014 .
          關鍵詞：
        - 摘要：Nair，V.  and  Hinton，G.（2010a）.  Rectified；restricted Boltzmann machines. In ICML'2010 .
          關鍵詞：
        - 摘要：linear  units
          關鍵詞：
        - 摘要：improve
          關鍵詞：
        - 摘要：Nair，V.  and  Hinton，G.  E.（2009）.  3d  object  recognition  with  deep；belief  nets.  In  Y.  Bengio，D.  Schuurmans，J.  D.  Lafferty，C.  K.  I.；Williams，and  A.  Culotta，editors，Advances
          關鍵詞：
        - 摘要：Nair，V.  and  Hinton，G.  E.（2010b）.  Rectified  linear  units  improve；restricted  Boltzmann  machines.  In  L.  Bottou  and  M.  Littman，editors，；Proceedings  of  the  Twenty-seventh  International  Conference  on  Machine
          關鍵詞：
        - 摘要：Narayanan，H.  and  Mitter，S.（2010）.  Sample  complexity  of  testing  the；manifold  hypothesis.  In  J.  Lafferty，C.  K.  I.  Williams，J.  Shawe-Taylor，；R.  Zemel，and  A.  Culotta，editors，Advances  in  Neural  Information
          關鍵詞：
        - 摘要：Naumann，U.（2008）.  Optimal  Jacobian  accumulation  is  NP-complete.；Mathematical Programming ，112 （2），427–441.
          關鍵詞：
        - 摘要：Navigli，R. and Velardi，P.（2005）. Structural semantic interconnections:；a  knowledge-based  approach  to  word  sense  disambiguation.  IEEE  Trans.
          關鍵詞：
        - 摘要：Pattern Analysis and Machine Intelligence ，27 （7），1075–1086.
          關鍵詞：
        - 摘要：Neal，R.  and  Hinton，G.（1999）.  A  view  of  the  EM  algorithm  that；incremental，sparse，and  other  variants.  In  M.  I.  Jordan，；justifies
          關鍵詞：
        - 摘要：Neal，R. M.（1990）. Learning stochastic feedforward networks. Technical；report.
          關鍵詞：
        - 摘要：Neal，R.  M.（1993）.  Probabilistic  inference  using  Markov  chain  Monte-；Carlo  methods.  Technical  Report  CRG-TR-93-1，Dept.  of  Computer；Science，University of Toronto.
          關鍵詞：
        - 摘要：Neal，R.  M.（1994）.  Sampling  from  multimodal  distributions  using；tempered transitions. Technical Report 9421，Dept. of Statistics，University；of Toronto.
          關鍵詞：
        - 摘要：Neal，R.  M.（1996）.  Bayesian  Learning  for  Neural  Networks  .  Lecture；Notes in Statistics. Springer.
          關鍵詞：
        - 摘要：Neal，R.  M.（2001）.  Annealed  importance  sampling.  Statistics  and；Computing ，11 （2），125–139.
          關鍵詞：
        - 摘要：Neal，R.  M.（2005）.  Estimating  ratios  of  normalizing  constants  using；linked importance sampling.
          關鍵詞：
        - 摘要：Nesterov，Y.（1983）.  A  method  of  solving  a  convex  programming；problem  with  convergence  rate  O（1/k  2  ）.  Soviet  Mathematics  Doklady；，27 ，372–376.
          關鍵詞：
        - 摘要：Nesterov，Y.（2004）.  Introductory  lectures  on  convex  optimization:  a；basic  course  .  Applied  optimization.  Kluwer  Academic  Publ.，Boston，；Dordrecht，London.
          關鍵詞：
        - 摘要：Netzer，Y.，Wang，T.，Coates，A.，Bissacco，A.，Wu，B.，and；Ng，A.  Y.（2011）.  Reading  digits  in  natural  images  with  unsupervised；feature  learning.  Deep  Learning  and  Unsupervised  Feature  Learning
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Workshop，NIPS.
          關鍵詞：
        - 摘要：Ney，H.  and  Kneser，R.（1993）.  Improved  clustering  techniques  for；class-based  statistical  language  modelling.  In  European  Conference  on；Speech Communication and Technology（Eurospeech） ，pages 973–976，
          關鍵詞：
        - 摘要：Ng，A.（2015）.；https://see.stanford.edu/materials/aimlcs229/ML-advice.pdf.
          關鍵詞：
        - 摘要：applying
          關鍵詞：
        - 摘要：Advice
          關鍵詞：
        - 摘要：for
          關鍵詞：
        - 摘要：machine
          關鍵詞：
        - 摘要：learning.
          關鍵詞：
        - 摘要：Niesler，T.  R.，Whittaker，E.  W.  D.，and  Woodland，P.  C.（1998）.；Comparison  of  part-of-speech  and  automatically  derived  category-based；language  models  for  speech  recognition.  In  International  Conference  on
          關鍵詞：
        - 摘要：Ning，F.，Delhomme，D.，LeCun，Y.，Piano，F.，Bottou，L.，and；Barbano，P.  E.（2005）.  To-ward  automatic  phenotyping  of  developing；embryos  from  videos.  Image  Processing，IEEE  Transactions  on  ，14
          關鍵詞：
        - 摘要：Nocedal，J. and Wright，S.（2006）. Numerical Optimization . Springer.
          關鍵詞：
        - 摘要：Norouzi，M. and Fleet，D.  J.（2011）. Minimal loss hashing for compact；binary codes. In ICML'2011 .
          關鍵詞：
        - 摘要：Nowlan，S.  J.（1990）.  Competing experts: An experimental investigation；of associative mixture models. Technical Report CRG-TR-90-5，University；of Toronto.
          關鍵詞：
        - 摘要：Nowlan，S.  J.  and  Hinton，G.  E.（1992）.  Adaptive  soft  weight  tying；using  Gaussian  mixtures.  In  J.  M.  S.  Hanson  and  R.  Lippmann，editors，；Advances in Neural Information Processing Systems 4（NIPS'91） ，pages
          關鍵詞：
        - 摘要：Olshausen，B.  and  Field，D.  J.（2005）.  How  close  are  we；understanding V1? Neural Computation ，17 ，1665–1699.
          關鍵詞：
        - 摘要：to
          關鍵詞：
        - 摘要：Olshausen，B.  A.  and  Field，D.  J.（1996）.  Emergence  of  simple-cell；receptivefield properties by learning a sparse code for natural images. Nature；，381 ，607–609.
          關鍵詞：
        - 摘要：Olshausen，B. A.，Anderson，C. H.，and Van Essen，D. C.（1993）. A；neurobiological  model  of  visual  attention  and  invariant  pattern  recognition；based on dynamic routing of information. J. Neurosci. ，13 （11），4700–
          關鍵詞：
        - 摘要：Opper，M.  and  Archambeau，C.（2009）.  The  variational  Gaussian；approximation revisited. Neural computation ，21 （3），786–792.
          關鍵詞：
        - 摘要：Oquab，M.，Bottou，L.，Laptev，I.，and  Sivic，J.（2014）.  Learning；and  transferring  mid-level  image  representations  using  convolutional  neural；networks.  In  Computer  Vision  and  Pattern  Recognition（CVPR），2014
          關鍵詞：
        - 摘要：Osindero，S.  and  Hinton，G.  E.（2008）.  Modeling  image  patches  with  a；directed  hierarchy  of  Markov  randomfields.  In  J.  Platt，D.  Koller，Y.；Singer，and  S.  Roweis，editors，Advances
          關鍵詞：
        - 摘要：in  Neural
          關鍵詞：
        - 摘要：Ovid and Martin，C.（2004）. Metamorphoses . W.W. Norton.
          關鍵詞：
        - 摘要：Paccanaro，A.  and  Hinton，G.  E.（2000）.  Extracting  distributed；representations  of  concepts  and  relations  from  positive  and  negative；propositions.
          關鍵詞：
        - 摘要：Conference
          關鍵詞：
        - 摘要：Joint
          關鍵詞：
        - 摘要：on
          關鍵詞：
        - 摘要：In
          關鍵詞：
        - 摘要：Paine，T.  L.，Khorrami，P.，Han，W.，and  Huang，T.  S.（2014）.  An；analysis  of  unsupervised  pre-training  in  light  of  recent  advances.  arXiv；preprint arXiv:1412.6597 .
          關鍵詞：
        - 摘要：Palatucci，M.，Pomerleau，D.，Hinton，G.  E.，and  Mitchell，T.  M.；（2009）. Zero-shot learning with semantic output codes. In Y. Bengio，D.；Schuurmans，J.  D.  Lafferty，C.  K.  I.  Williams，and  A.  Culotta，
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；editors，Advances  in  Neural  Information  Processing  Systems  22  ，pages；1410–1418. Curran Associates，Inc.
          關鍵詞：
        - 摘要：Parker，D.  B.（1985）.  Learning-logic.  Technical  Report  TR-47，Center；for Comp. Research in Economics and Management Sci.，MIT.
          關鍵詞：
        - 摘要：Pascanu，R.，Mikolov，T.，and  Bengio，Y.（2013a）.  On  the  difficulty；of training recurrent neural networks. In ICML'2013 .
          關鍵詞：
        - 摘要：Pascanu，R.，Mikolov，T.，and  Bengio，Y.（2013b）.  On  the  difficulty；of training recurrent neural networks. In ICM（1c）.
          關鍵詞：
        - 摘要：Pascanu，R.，Gulcehre，C.，Cho，K.，and  Bengio，Y.（2014a）.  How；to construct deep recurrent neural networks. In ICLR .
          關鍵詞：
        - 摘要：Pascanu，R.，Montufar，G.，and  Bengio，Y.（2014b）.  On  the  number；of  inference  regions  of  deep  feed  forward  networks  with  piece-wise  linear；activations. In ICL（1）.
          關鍵詞：
        - 摘要：Pati，Y.，Rezaiifar，R.，and  Krishnaprasad，P.（1993）.  Orthogonal；matching  pursuit:Recursive  function  approximation  with  applications  to；wavelet  decomposition.  In  Proceedings  of  the  27  th  Annual  Asilomar
          關鍵詞：
        - 摘要：Pearl，J.（1985）.  Bayesian  networks:  A  model  of  self-activated  memory；for  evidential  reasoning.  In  Proceedings  of  the  7th  Conference  of  the；Cognitive  Science  Society，University  of  California，Irvine  ，pages  329–
          關鍵詞：
        - 摘要：Pearl，J.（1988）. Probabilistic Reasoning in Intelligent Systems: Networks；of Plausible Inference . Morgan Kaufmann.
          關鍵詞：
        - 摘要：Perron，O.（1907）.  Zur  theorie  der  matrices.  Mathematische  Annalen；，64 （2），248–263.
          關鍵詞：
        - 摘要：Petersen，K.  B.  and  Pedersen，M.  S.（2006）.  The  matrix  cookbook.；Version 20051003.
          關鍵詞：
        - 摘要：Peterson，G.  B.（2004）.  A  day  of  great  illumination:  B.  F.  Skinner's；discovery of shaping. Journal of the Experimental Analysis of Behavior ，82；（3），317–328.
          關鍵詞：
        - 摘要：Pham，D.-T.，Garat，P.，and；Jutten，C.（1992）.  Separation  of  a；mixture of independent sources through a maximum likelihood approach. In
          關鍵詞：
        - 摘要：Pham，P.-H.，Jelaca，D.，Farabet，C.，Martini，B.，LeCun，Y.，and；Culurciello，E.（2012）. Neu-Flow: dataflow vision processing system-on-；a-chip.
          關鍵詞：
        - 摘要：In  Circuits  and  Systems（MWSCAS），2012
          關鍵詞：
        - 摘要：Pinheiro，P.  H.  O.  and  Collobert，R.（2014）.  Recurrent  convolutional；neural networks for scene labeling. In ICML'2014 .
          關鍵詞：
        - 摘要：Pinheiro，P. H. O. and Collobert，R.（2015）. From image-level to pixel-；level  labeling  with  con-volutional  networks.  In  Conference  on  Computer；Vision and Pattern Recognition（CVPR） .
          關鍵詞：
        - 摘要：Pinto，N.，Cox，D.  D.，and  DiCarlo，J.  J.（2008）.  Why  is  real-world；visual object recognition hard? PLoS Comput Biol ，4 .
          關鍵詞：
        - 摘要：Pinto，N.，Stone，Z.，Zickler，T.，and  Cox，D.（2011）.  Scaling  up；biologically-inspired  computer  vision:  A  case  study  in  unconstrained  face；recognition  on  facebook.  In  Computer  Vision  and  Pattern  Recognition
          關鍵詞：
        - 摘要：Pollack，J.  B.（1990）.  Recursive  distributed  representations.  Artificial；Intelligence ，46 （1），77–105.
          關鍵詞：
        - 摘要：Polyak，B.  and；stochastic；approximation  by  averaging.  SIAM  J.  Control  and  Optimization  ，30
          關鍵詞：
        - 摘要：Juditsky，A.（1992）.  Acceleration  of
          關鍵詞：
        - 摘要：Polyak，B.  T.（1964）.  Some  methods  of  speeding  up  the  convergence  of
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；iteration  methods.  USSR  Computational  Mathematics  and  Mathematical；Physics ，4 （5），1–17.
          關鍵詞：
        - 摘要：Poole，B.，Sohl-Dickstein，J.，and  Ganguli，S.（2014）.  Analyzing；noise in autoencoders and deep networks. CoRR ，abs/1406.1831 .
          關鍵詞：
        - 摘要：Poon，H.  and  Domingos，P.（2011）.  Sum-product  networks  for  deep；learning. In Learning Workshop ，Fort Lauderdale，FL.
          關鍵詞：
        - 摘要：Presley，R. K. and Haggard，R. L.（1994）. Afixed point implementation；of  the  backpropaga-tion  learning  algorithm.  In  Southeastcon  '94.  Creative；Technology  Transfer-A  Global  Affair.，Proceedings  of  the  1994  IEEE  ，
          關鍵詞：
        - 摘要：Price，R.（1958）. A useful theorem for nonlinear devices having Gaussian；inputs. IEEE Transactions on Information Theory ，4 （2），69–72.
          關鍵詞：
        - 摘要：Quiroga，R.  Q.，Reddy，L.，Kreiman，G.，Koch，C.，and  Fried，I.；（2005）.  Invariant  visual  representation  by  single  neurons  in  the  human；brain. Nature ，435 （7045），1102–1107.
          關鍵詞：
        - 摘要：Radford，A.，Metz，L.，and；Unsupervised；representation  learning  with  deep  convolutional  generative  adversarial
          關鍵詞：
        - 摘要：Chintala，S.（2015）.
          關鍵詞：
        - 摘要：Raiko，T.，Yao，L.，Cho，K.，and  Bengio，Y.（2014）.；Iterative；neural autoregressive distribution estimator（NADE-k）. Technical report，
          關鍵詞：
        - 摘要：Raina，R.，Madhavan，A.，and  Ng，A.  Y.（2009a）.  Large-scale  deep；unsupervised  learning  using  graphics  processors.  In  L.  Bottou  and  M.；Littman，editors，Proceedings of the Twenty-sixth International Conference
          關鍵詞：
        - 摘要：Raina，R.，Madhavan，A.，and  Ng，A.  Y.（2009b）.  Large-scale  deep；unsupervised learning using graphics processors. In ICML'2009 .
          關鍵詞：
        - 摘要：Ramsey，F.  P.（1926）.  Truth  and  probability.  In  R.  B.  Braithwaite，；editor，The Foundations of Mathematics and other Logical Essays ，chapter；7，pages  156–198.  McMaster  University  Archive  for  the  History  of
          關鍵詞：
        - 摘要：Ranzato，M.  and  Hinton，G.  H.（2010）.  Modeling  pixel  means  and；covariances using factorized third-order Boltzmann machines. In CVPR'2010；，pages 2551–2558.
          關鍵詞：
        - 摘要：Ranzato，M.，Poultney，C.，Chopra，S.，and；LeCun，Y.（2007a）.；Efficient  learning  of  sparse  representations  with  an  energy-based  model.  In
          關鍵詞：
        - 摘要：Ranzato，M.，Poultney，C.，Chopra，S.，and；LeCun，Y.（2007b）.；Efficient learning of sparse representations with an energy-based model. In B.
          關鍵詞：
        - 摘要：Ranzato，M.，Huang，F.，Boureau，Y.，and；LeCun，Y.（2007c）.；Unsupervised  learning  of  invariant  feature  hierarchies  with  applications  to
          關鍵詞：
        - 摘要：Ranzato，M.，Boureau，Y.，and  LeCun，Y.（2008）.  Sparse  feature；learning for deep belief networks. In NIPS'2007 .
          關鍵詞：
        - 摘要：Ranzato，M.，Krizhevsky，A.，and Hinton，G. E.（2010a）. Factored 3-；way  restricted  Boltzmann  machines  for  modeling  natural  images.  In；Proceedings of AISTATS 2010 .
          關鍵詞：
        - 摘要：Ranzato，M.，Mnih，V.，and  Hinton，G.（2010b）.  Generating  more；realistic images using gated MRFs. In NIPS'2010 .
          關鍵詞：
        - 摘要：Rao，C.（1945）. Information and the accuracy attainable in the estimation；of statistical param-eters. Bulletin of the Calcutta Mathematical Society ，37；，81–89.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Rasmus，A.，Valpola，H.，Honkala，M.，Berglund，M.，and  Raiko，；T.（2015）.  Semi-supervised  learning  with  ladder  network.  arXiv  preprint
          關鍵詞：
        - 摘要：Recht，B.，Re，C.，Wright，S.，and  Niu，F.（2011）.  Hogwild:  A；lock-free approach to parallelizing stochastic gradient descent. In NIPS'2011 .
          關鍵詞：
        - 摘要：Reichert，D.  P.，Seriès，P.，and  Storkey，A.  J.（2011）.  Neuronal；in  perceptual；adaptation
          關鍵詞：
        - 摘要：for  sampling-based  probabilistic
          關鍵詞：
        - 摘要：inference
          關鍵詞：
        - 摘要：Rezende，D.  J.，Mohamed，S.，and  Wierstra，D.（2014）.  Stochastic；backpropagation  and  approx-imate  inference  in  deep  generative  models.  In；ICML'2014 . Preprint:arXiv:1401.4082.
          關鍵詞：
        - 摘要：Rifai，S.，Vincent，P.，Muller，X.，Glorot，X.，and；Bengio，Y.；（2011a）.  Contractive  auto-encoders:  Explicit  invariance  during  feature
          關鍵詞：
        - 摘要：Rifai，S.，Mesnil，G.，Vincent，P.，Muller，X.，Bengio，Y.，；Dauphin，Y.，and  Glorot，X.（2011b）.  Higher  order  contractive  auto-；encoder. In ECML PKDD .
          關鍵詞：
        - 摘要：Rifai，S.，Dauphin，Y.，Vincent，P.，Bengio，Y.，and  Muller，X.；（2011c）. The manifold tangent classifier. In NIPS'2011 .
          關鍵詞：
        - 摘要：Rifai，S.，Dauphin，Y.，Vincent，P.，Bengio，Y.，and  Muller，X.；（2011d）.  The  manifold  tangent  classifier.  In  NIPS'2011  .  Student  paper；award.
          關鍵詞：
        - 摘要：Rifai，S.，Bengio，Y.，Dauphin，Y.，and  Vincent，P.（2012）.  A；generative process for sampling contractive auto-encoders. In ICML'2012 .
          關鍵詞：
        - 摘要：and  Shapley，R.（2004）.  Reverse；Ringach，D.；neurophysiology. Cognitive Science ，28 （2），147–166.
          關鍵詞：
        - 摘要：correlation
          關鍵詞：
        - 摘要：in
          關鍵詞：
        - 摘要：Roberts，S.  and  Everson，R.（2001）.  Independent  component  analysis:；principles and practice . Cambridge University Press.
          關鍵詞：
        - 摘要：Robinson，A.  J.  and  Fallside，F.（1991）.  A  recurrent  error  propagation；network  speech  recognition  system.  Computer  Speech  and  Language  ，5；（3），259–274.
          關鍵詞：
        - 摘要：Rockafellar，R.  T.（1997）.  Convex  analysis.  princeton  landmarks  in；mathematics.
          關鍵詞：
        - 摘要：Romero，A.，Ballas，N.，Ebrahimi Kahou，S.，Chassang，A.，Gatta，；C.，and  Bengio，Y.（2015）.  Fitnets:Hints  for；thin  deep  nets.  In
          關鍵詞：
        - 摘要：Rosen，J.  B.（1960）.  The  gradient  projection  method  for  nonlinear；programming. part i. linear constraints. Journal of the Society for Industrial；and Applied Mathematics ，8 （1），pp. 181–217.
          關鍵詞：
        - 摘要：Rosenblatt，F.（1958）.  The  perceptron:  A  probabilistic  model  for；information storage and organization in the brain. Psychological Review ，65；，386–408.
          關鍵詞：
        - 摘要：Rosenblatt，F.（1962）.  Principles  of  Neurodynamics  .  Spartan，New；York.
          關鍵詞：
        - 摘要：Rosenblatt，M.（1956）.  Remarks  on  some  nonparametric  estimates  of  a；density  function.  The  Annals  of  Mathematical  Statistics ，27  （3），832–；837.
          關鍵詞：
        - 摘要：Roweis，S.  and  Saul，L.  K.（2000）.  Nonlinear  dimensionality  reduction；by locally linear embedding. Science ，290 （5500）.
          關鍵詞：
        - 摘要：Roweis，S.，Saul，L.，and  Hinton，G.（2002）.  Global  coordination  of；local  linear  models.  In  T.  Dietterich，S.  Becker，and  Z.  Ghahramani，；editors，Advances
          關鍵詞：
        - 摘要：in  Neural
          關鍵詞：
        - 摘要：Information
          關鍵詞：
        - 摘要：Processing
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Rubin，D. B. et al. （1984）. Bayesianly justifiable and relevant frequency；calculations  for  the  applied  statistician.  The  Annals  of  Statistics  ，12
          關鍵詞：
        - 摘要：Rumelhart，D.，Hinton，G.，and  Williams，R.（1986a）.  Learning；representations by back-propagating errors. Nature ，323 ，533–536.
          關鍵詞：
        - 摘要：Rumelhart，D.  E.，Hinton，G.  E.，and  Williams，R.  J.（1986b）.；Learning  internal  representations  by  error  propagation.  In  D.  E.  Rumelhart；and  J.  L.  McClelland，editors，Parallel  Distributed  Processing  ，volume
          關鍵詞：
        - 摘要：Rumelhart，D.  E.，Hinton，G.  E.，and  Williams，R.；J.（1986c）.；Learning  representations  by  back-propagating  errors.  Nature ，323  ，533–
          關鍵詞：
        - 摘要：Rumelhart，D.  E.，McClelland，J.  L.，and；the  PDP  Research；Group（1986d）.  Parallel  Distributed  Processing:  Explorations  in  the
          關鍵詞：
        - 摘要：Russakovsky，O.，Deng，J.，Su，H.，Krause，J.，Satheesh，S.，；Ma，S.，Huang，Z.，Karpathy，A.，Khosla，A.，Bernstein，M.，；Berg，A.  C.，and  Fei-Fei，L.（2014a）.  ImageNet  Large  Scale  Visual
          關鍵詞：
        - 摘要：Russakovsky，O.，Deng，J.，Su，H.，Krause，J.，Satheesh，S.，；Ma，S.，Huang，Z.，Karpathy，A.，Khosla，A.，Bernstein，M.，et al.；（2014b）. Imagenet large scale visual recognition challenge. arXiv preprint
          關鍵詞：
        - 摘要：Russel，S.  J.  and  Norvig，P.（2003）.  Artificial  Intelligence:a  Modern；Approach . Prentice Hall.
          關鍵詞：
        - 摘要：Rust，N.，Schwartz，O.，Movshon，J.；Simoncelli，E.；（2005）.  Spatiotemporal  elements  of  macaque  V1  receptivefields.  Neuron
          關鍵詞：
        - 摘要：A.，and
          關鍵詞：
        - 摘要：Ramabhadran，B.；Sainath，T.，Mohamed，A.，Kingsbury，B.，and；（2013）. Deep convolutional neural networks for LVCSR. In ICASSP 2013
          關鍵詞：
        - 摘要：Salakhutdinov，R.（2010）.  Learning；in  Markov  randomfields  using；tempered  transitions.  In  Y.  Bengio，D.  Schuurmans，C.  Williams，J.
          關鍵詞：
        - 摘要：in  Neural
          關鍵詞：
        - 摘要：Salakhutdinov，R.  and  Hinton，G.（2009a）.  Deep  Boltzmann  machines.；In Proceedings of the International Conference on Artificial Intelligence and；Statistics ，volume 5，pages 448–455.
          關鍵詞：
        - 摘要：Salakhutdinov，R.  and  Hinton，G.（2009b）.  Semantic  hashing.；International Journal of Approximate Reasoning .
          關鍵詞：
        - 摘要：In
          關鍵詞：
        - 摘要：Salakhutdinov，R.  and  Hinton，G.  E.（2007a）.  Learning  a  nonlinear；embedding  by  preserving  class  neighbourhood  structure.  In  Proceedings  of；AISTATS-2007 .
          關鍵詞：
        - 摘要：Salakhutdinov，R.  and  Hinton，G.  E.（2007b）.  Semantic  hashing.  In；SIGIR'2007 .
          關鍵詞：
        - 摘要：Salakhutdinov，R.  and  Hinton，G.  E.（2008）.  Using  deep  belief  nets  to；learn  covariance  kernels  for  Gaussian  processes.  In  J.  Platt，D.  Koller，Y.；Singer，and  S.  Roweis，editors，Advances
          關鍵詞：
        - 摘要：in  Neural
          關鍵詞：
        - 摘要：Salakhutdinov，R. and Larochelle，H.（2010）. Efficient learning of deep；Boltzmann  machines.  In  Proceedings  of  the  Thirteenth  International；Conference  on  Artificial  Intelligence  and  Statistics（AISTATS  2010），
          關鍵詞：
        - 摘要：Salakhutdinov，R.；factorization. In NIPS'2008 .
          關鍵詞：
        - 摘要：and  Mnih，A.（2008）.
          關鍵詞：
        - 摘要：Probabilistic  matrix
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Salakhutdinov，R. and Murray，I.（2008）. On the quantitative analysis of；deep belief networks. In W. W. Cohen，A. McCallum，and S. T. Roweis，
          關鍵詞：
        - 摘要：Salakhutdinov，R.，Mnih，A.，and  Hinton，G.（2007）.  Restricted；Boltzmann machines for collab-orativefiltering. In ICML .
          關鍵詞：
        - 摘要：Sanger，T.  D.（1994）.  Neural  network；learning  control  of  robot；manipulators  using  gradually  increasing  task  difficulty.  IEEE  Transactions
          關鍵詞：
        - 摘要：Saul，L.  K.  and  Jordan，M.  I.（1996）.  Exploiting  tractable  substructures；in  intractable  networks.  In  D.  Touretzky，M.  Mozer，and  M.  Hasselmo，；editors，Advances in Neural Information Processing Systems 8（NIPS'95）
          關鍵詞：
        - 摘要：Saul，L. K.，Jaakkola，T.，and Jordan，M. I.（1996）. Meanfield theory；for sigmoid  belief  networks. Journal of Artificial Intelligence Research ，4；，61–76.
          關鍵詞：
        - 摘要：Savich，A.  W.，Moussa，M.，and  Areibi，S.（2007）.  The  impact  of；arithmetic representation on implementing mlp-bp on fpgas: A study. Neural；Networks，IEEE Transactions on ，18 （1），240–252.
          關鍵詞：
        - 摘要：Saxe，A.  M.，Koh，P.  W.，Chen，Z.，Bhand，M.，Suresh，B.，and；Ng，A.（2011）. On random weights and unsupervised feature learning. In；Proc. ICML'2011 . ACM.
          關鍵詞：
        - 摘要：Saxe，A.  M.，McClelland，J.  L.，and  Ganguli，S.（2013）.  Exact；solutions  to  the  nonlinear  dynamics  of  learning  in  deep  linear  neural；networks. In ICLR .
          關鍵詞：
        - 摘要：Schaul，T.，Antonoglou，I.，and  Silver，D.（2014）.  Unit；stochastic  optimization.；Representations .
          關鍵詞：
        - 摘要：tests  for；International  Conference  on  Learning
          關鍵詞：
        - 摘要：In
          關鍵詞：
        - 摘要：Schmidhuber，J.（1992）.  Learning  complex，extended  sequences  using；the principle of history compression. Neural Computation ，4 （2），234–；242.
          關鍵詞：
        - 摘要：Schmidhuber，J.（1996）.  Sequential  neural；Transactions on Neural Networks ，7 （1），142–146.
          關鍵詞：
        - 摘要：text  compression.  IEEE
          關鍵詞：
        - 摘要：Schmidhuber，J.（2012）.  Self-delimiting  neural  networks.  arXiv  preprint；arXiv:1210.0118 .
          關鍵詞：
        - 摘要：Schölkopf，B.  and  Smola，A.  J.（2002）.  Learning with kernels: Support；vector machines，regular-ization，optimization，and beyond . MIT press.
          關鍵詞：
        - 摘要：Schölkopf，B.，Burges，C. J. C.，and Smola，A. J.（1998a）. Advances；in kernel methods: support vector learning . MIT Press，Cambridge，MA.
          關鍵詞：
        - 摘要：Schölkopf，B.，Smola，A.，and  Müller，K.-R.（1998b）.  Nonlinear；component  analysis  as  a  kernel  eigenvalue  problem.  Neural  Computation；，10 ，1299–1319.
          關鍵詞：
        - 摘要：Schölkopf，B.，Burges，C.  J.  C.，and  Smola，A.  J.（1999）.  Advances；in  Kernel  Methods—Support  Vector  Learning  .  MIT  Press，Cambridge，；MA.
          關鍵詞：
        - 摘要：Schölkopf，B.，Janzing，D.，Peters，J.，Sgouritsa，E.，Zhang，K.，；and Mooij，J.（2012）. On causal and anticausal learning. In ICML'2012 ，；pages 1255–1262.
          關鍵詞：
        - 摘要：Schuster，M.（1999）.  On  supervised  learning  from  sequential  data  with；applications for speech recognition.
          關鍵詞：
        - 摘要：Schuster，M.  and  Paliwal，K.（1997）.  Bidirectional  recurrent  neural；networks.  IEEE  Transactions  on  Signal  Processing  ，45  （11），2673–；2681.
          關鍵詞：
        - 摘要：Schwenk，H.（2007）.  Continuous  space  language  models.  Computer；speech and language ，21 ，492–518.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Schwenk，H.（2010）.  Continuous  space  language  models  for  statistical；machine  translation.  The  Prague  Bulletin  of  Mathematical  Linguistics  ，93
          關鍵詞：
        - 摘要：Schwenk，H.（2014）. Cleaned subset of WMT '14 dataset.
          關鍵詞：
        - 摘要：Schwenk，H.  and  Bengio，Y.（1998）.  Training  methods  for  adaptive；boosting  of  neural  networks.  In  M.  Jordan，M.  Kearns，and  S.  Solla，；editors，Advances
          關鍵詞：
        - 摘要：in  Neural
          關鍵詞：
        - 摘要：Information
          關鍵詞：
        - 摘要：Processing
          關鍵詞：
        - 摘要：Schwenk，H.  and  Gauvain，J.-L.（2002）.  Connectionist；modeling；International
          關鍵詞：
        - 摘要：large  vocabulary  continuous  speech；Acoustics，Speech；Conference
          關鍵詞：
        - 摘要：for
          關鍵詞：
        - 摘要：on
          關鍵詞：
        - 摘要：and
          關鍵詞：
        - 摘要：recognition.
          關鍵詞：
        - 摘要：language；In；Signal
          關鍵詞：
        - 摘要：Schwenk，H.，Costa-jussà，M.  R.，and  Fonollosa，J.  A.  R.（2006）.；task.  In；Continuous  space
          關鍵詞：
        - 摘要：language  models  for
          關鍵詞：
        - 摘要：the  IWSLT  2006
          關鍵詞：
        - 摘要：Seide，F.，Li，G.，and  Yu，D.（2011）.  Conversational；speech；transcription  using  context-dependent  deep  neural  networks.  In  Interspeech
          關鍵詞：
        - 摘要：Sejnowski，T.（1987）.  Higher-order  Boltzmann  machines.；In  AIP；Conference  Proceedings  151  on  Neural  Networks  for  Computing  ，pages
          關鍵詞：
        - 摘要：Series，P.，Reichert，D.  P.，and  Storkey，A.  J.（2010）.  Hallucinations；in  Charles  Bonnet  syndrome  induced  by  homeostasis:  a  deep  Boltzmann；machine  model.  In  Advances  in  Neural  Information  Processing  Systems  ，
          關鍵詞：
        - 摘要：Sermanet，P.，Chintala，S.，and  LeCun，Y.（2012）.  Convolutional；In；neural  networks  applied
          關鍵詞：
        - 摘要：to  house  numbers  digit  classification.
          關鍵詞：
        - 摘要：LeCun，Y.；Sermanet，P.，Kavukcuoglu，K.，Chintala，S.，and；（2013）.  Pedestrian  detection  with  unsupervised  multi-stage  feature
          關鍵詞：
        - 摘要：Shilov，G.（1977）. Linear Algebra. Dover Books on Mathematics Series.；Dover Publications.
          關鍵詞：
        - 摘要：Siegelmann，H.（1995）.  Computation  beyond  the  Turing  limit.  Science；，268 （5210），545–548.
          關鍵詞：
        - 摘要：Siegelmann，H. and Sontag，E.（1991）. Turing computability with neural；nets. Applied Mathe-matics Letters ，4 （6），77–80.
          關鍵詞：
        - 摘要：Siegelmann，H.  T.  and  Sontag，E.  D.（1995）.  On  the  computational；power  of  neural  nets.  Journal  of  Computer  and  Systems  Sciences  ，50；（1），132–150.
          關鍵詞：
        - 摘要：Sietsma，J. and Dow，R.（1991）. Creating artificial neural networks that；generalize. Neural Networks ，4 （1），67–79.
          關鍵詞：
        - 摘要：Simard，D.，Steinkraus，P.  Y.，and  Platt，J.  C.（2003）.  Best  practices；for convolutional neural networks. In ICDAR'2003 .
          關鍵詞：
        - 摘要：Simard，P.；and  Graf，H.  P.（1994）.  Backpropagation  without；multiplication.  In  Advances  in  Neural  Information  Processing  Systems  ，
          關鍵詞：
        - 摘要：Simard，P.，Victorri，B.，LeCun，Y.，and；Denker，J.（1992）.；Tangent prop-A formalism for specifying selected invariances in an adaptive
          關鍵詞：
        - 摘要：Simard，P.  Y.，LeCun，Y.，and  Denker，J.（1993）.  Efficient  pattern；recognition using a new transformation distance. In NIPS'92 .
          關鍵詞：
        - 摘要：Simard，P.  Y.，LeCun，Y.  A.，Denker，J.  S.，and  Victorri，B.；（1998）.  Transformation；in  pattern  recognition—tangent
          關鍵詞：
        - 摘要：invariance
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；distance and tangent propagation. Lecture Notes in Computer Science ，1524；.
          關鍵詞：
        - 摘要：Simons，D.  J.  and  Levin，D.  T.（1998）.  Failure  to  detect  changes  to；people during a real-world interaction. Psychonomic Bulletin & Review ，5；（4），644–649.
          關鍵詞：
        - 摘要：Simonyan，K.  and  Zisserman，A.（2015）.  Very  deep  convolutional；networks for large-scale image recognition. In ICLR .
          關鍵詞：
        - 摘要：Sjöberg，J.  and  Ljung，L.（1995）.  Overtraining，regularization  and；searching for a minimum，with application to neural networks. International；Journal of Control ，62 （6），1391–1407.
          關鍵詞：
        - 摘要：Skinner，B. F.（1958）. Reinforcement today. American Psychologist ，13；，94–99.
          關鍵詞：
        - 摘要：Smolensky，P.（1986）.  Information  processing  in  dynamical  systems:；Foundations of harmony theory. In D. E. Rumelhart and J. L. McClelland，；editors，Parallel  Distributed  Processing  ，volume  1，chapter  6，pages
          關鍵詞：
        - 摘要：Snoek，J.，Larochelle，H.，and  Adams，R.  P.（2012）.  Practical；Bayesian optimization of machine learning algorithms. In NIPS'2012 .
          關鍵詞：
        - 摘要：Socher，R.，Huang，E.；Y.，and；Manning，C.  D.（2011a）.  Dynamic  pooling  and  unfolding  recursive
          關鍵詞：
        - 摘要：H.，Pennington，J.，Ng，A.
          關鍵詞：
        - 摘要：Socher，R.，Manning，C.，and  Ng，A.  Y.（2011b）.  Parsing  natural；scenes and natural language with recursive neural networks. In Proceedings；of
          關鍵詞：
        - 摘要：International  Conference
          關鍵詞：
        - 摘要：the
          關鍵詞：
        - 摘要：Y.，and；Socher，R.，Pennington，J.，Huang，E.；Manning，C.  D.（2011c）.  Semi-supervised  recursive  autoencoders  for
          關鍵詞：
        - 摘要：H.，Ng，A.
          關鍵詞：
        - 摘要：Y.，Chuang，J.，Manning，C.；Socher，R.，Perelygin，A.，Wu，J.；D.，Ng，A.  Y.，and  Potts，C.（2013a）.  Recursive  deep  models  for
          關鍵詞：
        - 摘要：Socher，R.，Ganjoo，M.，Manning，C.  D.，and  Ng，A.  Y.（2013b）.；Zero-shot learning through cross-modal transfer. In 27th Annual Conference；on Neural Information Processing Systems（NIPS 2013） .
          關鍵詞：
        - 摘要：A.，Maheswaranathan，N.，and；Sohl-Dickstein，J.，Weiss，E.；Ganguli，S.（2015）.  Deep  unsuper-vised  learning  using  nonequilibrium
          關鍵詞：
        - 摘要：Sohn，K.，Zhou，G.，and  Lee，H.（2013）.  Learning  and  selecting；features jointly with point-wise gated Boltzmann machines. In ICML'2013 .
          關鍵詞：
        - 摘要：Solomonoff，R.  J.（1989）.  A  system  for  incremental  learning  based  on；algorithmic probability.
          關鍵詞：
        - 摘要：Sontag，E.  D.（1998）.  VC  dimension  of  neural  networks.  NATO  ASI；Series F Computer and Systems Sciences ，168 ，69–96.
          關鍵詞：
        - 摘要：Sontag，E. D. and Sussman，H. J.（1989）. Backpropagation can give rise；to spurious local minima even for networks without hidden layers. Complex；Systems ，3 ，91–106.
          關鍵詞：
        - 摘要：Sparkes，B.（1996）.  The  Red  and  the  Black:  Studies  in  Greek  Pottery  .；Routledge.
          關鍵詞：
        - 摘要：Spitkovsky，V.  I.，Alshawi，H.，and  Jurafsky，D.（2010）.  From  baby；steps to leapfrog: how“less is more”in unsupervised dependency parsing. In；HLT'10 .
          關鍵詞：
        - 摘要：Squire，W.  and  Trapp，G.（1998）.  Using  complex  variables  to  estimate；derivatives of real functions. SIAM Rev. ，40 （1），110–112.
          關鍵詞：
        - 摘要：Srebro，N.  and  Shraibman，A.（2005）.  Rank，trace-norm  and  max-；norm. In Proceedings of the 18th Annual Conference on Learning Theory ，
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；pages 545–560. Springer-Verlag.
          關鍵詞：
        - 摘要：Srivastava，N.（2013）.  Improving  Neural  Networks  With  Dropout；Master's thesis，U. Toronto.
          關鍵詞：
        - 摘要：.
          關鍵詞：
        - 摘要：Srivastava，N.  and  Salakhutdinov，R.（2012）.  Multimodal  learning  with；deep Boltzmann machines. In NIPS'2012 .
          關鍵詞：
        - 摘要：Srivastava，N.，Salakhutdinov，R.  R.，and  Hinton，G.  E.（2013）.；Modeling  documents  with  deep  Boltzmann  machines.  arXiv  preprint；arXiv:1309.6865 .
          關鍵詞：
        - 摘要：Srivastava，N.，Hinton，G.，Krizhevsky，A.，Sutskever，I.，and；Salakhutdinov，R.（2014）.  Dropout:  A  simple  way  to  prevent  neural；networks  from  overfitting.  Journal  of  Machine  Learning  Research ，15  ，
          關鍵詞：
        - 摘要：Srivastava，R.  K.，Greff，K.，and  Schmidhuber，J.（2015）.  Highway；networks. arXiv:1505.00387 .
          關鍵詞：
        - 摘要：Steinkrau，D.，Simard，P.  Y.，and  Buck，I.（2005）.  Using  GPUs  for；machine  learning  algorithms.  2013  12th  International  Conference  on；Document Analysis and Recognition ，0 ，1115–1119.
          關鍵詞：
        - 摘要：Stoyanov，V.，Ropson，A.，and  Eisner，J.（2011）.  Empirical；risk；minimization  of  graphical  model  parameters  given  approximate  inference，
          關鍵詞：
        - 摘要：Sukhbaatar，S.，Szlam，A.，Weston，J.，and；Weakly supervised memory networks. arXiv preprint arXiv:1503.08895 .
          關鍵詞：
        - 摘要：Fergus，R.（2015）.
          關鍵詞：
        - 摘要：Supancic，J. and Ramanan，D.（2013）. Self-paced learning for long-term；tracking. In CVPR'2013 .
          關鍵詞：
        - 摘要：Sussillo，D.（2014）.  Random  walks:Training  very  deep  nonlinear  feed-；forward networks with smart initialization. CoRR ，abs/1412.6558 .
          關鍵詞：
        - 摘要：Sutskever，I.（2012）.  Training  Recurrent  Neural  Networks；thesis，Department of computer science，University of Toronto.
          關鍵詞：
        - 摘要：.  Ph.D.
          關鍵詞：
        - 摘要：Sutskever，I.  and  Hinton，G.  E.（2008）.  Deep  narrow  sigmoid  belief；networks  are  universal  approximators.  Neural  Computation ，20  （11），；2629–2636.
          關鍵詞：
        - 摘要：Sutskever，I.  and  Tieleman，T.（2010）.  On  the  Convergence  Properties；of Contrastive Divergence. In AISTATS'2010 .
          關鍵詞：
        - 摘要：Sutskever，I.，Hinton，G.，and  Taylor，G.（2009）.  The；temporal restricted Boltzmann machine. In NIPS'2008 .
          關鍵詞：
        - 摘要：recurrent
          關鍵詞：
        - 摘要：Sutskever，I.，Martens，J.，and  Hinton，G.  E.（2011）.  Generating  text；with recurrent neural networks. In ICML'2011 ，pages 1017–1024.
          關鍵詞：
        - 摘要：Sutskever，I.，Martens，J.，Dahl，G.，and Hinton，G.（2013）. On the；importance of initialization and momentum in deep learning. In ICML .
          關鍵詞：
        - 摘要：Sutskever，I.，Vinyals，O.，and  Le，Q.  V.（2014）.  Sequence；sequence learning with neural networks. In NIPS'2014，arXiv:1409.3215 .
          關鍵詞：
        - 摘要：to
          關鍵詞：
        - 摘要：Sutton，R.；Introduction . MIT Press.
          關鍵詞：
        - 摘要：and  Barto，A.（1998）.  Reinforcement  Learning:  An
          關鍵詞：
        - 摘要：Sutton，R.  S.，Mcallester，D.，Singh，S.，and  Mansour，Y.（2000）.；function；for
          關鍵詞：
        - 摘要：learning  with
          關鍵詞：
        - 摘要：reinforcement
          關鍵詞：
        - 摘要：Swersky，K.，Ranzato，M.，Buchman，D.，Marlin，B.，and；de；Freitas，N.（2011）. On autoencoders and score matching for energy based
          關鍵詞：
        - 摘要：Swersky，K.，Snoek，J.，and  Adams，R.  P.（2014）.  Freeze-thaw
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Bayesian optimization. arXiv preprint arXiv:1406.3896 .
          關鍵詞：
        - 摘要：Szegedy，C.，Liu，W.，Jia，Y.，Sermanet，P.，Reed，S.，；Anguelov，D.，Erhan，D.，Vanhoucke，V.，and；（2014a）.  Going  deeper  with  convolutions.  Technical
          關鍵詞：
        - 摘要：Rabinovich，A.；report，
          關鍵詞：
        - 摘要：Szegedy，C.，Zaremba，W.，Sutskever，I.，Bruna，J.，Erhan，D.，；Goodfellow，I.  J.，and  Fergus，R.（2014b）.  Intriguing  properties  of；neural networks. ICLR ，abs/1312.6199 .
          關鍵詞：
        - 摘要：Szegedy，C.，Vanhoucke，V.，Ioffe，S.，Shlens，J.，and  Wojna，Z.；（2015）. Rethinking the Inception Architecture for Computer Vision. ArXiv；e-prints .
          關鍵詞：
        - 摘要：Taigman，Y.，Yang，M.，Ranzato，M.，and；Wolf，L.（2014）.；DeepFace: Closing the gap to human-level performance in face verification.
          關鍵詞：
        - 摘要：Tandy，D.  W.（1997）.  Works  and  Days:  A  Translation  and  Commentary；for the Social Sciences. University of California Press.
          關鍵詞：
        - 摘要：Tang，Y.  and  Eliasmith，C.（2010）.  Deep  networks  for  robust  visual；recognition. In Proceedings of the 27th International Conference on Machine；Learning，June 21-24，2010，Haifa，Israel .
          關鍵詞：
        - 摘要：Tang，Y.，Salakhutdinov，R.，and  Hinton，G.（2012）.  Deep  mixtures；of factor analysers. arXiv preprint arXiv:1206.4635 .
          關鍵詞：
        - 摘要：Taylor，G.  and  Hinton，G.（2009）.  Factored  conditional  restricted；Boltzmann  machines  for  modeling  motion  style.  In  L.  Bottou  and  M.；Littman，editors，Proceedings of the Twenty-sixth International Conference
          關鍵詞：
        - 摘要：Taylor，G.，Hinton，G.  E.，and  Roweis，S.（2007）.  Modeling  human；motion  using  binary  latent  variables.  In  B.  Schölkopf，J.  Platt，and  T.
          關鍵詞：
        - 摘要：Hoffman，editors，Advances  in  Neural  Information  Processing  Systems；19（NIPS'06） ，pages 1345–1352. MIT Press，Cambridge，MA.
          關鍵詞：
        - 摘要：Teh，Y.，Welling，M.，Osindero，S.，and  Hinton，G.  E.（2003）.；Energy-based  models  for  sparse  overcomplete  representations.  Journal  of；Machine Learning Research ，4 ，1235–1260.
          關鍵詞：
        - 摘要：Tenenbaum，J.，de  Silva，V.，and  Langford，J.  C.（2000）.  A  global；geometric framework for nonlinear dimensionality reduction. Science ，290；（5500），2319–2323.
          關鍵詞：
        - 摘要：Theis，L.，van  den  Oord，A.，and  Bethge，M.（2015）.  A  note  on  the；evaluation of generative models. arXiv:1511.01844.
          關鍵詞：
        - 摘要：Thompson，J.，Jain，A.，LeCun，Y.，and  Bregler，C.（2014）.  Joint；training  of  a  convolutional  network  and  a  graphical  model  for  human  pose；estimation. In NIPS'2014 .
          關鍵詞：
        - 摘要：Thrun，S.（1995）. Learning to play the game of chess. In NIPS'1994 .
          關鍵詞：
        - 摘要：Tibshirani，R. J.（1995）. Regression shrinkage and selection via the lasso.；Journal of the Royal Statistical Society B ，58 ，267–288.
          關鍵詞：
        - 摘要：Tieleman，T.（2008）.  Training  restricted  Boltzmann  machines  using；approximations  to  the  like-lihood  gradient.  In  ICML'2008  ，pages  1064–；1071.
          關鍵詞：
        - 摘要：Tieleman，T.  and  Hinton，G.（2009）.  Using  fast  weights  to  improve；persistent contrastive diver-gence. In ICML'2009 .
          關鍵詞：
        - 摘要：Tipping，M.  E.  and  Bishop，C.  M.（1999）.  Probabilistic  principal；components  analysis.  Journal  of  the  Royal  Statistical  Society  B  ，61；（3），611–622.
          關鍵詞：
        - 摘要：Torralba，A.，Fergus，R.，and  Weiss，Y.（2008）.  Small  codes  and；large  databases for recognition. In Proceedings of the Computer Vision and；Pattern Recognition Conference（CVPR'08） ，pages 1–8.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Touretzky，D.  S.  and  Minton，G.  E.（1985）.  Symbols  among  the；neurons: Details of a con-nectionist inference architecture. In Proceedings of
          關鍵詞：
        - 摘要：Tu，K.  and  Honavar，V.（2011）.  On；unsupervised learning of probabilistic grammars. In IJCAI'2011 .
          關鍵詞：
        - 摘要：the  utility  of  curricula
          關鍵詞：
        - 摘要：in
          關鍵詞：
        - 摘要：Turaga，S.  C.，Murray，J.  F.，Jain，V.，Roth，F.，Helmstaedter，；M.，Briggman，K.，Denk，W.，and；S.（2010）.
          關鍵詞：
        - 摘要：Seung，H.
          關鍵詞：
        - 摘要：Turian，J.，Ratinov，L.，and；Word；representations:  A  simple  and  general  method  for  semi-supervised  learning.
          關鍵詞：
        - 摘要：Bengio，Y.（2010）.
          關鍵詞：
        - 摘要：Töscher，A.，Jahrer，M.，and  Bell，R.  M.（2009）.  The  BigChaos；solution to the Netflix grand prize.
          關鍵詞：
        - 摘要：Uria，B.，Murray，I.，and  Larochelle，H.（2013）.  Rnade:  The  real-；valued neural autoregressive density-estimator. In NIPS'2013 .
          關鍵詞：
        - 摘要：van  den  Oörd，A.，Dieleman，S.，and  Schrauwen，B.（2013）.  Deep；content-based music recom-mendation. In NIPS'2013 .
          關鍵詞：
        - 摘要：van  der  Maaten，L.  and  Hinton，G.  E.（2008）.  Visualizing  data  using  t-；SNE. J. Machine Learning Res. ，9 .
          關鍵詞：
        - 摘要：Vanhoucke，V.，Senior，A.，and  Mao，M.  Z.（2011）.  Improving  the；speed  of  neural  networks  on  CPUs.  In  Proc.  Deep  Learning  and；Unsupervised Feature Learning NIPS Workshop .
          關鍵詞：
        - 摘要：Vapnik，V.  N.（1982）.  Estimation  of  Dependences  Based  on  Empirical；Data . Springer-Verlag，Berlin.
          關鍵詞：
        - 摘要：Vapnik，V.  N.（1995）.  The  Nature  of  Statistical  Learning  Theory  .；Springer，New York.
          關鍵詞：
        - 摘要：Vapnik，V.  N.  and  Chervonenkis，A.  Y.（1971）.  On  the  uniform；convergence of relative frequencies of events to their probabilities. Theory of；Probability and Its Applications ，16 ，264–280.
          關鍵詞：
        - 摘要：Vincent，P.（2011）. A connection between score matching and denoising；autoencoders. Neural Computation ，23 （7）.
          關鍵詞：
        - 摘要：Vincent，P.  and  Bengio，Y.（2003）.  Manifold  Parzen  windows.  In；NIPS'2002 . MIT Press.
          關鍵詞：
        - 摘要：Vincent，P.，Larochelle，H.，Bengio，Y.，and；Manzagol，P.-A.；（2008a）.  Extracting  and  composing  robust  features  with  denoising
          關鍵詞：
        - 摘要：Manzagol，P.-A.；Vincent，P.，Larochelle，H.，Bengio，Y.，and；（2008b）.  Extracting  and  composing  robust  features  with  denoising
          關鍵詞：
        - 摘要：Vincent，P.，Larochelle，H.，Lajoie，I.，Bengio，Y.，and  Manzagol，；P.-A.（2010）.  Stacked  denoising；autoencoders:  Learning  useful
          關鍵詞：
        - 摘要：Vincent，P.，de  Brébisson，A.，and  Bouthillier，X.（2015）.  Efficient；exact  gradient  update  for  training  deep  networks  with  very  large  sparse；targets. In C. Cortes，N. D. Lawrence，D. D. Lee，M. Sugiyama，and R.
          關鍵詞：
        - 摘要：Vinyals，O.，Kaiser，L.，Koo，T.，Petrov，S.，Sutskever，I.，and；Hinton，G.（2014a）.  Grammar  as  a  foreign  language.  arXiv  preprint；arXiv:1412.7449 .
          關鍵詞：
        - 摘要：Vinyals，O.，Toshev，A.，Bengio，S.，and Erhan，D.（2014b）. Show
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；and tell:a neural image caption generator. arXiv 1411.4555.
          關鍵詞：
        - 摘要：Vinyals，O.，Fortunato，M.，and Jaitly，N.（2015a）. Pointer networks.；arXiv preprint arXiv:1506.03134 .
          關鍵詞：
        - 摘要：Vinyals，O.，Toshev，A.，Bengio，S.，and Erhan，D.（2015b）. Show；and tell:a neural image caption generator. In CVPR'2015 . arXiv:1411.4555.
          關鍵詞：
        - 摘要：Viola，P.  and  Jones，M.（2001）.  Robust  real-time  object  detection.  In；International Journal of Computer Vision .
          關鍵詞：
        - 摘要：Visin，F.，Kastner，K.，Cho，K.，Matteucci，M.，Courville，A.，and；Bengio，Y.（2015）.  ReNet:  A  recurrent  neural  network  based  alternative；to convolutional networks. arXiv preprint arXiv:1505.00393 .
          關鍵詞：
        - 摘要：Von  Melchner，L.，Pallas，S.  L.，and  Sur，M.（2000）.  Visual；behaviour  mediated  by  retinal  projections  directed  to  the  auditory  pathway.；Nature ，404 （6780），871–876.
          關鍵詞：
        - 摘要：Wager，S.，Wang，S.，and  Liang，P.（2013）.  Dropout；training  as；adaptive  regularization.  In  Advances  in  Neural  Information  Processing
          關鍵詞：
        - 摘要：Waibel，A.，Hanazawa，T.，Hinton，G.  E.，Shikano，K.，and  Lang，；K.（1989）.  Phoneme  recognition  using  time-delay  neural  networks.  IEEE；Transactions  on  Acoustics，Speech，and  Signal  Processing  ，37  ，328–
          關鍵詞：
        - 摘要：Wan，L.，Zeiler，M.，Zhang，S.，LeCun，Y.，and；Fergus，R.；（2013）.  Regularization  of  neural  networks  using  dropconnect.  In
          關鍵詞：
        - 摘要：Wang，S. and Manning，C.（2013）. Fast dropout training. In ICML'2013；.
          關鍵詞：
        - 摘要：Wang，Z.，Zhang，J.，Feng，J.，and  Chen，Z.（2014a）.  Knowledge；graph and text jointly embedding. In Proc. EMNLP'2014 .
          關鍵詞：
        - 摘要：Wang，Z.，Zhang，J.，Feng，J.，and  Chen，Z.（2014b）.  Knowledge；graph embedding by translating on hyperplanes. In Proc. AAAI'2014 .
          關鍵詞：
        - 摘要：Warde-Farley，D.，Goodfellow，I.  J.，Courville，A.，and  Bengio，Y.；（2014）. An empirical analysis of dropout in piecewise linear networks. In；ICL（1）.
          關鍵詞：
        - 摘要：Wawrzynek，J.，Asanovic，K.，Kingsbury，B.，Johnson，D.，Beck，；J.，and  Morgan，N.（1996）.  Spert-II:  A  vector  microprocessor  system.；Computer ，29 （3），79–86.
          關鍵詞：
        - 摘要：Weaver，L.  and  Tao，N.（2001）.  The  optimal  reward  baseline  for；gradient-based reinforcement learning. In Proc. UAI'2001 ，pages 538–545.
          關鍵詞：
        - 摘要：Weinberger，K.  Q.  and  Saul，L.  K.（2004a）.  Unsupervised  learning  of；image  manifolds  by  semidefi-nite  programming.  In  Proceedings  of  the；Computer  Vision  and  Pattern  Recognition  Conference（CVPR'04）  ，
          關鍵詞：
        - 摘要：Weinberger，K.  Q.  and  Saul，L.  K.（2004b）.  Unsupervised  learning  of；image manifolds by semidefinite programming. In CVPR'2004 ，pages 988–；995.
          關鍵詞：
        - 摘要：Weiss，Y.，Torralba，A.，and  Fergus，R.（2008）.  Spectral  hashing.  In；NIPS ，pages 1753–1760.
          關鍵詞：
        - 摘要：Welling，M.，Zemel，R. S.，and Hinton，G. E.（2002）. Self supervised；boosting.  In  Advances  in  Neural  Information  Processing  Systems  ，pages；665–672.
          關鍵詞：
        - 摘要：Welling，M.，Hinton，G.  E.，and  Osindero，S.（2003a）.  Learning；sparse  topographic  representa-tions  with  products  of  Student-t  distributions.；In NIPS'2002 .
          關鍵詞：
        - 摘要：Welling，M.，Zemel，R.，and  Hinton，G.  E.（2003b）.  Self-supervised；boosting.  In  S.  Becker，S.  Thrun，and  K.  Obermayer，editors，Advances；in Neural Information Processing Systems 15（NIPS'02） ，pages 665–672.
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；MIT Press.
          關鍵詞：
        - 摘要：Welling，M.，Rosen-Zvi，M.，and  Hinton，G.  E.（2005）.  Exponential；family harmoniums with an application to information retrieval. In L. Saul，；Y.  Weiss，and  L.  Bottou，editors，Advances
          關鍵詞：
        - 摘要：Werbos，P.  J.（1981）.  Applications  of  advances  in  nonlinear  sensitivity；analysis.  In  Proceedings  of  the  10th  IFIP  Conference，31.8-4.9，NYC  ，；pages 762–770.
          關鍵詞：
        - 摘要：Weston，J.，Bengio，S.，and  Usunier，N.（2010）.  Large  scale  image；annotation:  learning  to  rank  with  joint  word-image  embeddings.  Machine；Learning ，81 （1），21–35.
          關鍵詞：
        - 摘要：Weston，J.，Chopra，S.，and  Bordes，A.（2014）.  Memory  networks.；arXiv preprint arXiv:1410.3916 .
          關鍵詞：
        - 摘要：Widrow，B.  and  Hoff，M.  E.（1960）.  Adaptive  switching  circuits.  In；1960 IRE WESCON Convention Record ，volume 4，pages 96–104. IRE，；New York.
          關鍵詞：
        - 摘要：Wikipedia（2015）. List of animals by number of neurons—Wikipedia，the；free encyclopedia. ［Online；accessed 4-March-2015］.
          關鍵詞：
        - 摘要：Williams，C. K. I. and Agakov，F. V.（2002）. Products of Gaussians and；Probabilistic  Minor  Component  Analysis.  Neural  Computation  ，14（5）；，1169–1182.
          關鍵詞：
        - 摘要：Williams，C. K. I. and Rasmussen，C. E.（1996）. Gaussian processes for；regression.  In  D.  Touretzky，M.  Mozer，and  M.  Hasselmo，editors，；Advances in Neural Information Processing Systems 8（NIPS'95） ，pages
          關鍵詞：
        - 摘要：Williams，R.  J.（1992）.  Simple  statistical  gradient-following  algorithms；connectionist reinforcement learning. Machine Learning ，8 ，229–256.
          關鍵詞：
        - 摘要：Williams，R.  J.  and  Zipser，D.（1989）.  A；learning  algorithm  for；continually running fully recurrent neural networks. Neural Computation ，1
          關鍵詞：
        - 摘要：Wilson，D.  R.  and  Martinez，T.  R.（2003）.  The  general  inefficiency  of；batch training for gradient descent learning. Neural Networks ，16 （10），；1429–1451.
          關鍵詞：
        - 摘要：Wilson，J.  R.（1984）.  Variance；for  digital；simulation.  American  Journal  of  Mathematical  and  Management  Sciences
          關鍵詞：
        - 摘要：techniques
          關鍵詞：
        - 摘要：reduction
          關鍵詞：
        - 摘要：Wiskott，L.  and  Sejnowski，T.  J.（2002）.  Slow  feature  analysis:；Unsupervised  learning  of  invari-ances.  Neural  Computation  ，14  （4），；715–770.
          關鍵詞：
        - 摘要：Wolpert，D.  and  MacReady，W.（1997）.  No  free  lunch  theorems  for；optimization. IEEE Transactions on Evolutionary Computation ，1 ，67–82.
          關鍵詞：
        - 摘要：Wolpert，D. H.（1996）. The lack of a priori distinction between learning；algorithms. Neural Computation ，8 （7），1341–1390.
          關鍵詞：
        - 摘要：Wu，R.，Yan，S.，Shan，Y.，Dang，Q.，and  Sun，G.（2015）.  Deep；image: Scaling up image recognition. arXiv:1501.02876.
          關鍵詞：
        - 摘要：Wu，Z.（1997）.  Global  continuation  for  distance  geometry  problems.；SIAM Journal of Optimization ，7 ，814–836.
          關鍵詞：
        - 摘要：Xiong，H.  Y.，Barash，Y.，and  Frey，B.；J.（2011）.  Bayesian；prediction  of  tissue-regulated  splicing  using  RNA  sequence  and  cellular
          關鍵詞：
        - 摘要：L.，Kiros，R.，Cho，K.，Courville，A.，；Xu，K.，Ba，J.；Salakhutdinov，R.，Zemel，R.  S.，and  Bengio，Y.（2015）.  Show，
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Yildiz，I.  B.，Jaeger，H.，and  Kiebel，S.  J.（2012）.  Re-visiting  the；echo state property. Neural networks ，35 ，1–9.
          關鍵詞：
        - 摘要：Yosinski，J.，Clune，J.，Bengio，Y.，and  Lipson，H.（2014）.  How；transferable are features in deep neural networks? In NIPS 27 ，pages 3320–；3328. Curran Associates，Inc.
          關鍵詞：
        - 摘要：Younes，L.（1998）.  On；the  convergence  of  Markovian  stochastic；algorithms  with  rapidly  decreasing  ergodicity  rates.  In  Stochastics  and
          關鍵詞：
        - 摘要：Yu，D.，Wang，S.，and  Deng，L.（2010）.  Sequential  labeling  using；deep-structured conditional randomfields. IEEE Journal of Selected Topics in；Signal Processing .
          關鍵詞：
        - 摘要：Zaremba，W.  and  Sutskever，I.（2014）.  Learning  to  execute.  arXiv；1410.4615.
          關鍵詞：
        - 摘要：Zaremba，W.  and  Sutskever，I.（2015）.  Reinforcement  learning  neural；Turing machines. arXiv:1505.00521 .
          關鍵詞：
        - 摘要：Zaslavsky，T.（1975）. Facing Up to Arrangements: Face-Count Formulas；for Partitions of Space by Hyperplanes . Number no. 154 in Memoirs of the；American Mathematical Society. American Mathematical Society.
          關鍵詞：
        - 摘要：Zeiler，M.  D.  and  Fergus，R.（2014）.  Visualizing  and  understanding；convolutional networks. In ECCV'14 .
          關鍵詞：
        - 摘要：Zeiler，M. D.，Ranzato，M.，Monga，R.，Mao，M.，Yang，K.，Le，；Q.，Nguyen，P.，Senior，A.，Vanhoucke，V.，Dean，J.，and；Hinton，G.  E.（2013）.  On  rectified  linear  units  for  speech  processing.  In
          關鍵詞：
        - 摘要：Zhou，B.，Khosla，A.，Lapedriza，A.，Oliva，A.，and  Torralba，A.；（2015）.  Object  detectors  emerge  in  deep  scene  CNNs.  ICLR'2015，；arXiv:1412.6856.
          關鍵詞：
        - 摘要：Zhou，J.  and  Troyanskaya，O.  G.（2014）.  Deep  supervised  and；convolutional  generative  stochastic  network  for  protein  secondary  structure；prediction. In ICML'2014 .
          關鍵詞：
        - 摘要：Zhou，Y. and Chellappa，R.（1988）. Computation of opticalflow using a；International；neural  network.
          關鍵詞：
        - 摘要：In  Neural  Networks，1988.，IEEE
          關鍵詞：
        - 摘要：Zöhrer，M.  and  Pernkopf，F.（2014）.  General  stochastic  networks  for；classification. In NIPS'2014 .
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；索引
          關鍵詞：索引
        - 摘要：绝对值整流absolute value rectification
          關鍵詞：绝对值整流
        - 摘要：准确率accuracy
          關鍵詞：准确率
        - 摘要：声学acoustic
          關鍵詞：声学
        - 摘要：激活函数activation function
          關鍵詞：激活函数
        - 摘要：AdaGrad AdaGrad
          關鍵詞：
        - 摘要：对抗adversarial
          關鍵詞：对抗
        - 摘要：对抗样本adversarial example
          關鍵詞：对抗样本
        - 摘要：对抗训练adversarial training
          關鍵詞：对抗训练
        - 摘要：几乎处处almost everywhere
          關鍵詞：几乎处处
        - 摘要：几乎必然almost sure
          關鍵詞：几乎必然
        - 摘要：几乎必然收敛almost sure convergence
          關鍵詞：几乎必然收敛
        - 摘要：选择性剪接数据集alternative splicing dataset
          關鍵詞：选择性剪接数据集
        - 摘要：原始采样ancestral sampling
          關鍵詞：原始采样
        - 摘要：退火重要采样annealed importance sampling
          關鍵詞：退火重要采样
        - 摘要：专用集成电路application-specific integrated circuit
          關鍵詞：专用集成电路
        - 摘要：近似贝叶斯计算approximate Bayesian computa-tion
          關鍵詞：近似贝叶斯计算
        - 摘要：近似推断approximate inference
          關鍵詞：近似推断
        - 摘要：架构architecture
          關鍵詞：架构
        - 摘要：人工智能artificial intelligence
          關鍵詞：人工智能
        - 摘要：人工神经网络artificial neural network
          關鍵詞：人工神经网络
        - 摘要：渐近无偏asymptotically unbiased
          關鍵詞：渐近无偏
        - 摘要：异步随机梯度下降Asynchoronous Stochastic Gradient Descent
          關鍵詞：异步随机梯度下降
        - 摘要：异步asynchronous
          關鍵詞：异步
        - 摘要：注意力机制attention mechanism
          關鍵詞：注意力机制
        - 摘要：属性attribute
          關鍵詞：属性
        - 摘要：自编码器autoencoder
          關鍵詞：自编码器
        - 摘要：自动微分automatic differentiation
          關鍵詞：自动微分
        - 摘要：自动语音识别Automatic Speech Recognition
          關鍵詞：自动语音识别
        - 摘要：自回归网络auto-regressive network
          關鍵詞：自回归网络
        - 摘要：反向传播back propagation
          關鍵詞：反向传播
        - 摘要：回退back-off
          關鍵詞：回退
        - 摘要：反向传播backprop
          關鍵詞：反向传播
        - 摘要：通过时间反向传播back-propagation through time
          關鍵詞：通过时间反向传播
        - 摘要：词袋bag of words
          關鍵詞：词袋
        - 摘要：Bagging bootstrap aggregating
          關鍵詞：
        - 摘要：bandit bandit
          關鍵詞：
        - 摘要：批量batch
          關鍵詞：批量
        - 摘要：批标准化batch normalization
          關鍵詞：批标准化
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；贝叶斯误差Bayes error
          關鍵詞：贝叶斯误差
        - 摘要：贝叶斯规则Bayes' rule
          關鍵詞：贝叶斯规则
        - 摘要：贝叶斯推断Bayesian inference
          關鍵詞：贝叶斯推断
        - 摘要：贝叶斯网络Bayesian network
          關鍵詞：贝叶斯网络
        - 摘要：贝叶斯概率Bayesian probability
          關鍵詞：贝叶斯概率
        - 摘要：贝叶斯统计Bayesian statistics
          關鍵詞：贝叶斯统计
        - 摘要：基准bechmark
          關鍵詞：基准
        - 摘要：信念网络belief network
          關鍵詞：信念网络
        - 摘要：Bernoulli分布Bernoulli distribution
          關鍵詞：分布
        - 摘要：基准baseline
          關鍵詞：基准
        - 摘要：BFGS BFGS
          關鍵詞：
        - 摘要：偏置bias in affine function
          關鍵詞：偏置
        - 摘要：偏差bias in statistics
          關鍵詞：偏差
        - 摘要：有偏biased
          關鍵詞：有偏
        - 摘要：有偏重要采样biased importance sampling
          關鍵詞：有偏重要采样
        - 摘要：偏差biass
          關鍵詞：偏差
        - 摘要：二元语法bigram
          關鍵詞：二元语法
        - 摘要：二元关系binary relation
          關鍵詞：二元关系
        - 摘要：二值稀疏编码binary sparse coding
          關鍵詞：二值稀疏编码
        - 摘要：比特bit
          關鍵詞：比特
        - 摘要：块坐标下降block coordinate descent
          關鍵詞：块坐标下降
        - 摘要：块吉布斯采样block Gibbs Sampling
          關鍵詞：块吉布斯采样
        - 摘要：玻尔兹曼分布Boltzmann distribution
          關鍵詞：玻尔兹曼分布
        - 摘要：玻尔兹曼机Boltzmann Machine
          關鍵詞：玻尔兹曼机
        - 摘要：Boosting Boosting
          關鍵詞：
        - 摘要：桥式采样bridge sampling
          關鍵詞：桥式采样
        - 摘要：广播broadcasting
          關鍵詞：广播
        - 摘要：磨合Burning-in
          關鍵詞：磨合
        - 摘要：变分法calculus of variations
          關鍵詞：变分法
        - 摘要：容量capacity
          關鍵詞：容量
        - 摘要：级联cascade
          關鍵詞：级联
        - 摘要：灾难遗忘catastrophic forgetting
          關鍵詞：灾难遗忘
        - 摘要：范畴分布categorical distribution
          關鍵詞：范畴分布
        - 摘要：因果因子causal factor
          關鍵詞：因果因子
        - 摘要：因果模型causal modeling
          關鍵詞：因果模型
        - 摘要：中心差分centered difference
          關鍵詞：中心差分
        - 摘要：中心极限定理central limit theorem
          關鍵詞：中心极限定理
        - 摘要：链式法则chain rule
          關鍵詞：链式法则
        - 摘要：混沌chaos
          關鍵詞：混沌
        - 摘要：弦chord
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；弦图chordal graph
          關鍵詞：弦图
        - 摘要：梯度截断clip gradient
          關鍵詞：梯度截断
        - 摘要：截断梯度clipping the gradient
          關鍵詞：截断梯度
        - 摘要：团clique
          關鍵詞：
        - 摘要：团势能clique potential
          關鍵詞：团势能
        - 摘要：闭式解closed form solution
          關鍵詞：闭式解
        - 摘要：级联coalesced
          關鍵詞：级联
        - 摘要：编码code
          關鍵詞：编码
        - 摘要：协同过滤collaborativefiltering
          關鍵詞：协同过滤
        - 摘要：列column
          關鍵詞：
        - 摘要：列空间column space
          關鍵詞：列空间
        - 摘要：共因common cause
          關鍵詞：共因
        - 摘要：完全图complete graph
          關鍵詞：完全图
        - 摘要：复杂细胞complex cell
          關鍵詞：复杂细胞
        - 摘要：计算图computational graph
          關鍵詞：计算图
        - 摘要：计算机视觉Computer Vision
          關鍵詞：计算机视觉
        - 摘要：概念漂移concept drift
          關鍵詞：概念漂移
        - 摘要：条件计算conditional computation
          關鍵詞：条件计算
        - 摘要：条件概率conditional probability
          關鍵詞：条件概率
        - 摘要：条件独立的conditionally independent
          關鍵詞：条件独立的
        - 摘要：共轭conjugate
          關鍵詞：共轭
        - 摘要：共轭方向conjugate directions
          關鍵詞：共轭方向
        - 摘要：共轭梯度conjugate gradient
          關鍵詞：共轭梯度
        - 摘要：联结主义connectionism
          關鍵詞：联结主义
        - 摘要：一致性consistency
          關鍵詞：一致性
        - 摘要：约束优化constrained optimization
          關鍵詞：约束优化
        - 摘要：特定环境下的独立context-specific independences
          關鍵詞：特定环境下的独立
        - 摘要：contextual bandit contextual bandit
          關鍵詞：
        - 摘要：延拓法continuation method
          關鍵詞：延拓法
        - 摘要：收缩contractive
          關鍵詞：收缩
        - 摘要：收缩自编码器contractive autoencoder
          關鍵詞：收缩自编码器
        - 摘要：对比散度contrastive divergence
          關鍵詞：对比散度
        - 摘要：凸优化Convex optimization
          關鍵詞：凸优化
        - 摘要：卷积convolution
          關鍵詞：卷积
        - 摘要：卷积玻尔兹曼机Convolutional Boltzmann Machine
          關鍵詞：卷积玻尔兹曼机
        - 摘要：卷积网络convolutional net
          關鍵詞：卷积网络
        - 摘要：卷积神经网络convolutional neural network
          關鍵詞：卷积神经网络
        - 摘要：坐标上升coordinate ascent
          關鍵詞：坐标上升
        - 摘要：坐标下降coordinate descent
          關鍵詞：坐标下降
        - 摘要：共父coparent
          關鍵詞：共父
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；相关系数correlation
          關鍵詞：相关系数
        - 摘要：代价cost
          關鍵詞：代价
        - 摘要：代价函数cost function
          關鍵詞：代价函数
        - 摘要：协方差covariance
          關鍵詞：协方差
        - 摘要：协方差矩阵covariance matrix
          關鍵詞：协方差矩阵
        - 摘要：协方差RBM covariance RBM
          關鍵詞：协方差
        - 摘要：覆盖coverage
          關鍵詞：覆盖
        - 摘要：准则criterion
          關鍵詞：准则
        - 摘要：临界点critical point
          關鍵詞：临界点
        - 摘要：临界温度critical temperatures
          關鍵詞：临界温度
        - 摘要：互相关函数cross-correlation
          關鍵詞：互相关函数
        - 摘要：交叉熵cross-entropy
          關鍵詞：交叉熵
        - 摘要：累积函数cumulative function
          關鍵詞：累积函数
        - 摘要：课程学习curriculum learning
          關鍵詞：课程学习
        - 摘要：维数灾难curse of dimensionality
          關鍵詞：维数灾难
        - 摘要：曲率curvature
          關鍵詞：曲率
        - 摘要：控制论cybernetics
          關鍵詞：控制论
        - 摘要：衰减damping
          關鍵詞：衰减
        - 摘要：数据生成分布data generating distribution
          關鍵詞：数据生成分布
        - 摘要：数据生成过程data generating process
          關鍵詞：数据生成过程
        - 摘要：数据并行data parallelism
          關鍵詞：数据并行
        - 摘要：数据点data point
          關鍵詞：数据点
        - 摘要：数据集dataset
          關鍵詞：数据集
        - 摘要：数据集增强dataset augmentation
          關鍵詞：数据集增强
        - 摘要：决策树decision tree
          關鍵詞：决策树
        - 摘要：解码器decoder
          關鍵詞：解码器
        - 摘要：分解decompose
          關鍵詞：分解
        - 摘要：深度信念网络deep belief network
          關鍵詞：深度信念网络
        - 摘要：深度玻尔兹曼机Deep Boltzmann Machine
          關鍵詞：深度玻尔兹曼机
        - 摘要：深度回路deep circuit
          關鍵詞：深度回路
        - 摘要：深度前馈网络deep feedforward network
          關鍵詞：深度前馈网络
        - 摘要：深度生成模型deep generative model
          關鍵詞：深度生成模型
        - 摘要：深度学习deep learning
          關鍵詞：深度学习
        - 摘要：深度模型deep model
          關鍵詞：深度模型
        - 摘要：深度网络deep network
          關鍵詞：深度网络
        - 摘要：点积dot product
          關鍵詞：点积
        - 摘要：双反向传播double backprop
          關鍵詞：双反向传播
        - 摘要：双重分块循环矩阵doubly block circulant matrix
          關鍵詞：双重分块循环矩阵
        - 摘要：降采样downsampling
          關鍵詞：降采样
        - 摘要：Dropout Dropout
          關鍵詞：
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Dropout Boosting Dropout Boosting
          關鍵詞：
        - 摘要：d-分离d-separation
          關鍵詞：分离
        - 摘要：动态规划dynamic programming
          關鍵詞：动态规划
        - 摘要：动态结构dynamic structure
          關鍵詞：动态结构
        - 摘要：提前终止early stopping
          關鍵詞：提前终止
        - 摘要：回声状态网络echo state network
          關鍵詞：回声状态网络
        - 摘要：有效容量effective capacity
          關鍵詞：有效容量
        - 摘要：特征分解eigendecomposition
          關鍵詞：特征分解
        - 摘要：特征值eigenvalue
          關鍵詞：特征值
        - 摘要：特征向量eigenvector
          關鍵詞：特征向量
        - 摘要：基本单位向量elementary basis vectors
          關鍵詞：基本单位向量
        - 摘要：元素对应乘积element-wise product
          關鍵詞：元素对应乘积
        - 摘要：嵌入embedding
          關鍵詞：嵌入
        - 摘要：经验分布empirical distribution
          關鍵詞：经验分布
        - 摘要：经验频率empirical frequency
          關鍵詞：经验频率
        - 摘要：经验风险empirical risk
          關鍵詞：经验风险
        - 摘要：经验风险最小化empirical risk minimization
          關鍵詞：经验风险最小化
        - 摘要：编码器encoder
          關鍵詞：编码器
        - 摘要：端到端的end-to-end
          關鍵詞：端到端的
        - 摘要：能量函数energy function
          關鍵詞：能量函数
        - 摘要：基于能量的模型Energy-based model
          關鍵詞：基于能量的模型
        - 摘要：集成ensemble
          關鍵詞：集成
        - 摘要：集成学习ensemble learning
          關鍵詞：集成学习
        - 摘要：轮epoch
          關鍵詞：
        - 摘要：轮数epochs
          關鍵詞：轮数
        - 摘要：等式约束equality constraint
          關鍵詞：等式约束
        - 摘要：均衡分布Equilibrium Distribution
          關鍵詞：均衡分布
        - 摘要：等变equivariance
          關鍵詞：等变
        - 摘要：等变表示equivariant representations
          關鍵詞：等变表示
        - 摘要：误差条error bar
          關鍵詞：误差条
        - 摘要：误差函数error function
          關鍵詞：误差函数
        - 摘要：误差度量error metric
          關鍵詞：误差度量
        - 摘要：错误率error rate
          關鍵詞：错误率
        - 摘要：估计量estimator
          關鍵詞：估计量
        - 摘要：欧几里得范数Euclidean norm
          關鍵詞：欧几里得范数
        - 摘要：欧拉-拉格朗日方程Euler-Lagrange Equation
          關鍵詞：欧拉, 拉格朗日方程
        - 摘要：证据下界evidence lower bound
          關鍵詞：证据下界
        - 摘要：样本example
          關鍵詞：样本
        - 摘要：额外误差excess error
          關鍵詞：额外误差
        - 摘要：期望expectation
          關鍵詞：期望
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；期望最大化expectation maximization
          關鍵詞：期望最大化
        - 摘要：E步expectation step
          關鍵詞：
        - 摘要：期望值expected value
          關鍵詞：期望值
        - 摘要：经验experience，E
          關鍵詞：经验
        - 摘要：专家网络expert network
          關鍵詞：专家网络
        - 摘要：相消解释explaining away
          關鍵詞：相消解释
        - 摘要：相消解释作用explaining away effect
          關鍵詞：相消解释作用
        - 摘要：解释因子explanatory factort
          關鍵詞：解释因子
        - 摘要：梯度爆炸exploding gradient
          關鍵詞：梯度爆炸
        - 摘要：开发exploitation
          關鍵詞：开发
        - 摘要：探索exploration
          關鍵詞：探索
        - 摘要：指数分布exponential distribution
          關鍵詞：指数分布
        - 摘要：因子factor
          關鍵詞：因子
        - 摘要：因子分析factor analysis
          關鍵詞：因子分析
        - 摘要：因子图factor graph
          關鍵詞：因子图
        - 摘要：因子factorial
          關鍵詞：因子
        - 摘要：分解factorization
          關鍵詞：分解
        - 摘要：分解的factorized
          關鍵詞：分解的
        - 摘要：变差因素factors of variation
          關鍵詞：变差因素
        - 摘要：快速Dropout fast dropout
          關鍵詞：快速
        - 摘要：快速持续性对比散度fast persistent contrastive di-vergence
          關鍵詞：快速持续性对比散度
        - 摘要：可行feasible
          關鍵詞：可行
        - 摘要：特征feature
          關鍵詞：特征
        - 摘要：特征提取器feature extractor
          關鍵詞：特征提取器
        - 摘要：特征映射feature map
          關鍵詞：特征映射
        - 摘要：特征选择feature selection
          關鍵詞：特征选择
        - 摘要：反馈feedback
          關鍵詞：反馈
        - 摘要：前向feedforward
          關鍵詞：前向
        - 摘要：前馈分类器feedforward classifier
          關鍵詞：前馈分类器
        - 摘要：前馈网络feedforward network
          關鍵詞：前馈网络
        - 摘要：前馈神经网络feedforward neural network
          關鍵詞：前馈神经网络
        - 摘要：现场可编程门阵列field programmable gated array
          關鍵詞：现场可编程门阵列
        - 摘要：精调fine-tune
          關鍵詞：精调
        - 摘要：精调fine-tuning
          關鍵詞：精调
        - 摘要：有限差分finite difference
          關鍵詞：有限差分
        - 摘要：第一层first layer
          關鍵詞：第一层
        - 摘要：不动点方程fixed point equation
          關鍵詞：不动点方程
        - 摘要：定点运算fixed-point arithmetic
          關鍵詞：定点运算
        - 摘要：翻转flip
          關鍵詞：翻转
        - 摘要：浮点运算float-point arithmetic
          關鍵詞：浮点运算
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；遗忘门forget gate
          關鍵詞：遗忘门
        - 摘要：前向传播forward propagation
          關鍵詞：前向传播
        - 摘要：傅里叶变换Fourier transform
          關鍵詞：傅里叶变换
        - 摘要：中央凹fovea
          關鍵詞：中央凹
        - 摘要：自由能free energy
          關鍵詞：自由能
        - 摘要：频率派概率frequentist probability
          關鍵詞：频率派概率
        - 摘要：频率派统计frequentist statistics
          關鍵詞：频率派统计
        - 摘要：Frobenius范数Frobenius norm
          關鍵詞：范数
        - 摘要：F分数F-score
          關鍵詞：分数
        - 摘要：全full
          關鍵詞：
        - 摘要：泛函functional
          關鍵詞：泛函
        - 摘要：泛函导数functional derivative
          關鍵詞：泛函导数
        - 摘要：Gabor函数Gabor function
          關鍵詞：函数
        - 摘要：Gamma分布Gamma distribution
          關鍵詞：分布
        - 摘要：门控gated
          關鍵詞：门控
        - 摘要：门控循环网络gated recurrent net
          關鍵詞：门控循环网络
        - 摘要：门控循环单元gated recurrent unit
          關鍵詞：门控循环单元
        - 摘要：门控RNN gated RNN
          關鍵詞：门控
        - 摘要：选通器gater
          關鍵詞：选通器
        - 摘要：高斯分布Gaussian distribution
          關鍵詞：高斯分布
        - 摘要：高斯核Gaussian kernel
          關鍵詞：高斯核
        - 摘要：高斯混合模型Gaussian Mixture Model
          關鍵詞：高斯混合模型
        - 摘要：高斯混合体Gaussian mixtures
          關鍵詞：高斯混合体
        - 摘要：高斯输出分布Gaussian output distribution
          關鍵詞：高斯输出分布
        - 摘要：高斯RBM Gaussian RBM
          關鍵詞：高斯
        - 摘要：Gaussian-Bernoulli RBM Gaussian-Bernoulli RBM
          關鍵詞：
        - 摘要：通用GPU general purpose GPU
          關鍵詞：通用
        - 摘要：泛化generalization
          關鍵詞：泛化
        - 摘要：泛化误差generalization error
          關鍵詞：泛化误差
        - 摘要：广义函数generalized function
          關鍵詞：广义函数
        - 摘要：广义Lagrange函数generalized Lagrange function
          關鍵詞：函数, 广义
        - 摘要：广义Lagrangian generalized Lagrangian
          關鍵詞：广义
        - 摘要：广义伪似然generalized pseudolikelihood
          關鍵詞：广义伪似然
        - 摘要：广义伪似然估计generalized pseudolikelihood esti-mator
          關鍵詞：广义伪似然估计
        - 摘要：广义得分匹配generalized score matching
          關鍵詞：广义得分匹配
        - 摘要：生成式对抗框架generative adversarial framework
          關鍵詞：生成式对抗框架
        - 摘要：生成式对抗网络generative adversarial network
          關鍵詞：生成式对抗网络
        - 摘要：生成模型generative model
          關鍵詞：生成模型
        - 摘要：生成式建模generative modeling
          關鍵詞：生成式建模
        - 摘要：生成矩匹配网络generative moment matching net-work
          關鍵詞：生成矩匹配网络
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；生成随机网络generative stochastic network
          關鍵詞：生成随机网络
        - 摘要：生成器网络generator network
          關鍵詞：生成器网络
        - 摘要：吉布斯分布Gibbs distribution
          關鍵詞：吉布斯分布
        - 摘要：Gibbs采样Gibbs Sampling
          關鍵詞：采样
        - 摘要：吉布斯步数Gibbs steps
          關鍵詞：吉布斯步数
        - 摘要：全局对比度归一化Global contrast normalization
          關鍵詞：全局对比度归一化
        - 摘要：全局极小值global minima
          關鍵詞：全局极小值
        - 摘要：全局最小点global minimum
          關鍵詞：全局最小点
        - 摘要：梯度gradient
          關鍵詞：梯度
        - 摘要：梯度上升gradient ascent
          關鍵詞：梯度上升
        - 摘要：梯度截断gradient clipping
          關鍵詞：梯度截断
        - 摘要：梯度下降gradient descent
          關鍵詞：梯度下降
        - 摘要：图模型graphical model
          關鍵詞：图模型
        - 摘要：图形处理器Graphics Processing Unit
          關鍵詞：图形处理器
        - 摘要：贪心greedy
          關鍵詞：贪心
        - 摘要：贪心算法greedy algorithm
          關鍵詞：贪心算法
        - 摘要：贪心逐层预训练greedy layer-wise pretraining
          關鍵詞：贪心逐层预训练
        - 摘要：贪心逐层训练greedy layer-wise training
          關鍵詞：贪心逐层训练
        - 摘要：贪心逐层无监督预训练greedy layer-wise unsuper-vised pretraining
          關鍵詞：贪心逐层无监督预训练
        - 摘要：贪心监督预训练greedy supervised pretraining
          關鍵詞：贪心监督预训练
        - 摘要：贪心无监督预训练greedy unsupervised pretraining
          關鍵詞：贪心无监督预训练
        - 摘要：网格搜索grid search
          關鍵詞：网格搜索
        - 摘要：Hadamard乘积Hadamard product
          關鍵詞：乘积
        - 摘要：汉明距离Hamming distance
          關鍵詞：汉明距离
        - 摘要：硬专家混合体hard mixture of experts
          關鍵詞：硬专家混合体
        - 摘要：硬双曲正切函数hard tanh
          關鍵詞：硬双曲正切函数
        - 摘要：簧风琴harmonium
          關鍵詞：簧风琴
        - 摘要：哈里斯链Harris Chain
          關鍵詞：哈里斯链
        - 摘要：Helmholtz机Helmholtz machine
          關鍵詞：
        - 摘要：Hessian Hessian
          關鍵詞：
        - 摘要：异方差heteroscedastic
          關鍵詞：异方差
        - 摘要：隐藏层hidden layer
          關鍵詞：隐藏层
        - 摘要：隐马尔可夫模型Hidden Markov Model
          關鍵詞：隐马尔可夫模型
        - 摘要：隐藏单元hidden unit
          關鍵詞：隐藏单元
        - 摘要：隐藏变量hidden variable
          關鍵詞：隐藏变量
        - 摘要：爬山hill climbing
          關鍵詞：爬山
        - 摘要：超参数hyperparameter
          關鍵詞：超参数
        - 摘要：超参数优化hyperparameter optimization
          關鍵詞：超参数优化
        - 摘要：假设空间hypothesis space
          關鍵詞：假设空间
        - 摘要：同分布的identically distributed
          關鍵詞：同分布的
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；可辨认的identifiable
          關鍵詞：可辨认的
        - 摘要：单位矩阵identity matrix
          關鍵詞：单位矩阵
        - 摘要：独立同分布假设i.i.d. assumption
          關鍵詞：独立同分布假设
        - 摘要：病态ill conditioning
          關鍵詞：病态
        - 摘要：不道德immorality
          關鍵詞：不道德
        - 摘要：重要采样Importance Sampling
          關鍵詞：重要采样
        - 摘要：相互独立的independent
          關鍵詞：相互独立的
        - 摘要：独立成分分析independent component analysis
          關鍵詞：独立成分分析
        - 摘要：独立同分布independent identically distributed
          關鍵詞：独立同分布
        - 摘要：独立子空间分析independent subspace analysis
          關鍵詞：独立子空间分析
        - 摘要：索引index of matrix
          關鍵詞：索引
        - 摘要：不等式约束inequality constraint
          關鍵詞：不等式约束
        - 摘要：推断inference
          關鍵詞：推断
        - 摘要：无限infinite
          關鍵詞：无限
        - 摘要：信息检索information retrieval
          關鍵詞：信息检索
        - 摘要：内积inner product
          關鍵詞：内积
        - 摘要：输入input
          關鍵詞：输入
        - 摘要：输入分布input distribution
          關鍵詞：输入分布
        - 摘要：干预查询intervention query
          關鍵詞：干预查询
        - 摘要：不变invariant
          關鍵詞：不变
        - 摘要：求逆invert
          關鍵詞：求逆
        - 摘要：Isomap Isomap
          關鍵詞：
        - 摘要：各向同性isotropic
          關鍵詞：各向同性
        - 摘要：Jacobian Jacobian
          關鍵詞：
        - 摘要：Jacobian矩阵Jacobian matrix
          關鍵詞：矩阵
        - 摘要：联合概率分布joint probability distribution
          關鍵詞：联合概率分布
        - 摘要：Karush-Kuhn-Tucker Karush-Kuhn-Tucker
          關鍵詞：
        - 摘要：核函数kernel function
          關鍵詞：核函数
        - 摘要：核机器kernel machine
          關鍵詞：核机器
        - 摘要：核方法kernel method
          關鍵詞：核方法
        - 摘要：核技巧kernel trick
          關鍵詞：核技巧
        - 摘要：KL散度KL divergence
          關鍵詞：散度
        - 摘要：知识库knowledge base
          關鍵詞：知识库
        - 摘要：知识图谱knowledge graph
          關鍵詞：知识图谱
        - 摘要：Krylov方法Krylov method
          關鍵詞：方法
        - 摘要：KL散度Kullback-Leibler（KL） divergence
          關鍵詞：散度
        - 摘要：标签label
          關鍵詞：标签
        - 摘要：标注labeled
          關鍵詞：标注
        - 摘要：拉格朗日乘子Lagrange multiplier
          關鍵詞：拉格朗日乘子
        - 摘要：语言模型language model
          關鍵詞：语言模型
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Laplace分布Laplace distribution
          關鍵詞：分布
        - 摘要：大学习步骤large learning step
          關鍵詞：大学习步骤
        - 摘要：潜在latent
          關鍵詞：潜在
        - 摘要：潜层latent layer
          關鍵詞：潜层
        - 摘要：潜变量latent variable
          關鍵詞：潜变量
        - 摘要：大数定理Law of large number
          關鍵詞：大数定理
        - 摘要：逐层的layer-wise
          關鍵詞：逐层的
        - 摘要：L-BFGS L-BFGS
          關鍵詞：
        - 摘要：渗漏整流线性单元Leaky ReLU
          關鍵詞：渗漏整流线性单元
        - 摘要：渗漏单元leaky unit
          關鍵詞：渗漏单元
        - 摘要：学成learned
          關鍵詞：学成
        - 摘要：学习近似推断learned approximate inference
          關鍵詞：学习近似推断
        - 摘要：学习器learner
          關鍵詞：学习器
        - 摘要：学习率learning rate
          關鍵詞：学习率
        - 摘要：勒贝格可积Lebesgue-integrable
          關鍵詞：勒贝格可积
        - 摘要：左特征向量left eigenvector
          關鍵詞：左特征向量
        - 摘要：左奇异向量left singular vector
          關鍵詞：左奇异向量
        - 摘要：莱布尼兹法则Leibniz's rule
          關鍵詞：莱布尼兹法则
        - 摘要：似然likelihood
          關鍵詞：似然
        - 摘要：线搜索line search
          關鍵詞：线搜索
        - 摘要：线性自回归网络linear auto-regressive network
          關鍵詞：线性自回归网络
        - 摘要：线性分类器linear classifier
          關鍵詞：线性分类器
        - 摘要：线性组合linear combination
          關鍵詞：线性组合
        - 摘要：线性相关linear dependence
          關鍵詞：线性相关
        - 摘要：线性因子模型linear factor model
          關鍵詞：线性因子模型
        - 摘要：线性模型linear model
          關鍵詞：线性模型
        - 摘要：线性回归linear regression
          關鍵詞：线性回归
        - 摘要：线性阈值单元linear threshold units
          關鍵詞：线性阈值单元
        - 摘要：线性无关linearly independent
          關鍵詞：线性无关
        - 摘要：链接预测link prediction
          關鍵詞：链接预测
        - 摘要：链接重要采样linked importance sampling
          關鍵詞：链接重要采样
        - 摘要：Lipschitz Lipschitz
          關鍵詞：
        - 摘要：Lipschitz常数Lipschitz constant
          關鍵詞：常数
        - 摘要：Lipschitz连续Lipschitz continuous
          關鍵詞：连续
        - 摘要：流体状态机liquid state machine
          關鍵詞：流体状态机
        - 摘要：局部条件概率分布local conditional probability dis-tribution
          關鍵詞：局部条件概率分布
        - 摘要：局部不变性先验local constancy prior
          關鍵詞：局部不变性先验
        - 摘要：局部对比度归一化local contrast normalization
          關鍵詞：局部对比度归一化
        - 摘要：局部下降local descent
          關鍵詞：局部下降
        - 摘要：局部核local kernel
          關鍵詞：局部核
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；局部极大值local maxima
          關鍵詞：局部极大值
        - 摘要：局部极大点local maximum
          關鍵詞：局部极大点
        - 摘要：局部极小值local minima
          關鍵詞：局部极小值
        - 摘要：局部极小点local minimum
          關鍵詞：局部极小点
        - 摘要：对数尺度logarithmic scale
          關鍵詞：对数尺度
        - 摘要：逻辑回归logistic regression
          關鍵詞：逻辑回归
        - 摘要：logistic sigmoid logistic sigmoid
          關鍵詞：
        - 摘要：分对数logit
          關鍵詞：分对数
        - 摘要：对数线性模型log-linear model
          關鍵詞：对数线性模型
        - 摘要：长短期记忆long short-term memory
          關鍵詞：长短期记忆
        - 摘要：长期依赖long-term dependency
          關鍵詞：长期依赖
        - 摘要：环loop
          關鍵詞：
        - 摘要：环状信念传播loopy belief propagation
          關鍵詞：环状信念传播
        - 摘要：损失loss
          關鍵詞：损失
        - 摘要：损失函数loss function
          關鍵詞：损失函数
        - 摘要：机器学习machine learning
          關鍵詞：机器学习
        - 摘要：机器学习模型machine learning model
          關鍵詞：机器学习模型
        - 摘要：机器翻译machine translation
          關鍵詞：机器翻译
        - 摘要：主对角线main diagonal
          關鍵詞：主对角线
        - 摘要：流形manifold
          關鍵詞：流形
        - 摘要：流形假设manifold hypothesis
          關鍵詞：流形假设
        - 摘要：流形学习manifold learning
          關鍵詞：流形学习
        - 摘要：边缘概率分布marginal probability distribution
          關鍵詞：边缘概率分布
        - 摘要：马尔可夫链Markov Chain
          關鍵詞：马尔可夫链
        - 摘要：马尔可夫链蒙特卡罗Markov Chain Monte Carlo
          關鍵詞：马尔可夫链蒙特卡罗
        - 摘要：马尔可夫网络Markov network
          關鍵詞：马尔可夫网络
        - 摘要：马尔可夫随机场Markov randomfield
          關鍵詞：马尔可夫随机场
        - 摘要：掩码mask
          關鍵詞：掩码
        - 摘要：矩阵matrix
          關鍵詞：矩阵
        - 摘要：矩阵逆matrix inversion
          關鍵詞：矩阵逆
        - 摘要：矩阵乘积matrix product
          關鍵詞：矩阵乘积
        - 摘要：最大范数max norm
          關鍵詞：最大范数
        - 摘要：池pool
          關鍵詞：
        - 摘要：最大池化max pooling
          關鍵詞：最大池化
        - 摘要：极大值maxima
          關鍵詞：极大值
        - 摘要：M步maximization step
          關鍵詞：
        - 摘要：最大后验Maximum A Posteriori
          關鍵詞：最大后验
        - 摘要：最大似然maximum likelihood
          關鍵詞：最大似然
        - 摘要：最大似然估计maximum likelihood estimation
          關鍵詞：最大似然估计
        - 摘要：最大平均偏差maximum mean discrepancy
          關鍵詞：最大平均偏差
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；maxout maxout
          關鍵詞：
        - 摘要：maxout单元maxout unit
          關鍵詞：单元
        - 摘要：平均绝对误差mean absolute error
          關鍵詞：平均绝对误差
        - 摘要：均值和协方差RBM mean and covariance RBM
          關鍵詞：均值和协方差
        - 摘要：学生t 分布均值乘积mean product of Student t-distribution
          關鍵詞：分布均值乘积, 学生
        - 摘要：均方误差mean squared error
          關鍵詞：均方误差
        - 摘要：均值-协方差RBM mean-covariance restricted Boltzmann machine
          關鍵詞：协方差, 均值
        - 摘要：均匀场meanfield
          關鍵詞：均匀场
        - 摘要：均值场mean-field
          關鍵詞：均值场
        - 摘要：测度论measure theory
          關鍵詞：测度论
        - 摘要：零测度measure zero
          關鍵詞：零测度
        - 摘要：记忆网络memory network
          關鍵詞：记忆网络
        - 摘要：信息传输message passing
          關鍵詞：信息传输
        - 摘要：小批量minibatch
          關鍵詞：小批量
        - 摘要：小批量随机minibatch stochastic
          關鍵詞：小批量随机
        - 摘要：极小值minima
          關鍵詞：极小值
        - 摘要：极小点minimum
          關鍵詞：极小点
        - 摘要：混合Mixing
          關鍵詞：混合
        - 摘要：混合时间Mixing Time
          關鍵詞：混合时间
        - 摘要：混合密度网络mixture density network
          關鍵詞：混合密度网络
        - 摘要：混合分布mixture distribution
          關鍵詞：混合分布
        - 摘要：专家混合体mixture of experts
          關鍵詞：专家混合体
        - 摘要：模态modality
          關鍵詞：模态
        - 摘要：峰值mode
          關鍵詞：峰值
        - 摘要：模型model
          關鍵詞：模型
        - 摘要：模型平均model averaging
          關鍵詞：模型平均
        - 摘要：模型压缩model compression
          關鍵詞：模型压缩
        - 摘要：模型可辨识性model identifiability
          關鍵詞：模型可辨识性
        - 摘要：模型并行model parallelism
          關鍵詞：模型并行
        - 摘要：矩moment
          關鍵詞：
        - 摘要：矩匹配moment matching
          關鍵詞：矩匹配
        - 摘要：动量momentum
          關鍵詞：动量
        - 摘要：蒙特卡罗Monte Carlo
          關鍵詞：蒙特卡罗
        - 摘要：Moore-Penrose伪逆Moore-Penrose pseudoinverse
          關鍵詞：伪逆
        - 摘要：道德化moralization
          關鍵詞：道德化
        - 摘要：道德图moralized graph
          關鍵詞：道德图
        - 摘要：多层感知机multilayer perceptron
          關鍵詞：多层感知机
        - 摘要：多峰值multimodal
          關鍵詞：多峰值
        - 摘要：多模态学习multimodal learning
          關鍵詞：多模态学习
        - 摘要：多项式分布multinomial distribution
          關鍵詞：多项式分布
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；Multinoulli分布multinoulli distribution
          關鍵詞：分布
        - 摘要：多预测深度玻尔兹曼机multi-prediction deep Boltzmann machine
          關鍵詞：多预测深度玻尔兹曼机
        - 摘要：多任务学习multitask learning
          關鍵詞：多任务学习
        - 摘要：多维正态分布multivariate normal distribution
          關鍵詞：多维正态分布
        - 摘要：朴素贝叶斯naive Bayes
          關鍵詞：朴素贝叶斯
        - 摘要：奈特nats
          關鍵詞：奈特
        - 摘要：自然语言处理Natural Language Processing
          關鍵詞：自然语言处理
        - 摘要：最近邻nearest neighbor
          關鍵詞：最近邻
        - 摘要：最近邻图nearest neighbor graph
          關鍵詞：最近邻图
        - 摘要：最近邻回归nearest neighbor regression
          關鍵詞：最近邻回归
        - 摘要：负定negative definite
          關鍵詞：负定
        - 摘要：负部函数negative part function
          關鍵詞：负部函数
        - 摘要：负相negative phase
          關鍵詞：负相
        - 摘要：半负定negative semidefinite
          關鍵詞：半负定
        - 摘要：Nesterov动量Nesterov momentum
          關鍵詞：动量
        - 摘要：网络network
          關鍵詞：网络
        - 摘要：神经自回归密度估计器neural auto-regressive den-sity estimator
          關鍵詞：神经自回归密度估计器
        - 摘要：神经自回归网络neural auto-regressive network
          關鍵詞：神经自回归网络
        - 摘要：神经语言模型Neural Language Model
          關鍵詞：神经语言模型
        - 摘要：神经机器翻译Neural Machine Translation
          關鍵詞：神经机器翻译
        - 摘要：神经网络neural network
          關鍵詞：神经网络
        - 摘要：神经网络图灵机neural Turing machine
          關鍵詞：神经网络图灵机
        - 摘要：牛顿法Newton's method
          關鍵詞：牛顿法
        - 摘要：n -gram n-gram
          關鍵詞：
        - 摘要：没有免费午餐定理no free lunch theorem
          關鍵詞：没有免费午餐定理
        - 摘要：噪声noise
          關鍵詞：噪声
        - 摘要：噪声分布noise distribution
          關鍵詞：噪声分布
        - 摘要：噪声对比估计noise-contrastive estimation
          關鍵詞：噪声对比估计
        - 摘要：非凸nonconvex
          關鍵詞：非凸
        - 摘要：非分布式nondistributed
          關鍵詞：非分布式
        - 摘要：非分布式表示nondistributed representation
          關鍵詞：非分布式表示
        - 摘要：非线性共轭梯度nonlinear conjugate gradients
          關鍵詞：非线性共轭梯度
        - 摘要：非线性独立成分估计nonlinear independent com-ponents estimation
          關鍵詞：非线性独立成分估计
        - 摘要：非参数non-parametric
          關鍵詞：非参数
        - 摘要：范数norm
          關鍵詞：范数
        - 摘要：正态分布normal distribution
          關鍵詞：正态分布
        - 摘要：正规方程normal equation
          關鍵詞：正规方程
        - 摘要：归一化的normalized
          關鍵詞：归一化的
        - 摘要：标准初始化normalized initialization
          關鍵詞：标准初始化
        - 摘要：数值numeric value
          關鍵詞：数值
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；数值优化numerical optimization
          關鍵詞：数值优化
        - 摘要：对象识别object recognition
          關鍵詞：对象识别
        - 摘要：目标objective
          關鍵詞：目标
        - 摘要：目标函数objective function
          關鍵詞：目标函数
        - 摘要：奥卡姆剃刀Occam's razor
          關鍵詞：奥卡姆剃刀
        - 摘要：one-hot one-hot
          關鍵詞：
        - 摘要：一次学习one-shot learning
          關鍵詞：一次学习
        - 摘要：在线online
          關鍵詞：在线
        - 摘要：在线学习online learning
          關鍵詞：在线学习
        - 摘要：操作operation
          關鍵詞：操作
        - 摘要：最佳容量optimal capacity
          關鍵詞：最佳容量
        - 摘要：原点origin
          關鍵詞：原点
        - 摘要：正交orthogonal
          關鍵詞：正交
        - 摘要：正交矩阵orthogonal matrix
          關鍵詞：正交矩阵
        - 摘要：标准正交orthonormal
          關鍵詞：标准正交
        - 摘要：输出output
          關鍵詞：输出
        - 摘要：输出层output layer
          關鍵詞：输出层
        - 摘要：过完备overcomplete
          關鍵詞：过完备
        - 摘要：过估计overestimation
          關鍵詞：过估计
        - 摘要：过拟合overfitting
          關鍵詞：过拟合
        - 摘要：过拟合机制overfitting regime
          關鍵詞：过拟合机制
        - 摘要：上溢overflow
          關鍵詞：上溢
        - 摘要：并行分布式处理Parallel Distributed Processing
          關鍵詞：并行分布式处理
        - 摘要：并行回火parallel tempering
          關鍵詞：并行回火
        - 摘要：参数parameter
          關鍵詞：参数
        - 摘要：参数服务器parameter server
          關鍵詞：参数服务器
        - 摘要：参数共享parameter sharing
          關鍵詞：参数共享
        - 摘要：有参情况parametric case
          關鍵詞：有参情况
        - 摘要：参数化整流线性单元parametric ReLU
          關鍵詞：参数化整流线性单元
        - 摘要：偏导数partial derivative
          關鍵詞：偏导数
        - 摘要：配分函数Partition Function
          關鍵詞：配分函数
        - 摘要：性能度量performance measures
          關鍵詞：性能度量
        - 摘要：性能度量performance metrics
          關鍵詞：性能度量
        - 摘要：置换不变性permutation invariant
          關鍵詞：置换不变性
        - 摘要：持续性对比散度persistent contrastive divergence
          關鍵詞：持续性对比散度
        - 摘要：音素phoneme
          關鍵詞：音素
        - 摘要：语音phonetic
          關鍵詞：语音
        - 摘要：分段piecewise
          關鍵詞：分段
        - 摘要：点估计point estimator
          關鍵詞：点估计
        - 摘要：策略policy
          關鍵詞：策略
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；策略梯度policy gradient
          關鍵詞：策略梯度
        - 摘要：池化pooling
          關鍵詞：池化
        - 摘要：池化函数pooling function
          關鍵詞：池化函数
        - 摘要：病态条件poor conditioning
          關鍵詞：病态条件
        - 摘要：正定positive definite
          關鍵詞：正定
        - 摘要：正部函数positive part function
          關鍵詞：正部函数
        - 摘要：正相positive phase
          關鍵詞：正相
        - 摘要：半正定positive semidefinite
          關鍵詞：半正定
        - 摘要：后验概率posterior probability
          關鍵詞：后验概率
        - 摘要：幂方法power method
          關鍵詞：幂方法
        - 摘要：PR曲线PR curve
          關鍵詞：曲线
        - 摘要：精度precision
          關鍵詞：精度
        - 摘要：精度矩阵precision matrix
          關鍵詞：精度矩阵
        - 摘要：预测稀疏分解predictive sparse decomposition
          關鍵詞：预测稀疏分解
        - 摘要：预训练pretraining
          關鍵詞：预训练
        - 摘要：初级视觉皮层primary visual cortex
          關鍵詞：初级视觉皮层
        - 摘要：主成分分析principal components analysis
          關鍵詞：主成分分析
        - 摘要：先验概率prior probability
          關鍵詞：先验概率
        - 摘要：先验概率分布prior probability distribution
          關鍵詞：先验概率分布
        - 摘要：概率PCA probabilistic PCA
          關鍵詞：概率
        - 摘要：概率密度函数probability density function
          關鍵詞：概率密度函数
        - 摘要：概率分布probability distribution
          關鍵詞：概率分布
        - 摘要：概率质量函数probability mass function
          關鍵詞：概率质量函数
        - 摘要：专家之积product of expert
          關鍵詞：专家之积
        - 摘要：乘法法则product rule
          關鍵詞：乘法法则
        - 摘要：成比例proportional
          關鍵詞：成比例
        - 摘要：提议分布proposal distribution
          關鍵詞：提议分布
        - 摘要：伪似然pseudolikelihood
          關鍵詞：伪似然
        - 摘要：象限对quadrature pair
          關鍵詞：象限对
        - 摘要：量子力学quantum mechanics
          關鍵詞：量子力学
        - 摘要：径向基函数radial basis function
          關鍵詞：径向基函数
        - 摘要：随机搜索random search
          關鍵詞：随机搜索
        - 摘要：随机变量random variable
          關鍵詞：随机变量
        - 摘要：值域range
          關鍵詞：值域
        - 摘要：比率匹配ratio matching
          關鍵詞：比率匹配
        - 摘要：召回率recall
          關鍵詞：召回率
        - 摘要：接受域receptivefield
          關鍵詞：接受域
        - 摘要：再循环recirculation
          關鍵詞：再循环
        - 摘要：推荐系统recommender system
          關鍵詞：推荐系统
        - 摘要：重构reconstruction
          關鍵詞：重构
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；重构误差reconstruction error
          關鍵詞：重构误差
        - 摘要：整流线性rectified linear
          關鍵詞：整流线性
        - 摘要：整流线性变换rectified linear transformation
          關鍵詞：整流线性变换
        - 摘要：整流线性单元rectified linear unit
          關鍵詞：整流线性单元
        - 摘要：整流网络rectifier network
          關鍵詞：整流网络
        - 摘要：循环recurrence
          關鍵詞：循环
        - 摘要：循环卷积网络recurrent convolutional network
          關鍵詞：循环卷积网络
        - 摘要：循环网络recurrent network
          關鍵詞：循环网络
        - 摘要：循环神经网络recurrent neural network
          關鍵詞：循环神经网络
        - 摘要：回归regression
          關鍵詞：回归
        - 摘要：正则化regularization
          關鍵詞：正则化
        - 摘要：正则化regularize
          關鍵詞：正则化
        - 摘要：正则化项regularizer
          關鍵詞：正则化项
        - 摘要：强化学习reinforcement learning
          關鍵詞：强化学习
        - 摘要：关系relation
          關鍵詞：关系
        - 摘要：关系型数据库relational database
          關鍵詞：关系型数据库
        - 摘要：重参数化reparametrization
          關鍵詞：重参数化
        - 摘要：重参数化技巧reparametrization trick
          關鍵詞：重参数化技巧
        - 摘要：表示representation
          關鍵詞：表示
        - 摘要：表示学习representation learning
          關鍵詞：表示学习
        - 摘要：表示容量representational capacity
          關鍵詞：表示容量
        - 摘要：储层计算reservoir computing
          關鍵詞：储层计算
        - 摘要：受限玻尔兹曼机Restricted Boltzmann Machine
          關鍵詞：受限玻尔兹曼机
        - 摘要：反向相关reverse correlation
          關鍵詞：反向相关
        - 摘要：反向模式累加reverse mode accumulation
          關鍵詞：反向模式累加
        - 摘要：岭回归ridge regression
          關鍵詞：岭回归
        - 摘要：右特征向量right eigenvector
          關鍵詞：右特征向量
        - 摘要：右奇异向量right singular vector
          關鍵詞：右奇异向量
        - 摘要：风险risk
          關鍵詞：风险
        - 摘要：行row
          關鍵詞：
        - 摘要：扫视saccade
          關鍵詞：扫视
        - 摘要：鞍点saddle point
          關鍵詞：鞍点
        - 摘要：无鞍牛顿法saddle-free Newton method
          關鍵詞：无鞍牛顿法
        - 摘要：相同same
          關鍵詞：相同
        - 摘要：样本均值sample mean
          關鍵詞：样本均值
        - 摘要：样本方差sample variance
          關鍵詞：样本方差
        - 摘要：饱和saturate
          關鍵詞：饱和
        - 摘要：标量scalar
          關鍵詞：标量
        - 摘要：得分score
          關鍵詞：得分
        - 摘要：得分匹配score matching
          關鍵詞：得分匹配
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；二阶导数second derivative
          關鍵詞：二阶导数
        - 摘要：二阶导数测试second derivative test
          關鍵詞：二阶导数测试
        - 摘要：第二层second layer
          關鍵詞：第二层
        - 摘要：二阶方法second-order method
          關鍵詞：二阶方法
        - 摘要：自对比估计self-contrastive estimation
          關鍵詞：自对比估计
        - 摘要：自信息self-information
          關鍵詞：自信息
        - 摘要：语义哈希semantic hashing
          關鍵詞：语义哈希
        - 摘要：半受限波尔兹曼机semi-restricted Boltzmann Ma-chine
          關鍵詞：半受限波尔兹曼机
        - 摘要：半监督semi-supervised
          關鍵詞：半监督
        - 摘要：半监督学习semi-supervised learning
          關鍵詞：半监督学习
        - 摘要：可分离的separable
          關鍵詞：可分离的
        - 摘要：分离的separate
          關鍵詞：分离的
        - 摘要：分离separation
          關鍵詞：分离
        - 摘要：情景setting
          關鍵詞：情景
        - 摘要：浅度回路shadow circuit
          關鍵詞：浅度回路
        - 摘要：香农熵Shannon entropy
          關鍵詞：香农熵
        - 摘要：香农shannons
          關鍵詞：香农
        - 摘要：塑造shaping
          關鍵詞：塑造
        - 摘要：短列表shortlist
          關鍵詞：短列表
        - 摘要：sigmoid sigmoid
          關鍵詞：
        - 摘要：sigmoid信念网络sigmoid Belief Network
          關鍵詞：信念网络
        - 摘要：简单细胞simple cell
          關鍵詞：简单细胞
        - 摘要：奇异的singular
          關鍵詞：奇异的
        - 摘要：奇异值singular value
          關鍵詞：奇异值
        - 摘要：奇异值分解singular value decomposition
          關鍵詞：奇异值分解
        - 摘要：奇异向量singular vector
          關鍵詞：奇异向量
        - 摘要：跳跃连接skip connection
          關鍵詞：跳跃连接
        - 摘要：慢特征分析slow feature analysis
          關鍵詞：慢特征分析
        - 摘要：慢性原则slowness principle
          關鍵詞：慢性原则
        - 摘要：平滑smoothing
          關鍵詞：平滑
        - 摘要：平滑先验smoothness prior
          關鍵詞：平滑先验
        - 摘要：softmax softmax
          關鍵詞：
        - 摘要：softmax函数softmax function
          關鍵詞：函数
        - 摘要：softmax单元softmax unit
          關鍵詞：单元
        - 摘要：softplus softplus
          關鍵詞：
        - 摘要：softplus函数softplus function
          關鍵詞：函数
        - 摘要：生成子空间span
          關鍵詞：生成子空间
        - 摘要：稀疏sparse
          關鍵詞：稀疏
        - 摘要：稀疏激活sparse activation
          關鍵詞：稀疏激活
        - 摘要：稀疏编码sparse coding
          關鍵詞：稀疏编码
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；稀疏连接sparse connectivity
          關鍵詞：稀疏连接
        - 摘要：稀疏初始化sparse initialization
          關鍵詞：稀疏初始化
        - 摘要：稀疏交互sparse interactions
          關鍵詞：稀疏交互
        - 摘要：稀疏权重sparse weights
          關鍵詞：稀疏权重
        - 摘要：谱半径spectral radius
          關鍵詞：谱半径
        - 摘要：语音识别Speech Recognition
          關鍵詞：语音识别
        - 摘要：sphering sphering
          關鍵詞：
        - 摘要：尖峰和平板spike and slab
          關鍵詞：尖峰和平板
        - 摘要：尖峰和平板RBM spike and slab RBM
          關鍵詞：尖峰和平板
        - 摘要：虚假模态spurious modes
          關鍵詞：虚假模态
        - 摘要：方阵square
          關鍵詞：方阵
        - 摘要：标准差standard deviation
          關鍵詞：标准差
        - 摘要：标准差standard error
          關鍵詞：标准差
        - 摘要：标准正态分布standard normal distribution
          關鍵詞：标准正态分布
        - 摘要：声明statement
          關鍵詞：声明
        - 摘要：平稳的stationary
          關鍵詞：平稳的
        - 摘要：平稳分布Stationary Distribution
          關鍵詞：平稳分布
        - 摘要：驻点stationary point
          關鍵詞：驻点
        - 摘要：统计效率statistic efficiency
          關鍵詞：统计效率
        - 摘要：统计学习理论statistical learning theory
          關鍵詞：统计学习理论
        - 摘要：统计量statistics
          關鍵詞：统计量
        - 摘要：最陡下降steepest descent
          關鍵詞：最陡下降
        - 摘要：随机stochastic
          關鍵詞：随机
        - 摘要：随机课程stochastic curriculum
          關鍵詞：随机课程
        - 摘要：随机梯度上升Stochastic Gradient Ascent
          關鍵詞：随机梯度上升
        - 摘要：随机梯度下降stochastic gradient descent
          關鍵詞：随机梯度下降
        - 摘要：随机矩阵Stochastic Matrix
          關鍵詞：随机矩阵
        - 摘要：随机最大似然stochastic maximum likelihood
          關鍵詞：随机最大似然
        - 摘要：流stream
          關鍵詞：
        - 摘要：步幅stride
          關鍵詞：步幅
        - 摘要：结构学习structure learning
          關鍵詞：结构学习
        - 摘要：结构化概率模型structured probabilistic model
          關鍵詞：结构化概率模型
        - 摘要：结构化变分推断structured variational inference
          關鍵詞：结构化变分推断
        - 摘要：亚原子subatomic
          關鍵詞：亚原子
        - 摘要：子采样subsample
          關鍵詞：子采样
        - 摘要：求和法则sum rule
          關鍵詞：求和法则
        - 摘要：和–积网络sum-product network
          關鍵詞：积网络
        - 摘要：监督supervised
          關鍵詞：监督
        - 摘要：监督学习supervised learning
          關鍵詞：监督学习
        - 摘要：监督学习算法supervised learning algorithm
          關鍵詞：监督学习算法
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；监督模型supervised model
          關鍵詞：监督模型
        - 摘要：监督预训练supervised pretraining
          關鍵詞：监督预训练
        - 摘要：支持向量support vector
          關鍵詞：支持向量
        - 摘要：代理损失函数surrogate loss function
          關鍵詞：代理损失函数
        - 摘要：符号symbol
          關鍵詞：符号
        - 摘要：符号表示symbolic representation
          關鍵詞：符号表示
        - 摘要：对称symmetric
          關鍵詞：对称
        - 摘要：切面距离tangent distance
          關鍵詞：切面距离
        - 摘要：切平面tangent plane
          關鍵詞：切平面
        - 摘要：正切传播tangent prop
          關鍵詞：正切传播
        - 摘要：泰勒taylor
          關鍵詞：泰勒
        - 摘要：导师驱动过程teacher forcing
          關鍵詞：导师驱动过程
        - 摘要：温度temperature
          關鍵詞：温度
        - 摘要：回火转移tempered transition
          關鍵詞：回火转移
        - 摘要：回火tempering
          關鍵詞：回火
        - 摘要：张量tensor
          關鍵詞：张量
        - 摘要：测试误差test error
          關鍵詞：测试误差
        - 摘要：测试集test set
          關鍵詞：测试集
        - 摘要：碰撞情况the collider case
          關鍵詞：碰撞情况
        - 摘要：绑定的权重tied weights
          關鍵詞：绑定的权重
        - 摘要：Tikhonov正则Tikhonov regularization
          關鍵詞：正则
        - 摘要：平铺卷积tiled convolution
          關鍵詞：平铺卷积
        - 摘要：时延神经网络time delay neural network
          關鍵詞：时延神经网络
        - 摘要：时间步time step
          關鍵詞：时间步
        - 摘要：Toeplitz矩阵Toeplitz matrix
          關鍵詞：矩阵
        - 摘要：标记token
          關鍵詞：标记
        - 摘要：容差tolerance
          關鍵詞：容差
        - 摘要：地质ICA topographic ICA
          關鍵詞：地质
        - 摘要：训练误差training error
          關鍵詞：训练误差
        - 摘要：训练集training set
          關鍵詞：训练集
        - 摘要：转录transcribe
          關鍵詞：转录
        - 摘要：转录系统transcription system
          關鍵詞：转录系统
        - 摘要：迁移学习transfer learning
          關鍵詞：迁移学习
        - 摘要：转移transition
          關鍵詞：转移
        - 摘要：转置transpose
          關鍵詞：转置
        - 摘要：三角不等式triangle inequality
          關鍵詞：三角不等式
        - 摘要：三角形化triangulate
          關鍵詞：三角形化
        - 摘要：三角形化图triangulated graph
          關鍵詞：三角形化图
        - 摘要：三元语法trigram
          關鍵詞：三元语法
        - 摘要：无偏unbiased
          關鍵詞：无偏
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；无偏样本方差unbiased sample variance
          關鍵詞：无偏样本方差
        - 摘要：欠完备undercomplete
          關鍵詞：欠完备
        - 摘要：欠定的underdetermined
          關鍵詞：欠定的
        - 摘要：欠估计underestimation
          關鍵詞：欠估计
        - 摘要：欠拟合underfitting
          關鍵詞：欠拟合
        - 摘要：欠拟合机制underfitting regime
          關鍵詞：欠拟合机制
        - 摘要：下溢underflow
          關鍵詞：下溢
        - 摘要：潜在underlying
          關鍵詞：潜在
        - 摘要：潜在成因underlying cause
          關鍵詞：潜在成因
        - 摘要：无向undirected
          關鍵詞：无向
        - 摘要：无向模型undirected model
          關鍵詞：无向模型
        - 摘要：展开图unfolded graph
          關鍵詞：展开图
        - 摘要：展开unfolding
          關鍵詞：展开
        - 摘要：均匀分布uniform distribution
          關鍵詞：均匀分布
        - 摘要：一元语法unigram
          關鍵詞：一元语法
        - 摘要：单峰值unimodal
          關鍵詞：单峰值
        - 摘要：单元unit
          關鍵詞：单元
        - 摘要：单位范数unit norm
          關鍵詞：单位范数
        - 摘要：单位向量unit vector
          關鍵詞：单位向量
        - 摘要：万能近似定理universal approximation theorem
          關鍵詞：万能近似定理
        - 摘要：万能近似器universal approximator
          關鍵詞：万能近似器
        - 摘要：万能函数近似器universal function approximator
          關鍵詞：万能函数近似器
        - 摘要：未标注unlabeled
          關鍵詞：未标注
        - 摘要：未归一化概率函数unnormalized probability func-tion
          關鍵詞：未归一化概率函数
        - 摘要：非共享卷积unshared convolution
          關鍵詞：非共享卷积
        - 摘要：无监督unsupervised
          關鍵詞：无监督
        - 摘要：无监督学习unsupervised learning
          關鍵詞：无监督学习
        - 摘要：无监督学习算法unsupervised learning algorithm
          關鍵詞：无监督学习算法
        - 摘要：无监督预训练unsupervised pretraining
          關鍵詞：无监督预训练
        - 摘要：有效valid
          關鍵詞：有效
        - 摘要：验证集validation set
          關鍵詞：验证集
        - 摘要：梯度消失与爆炸问题vanishing and exploding gra-dient problem
          關鍵詞：梯度消失与爆炸问题
        - 摘要：梯度消失vanishing gradient
          關鍵詞：梯度消失
        - 摘要：Vapnik-Chervonenkis维度Vapnik-Chervonenkis dimension
          關鍵詞：维度
        - 摘要：变量消去variable elimination
          關鍵詞：变量消去
        - 摘要：方差variance
          關鍵詞：方差
        - 摘要：方差减小variance reduction
          關鍵詞：方差减小
        - 摘要：变分自编码器variational auto-encoder
          關鍵詞：变分自编码器
        - 摘要：变分导数variational derivative
          關鍵詞：变分导数
        - 摘要：变分自由能variational free energy
          關鍵詞：变分自由能
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)；变分推断variational inference
          關鍵詞：变分推断
        - 摘要：向量vector
          關鍵詞：向量
        - 摘要：虚拟对抗样本virtual adversarial example
          關鍵詞：虚拟对抗样本
        - 摘要：虚拟对抗训练virtual adversarial training
          關鍵詞：虚拟对抗训练
        - 摘要：可见层visible layer
          關鍵詞：可见层
        - 摘要：V-结构V-structure
          關鍵詞：结构
        - 摘要：醒眠wake sleep
          關鍵詞：醒眠
        - 摘要：warp warp
          關鍵詞：
        - 摘要：支持向量机support vector machine
          關鍵詞：支持向量机
        - 摘要：无向图模型undirected graphical model
          關鍵詞：无向图模型
        - 摘要：权重weight
          關鍵詞：权重
        - 摘要：权重衰减weight decay
          關鍵詞：权重衰减
        - 摘要：权重比例推断规则weight scaling inference rule
          關鍵詞：权重比例推断规则
        - 摘要：权重空间对称性weight space symmetry
          關鍵詞：权重空间对称性
        - 摘要：条件概率分布conditional probability distribution
          關鍵詞：条件概率分布
        - 摘要：白化whitening
          關鍵詞：白化
        - 摘要：宽度width
          關鍵詞：宽度
        - 摘要：赢者通吃winner-take-all
          關鍵詞：赢者通吃
        - 摘要：正切传播tangent propagation
          關鍵詞：正切传播
        - 摘要：流形正切分类器manifold tangent classifier
          關鍵詞：流形正切分类器
        - 摘要：词嵌入word embedding
          關鍵詞：词嵌入
        - 摘要：词义消歧word-sense disambiguation
          關鍵詞：词义消歧
        - 摘要：零数据学习zero-data learning
          關鍵詞：零数据学习
        - 摘要：零次学习zero-shot learning
          關鍵詞：零次学习
        - 摘要：Table of Contents
          關鍵詞：
        - 摘要：书名页
          關鍵詞：书名页
        - 摘要：版权页
          關鍵詞：版权页
        - 摘要：中文版推荐语（按姓氏拼音排序）
          關鍵詞：中文版推荐语, 按姓氏拼音排序
        - 摘要：译者序
          關鍵詞：译者序
        - 摘要：中文版致谢
          關鍵詞：中文版致谢
        - 摘要：英文原书致谢
          關鍵詞：英文原书致谢
        - 摘要：数学符号
          關鍵詞：数学符号
        - 摘要：目录
          關鍵詞：目录
        - 摘要：参考文献
          關鍵詞：参考文献
        - 摘要：索引
          關鍵詞：索引
        - 摘要：(cid:1783)(cid:1567)(cid:1480)(cid:4656)(cid:3456)(cid:4647)(cid:2584)(cid:3176)(cid:3526)(cid:3564)(cid:119)(cid:4078)(cid:3456)(cid:120)(cid:287)(cid:73)(cid:85)(cid:85)(cid:81)(cid:27)(cid:16)(cid:16)(cid:88)(cid:88)(cid:88)(cid:15)(cid:90)(cid:66)(cid:67)(cid:80)(cid:80)(cid:76)(cid:15)(cid:80)(cid:83)(cid:72)
          關鍵詞：